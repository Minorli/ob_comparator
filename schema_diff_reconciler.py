#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Copyright 2025 Minorli
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""

数据库对象对比工具 (V0.9.8.4 - Dump-Once, Compare-Locally + 依赖 + ALTER 修补 + 注释校验)
---------------------------------------------------------------------------
功能概要：
1. 对比 Oracle (源) 与 OceanBase (目标) 的：
   - TABLE, VIEW, MATERIALIZED VIEW
   - PROCEDURE, FUNCTION, PACKAGE, PACKAGE BODY, SYNONYM
   - JOB, SCHEDULE, TYPE, TYPE BODY
   - INDEX, CONSTRAINT (PK/UK/FK/CHECK)
   - SEQUENCE, TRIGGER

2. 对比规则：
   - TABLE：校验列名集合（忽略 OMS_* 列），并检查 VARCHAR/VARCHAR2 长度是否落在 [ceil(src*1.5), ceil(src*2.5)] 区间。
   - TABLE/列注释：基于 DBA_TAB_COMMENTS / DBA_COL_COMMENTS 对比 Remap 后的表/列注释（可通过 check_comments 开关关闭）。
   - VIEW/MVIEW/PLSQL/SYNONYM/JOB/SCHEDULE/TYPE：对比是否存在。
   - INDEX / CONSTRAINT：校验存在性与列组合（含唯一性/约束类型）。
   - SEQUENCE / TRIGGER：校验存在性；依赖：映射后生成期望依赖并对比目标端。

3. 性能架构 (V0.9.8.4 核心)：
   - OceanBase 侧采用“一次转储，本地对比”：
       使用少量 obclient 调用，分别 dump：
         DBA_OBJECTS
         DBA_TAB_COLUMNS
         DBA_INDEXES / DBA_IND_COLUMNS
         DBA_CONSTRAINTS / DBA_CONS_COLUMNS
         DBA_TRIGGERS
         DBA_SEQUENCES
       后续所有对比均在 Python 内存数据结构中完成。
   - 避免 V12 中在循环中大量调用 obclient 的性能黑洞。

4. 目标端订正 SQL 生成：
   - 缺失对象：
       TABLE / VIEW / PROCEDURE / FUNCTION / PACKAGE / PACKAGE BODY / SYNONYM /
       INDEX / CONSTRAINT / SEQUENCE / TRIGGER
       → 生成对应的 CREATE 语句脚本。
   - TABLE 列不匹配：
       → 生成 ALTER TABLE ADD/MODIFY（长度不足）脚本；
       → 对“多余列”生成注释掉的 DROP COLUMN 建议语句。
   - 依赖缺失 → 生成 ALTER ... COMPILE；跨 schema 调用/源端权限 → 生成 GRANT 脚本（受 generate_grants 控制）。
   - 所有脚本写入 fixup_scripts 目录下相应子目录，需人工审核后在 OceanBase 执行。

5. 健壮性：
   - 所有 obclient 调用增加 timeout（从 config.ini 的 [SETTINGS] -> obclient_timeout 读取，默认 60 秒）。
   - Instant Client / dbcat / JAVA_HOME / Remap 前置校验，发现致命问题立即终止。
"""

import argparse
import configparser
import hashlib
import json
import logging
import math
import os
import random
import re
import shutil
import socket
import subprocess
import sys
import tempfile
import textwrap
import threading
import time
import uuid
from collections import OrderedDict, defaultdict, deque
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed
from contextlib import contextmanager, nullcontext
from dataclasses import dataclass
from datetime import datetime, timedelta
from decimal import Decimal, InvalidOperation
from pathlib import Path
from typing import Callable, Dict, Iterable, Iterator, List, NamedTuple, NoReturn, Optional, Sequence, Set, Tuple, Union

__version__ = "0.9.8.4"
__author__ = "Minor Li"
REPO_URL = "https://github.com/Minorli/ob_comparator"
REPO_ISSUES_URL = f"{REPO_URL}/issues"

# 尝试导入 oracledb，如果失败则提示安装
try:
    import oracledb
except ImportError:
    print("错误: 未找到 'oracledb' 库。", file=sys.stderr)
    print("请先安装: pip install oracledb", file=sys.stderr)
    sys.exit(1)

# Rich 渲染写入文件时需要的简化字符映射，便于 vi/less 查看
BOX_ASCII_TRANS = str.maketrans({
    "┏": "+", "┓": "+", "┗": "+", "┛": "+", "┣": "+", "┫": "+", "┳": "+", "┻": "+", "╋": "+",
    "━": "-", "┃": "|", "─": "-", "│": "|",
    "┌": "+", "┐": "+", "└": "+", "┘": "+", "├": "+", "┤": "+", "┴": "+", "┬": "+", "┼": "+",
    "═": "-", "║": "|", "╔": "+", "╗": "+", "╚": "+", "╝": "+", "╠": "+", "╣": "+", "╦": "+", "╩": "+", "╬": "+",
})

# 简易 ANSI 转义去除（不依赖 rich 的 strip_ansi，兼容低版本 wheel）
ANSI_RE = re.compile(r"\x1b\[[0-9;]*[mGKF]")

def strip_ansi_text(text: str) -> str:
    return ANSI_RE.sub("", text)

# --- 日志配置 ---
LOG_TIME_FORMAT = "%Y-%m-%d %H:%M:%S"
LOG_FILE_FORMAT = "%(asctime)s | %(levelname)-8s | %(message)s"
LOG_SECTION_WIDTH = 80


class FatalError(RuntimeError):
    """致命错误：用于替代 sys.exit 以便并发任务可回传异常。"""


def abort_run(message: Optional[str] = None) -> NoReturn:
    raise FatalError(message or "fatal error")


def _build_console_handler(level: int) -> logging.Handler:
    try:
        from rich.logging import RichHandler
        handler = RichHandler(
            level=level,
            show_time=True,
            omit_repeated_times=False,
            show_level=True,
            show_path=False,
            rich_tracebacks=False,
            log_time_format=LOG_TIME_FORMAT
        )
        handler.setFormatter(logging.Formatter("%(message)s"))
        return handler
    except Exception:
        # 兜底：rich 处理异常时回退到标准日志，避免启动中断
        handler = logging.StreamHandler()
        handler.setLevel(level)
        handler.setFormatter(logging.Formatter(LOG_FILE_FORMAT, datefmt=LOG_TIME_FORMAT))
        return handler


def init_console_logging(level: int = logging.INFO) -> None:
    root_logger = logging.getLogger()
    root_logger.setLevel(level)
    for handler in list(root_logger.handlers):
        if isinstance(handler, logging.FileHandler):
            continue
        root_logger.removeHandler(handler)
    root_logger.addHandler(_build_console_handler(level))


def set_console_log_level(root_logger: logging.Logger, level: int) -> None:
    for handler in root_logger.handlers:
        if isinstance(handler, logging.FileHandler):
            continue
        handler.setLevel(level)


def resolve_console_log_level(level_name: Optional[str], *, is_tty: Optional[bool] = None) -> int:
    if is_tty is None:
        try:
            is_tty = sys.stdout.isatty()
        except Exception:
            # 兜底：某些环境 stdout 无法检测 TTY，默认按非 TTY 处理
            is_tty = False
    name = (level_name or "AUTO").strip().upper()
    if name == "AUTO":
        return logging.INFO if is_tty else logging.WARNING
    if hasattr(logging, name):
        return getattr(logging, name)
    return logging.INFO


def log_section(title: str, fill_char: str = "=") -> None:
    clean = f" {title.strip()} "
    if len(clean) >= LOG_SECTION_WIDTH:
        log.info("%s", title.strip())
        return
    log.info("%s", clean.center(LOG_SECTION_WIDTH, fill_char))


def log_subsection(title: str, fill_char: str = "-") -> None:
    clean = f" {title.strip()} "
    if len(clean) >= LOG_SECTION_WIDTH:
        log.info("%s", title.strip())
        return
    log.info("%s", clean.center(LOG_SECTION_WIDTH, fill_char))


init_console_logging()
log = logging.getLogger(__name__)

RUN_PHASE_ORDER: Tuple[str, ...] = (
    "加载配置与初始化",
    "对象映射准备",
    "OceanBase 元数据转储",
    "Oracle 元数据转储",
    "主对象校验",
    "扩展对象校验",
    "对象可用性校验",
    "依赖/授权校验",
    "修补脚本生成",
    "报告输出",
)


class RunPhaseInfo(NamedTuple):
    name: str
    duration: Optional[float]
    status: str


class RunSummary(NamedTuple):
    start_time: datetime
    end_time: datetime
    total_seconds: float
    phases: List[RunPhaseInfo]
    actions_done: List[str]
    actions_skipped: List[str]
    findings: List[str]
    attention: List[str]
    next_steps: List[str]


class RunSummaryContext(NamedTuple):
    start_time: datetime
    start_perf: float
    phase_durations: Dict[str, float]
    phase_skip_reasons: Dict[str, str]
    enabled_primary_types: Set[str]
    enabled_extra_types: Set[str]
    print_only_types: Set[str]
    total_checked: int
    enable_dependencies_check: bool
    enable_comment_check: bool
    enable_grant_generation: bool
    enable_schema_mapping_infer: bool
    fixup_enabled: bool
    fixup_dir: str
    dependency_chain_file: Optional[Path]
    view_chain_file: Optional[Path]
    trigger_list_summary: Optional[Dict[str, object]]
    report_start_perf: float


@contextmanager
def phase_timer(phase: str, durations: Dict[str, float]):
    start = time.perf_counter()
    try:
        yield
    finally:
        durations[phase] = durations.get(phase, 0.0) + (time.perf_counter() - start)


def format_duration(seconds: float) -> str:
    if seconds < 60:
        return f"{seconds:.2f}s"
    minutes, sec = divmod(seconds, 60)
    if minutes < 60:
        return f"{int(minutes)}m{sec:.0f}s"
    hours, rem = divmod(minutes, 60)
    return f"{int(hours)}h{int(rem)}m{sec:.0f}s"


def setup_run_logging(settings: Dict, timestamp: str) -> Optional[Path]:
    """
    为每次运行创建日志文件：
      - 日志目录默认 logs，可在 config.ini 的 [SETTINGS]->log_dir 覆盖
      - 控制台默认 auto（TTY=INFO, 非TTY=WARNING，可用 log_level 覆盖）
      - 文件记录 DEBUG 及以上，包含推导细节
    """
    try:
        log_dir_setting = (settings.get("log_dir") or "logs").strip() or "logs"
        log_dir = Path(log_dir_setting)
        log_dir.mkdir(parents=True, exist_ok=True)
        log_file = log_dir / f"run_{timestamp}.log"

        root_logger = logging.getLogger()
        # 允许 DEBUG 记录进入文件，但控制台按 log_level 过滤
        root_logger.setLevel(logging.DEBUG)
        existing_files = [
            handler for handler in root_logger.handlers
            if isinstance(handler, logging.FileHandler)
            and getattr(handler, "baseFilename", "") == str(log_file)
        ]
        if not existing_files:
            file_handler = logging.FileHandler(log_file, encoding="utf-8")
            file_handler.setLevel(logging.DEBUG)
            file_handler.setFormatter(logging.Formatter(LOG_FILE_FORMAT, datefmt=LOG_TIME_FORMAT))
            root_logger.addHandler(file_handler)

        level_name = (settings.get("log_level") or "AUTO").strip().upper()
        console_level = resolve_console_log_level(level_name)
        if level_name not in ("AUTO", "DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL"):
            log.warning("未知 log_level=%s，回退为 %s", level_name, logging.getLevelName(console_level))
        set_console_log_level(root_logger, console_level)

        log.info("本次运行日志将输出到: %s", log_file.resolve())
        if level_name == "AUTO":
            log.info(
                "日志级别: console=%s (AUTO), file=DEBUG",
                logging.getLevelName(console_level)
            )
        else:
            log.info("日志级别: console=%s, file=DEBUG", logging.getLevelName(console_level))
        return log_file
    except Exception as exc:
        log.warning("初始化日志文件失败，将仅输出到控制台: %s", exc)
        return None

# --- 类型别名 ---
OraConfig = Dict[str, str]
ObConfig = Dict[str, str]
RemapRules = Dict[str, str]
SourceObjectMap = Dict[str, Set[str]]  # {'OWNER.OBJ': {'TYPE1', 'TYPE2'}}
FullObjectMapping = Dict[str, Dict[str, str]]  # {'OWNER.OBJ': {'TYPE': 'TGT_OWNER.OBJ'}}
MasterCheckList = List[Tuple[str, str, str]]  # [(src_name, tgt_name, type)]
ReportResults = Dict[str, List]
PackageCompareResults = Dict[str, object]
# object_counts_summary keys: oracle/oceanbase/missing/extra -> {OBJECT_TYPE: count}
ObjectCountSummary = Dict[str, Dict[str, int]]
# 源端依赖集合的简化元组：(dep_owner, dep_name, dep_type, ref_owner, ref_name, ref_type)
SourceDependencyTuple = Tuple[str, str, str, str, str, str]
SourceDependencySet = Set[SourceDependencyTuple]
# 依赖图节点与图结构，用于递归推导目标 schema
DependencyNode = Tuple[str, str]  # (OWNER.OBJECT, OBJECT_TYPE)
DependencyGraph = Dict[DependencyNode, Set[DependencyNode]]
# 递归推导时每个节点的“最终引用表”缓存
TransitiveTableCache = Dict[DependencyNode, Set[str]]
RemapConflictMap = Dict[Tuple[str, str], str]  # {(SRC_FULL, TYPE): reason}
class BlacklistEntry(NamedTuple):
    black_type: str
    data_type: str
    source: str = ""


BlacklistEntryKey = Tuple[str, str]
BlacklistTableMap = Dict[Tuple[str, str], Dict[BlacklistEntryKey, BlacklistEntry]]
class BlacklistReportRow(NamedTuple):
    schema: str
    table: str
    black_type: str
    data_type: str
    reason: str
    status: str
    detail: str


class ObjectSupportReportRow(NamedTuple):
    obj_type: str
    src_full: str
    tgt_full: str
    support_state: str
    reason_code: str
    reason: str
    dependency: str
    action: str
    detail: str
    root_cause: str = ""


class ViewCompatResult(NamedTuple):
    support_state: str
    reason_code: str
    reason: str
    detail: str
    cleaned_ddl: str
    rewrite_notes: List[str]


class ViewConstraintReportRow(NamedTuple):
    view_full: str
    mode: str
    action: str
    reason: str
    constraints: str


class SupportClassificationResult(NamedTuple):
    support_state_map: Dict[Tuple[str, str], ObjectSupportReportRow]
    missing_detail_rows: List[ObjectSupportReportRow]
    unsupported_rows: List[ObjectSupportReportRow]
    extra_missing_rows: List[ObjectSupportReportRow]
    missing_support_counts: Dict[str, Dict[str, int]]
    extra_blocked_counts: Dict[str, int]
    unsupported_table_keys: Set[Tuple[str, str]]
    unsupported_view_keys: Set[Tuple[str, str]]
    view_compat_map: Dict[Tuple[str, str], ViewCompatResult]
    view_constraint_cleaned_rows: List[ViewConstraintReportRow]
    view_constraint_uncleanable_rows: List[ViewConstraintReportRow]


class IntervalPartitionInfo(NamedTuple):
    interval_expr: str
    partitioning_type: str
    subpartitioning_type: str
    last_partition_name: str
    last_high_value: str
    last_partition_position: int
    partition_key_columns: List[str]
    existing_partition_names: Set[str]


class DdlCleanReportRow(NamedTuple):
    obj_type: str
    obj_full: str
    replaced: int
    samples: List[Tuple[str, str]]


class DdlHintCleanReportRow(NamedTuple):
    obj_type: str
    obj_full: str
    policy: str
    total: int
    kept: int
    removed: int
    unknown: int
    kept_samples: List[str]
    removed_samples: List[str]
    unknown_samples: List[str]


class DdlFormatReportRow(NamedTuple):
    obj_type: str
    status: str
    reason: str
    size_bytes: int
    line_count: int
    path: str


USABILITY_STATUS_OK = "OK"
USABILITY_STATUS_UNUSABLE = "UNUSABLE"
USABILITY_STATUS_EXPECTED_UNUSABLE = "EXPECTED_UNUSABLE"
USABILITY_STATUS_UNEXPECTED_USABLE = "UNEXPECTED_USABLE"
USABILITY_STATUS_TIMEOUT = "TIMEOUT"
USABILITY_STATUS_SKIPPED = "SKIPPED"


class UsabilityCheckResult(NamedTuple):
    schema: str
    object_name: str
    object_type: str
    src_exists: bool
    src_usable: Optional[bool]
    tgt_exists: bool
    tgt_usable: Optional[bool]
    status: str
    src_error: str
    tgt_error: str
    root_cause: str
    recommendation: str
    src_time_ms: int
    tgt_time_ms: int


class UsabilitySummary(NamedTuple):
    total_candidates: int
    total_checked: int
    total_usable: int
    total_unusable: int
    total_expected_unusable: int
    total_unexpected_usable: int
    total_timeout: int
    total_skipped: int
    total_sampled_out: int
    duration_seconds: float
    results: List[UsabilityCheckResult]


@dataclass
class DdlFormatItem:
    path: Path
    obj_type: str
    rel_dir: str
    size_bytes: int
    line_count: int = 0


class HintFilterResult(NamedTuple):
    ddl: str
    total: int
    kept: int
    removed: int
    unknown: int
    kept_samples: List[str]
    removed_samples: List[str]
    unknown_samples: List[str]

# --- 全局 obclient timeout（秒），由配置初始化 ---
OBC_TIMEOUT: int = 60

# --- 模型定义 ---
class ObMetadata(NamedTuple):
    """
    一次性从 OceanBase dump 出来的元数据，用于本地对比。
    """
    objects_by_type: Dict[str, Set[str]]                 # OBJECT_TYPE -> {OWNER.OBJ}
    tab_columns: Dict[Tuple[str, str], Dict[str, Dict]]   # (OWNER, TABLE_NAME) -> {COLUMN_NAME: {type, length, etc.}}
    invisible_column_supported: bool                     # 是否支持读取 INVISIBLE_COLUMN 元数据
    identity_column_supported: bool                      # 是否支持读取 IDENTITY_COLUMN 元数据
    default_on_null_supported: bool                      # 是否支持读取 DEFAULT_ON_NULL 元数据
    indexes: Dict[Tuple[str, str], Dict[str, Dict]]      # (OWNER, TABLE_NAME) -> {INDEX_NAME: {uniqueness, columns[list]}}
    constraints: Dict[Tuple[str, str], Dict[str, Dict]]  # (OWNER, TABLE_NAME) -> {CONS_NAME: {type, columns[list], index_name}}
    triggers: Dict[Tuple[str, str], Dict[str, Dict]]     # (OWNER, TABLE_NAME) -> {TRG_NAME: {event, status}}
    sequences: Dict[str, Set[str]]                       # SEQUENCE_OWNER -> {SEQUENCE_NAME}
    sequence_attrs: Dict[str, Dict[str, Dict[str, object]]]  # OWNER -> {SEQUENCE_NAME: attrs}
    roles: Set[str]                                      # DBA_ROLES -> {ROLE}
    table_comments: Dict[Tuple[str, str], Optional[str]] # (OWNER, TABLE_NAME) -> COMMENT
    column_comments: Dict[Tuple[str, str], Dict[str, Optional[str]]]  # (OWNER, TABLE_NAME) -> {COLUMN_NAME: COMMENT}
    comments_complete: bool                              # 元数据是否完整加载（两端失败则跳过注释校验）
    object_statuses: Dict[Tuple[str, str, str], str]     # (OWNER, OBJECT_NAME, OBJECT_TYPE) -> STATUS
    package_errors: Dict[Tuple[str, str, str], "PackageErrorInfo"]  # (OWNER, NAME, TYPE) -> error summary
    package_errors_complete: bool                        # 目标端错误信息是否完整
    partition_key_columns: Dict[Tuple[str, str], List[str]]  # (OWNER, TABLE_NAME) -> [PARTITION_COLS...]
    constraint_deferrable_supported: bool = False         # 是否支持读取 DEFERRABLE/DEFERRED 元数据


class OracleMetadata(NamedTuple):
    """
    源端 Oracle 的元数据缓存，避免在循环中重复查询。
    """
    table_columns: Dict[Tuple[str, str], Dict[str, Dict]]   # (OWNER, TABLE_NAME) -> 列定义
    invisible_column_supported: bool                        # 是否支持读取 INVISIBLE_COLUMN 元数据
    identity_column_supported: bool                         # 是否支持读取 IDENTITY_COLUMN 元数据
    default_on_null_supported: bool                         # 是否支持读取 DEFAULT_ON_NULL 元数据
    indexes: Dict[Tuple[str, str], Dict[str, Dict]]        # (OWNER, TABLE_NAME) -> 索引
    constraints: Dict[Tuple[str, str], Dict[str, Dict]]    # (OWNER, TABLE_NAME) -> 约束 (含 index_name)
    triggers: Dict[Tuple[str, str], Dict[str, Dict]]       # (OWNER, TABLE_NAME) -> 触发器
    sequences: Dict[str, Set[str]]                         # OWNER -> {SEQUENCE_NAME}
    sequence_attrs: Dict[str, Dict[str, Dict[str, object]]]  # OWNER -> {SEQUENCE_NAME: attrs}
    table_comments: Dict[Tuple[str, str], Optional[str]]   # (OWNER, TABLE_NAME) -> COMMENT
    column_comments: Dict[Tuple[str, str], Dict[str, Optional[str]]]  # (OWNER, TABLE_NAME) -> {COLUMN_NAME: COMMENT}
    comments_complete: bool                                # 注释元数据是否加载完成
    blacklist_tables: BlacklistTableMap                    # (OWNER, TABLE) -> {(BLACK_TYPE, DATA_TYPE): entry}
    object_privileges: List["OracleObjectPrivilege"]         # DBA_TAB_PRIVS (对象权限)
    sys_privileges: List["OracleSysPrivilege"]               # DBA_SYS_PRIVS (系统权限)
    role_privileges: List["OracleRolePrivilege"]             # DBA_ROLE_PRIVS (角色授权)
    role_metadata: Dict[str, "OracleRoleInfo"]               # DBA_ROLES 角色元数据
    system_privilege_map: Set[str]                           # SYSTEM_PRIVILEGE_MAP
    table_privilege_map: Set[str]                            # TABLE_PRIVILEGE_MAP
    object_statuses: Dict[Tuple[str, str, str], str]         # (OWNER, OBJECT_NAME, OBJECT_TYPE) -> STATUS
    package_errors: Dict[Tuple[str, str, str], "PackageErrorInfo"]  # (OWNER, NAME, TYPE) -> error summary
    package_errors_complete: bool                            # 源端错误信息是否完整
    partition_key_columns: Dict[Tuple[str, str], List[str]]   # (OWNER, TABLE_NAME) -> [PARTITION_COLS...]
    interval_partitions: Dict[Tuple[str, str], IntervalPartitionInfo]  # (OWNER, TABLE) -> interval info


class DependencyRecord(NamedTuple):
    owner: str
    name: str
    object_type: str
    referenced_owner: str
    referenced_name: str
    referenced_type: str


class DependencyIssue(NamedTuple):
    dependent: str
    dependent_type: str
    referenced: str
    referenced_type: str
    reason: str


class SynonymMeta(NamedTuple):
    owner: str
    name: str
    table_owner: str
    table_name: str
    db_link: Optional[str]


DependencyReport = Dict[str, List[DependencyIssue]]


class OracleObjectPrivilege(NamedTuple):
    grantee: str
    owner: str
    object_name: str
    object_type: str
    privilege: str
    grantable: bool


class OracleSysPrivilege(NamedTuple):
    grantee: str
    privilege: str
    admin_option: bool


class OracleRolePrivilege(NamedTuple):
    grantee: str
    role: str
    admin_option: bool


class OracleRoleInfo(NamedTuple):
    role: str
    authentication_type: str
    password_required: bool
    oracle_maintained: Optional[bool]


class ObjectGrantEntry(NamedTuple):
    privilege: str
    object_full: str
    grantable: bool


class SystemGrantEntry(NamedTuple):
    privilege: str
    admin_option: bool


class RoleGrantEntry(NamedTuple):
    role: str
    admin_option: bool


class FilteredGrantEntry(NamedTuple):
    category: str  # SYSTEM | OBJECT
    grantee: str
    privilege: str
    object_full: str
    reason: str


class GrantPlan(NamedTuple):
    object_grants: Dict[str, Set[ObjectGrantEntry]]
    sys_privs: Dict[str, Set[SystemGrantEntry]]
    role_privs: Dict[str, Set[RoleGrantEntry]]
    role_ddls: List[str]
    filtered_grants: List[FilteredGrantEntry]
    view_grant_targets: Set[str]


class ObGrantCatalog(NamedTuple):
    object_privs: Set[Tuple[str, str, str]]          # (GRANTEE, PRIV, OWNER.OBJ)
    object_privs_grantable: Set[Tuple[str, str, str]]  # grantable subset
    sys_privs: Set[Tuple[str, str]]                  # (GRANTEE, PRIV)
    sys_privs_admin: Set[Tuple[str, str]]            # admin_option subset
    role_privs: Set[Tuple[str, str]]                 # (GRANTEE, ROLE)
    role_privs_admin: Set[Tuple[str, str]]           # admin_option subset


class ColumnLengthIssue(NamedTuple):
    column: str
    src_length: int
    tgt_length: int
    limit_length: int  # 下限或上限（根据 issue 标识）
    issue: str         # 'short' | 'oversize'


class ColumnTypeIssue(NamedTuple):
    column: str
    src_type: str
    tgt_type: str
    expected_type: str
    issue: str


class PackageErrorInfo(NamedTuple):
    count: int
    first_error: str


class PackageCompareRow(NamedTuple):
    src_full: str
    obj_type: str
    src_status: str
    tgt_full: str
    tgt_status: str
    result: str
    error_count: int
    first_error: str


# --- 对象类型常量 ---
PRIMARY_OBJECT_TYPES: Tuple[str, ...] = (
    'TABLE',
    'VIEW',
    'MATERIALIZED VIEW',
    'PROCEDURE',
    'FUNCTION',
    'PACKAGE',
    'PACKAGE BODY',
    'SYNONYM',
    'JOB',
    'SCHEDULE',
    'TYPE',
    'TYPE BODY'
)

PRINT_ONLY_PRIMARY_TYPES: Tuple[str, ...] = (
    'MATERIALIZED VIEW',
)

PRINT_ONLY_PRIMARY_REASONS: Dict[str, str] = {
    'MATERIALIZED VIEW': "OB 暂不支持 MATERIALIZED VIEW，仅打印不校验"
}

OB_FEATURE_GATE_VERSION = "4.4.2"
INTERVAL_PARTITION_FIXUP_MODE_VALUES: Set[str] = {"auto", "true", "false"}
INTERVAL_PARTITION_FIXUP_MODE_ALIASES: Dict[str, str] = {
    "on": "true",
    "yes": "true",
    "1": "true",
    "off": "false",
    "no": "false",
    "0": "false",
}
MVIEW_CHECK_FIXUP_MODE_VALUES: Set[str] = {"auto", "on", "off"}
MVIEW_CHECK_FIXUP_MODE_ALIASES: Dict[str, str] = {
    "true": "on",
    "1": "on",
    "yes": "on",
    "enable": "on",
    "enabled": "on",
    "false": "off",
    "0": "off",
    "no": "off",
    "disable": "off",
    "disabled": "off",
}

PACKAGE_OBJECT_TYPES: Tuple[str, ...] = (
    'PACKAGE',
    'PACKAGE BODY'
)

PLSQL_ORDER_TYPES: Tuple[str, ...] = (
    'TYPE',
    'TYPE BODY',
    'PACKAGE',
    'PACKAGE BODY',
    'PROCEDURE',
    'FUNCTION',
    'TRIGGER'
)

PLSQL_ORDER_PRIORITY: Dict[str, int] = {
    'TYPE': 0,
    'PACKAGE': 1,
    'PROCEDURE': 2,
    'FUNCTION': 3,
    'TRIGGER': 4,
    'TYPE BODY': 5,
    'PACKAGE BODY': 6,
}

# 这些类型不参与 schema 推导（除非显式 remap）
NO_INFER_SCHEMA_TYPES: Set[str] = {
    'VIEW',
    'MATERIALIZED VIEW',
    'TRIGGER',
    'PACKAGE',
    'PACKAGE BODY'
}

INVALID_STATUS_TYPES: Set[str] = {
    'VIEW',
    'PROCEDURE',
    'FUNCTION',
    'PACKAGE',
    'PACKAGE BODY',
    'TYPE',
    'TYPE BODY',
    'TRIGGER'
}

BLACKLIST_REASON_BY_TYPE: Dict[str, str] = {
    'SPE': "表字段存在不支持的类型，不支持创建，不需要生成DDL",
    'TEMP_TABLE': "临时表，不支持创建，不需要生成DDL",
    'TEMPORARY_TABLE': "源表是临时表，不需要生成DDL",
    'DIY': "表中字段存在自定义类型，不支持创建，不需要生成DDL",
    'LOB_OVERSIZE': "表中存在的LOB字段体积超过512 MiB，可以在目标端创建表，但是 OMS 不支持同步",
    'LONG': "LONG/LONG RAW 需转换为 CLOB/BLOB",
    'DBLINK': "源表可能是 IOT 表或者外部表，不需要生成DDL",
    'NAME_PATTERN': "表名命中黑名单关键字规则，建议排除迁移或人工确认",
    'RENAME': "表名命中黑名单关键字规则，建议排除迁移或人工确认",
}
BLACKLIST_MODES: Set[str] = {"auto", "table_only", "rules_only", "disabled"}
DEFAULT_BLACKLIST_RULES_PATH = str(Path(__file__).resolve().parent / "blacklist_rules.json")

SUPPORT_STATE_SUPPORTED = "SUPPORTED"
SUPPORT_STATE_UNSUPPORTED = "UNSUPPORTED"
SUPPORT_STATE_BLOCKED = "BLOCKED"

VIEW_UNSUPPORTED_DEFAULT_VIEWS: Set[str] = {
    "DBA_DATA_FILES",
    "ALL_RULES",
}
VIEW_PRIVILEGE_DEFAULT_VIEWS: Set[str] = {
    "DBA_JOBS",
    "DBA_OBJECTS",
    "DBA_SOURCE",
}
VIEW_UNSUPPORTED_PATTERNS: Tuple[str, ...] = (
    r'(?<!\w)"?SYS"?\s*\.\s*"?OBJ\$"?(?!\w)',
)
VIEW_X_DOLLAR_PATTERN = re.compile(
    r'(?<![A-Z0-9_\$#])"?X\$[A-Z0-9_#$]+"?(?![A-Z0-9_\$#])',
    flags=re.IGNORECASE
)
VIEW_DBLINK_POLICIES: Set[str] = {"block", "allow"}
VIEW_CONSTRAINT_CLEANUP_VALUES: Set[str] = {"auto", "force", "off"}
VIEW_CONSTRAINT_CLEANUP_ALIASES = {
    "on": "auto",
    "true": "auto",
    "yes": "auto",
    "1": "auto",
    "disable": "off",
    "false": "off",
    "0": "off",
}

# 主对象中除 TABLE 外均做存在性验证
PRIMARY_EXISTENCE_ONLY_TYPES: Tuple[str, ...] = tuple(
    obj for obj in PRIMARY_OBJECT_TYPES if obj != 'TABLE' and obj not in PRINT_ONLY_PRIMARY_TYPES
)

# 额外纳入 remap/依赖但不做列级主检查的对象
DEPENDENCY_EXTRA_OBJECT_TYPES: Tuple[str, ...] = (
    'TRIGGER',
    'SEQUENCE',
    'INDEX'
)

ALL_TRACKED_OBJECT_TYPES: Tuple[str, ...] = tuple(
    sorted(set(PRIMARY_OBJECT_TYPES) | set(DEPENDENCY_EXTRA_OBJECT_TYPES))
)

EXTRA_OBJECT_CHECK_TYPES: Tuple[str, ...] = (
    'INDEX',
    'CONSTRAINT',
    'SEQUENCE',
    'TRIGGER'
)

STATUS_DRIFT_CHECK_TYPES: Tuple[str, ...] = (
    'TRIGGER',
    'CONSTRAINT',
)

CONSTRAINT_STATUS_SYNC_MODE_VALUES: Set[str] = {"enabled_only", "full"}
CONSTRAINT_STATUS_SYNC_MODE_ALIASES: Dict[str, str] = {
    "enabled": "enabled_only",
    "on": "enabled_only",
    "true": "enabled_only",
    "1": "enabled_only",
    "all": "full",
}

CONSTRAINT_MISSING_FIXUP_VALIDATE_MODE_VALUES: Set[str] = {
    "safe_novalidate",
    "source",
    "force_validate",
}
CONSTRAINT_MISSING_FIXUP_VALIDATE_MODE_ALIASES: Dict[str, str] = {
    "safe": "safe_novalidate",
    "novalidate": "safe_novalidate",
    "safe_no_validate": "safe_novalidate",
    "source_only": "source",
    "auto": "source",
    "validate": "force_validate",
    "force": "force_validate",
    "always_validate": "force_validate",
}

TRIGGER_VALIDITY_SYNC_MODE_VALUES: Set[str] = {"off", "compile"}
TRIGGER_VALIDITY_SYNC_MODE_ALIASES: Dict[str, str] = {
    "on": "compile",
    "true": "compile",
    "1": "compile",
    "false": "off",
    "0": "off",
}

FIXUP_CREATE_REPLACE_TYPES: Set[str] = {
    'VIEW',
    'PROCEDURE',
    'FUNCTION',
    'PACKAGE',
    'PACKAGE BODY',
    'TRIGGER',
    'TYPE',
    'TYPE BODY',
    'SYNONYM',
}

FIXUP_IDEMPOTENT_DEFAULT_TYPES: Set[str] = set(FIXUP_CREATE_REPLACE_TYPES)

FIXUP_AUTO_GRANT_DEFAULT_TYPES_ORDERED: Tuple[str, ...] = (
    'VIEW',
    'MATERIALIZED VIEW',
    'SYNONYM',
    'PROCEDURE',
    'FUNCTION',
    'PACKAGE',
    'PACKAGE BODY',
    'TRIGGER',
    'TYPE',
    'TYPE BODY',
)
FIXUP_AUTO_GRANT_DEFAULT_TYPES: Set[str] = set(FIXUP_AUTO_GRANT_DEFAULT_TYPES_ORDERED)
FIXUP_AUTO_GRANT_ALLOWED_TYPES: Set[str] = set(PRIMARY_OBJECT_TYPES) | set(EXTRA_OBJECT_CHECK_TYPES)

# 注释比对时批量 IN 子句的大小，避免 ORA-01795
COMMENT_BATCH_SIZE = 200
# Oracle IN 列表最大表达式数量为 1000，预留余量
ORACLE_IN_BATCH_SIZE = 900
# 授权规模提示阈值（对象权限条数）
GRANT_WARN_THRESHOLD = 200000
# 扩展校验使用多进程的表数量阈值（过大会导致内存翻倍）
EXTRA_CHECK_PROCESS_MAX_TABLES = 2000

# OceanBase 目标端自动生成且需在列对比中忽略的 OMS 列
IGNORED_OMS_COLUMNS: Tuple[str, ...] = (
    "OMS_OBJECT_NUMBER",
    "OMS_RELATIVE_FNO",
    "OMS_BLOCK_NUMBER",
    "OMS_ROW_NUMBER",
)

# 系统/迁移工具自动生成的列（用于降噪与注释过滤）
AUTO_GENERATED_COLUMNS: Tuple[str, ...] = (
    "__PK_INCREMENT",
)
AUTO_SEQUENCE_PATTERNS = (
    re.compile(r"^ISEQ\$\$_", re.IGNORECASE),
)
SYS_NC_COLUMN_PATTERNS = (
    re.compile(r"^SYS_NC\d+\$", re.IGNORECASE),
    re.compile(r"^SYS_NC_[A-Z_]+\$", re.IGNORECASE),
)
# SYS_C* 可能带 $ 或其他后缀（目标端内部列），用前缀匹配避免漏判
SYS_C_COLUMN_PATTERNS = (
    re.compile(r"^SYS_C_?\d+", re.IGNORECASE),
)
NOISE_REASON_AUTO_COLUMN = "AUTO_COLUMN"
NOISE_REASON_AUTO_SEQUENCE = "AUTO_SEQUENCE"
NOISE_REASON_SYS_NC_COLUMN = "SYS_NC_COLUMN"
NOISE_REASON_OMS_HELPER_COLUMN = "OMS_HELPER_COLUMN"
NOISE_REASON_OMS_ROWID_INDEX = "OMS_ROWID_INDEX"
NOISE_REASON_OBNOTNULL_CONSTRAINT = "OBNOTNULL_CONSTRAINT"
OB_OBCHECK_NAME_PATTERN = re.compile(r"_OBCHECK_\d+$", re.IGNORECASE)


def normalize_identifier_name(name: Optional[str]) -> str:
    if not name:
        return ""
    return str(name).strip().strip('"').upper()


def parse_column_id(value: Optional[object]) -> Optional[int]:
    if value is None:
        return None
    try:
        num = int(str(value).strip())
    except (TypeError, ValueError):
        return None
    if num <= 0:
        return None
    return num


def is_sys_nc_column_name(name: Optional[str]) -> bool:
    name_u = normalize_identifier_name(name)
    if not name_u:
        return False
    return any(pattern.match(name_u) for pattern in SYS_NC_COLUMN_PATTERNS)


def is_sys_c_column_name(name: Optional[str]) -> bool:
    name_u = normalize_identifier_name(name)
    if not name_u:
        return False
    if any(pattern.match(name_u) for pattern in SYS_C_COLUMN_PATTERNS):
        return True
    # 兜底：SYS_C 后面必须尽快出现数字（避免误伤 SYS_CUSTOMER 等业务列）
    if not name_u.startswith("SYS_C"):
        return False
    probe = name_u[5:]
    if probe.startswith("_"):
        probe = probe[1:]
    return bool(probe) and probe[0].isdigit()


def classify_noise_column(name: Optional[str]) -> Optional[str]:
    name_u = normalize_identifier_name(name)
    if not name_u:
        return None
    if name_u in AUTO_GENERATED_COLUMNS:
        return NOISE_REASON_AUTO_COLUMN
    if is_sys_nc_column_name(name_u):
        return NOISE_REASON_SYS_NC_COLUMN
    if name_u in IGNORED_OMS_COLUMNS:
        return NOISE_REASON_OMS_HELPER_COLUMN
    return None


def is_oms_rowid_index_name(name: Optional[str]) -> bool:
    return "_OMS_ROWID" in normalize_identifier_name(name)


def is_auto_sequence_name(name: Optional[str]) -> bool:
    name_u = normalize_identifier_name(name)
    if not name_u:
        return False
    return any(pattern.match(name_u) for pattern in AUTO_SEQUENCE_PATTERNS)


def is_ignored_oms_column(col_name: Optional[str], col_meta: Optional[Dict] = None) -> bool:
    """
    OceanBase 端迁移工具可能添加 OMS_* 列（VISIBLE/INVISIBLE 均可）。
    只要列名命中已知 OMS_* 集合就忽略，不再依赖 hidden 标记。
    """
    if not col_name:
        return False
    col_u = col_name.strip('"').upper()
    return col_u in IGNORED_OMS_COLUMNS


def is_ignored_source_column(col_name: Optional[str], col_meta: Optional[Dict] = None) -> bool:
    """
    源端列忽略规则：
    - 命中 OMS_* 忽略名单的列
    - Oracle hidden/virtual 等隐藏列（如支持 HIDDEN_COLUMN 字段）
    """
    if is_ignored_oms_column(col_name, col_meta):
        return True
    if col_meta and col_meta.get("hidden"):
        return True
    return False


def select_tab_columns_view(
    primary_view: str,
    primary_support: Dict[str, bool],
    secondary_view: str,
    secondary_support: Dict[str, bool]
) -> Tuple[str, Dict[str, bool], List[str]]:
    """
    选择可提供更多列元数据的字典视图。
    当 secondary_view 提供 primary_view 缺失的字段时，切换到 secondary_view。
    返回 (view_name, support_map, missing_columns)。
    """
    missing_cols: List[str] = []
    for col_name, supported in (primary_support or {}).items():
        if not supported and (secondary_support or {}).get(col_name):
            missing_cols.append(col_name)
    if missing_cols:
        return secondary_view, secondary_support, missing_cols
    return primary_view, primary_support, []


VARCHAR_LEN_MIN_MULTIPLIER = 1.5  # 目标端 VARCHAR/2 长度需 >= ceil(src * 1.5)
VARCHAR_LEN_OVERSIZE_MULTIPLIER = 2.5  # 超过该倍数认为“过大”，需要提示
NUMBER_STAR_PRECISION = 38  # NUMBER(*) 等价精度


def normalize_black_type(value: Optional[str]) -> str:
    if value is None:
        return ""
    return str(value).strip().upper()


def normalize_black_data_type(value: Optional[str]) -> str:
    if value is None:
        return ""
    return str(value).strip().upper()


def normalize_blacklist_mode(value: Optional[str]) -> str:
    mode = (value or "auto").strip().lower()
    if mode not in BLACKLIST_MODES:
        log.warning("未知 blacklist_mode=%s，回退为 auto。", value)
        return "auto"
    return mode


def merge_blacklist_sources(existing: str, incoming: str) -> str:
    parts: List[str] = []
    for raw in (existing, incoming):
        if not raw:
            continue
        for item in str(raw).split(","):
            token = item.strip()
            if token:
                parts.append(token)
    merged: List[str] = []
    for item in parts:
        if item not in merged:
            merged.append(item)
    return ", ".join(merged)


def format_blacklist_source(source: str) -> str:
    if not source:
        return ""
    parts = [item.strip() for item in str(source).split(",") if item.strip()]
    parts = [item for item in parts if item.upper() != "TABLE"]
    return ", ".join(parts)


def merge_detail_parts(*parts: Optional[str]) -> str:
    items: List[str] = []
    for part in parts:
        if not part:
            continue
        token = str(part).strip()
        if not token or token == "-":
            continue
        items.append(token)
    return "; ".join(items) if items else "-"


def add_blacklist_entry(
    blacklist_tables: BlacklistTableMap,
    owner: str,
    table: str,
    black_type: Optional[str],
    data_type: Optional[str],
    source: str = ""
) -> bool:
    owner_u = (owner or "").strip().upper()
    table_u = (table or "").strip().upper()
    if not owner_u or not table_u:
        return False
    black_type_u = normalize_black_type(black_type) or "UNKNOWN"
    data_type_u = normalize_black_data_type(data_type)
    entry_key = (black_type_u, data_type_u)
    entries = blacklist_tables.setdefault((owner_u, table_u), {})
    existing = entries.get(entry_key)
    merged_source = merge_blacklist_sources(existing.source if existing else "", source)
    if existing and merged_source == existing.source:
        return False
    entries[entry_key] = BlacklistEntry(black_type_u, data_type_u, merged_source)
    return True


def blacklist_reason(black_type: Optional[str]) -> str:
    black_type_u = normalize_black_type(black_type)
    if not black_type_u:
        return "未知黑名单类型"
    return BLACKLIST_REASON_BY_TYPE.get(black_type_u, "未知黑名单类型")


def summarize_blacklist_entries(
    entries: Dict[BlacklistEntryKey, BlacklistEntry]
) -> Tuple[str, str, str]:
    """
    汇总黑名单条目，返回 (black_type, reason, detail)。
    """
    if not entries:
        return "", "", ""
    prefer_order = [
        "TEMPORARY_TABLE",
        "TEMP_TABLE",
        "NAME_PATTERN",
        "RENAME",
        "SPE",
        "DIY",
        "LONG",
        "LOB_OVERSIZE",
        "DBLINK",
    ]
    seen_types: List[str] = []
    detail_parts: List[str] = []
    for (black_type, data_type), entry in entries.items():
        bt = (black_type or "").upper()
        dt = (data_type or "").upper()
        if bt:
            seen_types.append(bt)
        detail = f"{bt}:{dt}" if bt or dt else ""
        if entry and entry.source:
            detail = f"{detail}({entry.source})" if detail else f"SOURCE={entry.source}"
        if detail:
            detail_parts.append(detail)
    chosen = ""
    for cand in prefer_order:
        if cand in seen_types:
            chosen = cand
            break
    if not chosen and seen_types:
        chosen = sorted(set(seen_types))[0]
    reason = blacklist_reason(chosen)
    detail = "; ".join(detail_parts)
    return chosen, reason, detail


def is_long_type(data_type: Optional[str]) -> bool:
    dt = (data_type or "").strip().upper()
    return dt in ("LONG", "LONG RAW")


def is_long_only_blacklist(entries: Optional[Dict[Tuple[str, str], BlacklistEntry]]) -> bool:
    """
    判断黑名单条目是否全部为 LONG/LONG RAW 类型。
    """
    if not entries:
        return False
    for (black_type, data_type), entry in entries.items():
        bt = (black_type or "").strip().upper()
        dt = (data_type or "").strip().upper()
        if bt != "LONG" and not is_long_type(dt):
            return False
        if entry and entry.black_type and entry.black_type.upper() != "LONG" and not is_long_type(entry.data_type):
            return False
    return True


def map_long_type_to_ob(data_type: Optional[str]) -> str:
    dt = (data_type or "").strip().upper()
    if dt == "LONG":
        return "CLOB"
    if dt == "LONG RAW":
        return "BLOB"
    return dt

def is_oms_index(name: str, columns: List[str]) -> bool:
    """识别迁移工具自动生成的 OMS_* 唯一索引，忽略之。"""
    if is_oms_rowid_index_name(name):
        return True
    name_u = (name or "").strip('"').upper()
    cols_u = [c.strip('"').upper() for c in (columns or []) if c]
    if not cols_u:
        return False
    # 检查名称是否以 _OMS_ROWID 结尾
    if not name_u.endswith("_OMS_ROWID"):
        return False
    
    # 检查列集合是否包含所有标准 OMS 列
    cols_set = set(cols_u)
    oms_cols_set = set(IGNORED_OMS_COLUMNS)
    
    # 如果包含所有4个OMS列，则认为是OMS索引（允许有额外列）
    return oms_cols_set.issubset(cols_set)


def extract_name_value(value: Optional[object]) -> str:
    if value is None:
        return ""
    if isinstance(value, str):
        return value
    if isinstance(value, (tuple, list)):
        for item in value:
            if isinstance(item, str) and item:
                return item
        if value:
            return extract_name_value(value[0])
    return str(value)


def extract_constraint_name(value: Optional[object]) -> str:
    return extract_name_value(value)


def is_notnull_check_condition(search_condition: Optional[str]) -> bool:
    if not search_condition:
        return False
    cond = normalize_sql_expression(search_condition)
    if not cond:
        return False
    cond_u = cond.upper().strip()
    while cond_u.startswith("(") and cond_u.endswith(")"):
        stripped = strip_wrapping_parentheses(cond_u)
        if stripped == cond_u:
            break
        cond_u = stripped.strip()
    cond_u = re.sub(r'"([A-Z0-9_#$]+)"', r"\1", cond_u)
    return bool(re.match(r"^[A-Z0-9_#$]+\s+IS\s+NOT\s+NULL$", cond_u))


def is_ob_notnull_constraint(
    name: Optional[object],
    search_condition: Optional[str] = None
) -> bool:
    """
    识别 OceanBase Oracle 模式下自动生成的 NOT NULL 类 CHECK 约束。
    典型命名：
      - *_OBNOTNULL_*
      - *_OBCHECK_<digits>   (仅在条件为单列 IS NOT NULL 时识别)
    """
    name_u = extract_constraint_name(name).upper()
    if "OBNOTNULL" in name_u:
        return True
    if OB_OBCHECK_NAME_PATTERN.search(name_u):
        if search_condition is None:
            return True
        return is_notnull_check_condition(search_condition)
    return False


def strip_wrapping_parentheses(text: str) -> str:
    if not text or not text.startswith("(") or not text.endswith(")"):
        return text
    depth = 0
    for idx, ch in enumerate(text):
        if ch == "(":
            depth += 1
        elif ch == ")":
            depth -= 1
            if depth == 0 and idx != len(text) - 1:
                return text
    if depth != 0:
        return text
    return text[1:-1].strip()


def tokenize_sql_expression(expr: str) -> List[Tuple[str, str, int, int]]:
    """
    将表达式拆分为 token，忽略空白，保留字符串字面量与括号位置。
    token 形式: (type, value, start, end)
    """
    tokens: List[Tuple[str, str, int, int]] = []
    if not expr:
        return tokens
    i = 0
    n = len(expr)
    while i < n:
        ch = expr[i]
        if ch.isspace():
            i += 1
            continue
        if ch == "'":
            start = i
            i += 1
            while i < n:
                if expr[i] == "'":
                    if i + 1 < n and expr[i + 1] == "'":
                        i += 2
                        continue
                    i += 1
                    break
                i += 1
            tokens.append(("STRING", expr[start:i], start, i))
            continue
        if ch.isalnum() or ch in "_$#":
            start = i
            i += 1
            while i < n and (expr[i].isalnum() or expr[i] in "_$#"):
                i += 1
            tokens.append(("WORD", expr[start:i], start, i))
            continue
        if ch in ("(", ")"):
            tokens.append(("PAREN", ch, i, i + 1))
            i += 1
            continue
        tokens.append(("SYMBOL", ch, i, i + 1))
        i += 1
    return tokens


def strip_redundant_predicate_parentheses(expr: str) -> str:
    """
    移除 AND/OR 链中冗余的谓词括号：
      (A > 0) AND (B IN (1,2,3)) -> A > 0 AND B IN (1,2,3)
    注意：仅移除顶层谓词括号，保留 NOT/IN/函数调用等必要语法。
    """
    if not expr:
        return expr
    tokens = tokenize_sql_expression(expr)
    if not tokens:
        return expr

    stack: List[Dict[str, object]] = []
    pairs: List[Dict[str, object]] = []

    for idx, (tok_type, tok_val, tok_start, tok_end) in enumerate(tokens):
        if tok_type == "STRING":
            continue
        if tok_type == "PAREN" and tok_val == "(":
            stack.append({
                "start_token_idx": idx,
                "start_pos": tok_start,
                "depth": len(stack),
                "has_boolean": False,
                "between_pending": False
            })
            continue
        if tok_type == "PAREN" and tok_val == ")":
            if not stack:
                continue
            ctx = stack.pop()
            ctx["end_token_idx"] = idx
            ctx["end_pos"] = tok_start
            pairs.append(ctx)
            continue
        if not stack:
            continue
        ctx = stack[-1]
        if tok_type == "WORD":
            word = tok_val.upper()
            if word == "BETWEEN":
                ctx["between_pending"] = True
            elif word == "AND":
                if ctx.get("between_pending"):
                    ctx["between_pending"] = False
                else:
                    ctx["has_boolean"] = True
            elif word == "OR":
                ctx["has_boolean"] = True

    def _prev_allows(token: Optional[Tuple[str, str, int, int]]) -> bool:
        if token is None:
            return True
        tok_type, tok_val, _, _ = token
        if tok_type == "WORD":
            return tok_val.upper() in ("AND", "OR")
        if tok_type == "PAREN":
            return tok_val == "("
        return False

    def _next_allows(token: Optional[Tuple[str, str, int, int]]) -> bool:
        if token is None:
            return True
        tok_type, tok_val, _, _ = token
        if tok_type == "WORD":
            return tok_val.upper() in ("AND", "OR")
        if tok_type == "PAREN":
            return tok_val == ")"
        return False

    remove = [False] * len(expr)
    for ctx in pairs:
        if ctx.get("depth") != 0:
            continue
        if ctx.get("has_boolean"):
            continue
        start_idx = int(ctx["start_token_idx"])
        end_idx = int(ctx["end_token_idx"])
        prev_tok = tokens[start_idx - 1] if start_idx > 0 else None
        next_tok = tokens[end_idx + 1] if end_idx + 1 < len(tokens) else None
        if not _prev_allows(prev_tok) or not _next_allows(next_tok):
            continue
        remove[int(ctx["start_pos"])] = True
        remove[int(ctx["end_pos"])] = True

    if not any(remove):
        return expr
    return "".join(ch for i, ch in enumerate(expr) if not remove[i])


OPERATOR_SPACE_CHARS = set("+-*/%<>=,|")


def strip_spaces_around_operators(text: str) -> str:
    if not text:
        return ""
    out: List[str] = []
    in_quote = False
    i = 0
    while i < len(text):
        ch = text[i]
        if ch == "'":
            out.append(ch)
            if in_quote:
                if i + 1 < len(text) and text[i + 1] == "'":
                    out.append("'")
                    i += 1
                else:
                    in_quote = False
            else:
                in_quote = True
            i += 1
            continue
        if not in_quote and ch.isspace():
            prev = out[-1] if out else ""
            j = i + 1
            while j < len(text) and text[j].isspace():
                j += 1
            next_ch = text[j] if j < len(text) else ""
            if prev in OPERATOR_SPACE_CHARS or next_ch in OPERATOR_SPACE_CHARS:
                i += 1
                continue
        out.append(ch)
        i += 1
    return "".join(out)


def normalize_sql_expression(expr: Optional[str]) -> str:
    if expr is None:
        return ""
    text = str(expr).strip()
    if not text:
        return ""
    text = text.replace("\r", " ").replace("\n", " ").replace("\t", " ")
    text = re.sub(r"\s+", " ", text).strip()
    if text.endswith(";"):
        text = text[:-1].strip()
    text = re.sub(r'"([A-Za-z0-9_#$]+)"', r'\1', text)
    while True:
        stripped = strip_wrapping_parentheses(text)
        if stripped == text:
            break
        text = stripped
    return strip_spaces_around_operators(text)


def uppercase_outside_single_quotes(text: str) -> str:
    if not text:
        return ""
    out: List[str] = []
    in_quote = False
    i = 0
    while i < len(text):
        ch = text[i]
        if ch == "'":
            out.append(ch)
            if in_quote:
                if i + 1 < len(text) and text[i + 1] == "'":
                    out.append("'")
                    i += 1
                else:
                    in_quote = False
            else:
                in_quote = True
        else:
            out.append(ch if in_quote else ch.upper())
        i += 1
    return "".join(out)


def normalize_sql_expression_casefold(expr: Optional[str]) -> str:
    return uppercase_outside_single_quotes(normalize_sql_expression(expr))


def is_system_notnull_check(cons_name: Optional[str], search_condition: Optional[str]) -> bool:
    if not cons_name or not search_condition:
        return False
    name_u = cons_name.upper()
    if not name_u.startswith("SYS_"):
        return False
    return is_notnull_check_condition(search_condition)


CHECK_SYS_CONTEXT_USERENV_RE = re.compile(r"SYS_CONTEXT\s*\(\s*['\"]USERENV['\"]", flags=re.IGNORECASE)
CHECK_LIKE_ESCAPE_REWRITE_RE = re.compile(
    r"([A-Z0-9_#$]+\s+LIKE)\s+REPLACE\('((?:''|[^'])*)','\\\\','\\\\\\\\'\)\s+ESCAPE\s+'\\\\'",
    flags=re.IGNORECASE
)
CHECK_RANGE_REWRITE_RE = re.compile(
    r"^([A-Z0-9_#$]+)\s*>=\s*(.+)\s+AND\s+\1\s*<=\s*(.+)$",
    flags=re.IGNORECASE
)
CHECK_RANGE_REWRITE_REV_RE = re.compile(
    r"^([A-Z0-9_#$]+)\s*<=\s*(.+)\s+AND\s+\1\s*>=\s*(.+)$",
    flags=re.IGNORECASE
)


def normalize_deferrable_flag(value: Optional[object]) -> str:
    text = str(value).strip().upper() if value is not None else ""
    if text in ("DEFERRABLE", "Y", "YES", "TRUE", "1"):
        return "DEFERRABLE"
    if text in ("NOT DEFERRABLE", "NOT_DEFERRABLE", "N", "NO", "FALSE", "0"):
        return "NOT DEFERRABLE"
    if text:
        return text
    return "NOT DEFERRABLE"


def normalize_deferred_flag(value: Optional[object]) -> str:
    text = str(value).strip().upper() if value is not None else ""
    if text in ("DEFERRED", "Y", "YES", "TRUE", "1"):
        return "DEFERRED"
    if text in ("IMMEDIATE", "NOT DEFERRED", "NOT_DEFERRED", "N", "NO", "FALSE", "0"):
        return "IMMEDIATE"
    if text:
        return text
    return "IMMEDIATE"


def normalize_check_constraint_expression(
    expr: Optional[str],
    cons_name: Optional[str]
) -> str:
    expr_norm = normalize_sql_expression_casefold(expr)
    expr_norm = strip_redundant_predicate_parentheses(expr_norm)
    expr_norm = uppercase_outside_single_quotes(normalize_sql_expression(expr_norm))
    # OceanBase 可能将 LIKE 常量重写为 REPLACE(... ) ESCAPE '\\'，此处回归到字面量语义。
    expr_norm = CHECK_LIKE_ESCAPE_REWRITE_RE.sub(r"\1 '\2'", expr_norm)
    # OceanBase 可能将 BETWEEN 展开为 >= AND <=，统一回 BETWEEN 以便跨库语义匹配。
    m = CHECK_RANGE_REWRITE_RE.fullmatch(expr_norm)
    if m:
        expr_norm = f"{m.group(1)} BETWEEN {m.group(2)} AND {m.group(3)}"
    else:
        m = CHECK_RANGE_REWRITE_REV_RE.fullmatch(expr_norm)
        if m:
            expr_norm = f"{m.group(1)} BETWEEN {m.group(3)} AND {m.group(2)}"
    name_u = (cons_name or "").upper()
    if not expr_norm:
        return f"__NO_EXPR__:{name_u}"
    return expr_norm


def normalize_check_constraint_signature(
    expr: Optional[str],
    cons_name: Optional[str],
    cons_meta: Optional[Dict]
) -> str:
    expr_norm = normalize_check_constraint_expression(expr, cons_name)
    deferrable = normalize_deferrable_flag((cons_meta or {}).get("deferrable"))
    deferred = normalize_deferred_flag((cons_meta or {}).get("deferred"))
    return f"{expr_norm}||DEFERRABLE={deferrable}||DEFERRED={deferred}"


CONSTRAINT_TYPE_LABELS = {
    "P": "PRIMARY KEY",
    "U": "UNIQUE",
    "R": "FOREIGN KEY",
    "C": "CHECK",
}


def classify_unsupported_constraint(cons_meta: Optional[Dict]) -> Optional[Tuple[str, str, str]]:
    if not cons_meta:
        return None
    ctype = (cons_meta.get("type") or "").upper()
    deferrable = normalize_deferrable_flag(cons_meta.get("deferrable"))
    deferred = normalize_deferred_flag(cons_meta.get("deferred"))
    if deferrable == "DEFERRABLE" or deferred == "DEFERRED":
        label = CONSTRAINT_TYPE_LABELS.get(ctype, ctype or "CONSTRAINT")
        reason_code = f"{label.replace(' ', '_')}_DEFERRABLE"
        return (
            reason_code,
            f"{label} 约束为 DEFERRABLE/DEFERRED，OceanBase 不支持。",
            "ORA-00900"
        )
    if ctype == "C":
        expr = cons_meta.get("search_condition") or ""
        if expr and CHECK_SYS_CONTEXT_USERENV_RE.search(str(expr)):
            return (
                "CHECK_SYS_CONTEXT",
                "CHECK 约束包含 SYS_CONTEXT('USERENV', ...)，OceanBase 不支持。",
                "ORA-02436"
            )
    return None


def normalize_delete_rule(value: Optional[str]) -> str:
    rule = (value or "").strip().upper()
    if not rule or rule == "NO ACTION":
        return ""
    return rule


def normalize_update_rule(value: Optional[str]) -> str:
    rule = (value or "").strip().upper()
    if not rule or rule == "NO ACTION":
        return ""
    return rule


def normalize_public_owner(owner: Optional[str]) -> Optional[str]:
    if owner is None:
        return None
    owner_str = str(owner).strip()
    if not owner_str:
        return None
    owner_u = owner_str.upper()
    if owner_u == "__PUBLIC":
        return "PUBLIC"
    return owner_u


def normalize_ob_metadata_public_owner(meta: ObMetadata) -> ObMetadata:
    def _norm_owner(owner: Optional[str]) -> Optional[str]:
        return normalize_public_owner(owner) if owner else owner

    def _norm_full_name(full_name: str) -> str:
        if not full_name:
            return full_name
        if '.' in full_name:
            owner, name = full_name.split('.', 1)
            owner_n = normalize_public_owner(owner)
            name_u = (name or "").upper()
            return f"{owner_n}.{name_u}"
        return full_name.upper()

    def _remap_owner_table_dict(
        data: Dict[Tuple[str, str], Dict[str, Dict]]
    ) -> Dict[Tuple[str, str], Dict[str, Dict]]:
        remapped: Dict[Tuple[str, str], Dict[str, Dict]] = {}
        for (owner, table), payload in data.items():
            owner_n = _norm_owner(owner)
            table_u = (table or "").upper()
            key = (owner_n, table_u)
            if key in remapped and isinstance(payload, dict):
                remapped[key].update(payload)
            else:
                remapped[key] = payload
        return remapped

    def _remap_owner_table_simple(
        data: Dict[Tuple[str, str], object]
    ) -> Dict[Tuple[str, str], object]:
        remapped: Dict[Tuple[str, str], object] = {}
        for (owner, table), payload in data.items():
            owner_n = _norm_owner(owner)
            table_u = (table or "").upper()
            key = (owner_n, table_u)
            remapped[key] = payload
        return remapped

    objects_by_type: Dict[str, Set[str]] = {}
    for obj_type, full_set in (meta.objects_by_type or {}).items():
        objects_by_type[obj_type] = {_norm_full_name(name) for name in (full_set or set())}

    object_statuses = {}
    for (owner, name, obj_type), status in (meta.object_statuses or {}).items():
        owner_n = _norm_owner(owner)
        name_u = (name or "").upper()
        obj_type_u = (obj_type or "").upper()
        object_statuses[(owner_n, name_u, obj_type_u)] = status

    package_errors = {}
    for (owner, name, obj_type), info in (meta.package_errors or {}).items():
        owner_n = _norm_owner(owner)
        name_u = (name or "").upper()
        obj_type_u = (obj_type or "").upper()
        package_errors[(owner_n, name_u, obj_type_u)] = info

    sequences = {}
    for owner, seqs in (meta.sequences or {}).items():
        owner_n = _norm_owner(owner)
        sequences[owner_n] = {str(s).upper() for s in (seqs or set())}

    sequence_attrs = {}
    for owner, attrs in (meta.sequence_attrs or {}).items():
        owner_n = _norm_owner(owner)
        sequence_attrs[owner_n] = attrs

    table_comments = _remap_owner_table_simple(meta.table_comments or {})
    column_comments = {}
    for (owner, table), cols in (meta.column_comments or {}).items():
        owner_n = _norm_owner(owner)
        table_u = (table or "").upper()
        column_comments[(owner_n, table_u)] = cols

    partition_key_columns = {}
    for (owner, table), cols in (meta.partition_key_columns or {}).items():
        owner_n = _norm_owner(owner)
        table_u = (table or "").upper()
        partition_key_columns[(owner_n, table_u)] = cols

    return meta._replace(
        objects_by_type=objects_by_type,
        tab_columns=_remap_owner_table_dict(meta.tab_columns or {}),
        indexes=_remap_owner_table_dict(meta.indexes or {}),
        constraints=_remap_owner_table_dict(meta.constraints or {}),
        triggers=_remap_owner_table_dict(meta.triggers or {}),
        sequences=sequences,
        sequence_attrs=sequence_attrs,
        table_comments=table_comments,
        column_comments=column_comments,
        object_statuses=object_statuses,
        package_errors=package_errors,
        partition_key_columns=partition_key_columns
    )

def is_index_expression_token(token: Optional[str]) -> bool:
    if not token:
        return False
    return bool(re.search(r"[()\s'\"+\-*/]|\bCASE\b", token, flags=re.IGNORECASE))


def normalize_number_meta(prec: Optional[object], scale: Optional[object]) -> Tuple[Optional[int], Optional[int]]:
    prec_int: Optional[int]
    scale_int: Optional[int]
    try:
        prec_int = int(prec) if prec is not None else None
    except (TypeError, ValueError):
        prec_int = None
    try:
        scale_int = int(scale) if scale is not None else None
    except (TypeError, ValueError):
        scale_int = None
    if prec_int is not None and scale_int is None:
        scale_int = 0
    return prec_int, scale_int


def normalize_number_signature(
    src_prec: Optional[int],
    src_scale: Optional[int],
    *,
    star_precision: int = NUMBER_STAR_PRECISION
) -> Tuple[Optional[int], Optional[int], bool]:
    if src_prec is None and src_scale is None:
        return None, None, True
    if src_prec is None:
        return star_precision, src_scale, False
    return src_prec, src_scale, False


def is_number_equivalent(
    src_prec: Optional[int],
    src_scale: Optional[int],
    tgt_prec: Optional[int],
    tgt_scale: Optional[int]
) -> bool:
    src_prec_n, src_scale_n, src_unbounded = normalize_number_signature(src_prec, src_scale)
    tgt_prec_n, tgt_scale_n, tgt_unbounded = normalize_number_signature(tgt_prec, tgt_scale)
    if src_unbounded:
        return tgt_unbounded
    if tgt_unbounded:
        return True
    if tgt_scale_n != src_scale_n:
        return False
    if tgt_prec_n is None or src_prec_n is None:
        return False
    return tgt_prec_n >= src_prec_n


def format_number_type_literal(data_type: str, precision: Optional[int], scale: Optional[int]) -> str:
    dt = (data_type or "").strip().upper() or "NUMBER"
    if precision is None:
        return dt
    if scale is None:
        return f"{dt}({int(precision)})"
    return f"{dt}({int(precision)},{int(scale)})"


def build_number_fixup_type(src_info: Dict, tgt_info: Dict) -> str:
    src_prec, src_scale = normalize_number_meta(
        src_info.get("data_precision"), src_info.get("data_scale")
    )
    tgt_prec, _tgt_scale = normalize_number_meta(
        tgt_info.get("data_precision"), tgt_info.get("data_scale")
    )
    if src_prec is None and src_scale is None:
        return format_number_type_literal(src_info.get("data_type") or "NUMBER", None, None)
    if src_prec is None:
        expected_scale = src_scale if src_scale is not None else 0
        return format_number_type_literal(
            src_info.get("data_type") or "NUMBER",
            NUMBER_STAR_PRECISION,
            expected_scale
        )
    expected_prec = src_prec
    if tgt_prec is not None and tgt_prec > expected_prec:
        expected_prec = tgt_prec
    expected_scale = src_scale if src_scale is not None else 0
    return format_number_type_literal(src_info.get("data_type") or "NUMBER", expected_prec, expected_scale)

OBJECT_COUNT_TYPES: Tuple[str, ...] = (
    'TABLE',
    'VIEW',
    'SYNONYM',
    'TRIGGER',
    'SEQUENCE',
    'PROCEDURE',
    'FUNCTION',
    'PACKAGE',
    'PACKAGE BODY',
    'TYPE',
    'TYPE BODY',
    'MATERIALIZED VIEW',
    'JOB',
    'SCHEDULE',
    'INDEX',
    'CONSTRAINT'
)


def parse_bool_flag(value: Optional[str], default: bool = True) -> bool:
    if value is None:
        return default
    return str(value).strip().lower() in ('1', 'true', 'yes', 'y', 'on')


def normalize_interval_partition_fixup_mode(raw_value: Optional[str]) -> str:
    value = (raw_value or "").strip().lower()
    if not value:
        return "auto"
    value = INTERVAL_PARTITION_FIXUP_MODE_ALIASES.get(value, value)
    if value not in INTERVAL_PARTITION_FIXUP_MODE_VALUES:
        log.warning(
            "generate_interval_partition_fixup=%s 不在支持范围内，将回退为 auto。",
            raw_value
        )
        return "auto"
    return value


def normalize_mview_check_fixup_mode(raw_value: Optional[str]) -> str:
    value = (raw_value or "").strip().lower()
    if not value:
        return "auto"
    value = MVIEW_CHECK_FIXUP_MODE_ALIASES.get(value, value)
    if value not in MVIEW_CHECK_FIXUP_MODE_VALUES:
        log.warning(
            "mview_check_fixup_mode=%s 不在支持范围内，将回退为 auto。",
            raw_value
        )
        return "auto"
    return value


def parse_type_list(
    raw_value: str,
    allowed: Set[str],
    label: str,
    default_all: bool = True
) -> Set[str]:
    if not raw_value or not raw_value.strip():
        return set(allowed) if default_all else set()
    parsed = {item.strip().upper() for item in raw_value.split(',') if item.strip()}
    unknown = parsed - allowed
    if unknown:
        log.warning("配置 %s 包含未知类型 %s，将被忽略。", label, sorted(unknown))
    return parsed & allowed


def parse_fixup_auto_grant_types(raw_value: str) -> Set[str]:
    if not raw_value or not raw_value.strip():
        return set(FIXUP_AUTO_GRANT_DEFAULT_TYPES)
    return parse_type_list(
        raw_value,
        FIXUP_AUTO_GRANT_ALLOWED_TYPES,
        'fixup_auto_grant_types',
        default_all=False
    )


def normalize_ddl_formatter(raw_value: Optional[str]) -> str:
    value = (raw_value or "").strip().lower()
    if not value:
        return DDL_FORMATTER_SQLCL
    if value not in DDL_FORMATTER_VALUES:
        log.warning("ddl_formatter=%s 不在支持范围内，将回退为 %s。", raw_value, DDL_FORMATTER_SQLCL)
        return DDL_FORMATTER_SQLCL
    return value


def normalize_ddl_format_fail_policy(raw_value: Optional[str]) -> str:
    value = (raw_value or "").strip().lower()
    if not value:
        return DDL_FORMAT_FAIL_FALLBACK
    if value not in DDL_FORMAT_FAIL_POLICIES:
        log.warning("ddl_format_fail_policy=%s 不在支持范围内，将回退为 %s。", raw_value, DDL_FORMAT_FAIL_FALLBACK)
        return DDL_FORMAT_FAIL_FALLBACK
    return value


def parse_ddl_format_types(raw_value: str, default_types: Optional[Set[str]] = None) -> Set[str]:
    if not raw_value or not raw_value.strip():
        return set(default_types or set())
    parsed_raw = [item.strip().upper() for item in raw_value.split(',') if item.strip()]
    normalized: Set[str] = set()
    for item in parsed_raw:
        key = item.replace(" ", "_")
        key = DDL_FORMAT_TYPE_ALIASES.get(key, item)
        normalized.add(key)
    unknown = normalized - DDL_FORMAT_TYPES
    if unknown:
        log.warning("配置 ddl_format_types 包含未知类型 %s，将被忽略。", sorted(unknown))
    return normalized & DDL_FORMAT_TYPES


def resolve_sqlcl_executable(raw_path: str) -> Optional[Path]:
    if not raw_path:
        return None
    path = Path(raw_path).expanduser()
    if path.is_dir():
        candidates = [
            path / "sql",
            path / "sql.exe",
            path / "bin" / "sql",
            path / "bin" / "sql.exe",
        ]
        for cand in candidates:
            if cand.exists():
                return cand
        return None
    if path.exists():
        return path
    return None


def infer_format_type_from_subdir(subdir: str) -> Optional[str]:
    if not subdir:
        return None
    parts = [p for p in subdir.replace("\\", "/").split("/") if p]
    if not parts:
        return None
    head = parts[0].lower()
    if head == "unsupported" and len(parts) > 1:
        head = parts[1].lower()
    if head in ("table_alter", "table-alter"):
        return "TABLE_ALTER"
    if head in ("tables_unsupported", "table"):
        return "TABLE"
    mapping = {
        "view": "VIEW",
        "materialized_view": "MATERIALIZED VIEW",
        "procedure": "PROCEDURE",
        "function": "FUNCTION",
        "package": "PACKAGE",
        "package_body": "PACKAGE BODY",
        "synonym": "SYNONYM",
        "job": "JOB",
        "schedule": "SCHEDULE",
        "type": "TYPE",
        "type_body": "TYPE BODY",
        "index": "INDEX",
        "constraint": "CONSTRAINT",
        "sequence": "SEQUENCE",
        "trigger": "TRIGGER",
    }
    return mapping.get(head)


def strip_plsql_trailing_slash(ddl: str) -> Tuple[str, bool]:
    if not ddl:
        return ddl, False
    lines = ddl.rstrip().splitlines()
    if not lines:
        return ddl, False
    idx = len(lines) - 1
    while idx >= 0 and not lines[idx].strip():
        idx -= 1
    if idx >= 0 and lines[idx].strip() == "/":
        return "\n".join(lines[:idx]).rstrip(), True
    return ddl, False

def parse_csv_set(raw_value: Optional[str]) -> Set[str]:
    """
    解析逗号分隔列表为大写集合。
    """
    if not raw_value or not str(raw_value).strip():
        return set()
    return {item.strip().upper() for item in str(raw_value).split(',') if item.strip()}


def parse_csv_list(raw_value: Optional[str]) -> List[str]:
    if not raw_value or not str(raw_value).strip():
        return []
    return [item.strip() for item in str(raw_value).split(',') if item and item.strip()]


def parse_blacklist_rule_tag_enabled(
    tag_value: Optional[object],
    default_enabled: bool = True
) -> bool:
    raw = (str(tag_value).strip().lower() if tag_value is not None else "")
    if not raw:
        return default_enabled
    if raw in ("enabled", "enable", "on", "true", "1", "yes", "y"):
        return True
    if raw in ("disabled", "disable", "off", "false", "0", "no", "n"):
        return False
    return default_enabled


def load_blacklist_name_patterns(
    inline_value: Optional[str],
    file_path: Optional[str]
) -> List[str]:
    patterns: List[str] = []
    seen: Set[str] = set()
    for item in parse_csv_list(inline_value):
        key = item.upper()
        if key not in seen:
            seen.add(key)
            patterns.append(item)
    path_raw = (file_path or "").strip()
    if path_raw:
        path = Path(path_raw).expanduser()
        if path.exists():
            try:
                for line in path.read_text(encoding="utf-8").splitlines():
                    token = line.strip()
                    if not token or token.startswith('#') or token.startswith(';'):
                        continue
                    key = token.upper()
                    if key in seen:
                        continue
                    seen.add(key)
                    patterns.append(token)
            except OSError as exc:
                log.debug("读取 blacklist_name_patterns_file 失败: %s (%s)", path, exc)
    return patterns


def build_blacklist_name_pattern_clause(patterns: Sequence[str]) -> str:
    clauses: List[str] = []
    for pattern in patterns:
        token = (pattern or "").strip()
        if not token:
            continue
        # 关键字按“字面包含”处理，避免 %/_ 被当作通配符
        escaped = token.upper()
        escaped = escaped.replace("!", "!!").replace("%", "!%").replace("_", "!_").replace("'", "''")
        clauses.append(f"UPPER(TABLE_NAME) LIKE '%{escaped}%' ESCAPE '!'")
    return " OR ".join(clauses)


def parse_interval_cutoff_date(value: Optional[str]) -> Optional[datetime]:
    raw = (value or "").strip()
    if not raw:
        return None
    if not re.match(r"^\d{8}$", raw):
        return None
    try:
        return datetime.strptime(raw, "%Y%m%d")
    except ValueError:
        return None


def parse_interval_cutoff_numeric(value: Optional[str]) -> Optional[Decimal]:
    raw = (value or "").strip()
    if not raw:
        return None
    try:
        numeric = Decimal(raw)
    except (InvalidOperation, ValueError):
        return None
    if numeric <= 0:
        return None
    return numeric


class BlacklistRule(NamedTuple):
    rule_id: str
    black_type: str
    sql: str
    min_ob_version: Optional[str] = None
    max_ob_version: Optional[str] = None
    enabled: bool = True
    tag: str = ""


def extract_ob_version_number(raw: Optional[str]) -> Optional[str]:
    if not raw:
        return None
    match = re.search(r"\d+(?:\.\d+){1,3}", str(raw))
    return match.group(0) if match else None


def load_blacklist_rules(path_value: Optional[str]) -> List[BlacklistRule]:
    if not path_value or not str(path_value).strip():
        return []
    path = Path(str(path_value).strip()).expanduser()
    if not path.exists():
        log.warning("blacklist_rules_path 不存在: %s", path)
        return []
    try:
        payload = json.loads(path.read_text(encoding="utf-8"))
    except Exception as exc:
        log.warning("blacklist_rules_path 解析失败: %s (%s)", path, exc)
        return []

    raw_rules: List[dict] = []
    if isinstance(payload, list):
        raw_rules = payload
    elif isinstance(payload, dict):
        raw_rules = payload.get("rules") or []
    else:
        log.warning("blacklist_rules_path 内容格式不支持: %s", path)
        return []

    rules: List[BlacklistRule] = []
    for idx, raw_rule in enumerate(raw_rules, 1):
        if not isinstance(raw_rule, dict):
            log.warning("黑名单规则格式异常(第%s条)，已跳过。", idx)
            continue
        rule_id = str(raw_rule.get("id") or "").strip()
        sql = str(raw_rule.get("sql") or "").strip()
        if not rule_id or not sql:
            log.warning("黑名单规则缺少 id/sql(第%s条)，已跳过。", idx)
            continue
        black_type = str(raw_rule.get("black_type") or "").strip().upper() or "UNKNOWN"
        tag = str(raw_rule.get("tag") or "").strip()
        enabled_raw = raw_rule.get("enabled", None)
        if enabled_raw is None:
            enabled = parse_blacklist_rule_tag_enabled(tag, True)
        else:
            enabled = parse_bool_flag(str(enabled_raw), True)
        rules.append(
            BlacklistRule(
                rule_id=rule_id,
                black_type=black_type,
                sql=sql,
                min_ob_version=str(raw_rule.get("min_ob_version") or "").strip() or None,
                max_ob_version=str(raw_rule.get("max_ob_version") or "").strip() or None,
                enabled=enabled,
                tag=tag,
            )
        )
    return rules


def is_blacklist_rule_enabled(
    rule: BlacklistRule,
    enable_set: Set[str],
    disable_set: Set[str],
    ob_version: Optional[str]
) -> Tuple[bool, str]:
    rule_id_u = rule.rule_id.strip().upper()
    if not rule.enabled:
        return False, "rule_disabled"
    if enable_set and rule_id_u not in enable_set:
        return False, "not_in_enable_set"
    if disable_set and rule_id_u in disable_set:
        return False, "in_disable_set"
    if ob_version:
        if rule.min_ob_version and compare_version(ob_version, rule.min_ob_version) < 0:
            return False, "below_min_version"
        if rule.max_ob_version and compare_version(ob_version, rule.max_ob_version) > 0:
            return False, "above_max_version"
    return True, ""


def collect_fixup_config_diagnostics(
    settings: Dict,
    enabled_primary_types: Set[str],
    enabled_extra_types: Set[str]
) -> List[str]:
    """
    汇总 check_* 与 fixup_* 的配置冲突或无效组合提示。
    仅做诊断提示，不影响执行。
    """
    diagnostics: List[str] = []
    fixup_types: Set[str] = set(settings.get("fixup_type_set", set()) or set())
    if not fixup_types:
        return diagnostics

    primary_types = set(PRIMARY_OBJECT_TYPES)
    extra_types = set(EXTRA_OBJECT_CHECK_TYPES)
    print_only_types = set(settings.get("effective_print_only_primary_types") or PRINT_ONLY_PRIMARY_TYPES)

    for obj_type in sorted(fixup_types):
        if obj_type in print_only_types:
            diagnostics.append(f"fixup_types 包含 {obj_type}，但该类型为仅打印对象，不会生成修补脚本。")
            continue
        if obj_type in primary_types and obj_type not in enabled_primary_types:
            diagnostics.append(f"fixup_types 包含 {obj_type}，但 check_primary_types 未启用该类型。")
        if obj_type in extra_types and obj_type not in enabled_extra_types:
            diagnostics.append(f"fixup_types 包含 {obj_type}，但 check_extra_types 未启用该类型。")

    if fixup_types & {"INDEX", "CONSTRAINT", "TRIGGER"} and "TABLE" not in enabled_primary_types:
        diagnostics.append("fixup_types 包含 INDEX/CONSTRAINT/TRIGGER，但 check_primary_types 未启用 TABLE，相关修补将无法生成。")

    trigger_list_path = settings.get("trigger_list", "").strip()
    if trigger_list_path and "TRIGGER" not in enabled_extra_types:
        diagnostics.append("trigger_list 已配置，但 check_extra_types 未启用 TRIGGER，仅进行清单格式校验。")

    interval_enabled = bool(
        settings.get(
            "effective_interval_fixup_enabled",
            settings.get("generate_interval_partition_fixup", False)
        )
    )
    if interval_enabled:
        if "TABLE" not in enabled_primary_types:
            diagnostics.append("已启用 interval 分区补齐，但 check_primary_types 未启用 TABLE，相关脚本将无法生成。")
        if not settings.get("interval_partition_cutoff_date") and not settings.get("interval_partition_cutoff_numeric_value"):
            diagnostics.append("interval_partition_cutoff 非法，interval 分区补齐将被跳过。")
        cutoff_numeric_raw = (settings.get("interval_partition_cutoff_numeric") or "").strip()
        if cutoff_numeric_raw and not settings.get("interval_partition_cutoff_numeric_value"):
            diagnostics.append("interval_partition_cutoff_numeric 非法，数值 interval 分区补齐将被跳过。")

    if settings.get("generate_interval_partition_fixup_mode") == "auto" and not settings.get("ob_version_known"):
        diagnostics.append(
            "generate_interval_partition_fixup=auto 但 OB 版本不可识别，已按旧行为回退为启用。"
        )
    if settings.get("mview_check_fixup_mode") == "auto" and not settings.get("ob_version_known"):
        diagnostics.append(
            "mview_check_fixup_mode=auto 但 OB 版本不可识别，已按旧行为回退为仅打印。"
        )

    status_check_types: Set[str] = set(settings.get("check_status_drift_type_set", set()) or set())
    if "TRIGGER" in status_check_types and "TRIGGER" not in enabled_extra_types:
        diagnostics.append("check_status_drift_types 包含 TRIGGER，但 check_extra_types 未启用 TRIGGER，状态漂移检查将被跳过。")
    if "CONSTRAINT" in status_check_types and "CONSTRAINT" not in enabled_extra_types:
        diagnostics.append("check_status_drift_types 包含 CONSTRAINT，但 check_extra_types 未启用 CONSTRAINT，状态漂移检查将被跳过。")

    if settings.get("generate_status_fixup"):
        if not parse_bool_flag(settings.get("generate_fixup", "true"), True):
            diagnostics.append("generate_status_fixup=true 但 generate_fixup=false，状态修复脚本不会生成。")
        status_fixup_types: Set[str] = set(settings.get("status_fixup_type_set", set()) or set())
        if not status_fixup_types:
            diagnostics.append("generate_status_fixup=true 但 status_fixup_types 为空，状态修复脚本不会生成。")
        for obj_type in sorted(status_fixup_types):
            if obj_type not in status_check_types:
                diagnostics.append(
                    f"status_fixup_types 包含 {obj_type}，但 check_status_drift_types 未启用该类型，状态修复脚本不会生成。"
                )

    if normalize_constraint_missing_fixup_validate_mode(
        settings.get("constraint_missing_fixup_validate_mode", "safe_novalidate")
    ) == "force_validate":
        diagnostics.append(
            "constraint_missing_fixup_validate_mode=force_validate，缺失约束修补遇到目标端脏数据时可能触发 ORA-02298。"
        )

    return diagnostics


SYNONYM_CHECK_SCOPE_VALUES = {"all", "public_only"}
SYNONYM_CHECK_SCOPE_ALIASES = {
    "public": "public_only",
    "public-only": "public_only",
    "publiconly": "public_only",
    "all-synonyms": "all",
    "all_synonyms": "all",
}


def normalize_synonym_check_scope(raw_value: Optional[str]) -> str:
    if not raw_value or not str(raw_value).strip():
        return "public_only"
    value = str(raw_value).strip().lower()
    value = SYNONYM_CHECK_SCOPE_ALIASES.get(value, value)
    if value not in SYNONYM_CHECK_SCOPE_VALUES:
        log.warning("synonym_check_scope=%s 不在支持范围内，将回退为 public_only。", raw_value)
        return "public_only"
    return value


SYNONYM_FIXUP_SCOPE_VALUES = {"all", "public_only"}
SYNONYM_FIXUP_SCOPE_ALIASES = dict(SYNONYM_CHECK_SCOPE_ALIASES)


def normalize_synonym_fixup_scope(raw_value: Optional[str]) -> str:
    if not raw_value or not str(raw_value).strip():
        return "public_only"
    value = str(raw_value).strip().lower()
    value = SYNONYM_FIXUP_SCOPE_ALIASES.get(value, value)
    if value not in SYNONYM_FIXUP_SCOPE_VALUES:
        log.warning("synonym_fixup_scope=%s 不在支持范围内，将回退为 public_only。", raw_value)
        return "public_only"
    return value


SEQUENCE_REMAP_POLICY_VALUES = {"infer", "source_only", "dominant_table"}
SEQUENCE_REMAP_POLICY_ALIASES = {
    "source": "source_only",
    "source-only": "source_only",
    "sourceonly": "source_only",
    "dominant": "dominant_table",
    "dominant-table": "dominant_table",
    "dominanttable": "dominant_table",
    "table": "dominant_table",
}


def normalize_sequence_remap_policy(raw_value: Optional[str]) -> str:
    if not raw_value or not str(raw_value).strip():
        return "source_only"
    value = str(raw_value).strip().lower()
    value = SEQUENCE_REMAP_POLICY_ALIASES.get(value, value)
    if value not in SEQUENCE_REMAP_POLICY_VALUES:
        log.warning("sequence_remap_policy=%s 不在支持范围内，将回退为 source_only。", raw_value)
        return "source_only"
    return value


REPORT_DIR_LAYOUT_VALUES = {"flat", "per_run"}
REPORT_DIR_LAYOUT_ALIASES = {
    "per-run": "per_run",
    "perrun": "per_run",
    "run": "per_run",
    "single": "flat",
}


def normalize_report_dir_layout(raw_value: Optional[str]) -> str:
    if not raw_value or not str(raw_value).strip():
        return "per_run"
    value = str(raw_value).strip().lower()
    value = REPORT_DIR_LAYOUT_ALIASES.get(value, value)
    if value not in REPORT_DIR_LAYOUT_VALUES:
        log.warning("report_dir_layout=%s 不在支持范围内，将回退为 per_run。", raw_value)
        return "per_run"
    return value


REPORT_DETAIL_MODE_VALUES = {"full", "split", "summary"}
REPORT_DETAIL_MODE_ALIASES = {
    "detail": "full",
    "details": "full",
    "all": "full",
    "simple": "summary",
    "lite": "summary",
}

REPORT_DB_DETAIL_MODE_VALUES = {"missing", "mismatched", "unsupported", "ok", "skipped"}
REPORT_DB_DETAIL_MODE_ALIASES = {
    "miss": "missing",
    "mismatch": "mismatched",
    "unsupport": "unsupported",
    "unsupported_only": "unsupported",
    "all": "all",
}
DEFAULT_REPORT_DB_DETAIL_MODES = {"missing", "mismatched", "unsupported"}
REPORT_DB_STORE_SCOPE_VALUES = {"summary", "core", "full"}
REPORT_DB_STORE_SCOPE_ALIASES = {
    "sum": "summary",
    "summary_only": "summary",
    "basic": "summary",
    "core_only": "core",
    "all": "full",
    "full_only": "full",
}


def normalize_report_detail_mode(raw_value: Optional[str]) -> str:
    if not raw_value or not str(raw_value).strip():
        return "split"
    value = str(raw_value).strip().lower()
    value = REPORT_DETAIL_MODE_ALIASES.get(value, value)
    if value not in REPORT_DETAIL_MODE_VALUES:
        log.warning("report_detail_mode=%s 不在支持范围内，将回退为 split。", raw_value)
        return "split"
    return value


def parse_report_db_detail_mode(raw_value: Optional[str]) -> Set[str]:
    if not raw_value or not str(raw_value).strip():
        return set(DEFAULT_REPORT_DB_DETAIL_MODES)
    tokens = [token.strip().lower() for token in str(raw_value).split(",") if token.strip()]
    if not tokens:
        return set(DEFAULT_REPORT_DB_DETAIL_MODES)
    normalized: Set[str] = set()
    unknown: List[str] = []
    for token in tokens:
        mapped = REPORT_DB_DETAIL_MODE_ALIASES.get(token, token)
        if mapped == "all":
            return set(REPORT_DB_DETAIL_MODE_VALUES)
        if mapped in REPORT_DB_DETAIL_MODE_VALUES:
            normalized.add(mapped)
        else:
            unknown.append(token)
    if unknown:
        log.warning("report_db_detail_mode 包含未知项: %s，将忽略。", ",".join(sorted(set(unknown))))
    if not normalized:
        log.warning(
            "report_db_detail_mode 为空或无有效值，将回退为 %s。",
            ",".join(sorted(DEFAULT_REPORT_DB_DETAIL_MODES))
        )
        return set(DEFAULT_REPORT_DB_DETAIL_MODES)
    return normalized


def normalize_report_db_store_scope(raw_value: Optional[str]) -> str:
    if not raw_value or not str(raw_value).strip():
        return "full"
    value = str(raw_value).strip().lower()
    value = REPORT_DB_STORE_SCOPE_ALIASES.get(value, value)
    if value not in REPORT_DB_STORE_SCOPE_VALUES:
        log.warning("report_db_store_scope=%s 不在支持范围内，将回退为 full。", raw_value)
        return "full"
    return value


FIXUP_IDEMPOTENT_MODE_VALUES = {"off", "guard", "replace", "drop_create"}
FIXUP_IDEMPOTENT_MODE_ALIASES = {
    "none": "off",
    "false": "off",
    "0": "off",
    "skip": "guard",
    "replace_only": "replace",
    "drop-create": "drop_create",
    "dropcreate": "drop_create",
}


def normalize_fixup_idempotent_mode(raw_value: Optional[str]) -> str:
    if not raw_value or not str(raw_value).strip():
        return "replace"
    value = str(raw_value).strip().lower()
    value = FIXUP_IDEMPOTENT_MODE_ALIASES.get(value, value)
    if value not in FIXUP_IDEMPOTENT_MODE_VALUES:
        log.warning("fixup_idempotent_mode=%s 不在支持范围内，将回退为 replace。", raw_value)
        return "replace"
    return value


COLUMN_VISIBILITY_POLICY_VALUES = {"auto", "enforce", "ignore"}
COLUMN_VISIBILITY_POLICY_ALIASES = {
    "on": "enforce",
    "true": "enforce",
    "off": "ignore",
    "false": "ignore",
}


def normalize_column_visibility_policy(raw_value: Optional[str]) -> str:
    if not raw_value or not str(raw_value).strip():
        return "auto"
    value = str(raw_value).strip().lower()
    value = COLUMN_VISIBILITY_POLICY_ALIASES.get(value, value)
    if value not in COLUMN_VISIBILITY_POLICY_VALUES:
        log.warning("column_visibility_policy=%s 不在支持范围内，将回退为 auto。", raw_value)
        return "auto"
    return value


def normalize_view_dblink_policy(raw_value: Optional[str]) -> str:
    if not raw_value or not str(raw_value).strip():
        return "block"
    value = str(raw_value).strip().lower()
    if value not in VIEW_DBLINK_POLICIES:
        log.warning("view_dblink_policy=%s 不在支持范围内，将回退为 block。", raw_value)
        return "block"
    return value


def normalize_view_constraint_cleanup(raw_value: Optional[str]) -> str:
    if not raw_value or not str(raw_value).strip():
        return "auto"
    value = str(raw_value).strip().lower()
    value = VIEW_CONSTRAINT_CLEANUP_ALIASES.get(value, value)
    if value not in VIEW_CONSTRAINT_CLEANUP_VALUES:
        log.warning("view_constraint_cleanup=%s 不在支持范围内，将回退为 auto。", raw_value)
        return "auto"
    return value


def normalize_constraint_status_sync_mode(raw_value: Optional[str]) -> str:
    if not raw_value or not str(raw_value).strip():
        return "enabled_only"
    value = str(raw_value).strip().lower()
    value = CONSTRAINT_STATUS_SYNC_MODE_ALIASES.get(value, value)
    if value not in CONSTRAINT_STATUS_SYNC_MODE_VALUES:
        log.warning(
            "constraint_status_sync_mode=%s 不在支持范围内，将回退为 enabled_only。",
            raw_value
        )
        return "enabled_only"
    return value


def normalize_constraint_missing_fixup_validate_mode(raw_value: Optional[str]) -> str:
    if not raw_value or not str(raw_value).strip():
        return "safe_novalidate"
    value = str(raw_value).strip().lower()
    value = CONSTRAINT_MISSING_FIXUP_VALIDATE_MODE_ALIASES.get(value, value)
    if value not in CONSTRAINT_MISSING_FIXUP_VALIDATE_MODE_VALUES:
        log.warning(
            "constraint_missing_fixup_validate_mode=%s 不在支持范围内，将回退为 safe_novalidate。",
            raw_value
        )
        return "safe_novalidate"
    return value


def normalize_trigger_validity_sync_mode(raw_value: Optional[str]) -> str:
    if not raw_value or not str(raw_value).strip():
        return "compile"
    value = str(raw_value).strip().lower()
    value = TRIGGER_VALIDITY_SYNC_MODE_ALIASES.get(value, value)
    if value not in TRIGGER_VALIDITY_SYNC_MODE_VALUES:
        log.warning(
            "trigger_validity_sync_mode=%s 不在支持范围内，将回退为 compile。",
            raw_value
        )
        return "compile"
    return value


def load_view_compat_rules(path_value: Optional[str]) -> Dict[str, object]:
    rules = {
        "unsupported_views": set(VIEW_UNSUPPORTED_DEFAULT_VIEWS),
        "privilege_views": set(VIEW_PRIVILEGE_DEFAULT_VIEWS),
        "unsupported_patterns": list(VIEW_UNSUPPORTED_PATTERNS),
        "compiled_patterns": [re.compile(pat, flags=re.IGNORECASE) for pat in VIEW_UNSUPPORTED_PATTERNS]
    }
    if not path_value or not str(path_value).strip():
        return rules
    path = Path(str(path_value).strip()).expanduser()
    if not path.exists():
        log.warning("view_compat_rules_path 不存在: %s", path)
        return rules
    try:
        payload = json.loads(path.read_text(encoding="utf-8"))
    except (OSError, json.JSONDecodeError) as exc:
        log.warning("view_compat_rules_path 解析失败: %s (%s)", path, exc)
        return rules

    def _merge_set(key: str, src: object) -> None:
        if isinstance(src, list):
            for item in src:
                if item:
                    rules[key].add(str(item).strip().upper())

    if isinstance(payload, dict):
        _merge_set("unsupported_views", payload.get("unsupported_views"))
        _merge_set("privilege_views", payload.get("privilege_views"))
        patterns = payload.get("unsupported_patterns")
        if isinstance(patterns, list):
            merged = []
            for item in patterns:
                if item:
                    merged.append(str(item).strip())
            if merged:
                rules["unsupported_patterns"] = merged
                rules["compiled_patterns"] = [re.compile(p, flags=re.IGNORECASE) for p in merged]
    else:
        log.warning("view_compat_rules_path 内容格式不支持: %s", path)
    return rules


def normalize_hint_policy(raw_value: Optional[str]) -> str:
    if not raw_value or not str(raw_value).strip():
        return DDL_HINT_POLICY_DEFAULT
    policy = str(raw_value).strip().lower()
    if policy not in DDL_HINT_POLICY_VALUES:
        log.warning(
            "ddl_hint_policy=%s 不在支持范围内，将回退为 %s。",
            raw_value,
            DDL_HINT_POLICY_DEFAULT
        )
        return DDL_HINT_POLICY_DEFAULT
    return policy


def load_hint_allowlist_file(path_value: Optional[str]) -> Set[str]:
    if not path_value or not str(path_value).strip():
        return set()
    path = Path(str(path_value).strip()).expanduser()
    try:
        raw_lines = path.read_text(encoding="utf-8").splitlines()
    except OSError as exc:
        log.warning("ddl_hint_allowlist_file 读取失败: %s (%s)", path, exc)
        return set()
    hints: Set[str] = set()
    for line in raw_lines:
        stripped = line.strip()
        if not stripped or stripped.startswith("#"):
            continue
        if "#" in stripped:
            stripped = stripped.split("#", 1)[0].strip()
        if not stripped:
            continue
        hints.add(stripped.upper())
    return hints


def chunk_list(items: List[str], size: int) -> List[List[str]]:
    """Split list into batches of given size."""
    return [items[i:i + size] for i in range(0, len(items), size)]


def build_bind_placeholders(count: int, offset: int = 0) -> str:
    if count <= 0:
        return ""
    return ",".join(f":{i+1+offset}" for i in range(count))


def iter_in_chunks(items: List[str], batch_size: int = ORACLE_IN_BATCH_SIZE) -> List[Tuple[str, List[str]]]:
    chunks: List[Tuple[str, List[str]]] = []
    for chunk in chunk_list(items, batch_size):
        if not chunk:
            continue
        chunks.append((build_bind_placeholders(len(chunk)), chunk))
    return chunks


# --- 扩展检查结果结构 ---
class IndexMismatch(NamedTuple):
    table: str
    missing_indexes: Set[str]
    extra_indexes: Set[str]
    detail_mismatch: List[str]


class IndexUnsupportedDetail(NamedTuple):
    table_full: str
    index_name: str
    columns: str
    reason_code: str
    reason: str
    ob_error_hint: str


class ConstraintMismatch(NamedTuple):
    table: str
    missing_constraints: Set[str]
    extra_constraints: Set[str]
    detail_mismatch: List[str]
    downgraded_pk_constraints: Set[str]


class SequenceMismatch(NamedTuple):
    src_schema: str
    tgt_schema: str
    missing_sequences: Set[str]
    extra_sequences: Set[str]
    note: Optional[str] = None
    missing_mappings: Optional[List[Tuple[str, str]]] = None
    detail_mismatch: Optional[List[str]] = None


class TriggerMismatch(NamedTuple):
    table: str
    missing_triggers: Set[str]
    extra_triggers: Set[str]
    detail_mismatch: List[str]
    missing_mappings: Optional[List[Tuple[str, str]]] = None


class TriggerListReportRow(NamedTuple):
    entry: str
    status: str
    detail: str


class TriggerStatusReportRow(NamedTuple):
    trigger_full: str
    src_event: str
    tgt_event: str
    src_enabled: str
    tgt_enabled: str
    src_valid: str
    tgt_valid: str
    detail: str


class ConstraintStatusDriftRow(NamedTuple):
    table_full: str
    constraint_type: str
    src_constraint: str
    tgt_constraint: str
    src_status: str
    tgt_status: str
    src_validated: str
    tgt_validated: str
    detail: str
    action_sql: str


class ConstraintValidateDeferredRow(NamedTuple):
    schema_name: str
    table_name: str
    constraint_name: str
    constraint_type: str
    src_validated: str
    applied_mode: str
    reason: str
    validate_sql: str


class ReportIndexEntry(NamedTuple):
    category: str
    path: str
    rows: str
    description: str


class ConstraintUnsupportedDetail(NamedTuple):
    table_full: str
    constraint_name: str
    search_condition: str
    reason_code: str
    reason: str
    ob_error_hint: str


class CommentMismatch(NamedTuple):
    table: str
    table_comment: Optional[Tuple[str, str]]  # (src, tgt) when different
    column_comment_diffs: List[Tuple[str, str, str]]  # (column, src_comment, tgt_comment)
    missing_columns: Set[str]
    extra_columns: Set[str]


class ColumnOrderMismatch(NamedTuple):
    table: str
    src_order: Tuple[str, ...]
    tgt_order: Tuple[str, ...]


ExtraCheckResults = Dict[str, List]


class NoiseSuppressedDetail(NamedTuple):
    category: str
    scope: str
    reason: str
    identifiers: str
    detail: str


class NoiseSuppressionResult(NamedTuple):
    tv_results: ReportResults
    extra_results: ExtraCheckResults
    comment_results: Dict[str, object]
    suppressed_details: List[NoiseSuppressedDetail]


TableEntry = Tuple[str, str, str, str]  # (SRC_SCHEMA, SRC_TABLE, TGT_SCHEMA, TGT_TABLE)


class ConstraintSignature(NamedTuple):
    pk: Set[Tuple[str, ...]]
    uk: Set[Tuple[str, ...]]
    fk: Set[Tuple[Tuple[str, ...], Optional[str], str, str]]
    ck: Set[str]


class IndexCompareCache(NamedTuple):
    src_map: Dict[Tuple[str, ...], Dict[str, Set[str]]]
    tgt_map: Dict[Tuple[str, ...], Dict[str, Set[str]]]
    src_sig: Dict[Tuple[str, ...], Set[str]]
    tgt_sig: Dict[Tuple[str, ...], Set[str]]
    constraint_index_cols: Set[Tuple[str, ...]]


class ConstraintCompareCache(NamedTuple):
    src_cons: Dict[str, Dict]
    tgt_cons: Dict[str, Dict]
    src_norm_cols: Dict[str, Tuple[str, ...]]
    tgt_norm_cols: Dict[str, Tuple[str, ...]]
    src_sig: ConstraintSignature
    tgt_sig: ConstraintSignature
    source_all_cols: Set[Tuple[str, ...]]
    partition_key_set: Set[str]


class TriggerCompareCache(NamedTuple):
    src_info_map: Dict[str, Dict[str, str]]
    tgt_info_map: Dict[str, Dict]
    src_sig: Set[Tuple[str, str, str, str]]
    tgt_sig: Set[Tuple[str, str, str, str]]


class ExtraTableResult(NamedTuple):
    tgt_name: str
    index_ok: Optional[bool]
    index_mismatch: Optional[IndexMismatch]
    constraint_ok: Optional[bool]
    constraint_mismatch: Optional[ConstraintMismatch]
    trigger_ok: Optional[bool]
    trigger_mismatch: Optional[TriggerMismatch]
    index_time: float
    constraint_time: float
    trigger_time: float


def normalize_column_sequence(columns: Optional[List[str]]) -> Tuple[str, ...]:
    if not columns:
        return tuple()
    seen: Set[str] = set()
    normalized: List[str] = []
    for col in columns:
        col_u = (col or '').upper()
        if not col_u:
            continue
        if col_u in seen:
            continue
        seen.add(col_u)
        normalized.append(col_u)
    return tuple(normalized)


def is_column_order_candidate(
    col_name: Optional[str],
    col_meta: Optional[Dict],
    *,
    is_source: bool,
    hidden_src_cols: Optional[Set[str]] = None,
    invisible_src_cols: Optional[Set[str]] = None
) -> bool:
    if not col_name:
        return False
    if classify_noise_column(col_name):
        return False
    if is_source:
        if is_ignored_source_column(col_name, col_meta):
            return False
    else:
        if is_ignored_oms_column(col_name, col_meta):
            return False
        if hidden_src_cols and col_name in hidden_src_cols:
            return False
        if invisible_src_cols and col_name in invisible_src_cols:
            return False
    if col_meta:
        if col_meta.get("hidden"):
            return False
        if col_meta.get("invisible") is True:
            return False
    return True


def build_column_order_sequence(
    col_meta: Dict[str, Dict],
    candidates: Set[str]
) -> Tuple[Optional[Tuple[str, ...]], Optional[str]]:
    if not candidates:
        return tuple(), None
    items: List[Tuple[int, str]] = []
    for name in candidates:
        meta = col_meta.get(name)
        if not meta:
            return None, "column_meta_missing"
        col_id = meta.get("column_id")
        if col_id is None:
            return None, "column_id_missing"
        if not isinstance(col_id, int):
            col_id = parse_column_id(col_id)
        if col_id is None:
            return None, "column_id_missing"
        items.append((col_id, name))
    ids = [entry[0] for entry in items]
    if len(ids) != len(set(ids)):
        return None, "column_id_duplicate"
    ordered = [name for _, name in sorted(items, key=lambda x: x[0])]
    return tuple(ordered), None


def normalize_index_columns(
    columns: List[str],
    expr_map: Optional[Dict[int, str]] = None
) -> Tuple[str, ...]:
    if not columns:
        return tuple()
    expr_map = expr_map or {}
    seen: Set[str] = set()
    normalized: List[str] = []
    for idx, col in enumerate(columns, start=1):
        expr = expr_map.get(idx)
        token = normalize_sql_expression_casefold(expr) if expr else (col or "").upper()
        if not token:
            continue
        if token in seen:
            continue
        seen.add(token)
        normalized.append(token)
    return tuple(normalized)


def index_has_desc(info: Optional[Dict]) -> bool:
    if not info:
        return False
    for flag in (info.get("descend") or []):
        if (flag or "").strip().upper() == "DESC":
            return True
    return False


def normalize_trigger_status(value: Optional[str]) -> str:
    status = (value or "").strip().upper()
    return status or "UNKNOWN"


def normalize_trigger_identity(
    trigger_key: Optional[str],
    info: Optional[Dict],
    default_owner: Optional[str]
) -> Tuple[str, str]:
    """
    Normalize trigger identity from either:
      1) key = TRIGGER_NAME + info.owner
      2) key = OWNER.TRIGGER_NAME (new canonical form)
    Returns (OWNER, TRIGGER_NAME).
    """
    info_dict = info if isinstance(info, dict) else {}
    key_u = (trigger_key or "").strip().upper()
    owner_u = ((info_dict.get("owner")) or default_owner or "").strip().upper()
    name_u = ((info_dict.get("name")) or "").strip().upper()
    if "." in key_u:
        key_owner, key_name = key_u.split(".", 1)
        if not owner_u:
            owner_u = key_owner
        if not name_u:
            name_u = key_name
    else:
        if not name_u:
            name_u = key_u
    if not owner_u and "." in key_u:
        owner_u = key_u.split(".", 1)[0]
    if not name_u and "." in key_u:
        name_u = key_u.split(".", 1)[1]
    return owner_u, name_u


def normalize_constraint_enabled_status(value: Optional[str]) -> str:
    status = (value or "").strip().upper()
    if status in {"ENABLED", "ENABLE"}:
        return "ENABLED"
    if status in {"DISABLED", "DISABLE"}:
        return "DISABLED"
    return "UNKNOWN"


def normalize_constraint_validated_status(value: Optional[str]) -> str:
    status = re.sub(r"\s+", " ", (value or "").strip().upper())
    if status in {"VALIDATED", "VALIDATE"}:
        return "VALIDATED"
    if status in {"NOT VALIDATED", "NOVALIDATE", "NOT VALIDATE"}:
        return "NOT VALIDATED"
    return "UNKNOWN"


def lookup_trigger_validity(
    meta: Union[OracleMetadata, ObMetadata],
    owner: str,
    name: str
) -> str:
    owner_u = (owner or "").upper()
    name_u = (name or "").upper()
    if not owner_u or not name_u:
        return "UNKNOWN"
    return normalize_trigger_status(meta.object_statuses.get((owner_u, name_u, "TRIGGER")))


def normalize_comment_text(text: Optional[str]) -> str:
    """
    统一注释文本：去除首尾空白、折叠多余空白，降低换行/制表差异的噪声。
    """
    if text is None:
        return ""
    # 去除控制字符，统一换行，并折叠空白
    sanitized = re.sub(r"[\x00-\x1f\x7f]", " ", str(text))
    collapsed = " ".join(sanitized.replace("\r\n", "\n").replace("\r", "\n").split())
    normalized = collapsed.strip()
    if normalized.upper() in {"NULL", "<NULL>", "NONE"}:
        return ""
    return normalized


def shorten_comment_preview(text: str, limit: int = 120) -> str:
    """
    将注释压缩为单行便于展示，控制长度。
    """
    if not text:
        return "<空>"
    single_line = text.replace("\n", "\\n")
    return single_line if len(single_line) <= limit else single_line[:limit - 3] + "..."

GRANT_PRIVILEGE_BY_TYPE: Dict[str, str] = {
    'TABLE': 'SELECT',
    'VIEW': 'SELECT',
    'MATERIALIZED VIEW': 'SELECT',
    'SYNONYM': 'SELECT',
    'SEQUENCE': 'SELECT',
    'INDEX': 'SELECT',
    'TYPE': 'EXECUTE',
    'TYPE BODY': 'EXECUTE',
    'PROCEDURE': 'EXECUTE',
    'FUNCTION': 'EXECUTE',
    'PACKAGE': 'EXECUTE',
    'PACKAGE BODY': 'EXECUTE',
    'JOB': 'EXECUTE',
    'SCHEDULE': 'EXECUTE'
}

# OceanBase Oracle 模式下常用对象权限（缺省白名单，可在配置中覆盖）
DEFAULT_SUPPORTED_OBJECT_PRIVS: Set[str] = {
    'SELECT',
    'INSERT',
    'UPDATE',
    'DELETE',
    'REFERENCES',
    'EXECUTE'
}

ROLE_AUTH_WARN_TYPES: Set[str] = {
    'PASSWORD',
    'EXTERNAL',
    'GLOBAL'
}

# 系统权限对对象权限的兜底映射（用于过滤已满足的 GRANT 建议）
SYS_PRIV_IMPLICATIONS: Dict[str, Set[str]] = {
    # SELECT ANY TABLE/SEQUENCE/DICTIONARY 可覆盖跨 schema SELECT
    'SELECT': {
        'SELECT ANY TABLE',
        'SELECT ANY SEQUENCE',
        'SELECT ANY DICTIONARY',
    },
    # EXECUTE ANY PROCEDURE/TYPE 可覆盖跨 schema EXECUTE
    'EXECUTE': {
        'EXECUTE ANY PROCEDURE',
        'EXECUTE ANY TYPE',
    },
    # REFERENCES ANY TABLE 可覆盖跨 schema REFERENCES
    'REFERENCES': {
        'REFERENCES ANY TABLE',
    },
}

DDL_OBJECT_TYPE_OVERRIDE: Dict[str, Tuple[str, bool]] = {
    'PROCEDURE': ('PROCEDURE', True),
    'FUNCTION': ('FUNCTION', True),
    'PACKAGE': ('PACKAGE', True),
    'PACKAGE BODY': ('PACKAGE BODY', True),
    'TRIGGER': ('TRIGGER', True)
}
DBCAT_OPTION_MAP: Dict[str, str] = {
    'TABLE': '--table',
    'VIEW': '--view',
    'MATERIALIZED VIEW': '--mview',
    'PROCEDURE': '--procedure',
    'FUNCTION': '--function',
    'PACKAGE': '--package',
    'PACKAGE BODY': '--package-body',
    'SYNONYM': '--synonym',
    'SEQUENCE': '--sequence',
    'TRIGGER': '--trigger',
    'TYPE': '--type',
    'TYPE BODY': '--type-body',
    'MVIEW LOG': '--mview-log',
    'TABLEGROUP': '--tablegroup'
}

DBCAT_OUTPUT_DIR_HINTS: Dict[str, Tuple[str, ...]] = {
    'TABLE': ('TABLE',),
    'VIEW': ('VIEW',),
    'MATERIALIZED VIEW': ('MVIEW', 'MATERIALIZED VIEW'),
    'PROCEDURE': ('PROCEDURE',),
    'FUNCTION': ('FUNCTION',),
    'PACKAGE': ('PACKAGE',),
    'PACKAGE BODY': ('PACKAGE BODY', 'PACKAGE_BODY'),
    'SYNONYM': ('SYNONYM',),
    'SEQUENCE': ('SEQUENCE',),
    'TRIGGER': ('TRIGGER',),
    'TYPE': ('TYPE',),
    'TYPE BODY': ('TYPE BODY', 'TYPE_BODY'),
    'MVIEW LOG': ('MVIEW LOG', 'MVIEW_LOG'),
    'TABLEGROUP': ('TABLEGROUP',)
}
DBCAT_DIR_TO_TYPE: Dict[str, str] = {
    hint.upper(): obj_type
    for obj_type, hints in DBCAT_OUTPUT_DIR_HINTS.items()
    for hint in hints
}

DDL_HINT_POLICY_DROP_ALL = "drop_all"
DDL_HINT_POLICY_KEEP_SUPPORTED = "keep_supported"
DDL_HINT_POLICY_KEEP_ALL = "keep_all"
DDL_HINT_POLICY_REPORT_ONLY = "report_only"
DDL_HINT_POLICY_VALUES = {
    DDL_HINT_POLICY_DROP_ALL,
    DDL_HINT_POLICY_KEEP_SUPPORTED,
    DDL_HINT_POLICY_KEEP_ALL,
    DDL_HINT_POLICY_REPORT_ONLY
}
DDL_HINT_POLICY_DEFAULT = DDL_HINT_POLICY_KEEP_SUPPORTED

DDL_FORMATTER_SQLCL = "sqlcl"
DDL_FORMATTER_NONE = "none"
DDL_FORMATTER_VALUES = {DDL_FORMATTER_SQLCL, DDL_FORMATTER_NONE}

DDL_FORMAT_FAIL_FALLBACK = "fallback"
DDL_FORMAT_FAIL_ERROR = "error"
DDL_FORMAT_FAIL_POLICIES = {DDL_FORMAT_FAIL_FALLBACK, DDL_FORMAT_FAIL_ERROR}

DDL_FORMAT_TYPE_ALIASES = {
    "MATERIALIZED_VIEW": "MATERIALIZED VIEW",
    "MVIEW": "MATERIALIZED VIEW",
    "PACKAGE_BODY": "PACKAGE BODY",
    "PACKAGEBODY": "PACKAGE BODY",
    "TYPE_BODY": "TYPE BODY",
    "TYPEBODY": "TYPE BODY",
    "TABLE_ALTER": "TABLE_ALTER",
    "TABLE-ALTER": "TABLE_ALTER",
    "TABLEALTER": "TABLE_ALTER",
}

DDL_FORMAT_TYPES: Set[str] = {
    "TABLE",
    "VIEW",
    "MATERIALIZED VIEW",
    "INDEX",
    "SEQUENCE",
    "SYNONYM",
    "PROCEDURE",
    "FUNCTION",
    "PACKAGE",
    "PACKAGE BODY",
    "TRIGGER",
    "TYPE",
    "TYPE BODY",
    "CONSTRAINT",
    "TABLE_ALTER",
    "JOB",
    "SCHEDULE",
}

DDL_FORMAT_PLSQL_TYPES = {
    "PROCEDURE",
    "FUNCTION",
    "PACKAGE",
    "PACKAGE BODY",
    "TRIGGER",
    "TYPE",
    "TYPE BODY",
}

OB_ORACLE_HINT_ALLOWLIST: Set[str] = {
    "AGGR_FIRST_UNNEST",
    "APPEND",
    "BEGIN_OUTLINE_DATA",
    "COALESCE_SQ",
    "COUNT_TO_EXISTS",
    "CURSOR_SHARING_EXACT",
    "DECORRELATE",
    "DIRECT",
    "DISABLE_PARALLEL_DAS_DML",
    "DISABLE_PARALLEL_DML",
    "DISTINCT_PUSHDOWN",
    "DYNAMIC_SAMPLING",
    "ELIMINATE_JOIN",
    "ENABLE_PARALLEL_DAS_DML",
    "ENABLE_PARALLEL_DML",
    "END_OUTLINE_DATA",
    "FAST_MINMAX",
    "FULL",
    "GATHER_OPTIMIZER_STATISTICS",
    "GBY_PUSHDOWN",
    "INDEX",
    "INDEX_SS",
    "INLINE",
    "JOIN_FIRST_UNNEST",
    "LEADING",
    "LEFT_TO_ANTI",
    "LOAD_BATCH_SIZE",
    "LOG_LEVEL",
    "MATERIALIZE",
    "MAX_CONCURRENT",
    "MERGE",
    "MONITOR",
    "MV_REWRITE",
    "NO_AGGR_FIRST_UNNEST",
    "NO_COALESCE_SQ",
    "NO_COST_BASED_QUERY_TRANSFORMATION",
    "NO_COUNT_TO_EXISTS",
    "NO_DECORRELATE",
    "NO_DIRECT",
    "NO_DISTINCT_PUSHDOWN",
    "NO_ELIMINATE_JOIN",
    "NO_EXPAND",
    "NO_FAST_MINMAX",
    "NO_GATHER_OPTIMIZER_STATISTICS",
    "NO_GBY_PUSHDOWN",
    "NO_INDEX",
    "NO_JOIN_FIRST_UNNEST",
    "NO_LEFT_TO_ANTI",
    "NO_MERGE",
    "NO_MV_REWRITE",
    "NO_OUTER_TO_INNER",
    "NO_PARALLEL",
    "NO_PLACE_GROUP_BY",
    "NO_PRED_DEDUCE",
    "NO_PROJECT_PRUNE",
    "NO_PULLUP_EXPR",
    "NO_PUSH_LIMIT",
    "NO_PX_JOIN_FILTER",
    "NO_PX_PART_JOIN_FILTER",
    "NO_QUERY_TRANSFORMATION",
    "NO_REPLACE_CONST",
    "NO_REWRITE",
    "NO_SEMI_TO_INNER",
    "NO_SIMPLIFY_DISTINCT",
    "NO_SIMPLIFY_EXPR",
    "NO_SIMPLIFY_GROUP_BY",
    "NO_SIMPLIFY_LIMIT",
    "NO_SIMPLIFY_ORDER_BY",
    "NO_SIMPLIFY_SET",
    "NO_SIMPLIFY_SUBQUERY",
    "NO_SIMPLIFY_WINFUNC",
    "NO_UNNEST",
    "NO_USE_COLUMN_TABLE",
    "NO_USE_DAS",
    "NO_USE_DISTRIBUTED_DML",
    "NO_USE_HASH",
    "NO_USE_HASH_AGGREGATION",
    "NO_USE_HASH_DISTINCT",
    "NO_USE_HASH_SET",
    "NO_USE_MERGE",
    "NO_USE_NL",
    "NO_USE_NL_MATERIALIZATION",
    "NO_WIN_MAGIC",
    "OPTIMIZER_FEATURES_ENABLE",
    "OPT_PARAM",
    "ORDERED",
    "OUTER_TO_INNER",
    "PARALLEL",
    "PLACE_GROUP_BY",
    "PQ_DISTRIBUTE",
    "PQ_MAP",
    "PQ_SET",
    "PRED_DEDUCE",
    "PROJECT_PRUNE",
    "PULLUP_EXPR",
    "PUSH_LIMIT",
    "PX_JOIN_FILTER",
    "PX_PART_JOIN_FILTER",
    "QB_NAME",
    "QUERY_TIMEOUT",
    "READ_CONSISTENCY",
    "REPLACE_CONST",
    "RESOURCE_GROUP",
    "SEMI_TO_INNER",
    "SIMPLIFY_DISTINCT",
    "SIMPLIFY_EXPR",
    "SIMPLIFY_GROUP_BY",
    "SIMPLIFY_LIMIT",
    "SIMPLIFY_ORDER_BY",
    "SIMPLIFY_SET",
    "SIMPLIFY_SUBQUERY",
    "SIMPLIFY_WINFUNC",
    "STAT",
    "TRACING",
    "TRANS_PARAM",
    "UNNEST",
    "USE_COLUMN_TABLE",
    "USE_CONCAT",
    "USE_DAS",
    "USE_DISTRIBUTED_DML",
    "USE_HASH",
    "USE_HASH_AGGREGATION",
    "USE_HASH_DISTINCT",
    "USE_HASH_SET",
    "USE_MERGE",
    "USE_NL",
    "USE_NL_MATERIALIZATION",
    "USE_PLAN_CACHE",
    "USE_PX",
    "WIN_MAGIC",
}

# ====================== 通用配置和基础函数 ======================

def load_config(config_file: str) -> Tuple[OraConfig, ObConfig, Dict]:
    """读取 config.ini 配置文件"""
    log.info(f"正在加载配置文件: {config_file}")
    config = configparser.ConfigParser(interpolation=None)
    if not config.read(config_file):
        log.error(f"严重错误: 配置文件 {config_file} 未找到或无法读取。")
        abort_run()

    try:
        ora_cfg = dict(config['ORACLE_SOURCE'])
        ob_cfg = dict(config['OCEANBASE_TARGET'])
        settings = dict(config['SETTINGS'])

        schemas_raw = settings.get('source_schemas', '')
        schemas_list = [s.strip().upper() for s in schemas_raw.split(',') if s.strip()]
        if not schemas_list:
            log.error(f"严重错误: [SETTINGS] 中的 'source_schemas' 未配置或为空。")
            abort_run()
        settings['source_schemas_list'] = schemas_list

        # remap 规则文件（可为空，表示按 1:1 映射）
        settings.setdefault('remap_file', '')

        # fixup 脚本目录
        settings.setdefault('fixup_dir', 'fixup_scripts')
        settings.setdefault('fixup_dir_allow_outside_repo', 'true')
        settings.setdefault('fixup_max_sql_file_mb', '50')
        settings.setdefault('fixup_force_clean', 'true')
        settings.setdefault('fixup_drop_sys_c_columns', 'true')
        settings.setdefault('generate_extra_cleanup', 'false')
        # obclient 超时时间 (秒)
        settings.setdefault('obclient_timeout', '60')
        # 报告输出目录
        settings.setdefault('report_dir', 'main_reports')
        settings.setdefault('report_dir_layout', 'per_run')
        settings.setdefault('report_detail_mode', 'split')
        settings.setdefault('report_to_db', 'true')
        settings.setdefault('report_db_schema', '')
        settings.setdefault('report_retention_days', '90')
        settings.setdefault('report_db_fail_abort', 'false')
        settings.setdefault('report_db_store_scope', 'full')
        settings.setdefault('report_db_detail_mode', 'missing,mismatched,unsupported')
        settings.setdefault('report_db_detail_max_rows', '0')
        settings.setdefault('report_db_detail_item_enable', '')
        settings.setdefault('report_db_detail_item_max_rows', '0')
        settings.setdefault('report_db_insert_batch', '200')
        settings.setdefault('report_db_save_full_json', 'false')
        # Oracle Instant Client 目录 (Thick Mode)
        settings.setdefault('oracle_client_lib_dir', '')
        # dbcat 相关配置
        settings.setdefault('dbcat_bin', '')
        settings.setdefault('dbcat_from', '')
        settings.setdefault('dbcat_to', '')
        settings.setdefault('dbcat_output_dir', 'dbcat_output')
        settings.setdefault('dbcat_no_cal_dep', 'false')
        settings.setdefault('dbcat_query_meta_thread', '')
        settings.setdefault('dbcat_progress_interval', '15')
        settings.setdefault('java_home', os.environ.get('JAVA_HOME', ''))
        # fixup 定向生成选项
        settings.setdefault('fixup_schemas', '')
        settings.setdefault('fixup_types', '')
        settings.setdefault('fixup_idempotent_mode', 'replace')
        settings.setdefault('fixup_idempotent_types', '')
        settings.setdefault('fixup_auto_grant', 'true')
        settings.setdefault(
            'fixup_auto_grant_types',
            ",".join(FIXUP_AUTO_GRANT_DEFAULT_TYPES_ORDERED)
        )
        settings.setdefault('fixup_auto_grant_fallback', 'true')
        settings.setdefault('fixup_auto_grant_cache_limit', '10000')
        settings.setdefault('synonym_check_scope', 'public_only')
        settings.setdefault('synonym_fixup_scope', 'public_only')
        settings.setdefault('trigger_list', '')
        settings.setdefault('trigger_qualify_schema', 'true')
        settings.setdefault('check_status_drift_types', 'trigger,constraint')
        settings.setdefault('generate_status_fixup', 'true')
        settings.setdefault('status_fixup_types', 'trigger,constraint')
        settings.setdefault('constraint_status_sync_mode', 'enabled_only')
        settings.setdefault('constraint_missing_fixup_validate_mode', 'safe_novalidate')
        settings.setdefault('trigger_validity_sync_mode', 'compile')
        settings.setdefault('sequence_remap_policy', 'source_only')
        settings.setdefault('generate_grants', 'true')
        settings.setdefault('grant_tab_privs_scope', 'owner')
        settings.setdefault('grant_merge_privileges', 'true')
        settings.setdefault('grant_merge_grantees', 'true')
        settings.setdefault('grant_supported_sys_privs', '')
        settings.setdefault('grant_supported_object_privs', '')
        settings.setdefault('grant_include_oracle_maintained_roles', 'false')
        # 检查范围开关
        settings.setdefault('check_primary_types', '')
        settings.setdefault('check_extra_types', '')
        settings.setdefault('check_dependencies', 'true')
        settings.setdefault('print_dependency_chains', 'true')
        settings.setdefault('check_comments', 'true')
        settings.setdefault('check_column_order', 'false')
        settings.setdefault('check_object_usability', 'false')
        settings.setdefault('check_source_usability', 'true')
        settings.setdefault('usability_check_timeout', '10')
        settings.setdefault('usability_check_workers', '10')
        settings.setdefault('max_usability_objects', '')
        settings.setdefault('usability_sample_ratio', '')
        settings.setdefault('generate_interval_partition_fixup', 'auto')
        settings.setdefault('mview_check_fixup_mode', 'auto')
        settings.setdefault('interval_partition_cutoff', '20280301')
        settings.setdefault('interval_partition_cutoff_numeric', '')
        settings.setdefault('blacklist_mode', 'auto')
        settings.setdefault('blacklist_rules_path', '')
        settings.setdefault('blacklist_rules_enable', '')
        settings.setdefault('blacklist_rules_disable', '')
        settings.setdefault('blacklist_name_patterns', '_RENAME')
        settings.setdefault('blacklist_name_patterns_file', '')
        settings.setdefault('blacklist_lob_max_mb', '512')
        settings.setdefault('infer_schema_mapping', 'true')
        settings.setdefault('ddl_punct_sanitize', 'true')
        settings.setdefault('ddl_hint_policy', DDL_HINT_POLICY_DEFAULT)
        settings.setdefault('ddl_hint_allowlist', '')
        settings.setdefault('ddl_hint_denylist', '')
        settings.setdefault('ddl_hint_allowlist_file', '')
        settings.setdefault('ddl_format_enable', 'false')
        settings.setdefault('ddl_format_types', '')
        settings.setdefault('ddl_formatter', DDL_FORMATTER_SQLCL)
        settings.setdefault('ddl_format_fail_policy', DDL_FORMAT_FAIL_FALLBACK)
        settings.setdefault('ddl_format_batch_size', '200')
        settings.setdefault('ddl_format_timeout', '60')
        settings.setdefault('ddl_format_max_lines', '30000')
        settings.setdefault('ddl_format_max_bytes', '2000000')
        settings.setdefault('sqlcl_bin', '')
        settings.setdefault('sqlcl_profile_path', '')
        settings.setdefault('view_compat_rules_path', '')
        settings.setdefault('view_dblink_policy', 'block')
        settings.setdefault('view_constraint_cleanup', 'auto')
        settings.setdefault('column_visibility_policy', 'auto')
        settings.setdefault('dbcat_chunk_size', '150')
        settings.setdefault('fixup_workers', '')
        settings.setdefault('progress_log_interval', '10')
        settings.setdefault('extra_check_workers', '16')
        settings.setdefault('extra_check_chunk_size', '200')
        settings.setdefault('extra_check_progress_interval', '10')
        settings.setdefault('report_width', '160')  # 报告宽度，避免nohup时被截断为80
        # 运行日志目录与级别
        settings.setdefault('log_dir', 'logs')
        settings.setdefault('log_level', 'auto')

        enabled_primary_types = parse_type_list(
            settings.get('check_primary_types', ''),
            set(PRIMARY_OBJECT_TYPES),
            'check_primary_types'
        )
        enabled_extra_types = parse_type_list(
            settings.get('check_extra_types', ''),
            set(EXTRA_OBJECT_CHECK_TYPES),
            'check_extra_types'
        )
        settings['enabled_primary_types'] = enabled_primary_types
        settings['enabled_extra_types'] = enabled_extra_types
        settings['enable_dependencies_check'] = parse_bool_flag(
            settings.get('check_dependencies', 'true'),
            True
        )
        settings['enable_grant_generation'] = parse_bool_flag(
            settings.get('generate_grants', 'true'),
            True
        )
        settings['grant_tab_privs_scope'] = settings.get('grant_tab_privs_scope', 'owner').strip().lower()
        settings['grant_supported_sys_privs_set'] = parse_csv_set(settings.get('grant_supported_sys_privs', ''))
        settings['grant_supported_object_privs_set'] = parse_csv_set(
            settings.get('grant_supported_object_privs', '')
        )
        settings['grant_include_oracle_maintained_roles'] = parse_bool_flag(
            settings.get('grant_include_oracle_maintained_roles', 'false'),
            False
        )
        settings['enable_comment_check'] = parse_bool_flag(
            settings.get('check_comments', 'true'),
            True
        )
        settings['generate_extra_cleanup'] = parse_bool_flag(
            settings.get('generate_extra_cleanup', 'false'),
            False
        )
        settings['check_object_usability'] = parse_bool_flag(
            settings.get('check_object_usability', 'false'),
            False
        )
        settings['check_source_usability'] = parse_bool_flag(
            settings.get('check_source_usability', 'true'),
            True
        )
        try:
            settings['usability_check_timeout'] = int(settings.get('usability_check_timeout', '10'))
        except (TypeError, ValueError):
            settings['usability_check_timeout'] = 10
        if settings['usability_check_timeout'] <= 0:
            settings['usability_check_timeout'] = 10
        try:
            settings['usability_check_workers'] = int(settings.get('usability_check_workers', '10'))
        except (TypeError, ValueError):
            settings['usability_check_workers'] = 10
        if settings['usability_check_workers'] <= 0:
            settings['usability_check_workers'] = 10
        try:
            settings['max_usability_objects'] = int(settings.get('max_usability_objects', '0') or 0)
        except (TypeError, ValueError):
            settings['max_usability_objects'] = 0
        try:
            settings['usability_sample_ratio'] = float(settings.get('usability_sample_ratio', '0') or 0)
        except (TypeError, ValueError):
            settings['usability_sample_ratio'] = 0.0
        if settings['usability_sample_ratio'] < 0:
            settings['usability_sample_ratio'] = 0.0
        settings['fixup_drop_sys_c_columns'] = parse_bool_flag(
            settings.get('fixup_drop_sys_c_columns', 'true'),
            False
        )
        settings['enable_column_order_check'] = parse_bool_flag(
            settings.get('check_column_order', 'false'),
            False
        )
        settings['generate_interval_partition_fixup_mode'] = normalize_interval_partition_fixup_mode(
            settings.get('generate_interval_partition_fixup', 'auto')
        )
        settings['mview_check_fixup_mode'] = normalize_mview_check_fixup_mode(
            settings.get('mview_check_fixup_mode', 'auto')
        )
        # 运行态会基于 OB 版本重新计算该布尔值；这里保留预估值供前置逻辑使用。
        settings['generate_interval_partition_fixup'] = (
            settings['generate_interval_partition_fixup_mode'] != 'false'
        )
        cutoff_raw = (settings.get('interval_partition_cutoff', '20280301') or '').strip()
        settings['interval_partition_cutoff'] = cutoff_raw or '20280301'
        settings['interval_partition_cutoff_date'] = parse_interval_cutoff_date(
            settings.get('interval_partition_cutoff', '20280301')
        )
        cutoff_numeric_raw = (settings.get('interval_partition_cutoff_numeric', '') or '').strip()
        settings['interval_partition_cutoff_numeric'] = cutoff_numeric_raw
        settings['interval_partition_cutoff_numeric_value'] = parse_interval_cutoff_numeric(
            cutoff_numeric_raw
        )
        # interval 相关校验在主流程应用版本门控后再做，避免 auto 模式误报。
        settings['blacklist_mode'] = normalize_blacklist_mode(settings.get('blacklist_mode', 'auto'))
        settings['blacklist_rules_enable_set'] = parse_csv_set(settings.get('blacklist_rules_enable', ''))
        settings['blacklist_rules_disable_set'] = parse_csv_set(settings.get('blacklist_rules_disable', ''))
        rules_path = (settings.get('blacklist_rules_path') or "").strip()
        settings['blacklist_rules_path'] = rules_path or DEFAULT_BLACKLIST_RULES_PATH
        settings['blacklist_name_patterns_list'] = load_blacklist_name_patterns(
            settings.get('blacklist_name_patterns', ''),
            settings.get('blacklist_name_patterns_file', '')
        )
        settings['blacklist_name_pattern_clause'] = build_blacklist_name_pattern_clause(
            settings.get('blacklist_name_patterns_list', [])
        )
        try:
            settings['blacklist_lob_max_mb'] = int(settings.get('blacklist_lob_max_mb', '512'))
        except (TypeError, ValueError):
            settings['blacklist_lob_max_mb'] = 512
        if settings['blacklist_lob_max_mb'] <= 0:
            settings['blacklist_lob_max_mb'] = 512
        settings['enable_ddl_punct_sanitize'] = parse_bool_flag(
            settings.get('ddl_punct_sanitize', 'true'),
            True
        )
        settings['ddl_hint_policy'] = normalize_hint_policy(settings.get('ddl_hint_policy'))
        settings['ddl_hint_allowlist_set'] = parse_csv_set(settings.get('ddl_hint_allowlist', ''))
        settings['ddl_hint_denylist_set'] = parse_csv_set(settings.get('ddl_hint_denylist', ''))
        settings['ddl_hint_allowlist_file_set'] = load_hint_allowlist_file(
            settings.get('ddl_hint_allowlist_file', '')
        )
        settings['ddl_format_enable'] = parse_bool_flag(
            settings.get('ddl_format_enable', 'false'),
            False
        )
        settings['ddl_formatter'] = normalize_ddl_formatter(
            settings.get('ddl_formatter', DDL_FORMATTER_SQLCL)
        )
        settings['ddl_format_fail_policy'] = normalize_ddl_format_fail_policy(
            settings.get('ddl_format_fail_policy', DDL_FORMAT_FAIL_FALLBACK)
        )
        try:
            settings['ddl_format_batch_size'] = int(settings.get('ddl_format_batch_size', '200'))
        except (TypeError, ValueError):
            settings['ddl_format_batch_size'] = 200
        if settings['ddl_format_batch_size'] <= 0:
            settings['ddl_format_batch_size'] = 200
        try:
            settings['ddl_format_timeout'] = int(settings.get('ddl_format_timeout', '60'))
        except (TypeError, ValueError):
            settings['ddl_format_timeout'] = 60
        if settings['ddl_format_timeout'] < 0:
            settings['ddl_format_timeout'] = 60
        try:
            settings['ddl_format_max_lines'] = int(settings.get('ddl_format_max_lines', '30000'))
        except (TypeError, ValueError):
            settings['ddl_format_max_lines'] = 30000
        if settings['ddl_format_max_lines'] < 0:
            settings['ddl_format_max_lines'] = 0
        try:
            settings['ddl_format_max_bytes'] = int(settings.get('ddl_format_max_bytes', '2000000'))
        except (TypeError, ValueError):
            settings['ddl_format_max_bytes'] = 2000000
        if settings['ddl_format_max_bytes'] < 0:
            settings['ddl_format_max_bytes'] = 0
        if settings['ddl_format_enable']:
            settings['ddl_format_type_set'] = parse_ddl_format_types(
                settings.get('ddl_format_types', ''),
                default_types={'VIEW'}
            )
        else:
            settings['ddl_format_type_set'] = set()
        settings['synonym_check_scope'] = normalize_synonym_check_scope(
            settings.get('synonym_check_scope', 'public_only')
        )
        settings['synonym_fixup_scope'] = normalize_synonym_fixup_scope(
            settings.get('synonym_fixup_scope', 'public_only')
        )
        settings['report_dir_layout'] = normalize_report_dir_layout(
            settings.get('report_dir_layout', 'per_run')
        )
        settings['report_detail_mode'] = normalize_report_detail_mode(
            settings.get('report_detail_mode', 'split')
        )
        settings['report_to_db'] = parse_bool_flag(
            settings.get('report_to_db', 'true'),
            True
        )
        settings['report_db_schema'] = (settings.get('report_db_schema', '') or '').strip().upper()
        try:
            settings['report_retention_days'] = int(settings.get('report_retention_days', '90'))
        except (TypeError, ValueError):
            settings['report_retention_days'] = 90
        if settings['report_retention_days'] < 0:
            settings['report_retention_days'] = 0
        settings['report_db_fail_abort'] = parse_bool_flag(
            settings.get('report_db_fail_abort', 'false'),
            False
        )
        settings['report_db_store_scope'] = normalize_report_db_store_scope(
            settings.get('report_db_store_scope', 'full')
        )
        settings['report_db_detail_mode_set'] = parse_report_db_detail_mode(
            settings.get('report_db_detail_mode', 'missing,mismatched,unsupported')
        )
        try:
            settings['report_db_detail_max_rows'] = int(settings.get('report_db_detail_max_rows', '0'))
        except (TypeError, ValueError):
            settings['report_db_detail_max_rows'] = 0
        if settings['report_db_detail_max_rows'] < 0:
            settings['report_db_detail_max_rows'] = 0
        raw_detail_item_enable = settings.get('report_db_detail_item_enable', '')
        if raw_detail_item_enable is None or not str(raw_detail_item_enable).strip():
            settings['report_db_detail_item_enable'] = (settings['report_db_store_scope'] == 'full')
        else:
            settings['report_db_detail_item_enable'] = parse_bool_flag(raw_detail_item_enable, False)
        try:
            settings['report_db_detail_item_max_rows'] = int(
                settings.get('report_db_detail_item_max_rows', settings['report_db_detail_max_rows'])
            )
        except (TypeError, ValueError):
            settings['report_db_detail_item_max_rows'] = settings['report_db_detail_max_rows']
        if settings['report_db_detail_item_max_rows'] < 0:
            settings['report_db_detail_item_max_rows'] = 0
        try:
            settings['report_db_insert_batch'] = int(settings.get('report_db_insert_batch', '200'))
        except (TypeError, ValueError):
            settings['report_db_insert_batch'] = 200
        if settings['report_db_insert_batch'] <= 0:
            settings['report_db_insert_batch'] = 200
        settings['report_db_save_full_json'] = parse_bool_flag(
            settings.get('report_db_save_full_json', 'false'),
            False
        )
        settings['view_dblink_policy'] = normalize_view_dblink_policy(
            settings.get('view_dblink_policy', 'block')
        )
        settings['view_constraint_cleanup'] = normalize_view_constraint_cleanup(
            settings.get('view_constraint_cleanup', 'auto')
        )
        settings['view_compat_rules'] = load_view_compat_rules(
            settings.get('view_compat_rules_path', '')
        )
        settings['sequence_remap_policy'] = normalize_sequence_remap_policy(
            settings.get('sequence_remap_policy', 'source_only')
        )
        settings['fixup_idempotent_mode'] = normalize_fixup_idempotent_mode(
            settings.get('fixup_idempotent_mode', 'replace')
        )
        idempotent_allowed = set(ALL_TRACKED_OBJECT_TYPES) | set(EXTRA_OBJECT_CHECK_TYPES)
        idempotent_types = parse_type_list(
            settings.get('fixup_idempotent_types', ''),
            idempotent_allowed,
            'fixup_idempotent_types',
            default_all=False
        )
        if not idempotent_types:
            idempotent_types = set(FIXUP_IDEMPOTENT_DEFAULT_TYPES)
        settings['fixup_idempotent_types_set'] = idempotent_types
        settings['fixup_auto_grant'] = parse_bool_flag(
            settings.get('fixup_auto_grant', 'true'),
            True
        )
        settings['fixup_auto_grant_types_set'] = parse_fixup_auto_grant_types(
            settings.get('fixup_auto_grant_types', '')
        )
        settings['fixup_auto_grant_fallback'] = parse_bool_flag(
            settings.get('fixup_auto_grant_fallback', 'true'),
            True
        )
        settings['fixup_dir_allow_outside_repo'] = parse_bool_flag(
            settings.get('fixup_dir_allow_outside_repo', 'true'),
            True
        )
        try:
            settings['fixup_max_sql_file_mb'] = str(
                int(settings.get('fixup_max_sql_file_mb', '50'))
            )
        except Exception:
            settings['fixup_max_sql_file_mb'] = '50'
        try:
            settings['fixup_auto_grant_cache_limit'] = str(
                int(settings.get('fixup_auto_grant_cache_limit', '10000'))
            )
        except Exception:
            settings['fixup_auto_grant_cache_limit'] = '10000'
        settings['column_visibility_policy'] = normalize_column_visibility_policy(
            settings.get('column_visibility_policy', 'auto')
        )
        settings['trigger_qualify_schema'] = parse_bool_flag(
            settings.get('trigger_qualify_schema', 'true'),
            True
        )
        settings['check_status_drift_type_set'] = parse_type_list(
            settings.get('check_status_drift_types', ''),
            set(STATUS_DRIFT_CHECK_TYPES),
            'check_status_drift_types'
        )
        settings['generate_status_fixup'] = parse_bool_flag(
            settings.get('generate_status_fixup', 'true'),
            True
        )
        settings['status_fixup_type_set'] = parse_type_list(
            settings.get('status_fixup_types', ''),
            set(STATUS_DRIFT_CHECK_TYPES),
            'status_fixup_types'
        )
        settings['constraint_status_sync_mode'] = normalize_constraint_status_sync_mode(
            settings.get('constraint_status_sync_mode', 'enabled_only')
        )
        settings['constraint_missing_fixup_validate_mode'] = normalize_constraint_missing_fixup_validate_mode(
            settings.get('constraint_missing_fixup_validate_mode', 'safe_novalidate')
        )
        settings['trigger_validity_sync_mode'] = normalize_trigger_validity_sync_mode(
            settings.get('trigger_validity_sync_mode', 'compile')
        )
        settings['print_dependency_chains'] = parse_bool_flag(
            settings.get('print_dependency_chains', 'true'),
            True
        )
        settings['enable_schema_mapping_infer'] = parse_bool_flag(
            settings.get('infer_schema_mapping', 'true'),
            True
        )
        settings['fixup_schema_list'] = [
            s.strip().upper() for s in settings.get('fixup_schemas', '').split(',') if s.strip()
        ]
        # 注意：fixup 类型默认值需要包含 CONSTRAINT，否则约束订正 SQL 不会生成
        settings['fixup_type_set'] = parse_type_list(
            settings.get('fixup_types', ''),
            set(ALL_TRACKED_OBJECT_TYPES) | set(EXTRA_OBJECT_CHECK_TYPES),
            'fixup_types'
        )
        cpu_default = max(1, os.cpu_count() or 1)
        try:
            settings['fixup_workers'] = int(settings.get('fixup_workers') or min(12, cpu_default))
            if settings['fixup_workers'] <= 0:
                settings['fixup_workers'] = min(12, cpu_default)
        except (TypeError, ValueError):
            settings['fixup_workers'] = min(12, cpu_default)
        try:
            settings['extra_check_workers'] = int(
                settings.get('extra_check_workers') or min(16, cpu_default)
            )
            if settings['extra_check_workers'] <= 0:
                settings['extra_check_workers'] = min(16, cpu_default)
        except (TypeError, ValueError):
            settings['extra_check_workers'] = min(16, cpu_default)
        settings['extra_check_workers'] = max(1, min(int(settings['extra_check_workers']), cpu_default))
        try:
            settings['dbcat_chunk_size'] = int(settings.get('dbcat_chunk_size', '150'))
        except ValueError:
            settings['dbcat_chunk_size'] = 150
        try:
            settings['extra_check_chunk_size'] = int(settings.get('extra_check_chunk_size', '200'))
        except (TypeError, ValueError):
            settings['extra_check_chunk_size'] = 200
        if settings['extra_check_chunk_size'] < 1:
            settings['extra_check_chunk_size'] = 200
        try:
            settings['extra_check_progress_interval'] = float(
                settings.get('extra_check_progress_interval', '10')
            )
        except (TypeError, ValueError):
            settings['extra_check_progress_interval'] = 10.0
        if settings['extra_check_progress_interval'] < 1:
            settings['extra_check_progress_interval'] = 1.0
        settings['dbcat_no_cal_dep'] = parse_bool_flag(settings.get('dbcat_no_cal_dep', 'false'), False)
        try:
            settings['dbcat_query_meta_thread'] = int(settings.get('dbcat_query_meta_thread') or 0)
        except (TypeError, ValueError):
            settings['dbcat_query_meta_thread'] = 0
        if settings['dbcat_query_meta_thread'] < 0:
            settings['dbcat_query_meta_thread'] = 0
        try:
            settings['dbcat_progress_interval'] = int(settings.get('dbcat_progress_interval', '15'))
        except (TypeError, ValueError):
            settings['dbcat_progress_interval'] = 15
        if settings['dbcat_progress_interval'] < 0:
            settings['dbcat_progress_interval'] = 0

        global OBC_TIMEOUT
        try:
            OBC_TIMEOUT = int(settings['obclient_timeout'])
        except ValueError:
            OBC_TIMEOUT = 60

        log.info(f"成功加载配置，将扫描 {len(schemas_list)} 个源 schema。")
        log.info(f"obclient 超时时间: {OBC_TIMEOUT} 秒")
        log.warning(
            "注意：程序将从 DBA_* 视图读取 Oracle/OceanBase 元数据，请确保运行账号具备 DBA/SELECT ANY DICTIONARY/SELECT_CATALOG_ROLE 等等价权限，否则结果将不完整。"
        )
        return ora_cfg, ob_cfg, settings
    except KeyError as e:
        log.error(f"严重错误: 配置文件中缺少必要的部分: {e}")
        abort_run()


def validate_runtime_paths(settings: Dict, ob_cfg: ObConfig) -> None:
    """在正式连接前，对关键路径和依赖做友好校验与提示。"""
    errors: List[str] = []
    warnings: List[str] = []

    # obclient 可执行文件
    obclient_path = Path(ob_cfg.get('executable', '')).expanduser()
    if not obclient_path.exists():
        errors.append(
            f"未找到 obclient 可执行文件: {obclient_path}。请在 config.ini 的 [OCEANBASE_TARGET] 中配置 executable 绝对路径。"
        )
    elif not os.access(obclient_path, os.X_OK):
        warnings.append(f"obclient 路径存在但不可执行: {obclient_path}，请检查权限。")

    # remap 文件
    remap_file = settings.get('remap_file', '').strip()
    if remap_file and not Path(remap_file).expanduser().exists():
        warnings.append(f"Remap 文件不存在: {remap_file}（将按 1:1 继续，可确认路径是否正确）。")

    trigger_list_path = settings.get('trigger_list', '').strip()
    if trigger_list_path and not Path(trigger_list_path).expanduser().exists():
        warnings.append(
            f"trigger_list 文件不存在: {trigger_list_path}（将记录在报告中并回退全量触发器生成）。"
        )

    blacklist_mode = settings.get('blacklist_mode', 'auto')
    if blacklist_mode in ("auto", "rules_only"):
        rules_path = settings.get('blacklist_rules_path', '').strip()
        if rules_path and not Path(rules_path).expanduser().exists():
            warnings.append(
                f"blacklist_rules_path 不存在: {rules_path}（将跳过黑名单规则）。"
            )
        patterns_file = settings.get('blacklist_name_patterns_file', '').strip()
        if patterns_file and not Path(patterns_file).expanduser().exists():
            warnings.append(
                f"blacklist_name_patterns_file 不存在: {patterns_file}（将仅使用 blacklist_name_patterns 内联关键字）。"
            )

    # dbcat / JAVA_HOME 仅在生成 fixup 时提示
    generate_fixup_enabled = settings.get('generate_fixup', 'true').strip().lower() in ('true', '1', 'yes')
    if generate_fixup_enabled:
        dbcat_bin = settings.get('dbcat_bin', '').strip()
        if not dbcat_bin:
            warnings.append("generate_fixup 已开启，但未配置 dbcat_bin；如需生成订正 SQL，请在 [SETTINGS] 中填写 dbcat 目录或 bin/dbcat 路径。")
        else:
            dbcat_path = Path(dbcat_bin).expanduser()
            if not dbcat_path.exists():
                errors.append(f"dbcat 路径不存在: {dbcat_path}。请确认路径或关闭 generate_fixup。")
            else:
                candidate = dbcat_path / "bin" / "dbcat" if dbcat_path.is_dir() else dbcat_path
                if not candidate.exists():
                    warnings.append(f"未在 {dbcat_path} 下找到 dbcat 可执行文件（期望 bin/dbcat 或可执行文件）；生成脚本可能失败。")

        java_home = settings.get('java_home', '').strip() or os.environ.get('JAVA_HOME', '')
        if not java_home:
            warnings.append("generate_fixup 已开启，但 JAVA_HOME 未配置；dbcat 运行可能失败。")
        elif not Path(java_home).expanduser().exists():
            warnings.append(f"JAVA_HOME 指向的目录不存在: {java_home}，请确认 JDK 路径。")

    ddl_format_enabled = settings.get('ddl_format_enable', False) and settings.get('ddl_formatter') == DDL_FORMATTER_SQLCL
    if ddl_format_enabled:
        sqlcl_raw = settings.get('sqlcl_bin', '').strip()
        sqlcl_exec = resolve_sqlcl_executable(sqlcl_raw)
        if not sqlcl_raw:
            errors.append("ddl_format_enable 已开启但未配置 sqlcl_bin；请提供 SQLcl 根目录或 bin/sql 可执行文件路径。")
        elif not sqlcl_exec or not sqlcl_exec.exists():
            errors.append(f"SQLcl 路径不可用: {sqlcl_raw}（未解析到 bin/sql）。")
        elif not os.access(sqlcl_exec, os.X_OK):
            warnings.append(f"SQLcl 路径存在但不可执行: {sqlcl_exec}，请检查权限。")
        profile_path = settings.get('sqlcl_profile_path', '').strip()
        if profile_path and not Path(profile_path).expanduser().exists():
            warnings.append(f"sqlcl_profile_path 不存在: {profile_path}（将忽略格式化 profile）。")
        if not generate_fixup_enabled:
            warnings.append("ddl_format_enable 已开启但 generate_fixup=false，本次不会产生可格式化的 DDL。")

    # 提示输出目录
    for key in ('fixup_dir', 'report_dir', 'dbcat_output_dir'):
        val = settings.get(key, '').strip()
        if not val:
            continue
        p = Path(val).expanduser()
        if not p.exists():
            warnings.append(f"目录 {p} 不存在，将在运行时尝试创建。请确保有写权限。")

    if warnings:
        for msg in warnings:
            log.warning(msg)
    if errors:
        for msg in errors:
            log.error(msg)
        log.error("关键路径缺失或不可用，已终止。请按提示修复后重试。")
        abort_run()


def run_config_wizard(config_path: Path) -> None:
    """
    交互式向导：在缺失或无效配置时提示用户输入并回写 config.ini。
    若标准输入不可用则直接退出，以免阻塞自动化流水线。
    """
    if not sys.stdin.isatty():
        log.error("交互式向导需要可用的标准输入/终端。请在可交互环境运行或直接编辑 config.ini。")
        abort_run()

    cfg = configparser.ConfigParser(interpolation=None)
    if config_path.exists():
        cfg.read(config_path, encoding="utf-8")
        log.info("已加载现有配置，将检查缺失/无效项后写回: %s", config_path)
    else:
        log.warning("未找到配置文件，将创建: %s", config_path)

    for section in ("ORACLE_SOURCE", "OCEANBASE_TARGET", "SETTINGS"):
        if not cfg.has_section(section):
            cfg[section] = {}

    def _prompt_field(
        section: str,
        key: str,
        message: str,
        *,
        default: Optional[str] = None,
        required: bool = False,
        validator: Optional[Callable[[str], Tuple[bool, str]]] = None,
        transform: Optional[Callable[[str], str]] = None,
    ) -> str:
        current = cfg.get(section, key, fallback="").strip()
        display_default = current or (default or "")
        while True:
            user_input = input(f"{message} [{display_default}]: ").strip()
            value = user_input or display_default
            if required and not value:
                print("该项必填，请输入。")
                continue
            if validator:
                ok, reason = validator(value)
                if not ok:
                    print(f"无效输入: {reason}")
                    continue
            if transform:
                value = transform(value)
            cfg[section][key] = value
            return value

    def _validate_path_exists(p: str) -> Tuple[bool, str]:
        path = Path(p).expanduser()
        return (path.exists(), "路径不存在") if p else (False, "为空")

    def _validate_exec_path(p: str) -> Tuple[bool, str]:
        path = Path(p).expanduser()
        if not path.exists():
            return False, "路径不存在"
        if not os.access(path, os.X_OK):
            return False, "文件不可执行"
        return True, ""

    def _validate_sqlcl_path(p: str) -> Tuple[bool, str]:
        exec_path = resolve_sqlcl_executable(p.strip())
        if not exec_path or not exec_path.exists():
            return False, "未解析到 bin/sql"
        if not os.access(exec_path, os.X_OK):
            return False, "SQLcl 不可执行"
        return True, ""

    def _validate_positive_int(val: str) -> Tuple[bool, str]:
        try:
            return int(val) > 0, "需要正整数"
        except ValueError:
            return False, "需要正整数"

    def _validate_non_negative_int(val: str) -> Tuple[bool, str]:
        try:
            return int(val) >= 0, "需要非负整数"
        except ValueError:
            return False, "需要非负整数"

    def _validate_ratio_0_1(val: str) -> Tuple[bool, str]:
        if not val.strip():
            return True, ""
        try:
            ratio = float(val)
        except ValueError:
            return False, "需要 0~1 之间的小数"
        if 0 <= ratio <= 1:
            return True, ""
        return False, "需要 0~1 之间的小数"

    def _validate_grant_scope(val: str) -> Tuple[bool, str]:
        v = val.strip().lower()
        if v in ("owner", "owner_or_grantee"):
            return True, ""
        return False, "仅支持 owner 或 owner_or_grantee"

    def _validate_synonym_fixup_scope(val: str) -> Tuple[bool, str]:
        if not val.strip():
            return True, ""
        normalized = SYNONYM_FIXUP_SCOPE_ALIASES.get(val.strip().lower(), val.strip().lower())
        if normalized in SYNONYM_FIXUP_SCOPE_VALUES:
            return True, ""
        return False, "仅支持 all 或 public_only"

    def _validate_synonym_check_scope(val: str) -> Tuple[bool, str]:
        if not val.strip():
            return True, ""
        normalized = SYNONYM_CHECK_SCOPE_ALIASES.get(val.strip().lower(), val.strip().lower())
        if normalized in SYNONYM_CHECK_SCOPE_VALUES:
            return True, ""
        return False, "仅支持 all 或 public_only"

    def _validate_sequence_remap_policy(val: str) -> Tuple[bool, str]:
        if not val.strip():
            return True, ""
        normalized = SEQUENCE_REMAP_POLICY_ALIASES.get(val.strip().lower(), val.strip().lower())
        if normalized in SEQUENCE_REMAP_POLICY_VALUES:
            return True, ""
        return False, "仅支持 infer/source_only/dominant_table"

    def _validate_blacklist_mode(val: str) -> Tuple[bool, str]:
        if not val.strip():
            return True, ""
        if val.strip().lower() in BLACKLIST_MODES:
            return True, ""
        return False, "仅支持 auto/table_only/rules_only/disabled"

    def _validate_fixup_idempotent_mode(val: str) -> Tuple[bool, str]:
        if not val.strip():
            return True, ""
        normalized = FIXUP_IDEMPOTENT_MODE_ALIASES.get(val.strip().lower(), val.strip().lower())
        if normalized in FIXUP_IDEMPOTENT_MODE_VALUES:
            return True, ""
        return False, "仅支持 off/guard/replace/drop_create"

    def _validate_interval_fixup_mode(val: str) -> Tuple[bool, str]:
        if not val.strip():
            return True, ""
        normalized = INTERVAL_PARTITION_FIXUP_MODE_ALIASES.get(val.strip().lower(), val.strip().lower())
        if normalized in INTERVAL_PARTITION_FIXUP_MODE_VALUES:
            return True, ""
        return False, "仅支持 auto/true/false"

    def _validate_mview_mode(val: str) -> Tuple[bool, str]:
        if not val.strip():
            return True, ""
        normalized = MVIEW_CHECK_FIXUP_MODE_ALIASES.get(val.strip().lower(), val.strip().lower())
        if normalized in MVIEW_CHECK_FIXUP_MODE_VALUES:
            return True, ""
        return False, "仅支持 auto/on/off"

    def _validate_column_visibility_policy(val: str) -> Tuple[bool, str]:
        if not val.strip():
            return True, ""
        normalized = COLUMN_VISIBILITY_POLICY_ALIASES.get(val.strip().lower(), val.strip().lower())
        if normalized in COLUMN_VISIBILITY_POLICY_VALUES:
            return True, ""
        return False, "仅支持 auto/enforce/ignore"

    def _validate_report_dir_layout(val: str) -> Tuple[bool, str]:
        if not val.strip():
            return True, ""
        normalized = REPORT_DIR_LAYOUT_ALIASES.get(val.strip().lower(), val.strip().lower())
        if normalized in REPORT_DIR_LAYOUT_VALUES:
            return True, ""
        return False, "仅支持 flat/per_run"

    def _validate_report_detail_mode(val: str) -> Tuple[bool, str]:
        if not val.strip():
            return True, ""
        normalized = REPORT_DETAIL_MODE_ALIASES.get(val.strip().lower(), val.strip().lower())
        if normalized in REPORT_DETAIL_MODE_VALUES:
            return True, ""
        return False, "仅支持 full/split/summary"

    def _validate_report_db_detail_mode(val: str) -> Tuple[bool, str]:
        if not val.strip():
            return True, ""
        tokens = [token.strip().lower() for token in val.split(",") if token.strip()]
        if not tokens:
            return True, ""
        for token in tokens:
            normalized = REPORT_DB_DETAIL_MODE_ALIASES.get(token, token)
            if normalized == "all":
                return True, ""
            if normalized not in REPORT_DB_DETAIL_MODE_VALUES:
                return False, "仅支持 missing/mismatched/unsupported/ok/skipped/all"
        return True, ""

    def _validate_report_db_store_scope(val: str) -> Tuple[bool, str]:
        if not val.strip():
            return True, ""
        normalized = REPORT_DB_STORE_SCOPE_ALIASES.get(val.strip().lower(), val.strip().lower())
        if normalized in REPORT_DB_STORE_SCOPE_VALUES:
            return True, ""
        return False, "仅支持 summary/core/full"

    def _validate_view_dblink_policy(val: str) -> Tuple[bool, str]:
        if not val.strip():
            return True, ""
        if val.strip().lower() in VIEW_DBLINK_POLICIES:
            return True, ""
        return False, "仅支持 block/allow"

    def _validate_view_constraint_cleanup(val: str) -> Tuple[bool, str]:
        if not val.strip():
            return True, ""
        normalized = VIEW_CONSTRAINT_CLEANUP_ALIASES.get(val.strip().lower(), val.strip().lower())
        if normalized in VIEW_CONSTRAINT_CLEANUP_VALUES:
            return True, ""
        return False, "仅支持 auto/force/off"

    def _validate_constraint_status_sync_mode(val: str) -> Tuple[bool, str]:
        if not val.strip():
            return True, ""
        normalized = CONSTRAINT_STATUS_SYNC_MODE_ALIASES.get(val.strip().lower(), val.strip().lower())
        if normalized in CONSTRAINT_STATUS_SYNC_MODE_VALUES:
            return True, ""
        return False, "仅支持 enabled_only/full"

    def _validate_constraint_missing_fixup_validate_mode(val: str) -> Tuple[bool, str]:
        if not val.strip():
            return True, ""
        normalized = CONSTRAINT_MISSING_FIXUP_VALIDATE_MODE_ALIASES.get(
            val.strip().lower(),
            val.strip().lower()
        )
        if normalized in CONSTRAINT_MISSING_FIXUP_VALIDATE_MODE_VALUES:
            return True, ""
        return False, "仅支持 safe_novalidate/source/force_validate"

    def _validate_trigger_validity_sync_mode(val: str) -> Tuple[bool, str]:
        if not val.strip():
            return True, ""
        normalized = TRIGGER_VALIDITY_SYNC_MODE_ALIASES.get(val.strip().lower(), val.strip().lower())
        if normalized in TRIGGER_VALIDITY_SYNC_MODE_VALUES:
            return True, ""
        return False, "仅支持 off/compile"

    def _validate_ddl_format_fail_policy(val: str) -> Tuple[bool, str]:
        if not val.strip():
            return True, ""
        if val.strip().lower() in DDL_FORMAT_FAIL_POLICIES:
            return True, ""
        return False, "仅支持 fallback/error"

    def _bool_transform(val: str) -> str:
        v = val.strip().lower()
        if v in ("true", "1", "yes", "y", "on"):
            return "true"
        if v in ("false", "0", "no", "n", "off"):
            return "false"
        return val or "true"

    print("\n=== 交互式配置向导 (空回车使用括号内默认值) ===")

    # ORACLE_SOURCE
    _prompt_field("ORACLE_SOURCE", "user", "Oracle 用户 (ORACLE_SOURCE.user)", required=True)
    _prompt_field("ORACLE_SOURCE", "password", "Oracle 密码 (ORACLE_SOURCE.password)", required=True)
    _prompt_field("ORACLE_SOURCE", "dsn", "Oracle DSN (host:port/service_name)", required=True)

    # OCEANBASE_TARGET
    _prompt_field(
        "OCEANBASE_TARGET",
        "executable",
        "obclient 可执行文件路径",
        validator=_validate_exec_path,
        required=True,
    )
    _prompt_field("OCEANBASE_TARGET", "host", "OceanBase 主机名/IP", required=True)
    _prompt_field(
        "OCEANBASE_TARGET",
        "port",
        "OceanBase 端口",
        default="2883",
        validator=_validate_positive_int,
        required=True,
    )
    _prompt_field("OCEANBASE_TARGET", "user_string", "-u 参数（含租户/库）", required=True)
    _prompt_field("OCEANBASE_TARGET", "password", "OceanBase 密码", required=True)

    # SETTINGS (关键路径与开关)
    _prompt_field(
        "SETTINGS",
        "oracle_client_lib_dir",
        "Oracle Instant Client 目录 (libclntsh.so 所在)",
        validator=_validate_path_exists,
        required=True,
    )
    _prompt_field(
        "SETTINGS",
        "source_schemas",
        "源 schema 列表 (逗号分隔)",
        required=True,
    )
    _prompt_field(
        "SETTINGS",
        "remap_file",
        "Remap 文件路径 (可选，默认 remap_rules.txt)",
        default="remap_rules.txt",
    )
    _prompt_field(
        "SETTINGS",
        "generate_fixup",
        "是否生成目标端订正 SQL (true/false)",
        default=cfg.get("SETTINGS", "generate_fixup", fallback="true"),
        transform=_bool_transform,
    )
    _prompt_field(
        "SETTINGS",
        "mview_check_fixup_mode",
        "MATERIALIZED VIEW 校验/修补模式 (auto/on/off)",
        default=cfg.get("SETTINGS", "mview_check_fixup_mode", fallback="auto"),
        validator=_validate_mview_mode,
        transform=normalize_mview_check_fixup_mode,
    )
    _prompt_field(
        "SETTINGS",
        "generate_interval_partition_fixup",
        "interval 分区补齐脚本模式 (auto/true/false)",
        default=cfg.get("SETTINGS", "generate_interval_partition_fixup", fallback="auto"),
        validator=_validate_interval_fixup_mode,
        transform=normalize_interval_partition_fixup_mode,
    )
    _prompt_field(
        "SETTINGS",
        "interval_partition_cutoff",
        "interval 分区补齐截止日期 (YYYYMMDD)",
        default=cfg.get("SETTINGS", "interval_partition_cutoff", fallback="20280301"),
    )
    _prompt_field(
        "SETTINGS",
        "interval_partition_cutoff_numeric",
        "数值 interval 分区补齐上限（可选，留空跳过）",
        default=cfg.get("SETTINGS", "interval_partition_cutoff_numeric", fallback=""),
    )
    _prompt_field(
        "SETTINGS",
        "generate_extra_cleanup",
        "是否生成目标端多余对象清理候选（仅注释，不自动执行）(true/false)",
        default=cfg.get("SETTINGS", "generate_extra_cleanup", fallback="false"),
        transform=_bool_transform,
    )
    _prompt_field(
        "SETTINGS",
        "fixup_drop_sys_c_columns",
        "是否对 SYS_C* 列生成 ALTER TABLE FORCE (true/false)",
        default=cfg.get("SETTINGS", "fixup_drop_sys_c_columns", fallback="true"),
        transform=_bool_transform,
    )
    _prompt_field(
        "SETTINGS",
        "generate_grants",
        "是否生成授权脚本并附加到修复 DDL (true/false)",
        default=cfg.get("SETTINGS", "generate_grants", fallback="true"),
        transform=_bool_transform,
    )
    _prompt_field(
        "SETTINGS",
        "fixup_auto_grant",
        "run_fixup 自动补权限 (true/false)",
        default=cfg.get("SETTINGS", "fixup_auto_grant", fallback="true"),
        transform=_bool_transform,
    )
    _prompt_field(
        "SETTINGS",
        "fixup_auto_grant_types",
        "自动补权限对象类型 (逗号分隔)",
        default=cfg.get(
            "SETTINGS",
            "fixup_auto_grant_types",
            fallback=",".join(FIXUP_AUTO_GRANT_DEFAULT_TYPES_ORDERED),
        ),
    )
    _prompt_field(
        "SETTINGS",
        "fixup_auto_grant_fallback",
        "自动补权限兜底生成 GRANT (true/false)",
        default=cfg.get("SETTINGS", "fixup_auto_grant_fallback", fallback="true"),
        transform=_bool_transform,
    )
    _prompt_field(
        "SETTINGS",
        "fixup_auto_grant_cache_limit",
        "自动补权限缓存大小（条目数，<=0 表示不限制）",
        default=cfg.get("SETTINGS", "fixup_auto_grant_cache_limit", fallback="10000"),
        validator=_validate_non_negative_int,
    )
    _prompt_field(
        "SETTINGS",
        "synonym_fixup_scope",
        "同义词修补范围 (all/public_only)",
        default=cfg.get("SETTINGS", "synonym_fixup_scope", fallback="public_only"),
        validator=_validate_synonym_fixup_scope,
        transform=normalize_synonym_fixup_scope,
    )
    _prompt_field(
        "SETTINGS",
        "sequence_remap_policy",
        "SEQUENCE 目标 schema 推导策略 (infer/source_only/dominant_table)",
        default=cfg.get("SETTINGS", "sequence_remap_policy", fallback="source_only"),
        validator=_validate_sequence_remap_policy,
        transform=normalize_sequence_remap_policy,
    )
    _prompt_field(
        "SETTINGS",
        "blacklist_mode",
        "黑名单来源模式 (auto/table_only/rules_only/disabled)",
        default=cfg.get("SETTINGS", "blacklist_mode", fallback="auto"),
        validator=_validate_blacklist_mode,
        transform=normalize_blacklist_mode,
    )
    _prompt_field(
        "SETTINGS",
        "blacklist_rules_path",
        "黑名单规则文件路径（JSON）",
        default=cfg.get("SETTINGS", "blacklist_rules_path", fallback="blacklist_rules.json"),
    )
    _prompt_field(
        "SETTINGS",
        "blacklist_rules_enable",
        "仅启用指定规则（逗号分隔，留空全量）",
        default=cfg.get("SETTINGS", "blacklist_rules_enable", fallback=""),
    )
    _prompt_field(
        "SETTINGS",
        "blacklist_rules_disable",
        "禁用指定规则（逗号分隔）",
        default=cfg.get("SETTINGS", "blacklist_rules_disable", fallback=""),
    )
    _prompt_field(
        "SETTINGS",
        "blacklist_name_patterns",
        "表名黑名单关键字（逗号分隔，默认 _RENAME）",
        default=cfg.get("SETTINGS", "blacklist_name_patterns", fallback="_RENAME"),
    )
    _prompt_field(
        "SETTINGS",
        "blacklist_name_patterns_file",
        "表名黑名单关键字文件（每行一条，可选）",
        default=cfg.get("SETTINGS", "blacklist_name_patterns_file", fallback=""),
    )
    _prompt_field(
        "SETTINGS",
        "blacklist_lob_max_mb",
        "LOB 超限阈值（MB）",
        default=cfg.get("SETTINGS", "blacklist_lob_max_mb", fallback="512"),
        validator=_validate_positive_int,
    )
    _prompt_field(
        "SETTINGS",
        "grant_tab_privs_scope",
        "授权抽取范围 (owner/owner_or_grantee)",
        default=cfg.get("SETTINGS", "grant_tab_privs_scope", fallback="owner"),
        validator=_validate_grant_scope,
    )
    _prompt_field(
        "SETTINGS",
        "grant_merge_privileges",
        "授权合并: 合并同一对象的多权限 (true/false)",
        default=cfg.get("SETTINGS", "grant_merge_privileges", fallback="true"),
        transform=_bool_transform,
    )
    _prompt_field(
        "SETTINGS",
        "grant_merge_grantees",
        "授权合并: 合并同权限的多 grantee (true/false)",
        default=cfg.get("SETTINGS", "grant_merge_grantees", fallback="true"),
        transform=_bool_transform,
    )
    _prompt_field(
        "SETTINGS",
        "grant_supported_sys_privs",
        "支持的系统权限清单 (逗号分隔，留空自动探测)",
        default=cfg.get("SETTINGS", "grant_supported_sys_privs", fallback=""),
    )
    _prompt_field(
        "SETTINGS",
        "grant_supported_object_privs",
        "支持的对象权限清单 (逗号分隔，留空使用默认)",
        default=cfg.get("SETTINGS", "grant_supported_object_privs", fallback=""),
    )
    _prompt_field(
        "SETTINGS",
        "grant_include_oracle_maintained_roles",
        "是否生成 ORACLE_MAINTAINED 角色 (true/false)",
        default=cfg.get("SETTINGS", "grant_include_oracle_maintained_roles", fallback="false"),
        transform=_bool_transform,
    )
    _prompt_field(
        "SETTINGS",
        "check_dependencies",
        "是否校验依赖 (true/false)",
        default=cfg.get("SETTINGS", "check_dependencies", fallback="true"),
        transform=_bool_transform,
    )
    _prompt_field(
        "SETTINGS",
        "check_comments",
        "是否比对表/列注释 (true/false)",
        default=cfg.get("SETTINGS", "check_comments", fallback="true"),
        transform=_bool_transform,
    )
    _prompt_field(
        "SETTINGS",
        "check_column_order",
        "是否校验列顺序 (true/false，默认 false)",
        default=cfg.get("SETTINGS", "check_column_order", fallback="false"),
        transform=_bool_transform,
    )
    _prompt_field(
        "SETTINGS",
        "check_object_usability",
        "是否校验 VIEW/SYNONYM 可用性 (true/false，默认 false)",
        default=cfg.get("SETTINGS", "check_object_usability", fallback="false"),
        transform=_bool_transform,
    )
    _prompt_field(
        "SETTINGS",
        "check_source_usability",
        "是否同时校验源端可用性 (true/false，默认 true)",
        default=cfg.get("SETTINGS", "check_source_usability", fallback="true"),
        transform=_bool_transform,
    )
    _prompt_field(
        "SETTINGS",
        "usability_check_timeout",
        "可用性校验超时（秒，默认 10）",
        default=cfg.get("SETTINGS", "usability_check_timeout", fallback="10"),
        validator=_validate_positive_int,
    )
    _prompt_field(
        "SETTINGS",
        "usability_check_workers",
        "可用性校验并发线程数（默认 10）",
        default=cfg.get("SETTINGS", "usability_check_workers", fallback="10"),
        validator=_validate_positive_int,
    )
    _prompt_field(
        "SETTINGS",
        "max_usability_objects",
        "可用性校验抽样阈值（0 表示不抽样）",
        default=cfg.get("SETTINGS", "max_usability_objects", fallback="0"),
        validator=_validate_non_negative_int,
    )
    _prompt_field(
        "SETTINGS",
        "usability_sample_ratio",
        "可用性校验抽样比例（0~1，留空/0 表示不抽样）",
        default=cfg.get("SETTINGS", "usability_sample_ratio", fallback="0"),
        validator=_validate_ratio_0_1,
    )
    _prompt_field(
        "SETTINGS",
        "column_visibility_policy",
        "列可见性(INVISIBLE)处理策略 (auto/enforce/ignore)",
        default=cfg.get("SETTINGS", "column_visibility_policy", fallback="auto"),
        validator=_validate_column_visibility_policy,
        transform=normalize_column_visibility_policy,
    )
    _prompt_field(
        "SETTINGS",
        "infer_schema_mapping",
        "是否自动推导 schema 映射 (true/false，默认 true，建议按 remap 需求决定)",
        default=cfg.get("SETTINGS", "infer_schema_mapping", fallback="true"),
        transform=_bool_transform,
    )
    _prompt_field(
        "SETTINGS",
        "dbcat_chunk_size",
        "dbcat 单批对象数量 (默认 150，可适当增大)",
        default=cfg.get("SETTINGS", "dbcat_chunk_size", fallback="150"),
        validator=_validate_positive_int,
    )
    _prompt_field(
        "SETTINGS",
        "check_primary_types",
        "主对象过滤 (留空为全量，例如 TABLE,VIEW)",
        default=cfg.get("SETTINGS", "check_primary_types", fallback=""),
    )
    _prompt_field(
        "SETTINGS",
        "synonym_check_scope",
        "同义词校验范围 (all/public_only)",
        default=cfg.get("SETTINGS", "synonym_check_scope", fallback="public_only"),
        validator=_validate_synonym_check_scope,
        transform=normalize_synonym_check_scope,
    )
    _prompt_field(
        "SETTINGS",
        "check_extra_types",
        "扩展对象过滤 (留空或 index,constraint,sequence,trigger)",
        default=cfg.get("SETTINGS", "check_extra_types", fallback="index,constraint,sequence,trigger"),
    )
    _prompt_field(
        "SETTINGS",
        "check_status_drift_types",
        "状态漂移检查类型 (留空或 trigger,constraint)",
        default=cfg.get("SETTINGS", "check_status_drift_types", fallback="trigger,constraint"),
    )
    _prompt_field(
        "SETTINGS",
        "extra_check_workers",
        "扩展对象校验并发进程数 (建议 8 或 16)",
        default=cfg.get("SETTINGS", "extra_check_workers", fallback="16"),
        validator=_validate_positive_int,
    )
    _prompt_field(
        "SETTINGS",
        "extra_check_chunk_size",
        "扩展对象校验单批表数量 (默认 200)",
        default=cfg.get("SETTINGS", "extra_check_chunk_size", fallback="200"),
        validator=_validate_positive_int,
    )
    _prompt_field(
        "SETTINGS",
        "extra_check_progress_interval",
        "扩展对象校验进度日志间隔（秒）",
        default=cfg.get("SETTINGS", "extra_check_progress_interval", fallback="10"),
        validator=_validate_positive_int,
    )
    _prompt_field(
        "SETTINGS",
        "fixup_dir",
        "订正 SQL 输出目录",
        default=cfg.get("SETTINGS", "fixup_dir", fallback="fixup_scripts"),
    )
    _prompt_field(
        "SETTINGS",
        "fixup_dir_allow_outside_repo",
        "是否允许 fixup_dir 指向项目目录外 (true/false)",
        default=cfg.get("SETTINGS", "fixup_dir_allow_outside_repo", fallback="true"),
        transform=_bool_transform,
    )
    _prompt_field(
        "SETTINGS",
        "fixup_max_sql_file_mb",
        "run_fixup 单文件最大读取大小（MB，<=0 不限制）",
        default=cfg.get("SETTINGS", "fixup_max_sql_file_mb", fallback="50"),
        validator=_validate_non_negative_int,
    )
    _prompt_field(
        "SETTINGS",
        "fixup_force_clean",
        "是否强制清理 fixup_dir（即使目录在项目外）(true/false)",
        default=cfg.get("SETTINGS", "fixup_force_clean", fallback="true"),
        transform=_bool_transform,
    )
    _prompt_field(
        "SETTINGS",
        "fixup_schemas",
        "限定生成订正 SQL 的目标 schema 列表 (逗号分隔，留空为全部)",
        default=cfg.get("SETTINGS", "fixup_schemas", fallback=""),
    )
    _prompt_field(
        "SETTINGS",
        "fixup_types",
        "限定生成订正 SQL 的对象类型 (留空为全部，如 TABLE,VIEW,TRIGGER)",
        default=cfg.get("SETTINGS", "fixup_types", fallback=""),
    )
    _prompt_field(
        "SETTINGS",
        "fixup_idempotent_mode",
        "修补脚本幂等模式 (off/guard/replace/drop_create)",
        default=cfg.get("SETTINGS", "fixup_idempotent_mode", fallback="replace"),
        validator=_validate_fixup_idempotent_mode,
        transform=normalize_fixup_idempotent_mode,
    )
    _prompt_field(
        "SETTINGS",
        "fixup_idempotent_types",
        "幂等模式作用对象类型 (逗号分隔，留空用默认)",
        default=cfg.get("SETTINGS", "fixup_idempotent_types", fallback=""),
    )
    _prompt_field(
        "SETTINGS",
        "trigger_list",
        "可选：触发器清单文件 (SCHEMA.TRIGGER_NAME，每行一条)",
        default=cfg.get("SETTINGS", "trigger_list", fallback=""),
    )
    _prompt_field(
        "SETTINGS",
        "trigger_qualify_schema",
        "触发器 DDL 是否强制补全 schema 前缀 (true/false)",
        default=cfg.get("SETTINGS", "trigger_qualify_schema", fallback="true"),
        transform=_bool_transform,
    )
    _prompt_field(
        "SETTINGS",
        "generate_status_fixup",
        "是否生成状态漂移修复脚本 (true/false，默认 true)",
        default=cfg.get("SETTINGS", "generate_status_fixup", fallback="true"),
        transform=_bool_transform,
    )
    _prompt_field(
        "SETTINGS",
        "status_fixup_types",
        "状态修复脚本对象类型 (trigger,constraint)",
        default=cfg.get("SETTINGS", "status_fixup_types", fallback="trigger,constraint"),
    )
    _prompt_field(
        "SETTINGS",
        "constraint_status_sync_mode",
        "约束状态同步模式 (enabled_only/full)",
        default=cfg.get("SETTINGS", "constraint_status_sync_mode", fallback="enabled_only"),
        validator=_validate_constraint_status_sync_mode,
        transform=normalize_constraint_status_sync_mode,
    )
    _prompt_field(
        "SETTINGS",
        "constraint_missing_fixup_validate_mode",
        "缺失约束修补 VALIDATE 策略 (safe_novalidate/source/force_validate)",
        default=cfg.get("SETTINGS", "constraint_missing_fixup_validate_mode", fallback="safe_novalidate"),
        validator=_validate_constraint_missing_fixup_validate_mode,
        transform=normalize_constraint_missing_fixup_validate_mode,
    )
    _prompt_field(
        "SETTINGS",
        "trigger_validity_sync_mode",
        "触发器有效性同步模式 (off/compile)",
        default=cfg.get("SETTINGS", "trigger_validity_sync_mode", fallback="compile"),
        validator=_validate_trigger_validity_sync_mode,
        transform=normalize_trigger_validity_sync_mode,
    )
    _prompt_field(
        "SETTINGS",
        "report_dir",
        "报告输出目录",
        default=cfg.get("SETTINGS", "report_dir", fallback="main_reports"),
    )
    _prompt_field(
        "SETTINGS",
        "report_dir_layout",
        "报告目录布局 (flat/per_run)",
        default=cfg.get("SETTINGS", "report_dir_layout", fallback="per_run"),
        validator=_validate_report_dir_layout,
        transform=normalize_report_dir_layout,
    )
    _prompt_field(
        "SETTINGS",
        "report_detail_mode",
        "报告内容模式 (full/split/summary)",
        default=cfg.get("SETTINGS", "report_detail_mode", fallback="split"),
        validator=_validate_report_detail_mode,
        transform=normalize_report_detail_mode,
    )
    _prompt_field(
        "SETTINGS",
        "report_to_db",
        "是否将报告存储到 OceanBase (true/false，默认 true)",
        default=cfg.get("SETTINGS", "report_to_db", fallback="true"),
        transform=_bool_transform,
    )
    _prompt_field(
        "SETTINGS",
        "report_db_store_scope",
        "报告写库范围 (summary/core/full，默认 full)",
        default=cfg.get("SETTINGS", "report_db_store_scope", fallback="full"),
        validator=_validate_report_db_store_scope,
        transform=normalize_report_db_store_scope,
    )
    _prompt_field(
        "SETTINGS",
        "report_db_schema",
        "报告存库 schema（可选，留空使用目标连接用户）",
        default=cfg.get("SETTINGS", "report_db_schema", fallback=""),
    )
    _prompt_field(
        "SETTINGS",
        "report_retention_days",
        "报告保留天数（0 表示不清理）",
        default=cfg.get("SETTINGS", "report_retention_days", fallback="90"),
        validator=_validate_non_negative_int,
    )
    _prompt_field(
        "SETTINGS",
        "report_db_fail_abort",
        "报告写库失败是否中止 (true/false)",
        default=cfg.get("SETTINGS", "report_db_fail_abort", fallback="false"),
        transform=_bool_transform,
    )
    _prompt_field(
        "SETTINGS",
        "report_db_detail_mode",
        "写入明细范围 (missing,mismatched,unsupported,ok,skipped,all)",
        default=cfg.get("SETTINGS", "report_db_detail_mode", fallback="missing,mismatched,unsupported"),
        validator=_validate_report_db_detail_mode,
    )
    _prompt_field(
        "SETTINGS",
        "report_db_detail_max_rows",
        "报告明细最大写入行数（0 不限制）",
        default=cfg.get("SETTINGS", "report_db_detail_max_rows", fallback="0"),
        validator=_validate_non_negative_int,
    )
    _prompt_field(
        "SETTINGS",
        "report_db_detail_item_enable",
        "是否写入明细行化表 DIFF_REPORT_DETAIL_ITEM (true/false，默认 full 时启用)",
        default=cfg.get("SETTINGS", "report_db_detail_item_enable", fallback=""),
    )
    _prompt_field(
        "SETTINGS",
        "report_db_detail_item_max_rows",
        "明细行化表最大写入行数（0 不限制）",
        default=cfg.get("SETTINGS", "report_db_detail_item_max_rows", fallback="0"),
        validator=_validate_non_negative_int,
    )
    _prompt_field(
        "SETTINGS",
        "report_db_insert_batch",
        "报告写库批量大小（INSERT ALL）",
        default=cfg.get("SETTINGS", "report_db_insert_batch", fallback="200"),
        validator=_validate_positive_int,
    )
    _prompt_field(
        "SETTINGS",
        "report_db_save_full_json",
        "是否保存完整报告 JSON (true/false)",
        default=cfg.get("SETTINGS", "report_db_save_full_json", fallback="false"),
        transform=_bool_transform,
    )
    _prompt_field(
        "SETTINGS",
        "dbcat_output_dir",
        "dbcat 输出缓存目录",
        default=cfg.get("SETTINGS", "dbcat_output_dir", fallback="dbcat_output"),
    )
    _prompt_field(
        "SETTINGS",
        "obclient_timeout",
        "obclient 超时（秒）",
        default=cfg.get("SETTINGS", "obclient_timeout", fallback="60"),
        validator=_validate_positive_int,
    )
    _prompt_field(
        "SETTINGS",
        "cli_timeout",
        "dbcat CLI 超时（秒）",
        default=cfg.get("SETTINGS", "cli_timeout", fallback="600"),
        validator=_validate_positive_int,
    )
    _prompt_field(
        "SETTINGS",
        "view_dblink_policy",
        "视图 DBLINK 处理策略 (block/allow)",
        default=cfg.get("SETTINGS", "view_dblink_policy", fallback="block"),
        validator=_validate_view_dblink_policy,
        transform=normalize_view_dblink_policy,
    )
    _prompt_field(
        "SETTINGS",
        "view_constraint_cleanup",
        "VIEW 列清单约束清洗策略 (auto/force/off)",
        default=cfg.get("SETTINGS", "view_constraint_cleanup", fallback="auto"),
        validator=_validate_view_constraint_cleanup,
        transform=normalize_view_constraint_cleanup,
    )
    _prompt_field(
        "SETTINGS",
        "view_compat_rules_path",
        "视图兼容性规则 JSON 路径 (可选)",
        default=cfg.get("SETTINGS", "view_compat_rules_path", fallback=""),
    )

    _prompt_field(
        "SETTINGS",
        "ddl_format_enable",
        "是否启用 SQLcl DDL 格式化 (true/false)",
        default=cfg.get("SETTINGS", "ddl_format_enable", fallback="false"),
        transform=_bool_transform,
    )
    ddl_format_enabled = parse_bool_flag(cfg.get("SETTINGS", "ddl_format_enable", fallback="false"))
    if ddl_format_enabled:
        _prompt_field(
            "SETTINGS",
            "sqlcl_bin",
            "SQLcl 根目录或 bin/sql 路径",
            default=cfg.get("SETTINGS", "sqlcl_bin", fallback=""),
            validator=_validate_sqlcl_path,
            required=True,
        )
        _prompt_field(
            "SETTINGS",
            "sqlcl_profile_path",
            "SQLcl 格式化规则文件 (可选)",
            default=cfg.get("SETTINGS", "sqlcl_profile_path", fallback=""),
        )
        _prompt_field(
            "SETTINGS",
            "ddl_format_types",
            "需要格式化的对象类型 (逗号分隔，默认 VIEW)",
            default=cfg.get("SETTINGS", "ddl_format_types", fallback="VIEW"),
        )
        _prompt_field(
            "SETTINGS",
            "ddl_format_fail_policy",
            "格式化失败策略 (fallback/error)",
            default=cfg.get("SETTINGS", "ddl_format_fail_policy", fallback="fallback"),
            validator=_validate_ddl_format_fail_policy,
            transform=normalize_ddl_format_fail_policy,
        )
        _prompt_field(
            "SETTINGS",
            "ddl_format_batch_size",
            "SQLcl 批量格式化对象数",
            default=cfg.get("SETTINGS", "ddl_format_batch_size", fallback="200"),
            validator=_validate_positive_int,
        )
        _prompt_field(
            "SETTINGS",
            "ddl_format_timeout",
            "SQLcl 批处理超时（秒，0 为不超时）",
            default=cfg.get("SETTINGS", "ddl_format_timeout", fallback="60"),
            validator=_validate_non_negative_int,
        )
        _prompt_field(
            "SETTINGS",
            "ddl_format_max_lines",
            "单个 DDL 最大行数（超过则跳过，0 表示不限制）",
            default=cfg.get("SETTINGS", "ddl_format_max_lines", fallback="30000"),
            validator=_validate_non_negative_int,
        )
        _prompt_field(
            "SETTINGS",
            "ddl_format_max_bytes",
            "单个 DDL 最大字节数（超过则跳过，0 表示不限制）",
            default=cfg.get("SETTINGS", "ddl_format_max_bytes", fallback="2000000"),
            validator=_validate_non_negative_int,
        )

    # 只有生成 fixup 时才校验 dbcat/JAVA_HOME
    gen_fixup_val = cfg.get("SETTINGS", "generate_fixup", fallback="true").lower()
    gen_fixup_enabled = gen_fixup_val in ("true", "1", "yes", "y", "on")
    if gen_fixup_enabled:
        _prompt_field(
            "SETTINGS",
            "dbcat_bin",
            "dbcat 路径（目录或 bin/dbcat 可执行文件）",
            validator=_validate_path_exists,
            required=True,
        )
        _prompt_field(
            "SETTINGS",
            "java_home",
            "JAVA_HOME (dbcat 需要)",
            default=cfg.get("SETTINGS", "java_home", fallback=os.environ.get("JAVA_HOME", "")),
            validator=_validate_path_exists,
            required=True,
        )
        _prompt_field(
            "SETTINGS",
            "dbcat_from",
            "dbcat from profile",
            default=cfg.get("SETTINGS", "dbcat_from", fallback="oracle19c"),
            required=True,
        )
        _prompt_field(
            "SETTINGS",
            "dbcat_to",
            "dbcat to profile",
            default=cfg.get("SETTINGS", "dbcat_to", fallback="oboracle422"),
            required=True,
        )

    config_path.parent.mkdir(parents=True, exist_ok=True)
    with open(config_path, "w", encoding="utf-8") as fp:
        cfg.write(fp)
    log.info("配置已保存: %s", config_path)


def load_remap_rules(file_path: str) -> RemapRules:
    """从 txt 文件加载 remap 规则"""
    log.info(f"正在加载 Remap 规则文件: {file_path}")
    rules: RemapRules = {}
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            for i, line in enumerate(f):
                line = line.strip()
                if not line or line.startswith('#'):
                    continue

                if '=' not in line:
                    log.warning(f"  [规则警告] 第 {i+1} 行格式错误，已跳过: {line}")
                    continue

                try:
                    src_obj, tgt_obj = line.split('=', 1)
                    src_obj = src_obj.strip().upper()
                    tgt_obj = tgt_obj.strip().upper()
                    if not src_obj or not tgt_obj or '.' not in src_obj or '.' not in tgt_obj:
                        log.warning(f"  [规则警告] 第 {i+1} 行格式无效 (必须为 'SCHEMA.OBJ')，已跳过: {line}")
                        continue
                    rules[src_obj] = tgt_obj
                except Exception:
                    log.warning(f"  [规则警告] 第 {i+1} 行解析失败，已跳过: {line}")

    except FileNotFoundError:
        log.warning(f"  [警告] Remap 文件 {file_path} 未找到。将按 1:1 规则继续。")
        return {}

    log.info(f"加载了 {len(rules)} 条 Remap 规则。")
    return rules


def parse_trigger_list_file(
    file_path: str
) -> Tuple[Set[str], List[Tuple[int, str, str]], List[Tuple[int, str]], int, Optional[str]]:
    """
    解析 trigger_list 文件，每行格式为 SCHEMA.TRIGGER_NAME。
    返回 (entries, invalid_entries, duplicate_entries, total_lines, error).
    """
    entries: Set[str] = set()
    invalid_entries: List[Tuple[int, str, str]] = []
    duplicate_entries: List[Tuple[int, str]] = []
    total_lines = 0
    if not file_path:
        return entries, invalid_entries, duplicate_entries, total_lines, None

    path = Path(file_path).expanduser()
    try:
        with path.open("r", encoding="utf-8") as fp:
            for line_no, raw in enumerate(fp, start=1):
                line = raw.strip()
                if not line or line.startswith("#"):
                    continue
                if "#" in line:
                    line = line.split("#", 1)[0].strip()
                    if not line:
                        continue
                total_lines += 1
                if "." not in line:
                    invalid_entries.append((line_no, raw.strip(), "缺少 schema 前缀 (SCHEMA.TRIGGER_NAME)"))
                    continue
                schema, name = line.split(".", 1)
                schema = schema.strip().strip('"')
                name = name.strip().strip('"')
                if not schema or not name:
                    invalid_entries.append((line_no, raw.strip(), "schema 或 trigger 名称为空"))
                    continue
                full_name = f"{schema.upper()}.{name.upper()}"
                if full_name in entries:
                    duplicate_entries.append((line_no, full_name))
                    continue
                entries.add(full_name)
        return entries, invalid_entries, duplicate_entries, total_lines, None
    except FileNotFoundError:
        return set(), [], [], 0, f"文件不存在: {path}"
    except OSError as exc:
        return set(), [], [], 0, f"读取失败: {exc}"


def build_trigger_full_set(
    triggers: Dict[Tuple[str, str], Dict[str, Dict]]
) -> Set[str]:
    """将元数据中的触发器转换为 OWNER.TRIGGER_NAME 集合。"""
    full_set: Set[str] = set()
    for (owner, _), trg_map in triggers.items():
        owner_u = (owner or "").upper()
        for trg_name, info in (trg_map or {}).items():
            trg_owner, name_u = normalize_trigger_identity(trg_name, info, owner_u)
            if trg_owner and name_u:
                full_set.add(f"{trg_owner}.{name_u}")
    return full_set


def build_trigger_full_map(
    triggers: Dict[Tuple[str, str], Dict[str, Dict]]
) -> Dict[str, Dict]:
    """将元数据中的触发器转换为 OWNER.TRIGGER_NAME -> info 映射。"""
    full_map: Dict[str, Dict] = {}
    for (owner, _), trg_map in triggers.items():
        owner_u = (owner or "").upper()
        for trg_name, info in (trg_map or {}).items():
            trg_owner, name_u = normalize_trigger_identity(trg_name, info, owner_u)
            if trg_owner and name_u:
                full_map[f"{trg_owner}.{name_u}"] = info
    return full_map


def collect_missing_trigger_mappings(
    extra_results: ExtraCheckResults
) -> Tuple[Dict[str, str], Set[str], int]:
    """
    从 trigger_mismatched 结果中提取缺失触发器映射。
    返回 (src_to_tgt_map, missing_targets, total_missing).
    """
    src_to_tgt: Dict[str, str] = {}
    missing_targets: Set[str] = set()
    total_missing = 0
    for item in extra_results.get("trigger_mismatched", []):
        total_missing += len(item.missing_triggers)
        for tgt_full in item.missing_triggers:
            if tgt_full:
                missing_targets.add(tgt_full.upper())
        if item.missing_mappings:
            for src_full, tgt_full in item.missing_mappings:
                if not src_full or not tgt_full:
                    continue
                src_u = src_full.upper()
                tgt_u = tgt_full.upper()
                src_to_tgt[src_u] = tgt_u
                missing_targets.add(tgt_u)
    return src_to_tgt, missing_targets, total_missing


def build_trigger_list_report(
    trigger_list_path: str,
    entries: Set[str],
    invalid_entries: List[Tuple[int, str, str]],
    duplicate_entries: List[Tuple[int, str]],
    total_lines: int,
    read_error: Optional[str],
    extra_results: ExtraCheckResults,
    oracle_meta: OracleMetadata,
    ob_meta: ObMetadata,
    full_object_mapping: FullObjectMapping,
    trigger_check_enabled: bool
) -> Tuple[List[TriggerListReportRow], Dict[str, object]]:
    """
    基于 trigger_list 与缺失触发器结果构造报告行与汇总信息。
    """
    summary: Dict[str, object] = {
        "enabled": True,
        "path": str(trigger_list_path),
        "total_lines": total_lines,
        "valid_entries": len(entries),
        "invalid_entries": len(invalid_entries),
        "duplicate_entries": len(duplicate_entries),
        "selected_missing": 0,
        "missing_not_listed": 0,
        "not_found": 0,
        "not_missing": 0,
        "check_disabled": False,
        "error": read_error or "",
        "fallback_full": False,
        "fallback_reason": ""
    }
    rows: List[TriggerListReportRow] = []

    if read_error:
        summary["fallback_full"] = True
        summary["fallback_reason"] = "read_error"
        _, _, total_missing = collect_missing_trigger_mappings(extra_results)
        summary["missing_not_listed"] = total_missing
        rows.append(TriggerListReportRow(
            entry=str(trigger_list_path),
            status="ERROR",
            detail=read_error
        ))
        return rows, summary

    for line_no, raw, reason in invalid_entries:
        rows.append(TriggerListReportRow(
            entry=f"line {line_no}: {raw}",
            status="INVALID",
            detail=reason
        ))
    for line_no, entry in duplicate_entries:
        rows.append(TriggerListReportRow(
            entry=f"line {line_no}: {entry}",
            status="DUPLICATE",
            detail="重复条目"
        ))

    if not trigger_check_enabled:
        summary["check_disabled"] = True
        for entry in sorted(entries):
            rows.append(TriggerListReportRow(
                entry=entry,
                status="CHECK_DISABLED",
                detail="TRIGGER 未启用检查，无法判定缺失状态"
            ))
        return rows, summary

    if not entries:
        summary["fallback_full"] = True
        summary["fallback_reason"] = "empty_list"
        _, _, total_missing = collect_missing_trigger_mappings(extra_results)
        summary["missing_not_listed"] = total_missing
        rows.append(TriggerListReportRow(
            entry=str(trigger_list_path),
            status="EMPTY",
            detail="清单为空，已回退全量触发器"
        ))
        return rows, summary

    src_to_tgt, missing_targets, total_missing = collect_missing_trigger_mappings(extra_results)
    missing_src_set = set(src_to_tgt.keys())
    missing_by_tgt = {tgt: src for src, tgt in src_to_tgt.items()}
    source_triggers = build_trigger_full_set(oracle_meta.triggers or {})
    target_triggers = build_trigger_full_set(ob_meta.triggers or {})

    selected_missing_targets: Set[str] = set()
    not_found = 0
    not_missing = 0

    for entry in sorted(entries):
        entry_u = entry.upper()
        if entry_u in missing_src_set:
            tgt_full = src_to_tgt.get(entry_u)
            if tgt_full:
                selected_missing_targets.add(tgt_full)
            rows.append(TriggerListReportRow(
                entry=entry_u,
                status="SELECTED_MISSING",
                detail=f"目标缺失: {tgt_full}" if tgt_full else "目标缺失"
            ))
            continue
        if entry_u in missing_targets:
            src_full = missing_by_tgt.get(entry_u)
            selected_missing_targets.add(entry_u)
            rows.append(TriggerListReportRow(
                entry=entry_u,
                status="SELECTED_MISSING",
                detail=f"源触发器: {src_full}" if src_full else "目标缺失"
            ))
            continue
        if entry_u not in source_triggers:
            if entry_u in target_triggers:
                not_missing += 1
                rows.append(TriggerListReportRow(
                    entry=entry_u,
                    status="EXISTS_IN_TARGET",
                    detail="目标端已存在"
                ))
            else:
                not_found += 1
                rows.append(TriggerListReportRow(
                    entry=entry_u,
                    status="NOT_FOUND_IN_SOURCE",
                    detail="源端未找到触发器"
                ))
            continue

        target_full = get_mapped_target(full_object_mapping, entry_u, 'TRIGGER') or entry_u
        if target_full.upper() in target_triggers:
            not_missing += 1
            rows.append(TriggerListReportRow(
                entry=entry_u,
                status="EXISTS_IN_TARGET",
                detail=f"目标端已存在: {target_full.upper()}"
            ))
        else:
            not_missing += 1
            rows.append(TriggerListReportRow(
                entry=entry_u,
                status="NOT_MISSING_OR_OUT_OF_SCOPE",
                detail="未在缺失清单中或不在本次校验范围"
            ))

    summary["selected_missing"] = len(selected_missing_targets)
    summary["missing_not_listed"] = max(0, total_missing - len(selected_missing_targets))
    summary["not_found"] = not_found
    summary["not_missing"] = not_missing
    return rows, summary


def collect_trigger_status_rows(
    oracle_meta: OracleMetadata,
    ob_meta: ObMetadata,
    full_object_mapping: FullObjectMapping,
    unsupported_table_keys: Optional[Set[Tuple[str, str]]] = None
) -> List[TriggerStatusReportRow]:
    """
    收集两端触发器的启用/有效性/事件差异明细。
    仅对源端存在且目标端存在的触发器输出差异行。
    """
    rows: List[TriggerStatusReportRow] = []
    src_map = build_trigger_full_map(oracle_meta.triggers or {})
    tgt_map = build_trigger_full_map(ob_meta.triggers or {})
    if not src_map or not tgt_map:
        return rows

    unsupported_keys = {(s.upper(), t.upper()) for s, t in (unsupported_table_keys or set())}
    trigger_parent_map: Dict[str, Tuple[str, str]] = {}
    if unsupported_keys:
        for (owner, table), trg_map in (oracle_meta.triggers or {}).items():
            table_key = (owner.upper(), table.upper())
            for trigger_key, info in (trg_map or {}).items():
                trg_owner, name_u = normalize_trigger_identity(trigger_key, info, owner)
                if not name_u:
                    continue
                trigger_parent_map[f"{trg_owner}.{name_u}"] = table_key

    for src_full, src_info in src_map.items():
        if unsupported_keys:
            parent_key = trigger_parent_map.get(src_full.upper())
            if parent_key and parent_key in unsupported_keys:
                continue
        mapped = get_mapped_target(full_object_mapping, src_full, 'TRIGGER') or src_full
        mapped_u = mapped.upper()
        tgt_info = tgt_map.get(mapped_u)
        if not tgt_info:
            continue
        if "." not in src_full or "." not in mapped_u:
            continue
        src_owner, src_name = src_full.split(".", 1)
        tgt_owner, tgt_name = mapped_u.split(".", 1)
        s_event = (src_info.get("event") or "").strip()
        t_event = (tgt_info.get("event") or "").strip()
        s_enabled = normalize_trigger_status(src_info.get("status"))
        t_enabled = normalize_trigger_status(tgt_info.get("status"))
        s_valid = lookup_trigger_validity(oracle_meta, src_owner, src_name)
        t_valid = lookup_trigger_validity(ob_meta, tgt_owner, tgt_name)
        if s_event == t_event and s_enabled == t_enabled and s_valid == t_valid:
            continue
        diffs: List[str] = []
        if s_event != t_event:
            diffs.append("EVENT")
        if s_enabled != t_enabled:
            diffs.append("ENABLED")
        if s_valid != t_valid:
            diffs.append("VALID")
        rows.append(TriggerStatusReportRow(
            trigger_full=mapped_u,
            src_event=s_event,
            tgt_event=t_event,
            src_enabled=s_enabled,
            tgt_enabled=t_enabled,
            src_valid=s_valid,
            tgt_valid=t_valid,
            detail=",".join(diffs) if diffs else "-"
        ))
    return rows


def build_trigger_status_fixup_sqls(
    row: TriggerStatusReportRow,
    validity_mode: str = "off"
) -> List[str]:
    trigger_full = (row.trigger_full or "").upper()
    parsed = parse_full_object_name(trigger_full)
    if not parsed:
        return []
    schema_u, trigger_u = parsed
    trigger_full_quoted = quote_qualified_parts(schema_u, trigger_u)
    statements: List[str] = []

    src_enabled = normalize_trigger_status(row.src_enabled)
    tgt_enabled = normalize_trigger_status(row.tgt_enabled)
    if src_enabled in {"ENABLED", "DISABLED"} and tgt_enabled in {"ENABLED", "DISABLED"} and src_enabled != tgt_enabled:
        action = "ENABLE" if src_enabled == "ENABLED" else "DISABLE"
        statements.append(f"ALTER TRIGGER {trigger_full_quoted} {action};")

    if normalize_trigger_validity_sync_mode(validity_mode) == "compile":
        src_valid = normalize_trigger_status(row.src_valid)
        tgt_valid = normalize_trigger_status(row.tgt_valid)
        if src_valid == "VALID" and tgt_valid == "INVALID":
            statements.append(f"ALTER TRIGGER {trigger_full_quoted} COMPILE;")

    return statements


def _build_constraint_semantic_key(
    name: str,
    meta: Dict,
    *,
    is_source: bool,
    full_object_mapping: FullObjectMapping
) -> Optional[Tuple]:
    ctype = (meta.get("type") or "").upper()
    if ctype not in {"P", "U", "R", "C"}:
        return None
    cols = tuple(col.upper() for col in (meta.get("columns") or []) if col)
    if ctype in {"P", "U"}:
        return (ctype, cols)
    if ctype == "R":
        ref_owner = (meta.get("ref_table_owner") or meta.get("r_owner") or "").upper()
        ref_table = (meta.get("ref_table_name") or "").upper()
        ref_target = ""
        if ref_owner and ref_table:
            ref_src_full = f"{ref_owner}.{ref_table}"
            if is_source:
                ref_target = (get_mapped_target(full_object_mapping, ref_src_full, "TABLE") or ref_src_full).upper()
            else:
                ref_target = ref_src_full
        # 不再回退到 R_CONSTRAINT_NAME（SYS_C*/OBPK* 跨库命名差异会制造噪声）
        return (
            ctype,
            cols,
            ref_target,
            normalize_delete_rule(meta.get("delete_rule")),
            normalize_update_rule(meta.get("update_rule")),
        )
    expr = normalize_check_constraint_expression(meta.get("search_condition"), name)
    return (ctype, expr)


def _build_constraint_status_drift_action_sql(
    table_schema: str,
    table_name: str,
    constraint_name: str,
    src_status: str,
    tgt_status: str,
    src_validated: str,
    tgt_validated: str,
    sync_mode: str
) -> str:
    sync_mode = normalize_constraint_status_sync_mode(sync_mode)
    table_full_quoted = quote_qualified_parts(table_schema, table_name)
    cons_quoted = quote_identifier(constraint_name)
    src_status_n = normalize_constraint_enabled_status(src_status)
    tgt_status_n = normalize_constraint_enabled_status(tgt_status)
    src_validated_n = normalize_constraint_validated_status(src_validated)
    tgt_validated_n = normalize_constraint_validated_status(tgt_validated)

    if src_status_n != tgt_status_n and src_status_n in {"ENABLED", "DISABLED"} and tgt_status_n in {"ENABLED", "DISABLED"}:
        if src_status_n == "DISABLED":
            return f"ALTER TABLE {table_full_quoted} DISABLE CONSTRAINT {cons_quoted};"
        if sync_mode == "full":
            if src_validated_n == "VALIDATED":
                return f"ALTER TABLE {table_full_quoted} ENABLE VALIDATE CONSTRAINT {cons_quoted};"
            if src_validated_n == "NOT VALIDATED":
                return f"ALTER TABLE {table_full_quoted} ENABLE NOVALIDATE CONSTRAINT {cons_quoted};"
        return f"ALTER TABLE {table_full_quoted} ENABLE CONSTRAINT {cons_quoted};"

    if (
        sync_mode == "full"
        and src_status_n == "ENABLED"
        and tgt_status_n == "ENABLED"
        and src_validated_n != tgt_validated_n
    ):
        if src_validated_n == "VALIDATED":
            return f"ALTER TABLE {table_full_quoted} ENABLE VALIDATE CONSTRAINT {cons_quoted};"
        if src_validated_n == "NOT VALIDATED":
            return f"ALTER TABLE {table_full_quoted} ENABLE NOVALIDATE CONSTRAINT {cons_quoted};"
    return ""


def collect_constraint_status_drift_rows(
    oracle_meta: OracleMetadata,
    ob_meta: ObMetadata,
    master_list: MasterCheckList,
    full_object_mapping: FullObjectMapping,
    sync_mode: str = "enabled_only",
    unsupported_table_keys: Optional[Set[Tuple[str, str]]] = None
) -> List[ConstraintStatusDriftRow]:
    rows: List[ConstraintStatusDriftRow] = []
    seen_tables: Set[Tuple[str, str, str, str]] = set()
    unsupported_keys = {(s.upper(), t.upper()) for s, t in (unsupported_table_keys or set())}
    sync_mode = normalize_constraint_status_sync_mode(sync_mode)

    for src_full, tgt_full, obj_type in master_list:
        if (obj_type or "").upper() != "TABLE":
            continue
        src_parsed = parse_full_object_name(src_full)
        tgt_parsed = parse_full_object_name(tgt_full)
        if not src_parsed or not tgt_parsed:
            continue
        src_schema, src_table = src_parsed
        tgt_schema, tgt_table = tgt_parsed
        table_key = (src_schema, src_table, tgt_schema, tgt_table)
        if table_key in seen_tables:
            continue
        seen_tables.add(table_key)
        if unsupported_keys and (src_schema.upper(), src_table.upper()) in unsupported_keys:
            continue

        src_map = oracle_meta.constraints.get((src_schema.upper(), src_table.upper()), {}) or {}
        tgt_map = ob_meta.constraints.get((tgt_schema.upper(), tgt_table.upper()), {}) or {}
        if not src_map or not tgt_map:
            continue

        src_entries: List[Dict[str, object]] = []
        for src_name, src_meta in src_map.items():
            src_name_u = (src_name or "").upper()
            if not src_name_u:
                continue
            key = _build_constraint_semantic_key(
                src_name_u,
                src_meta or {},
                is_source=True,
                full_object_mapping=full_object_mapping
            )
            if key is None:
                continue
            src_entries.append({
                "name": src_name_u,
                "type": (src_meta.get("type") or "").upper(),
                "key": key,
                "status": normalize_constraint_enabled_status(src_meta.get("status")),
                "validated": normalize_constraint_validated_status(src_meta.get("validated")),
            })

        tgt_entries: List[Dict[str, object]] = []
        for tgt_name, tgt_meta in tgt_map.items():
            tgt_name_u = (tgt_name or "").upper()
            if not tgt_name_u:
                continue
            key = _build_constraint_semantic_key(
                tgt_name_u,
                tgt_meta or {},
                is_source=False,
                full_object_mapping=full_object_mapping
            )
            if key is None:
                continue
            tgt_entries.append({
                "name": tgt_name_u,
                "type": (tgt_meta.get("type") or "").upper(),
                "key": key,
                "status": normalize_constraint_enabled_status(tgt_meta.get("status")),
                "validated": normalize_constraint_validated_status(tgt_meta.get("validated")),
            })

        if not src_entries or not tgt_entries:
            continue

        tgt_by_name: Dict[str, Dict[str, object]] = {entry["name"]: entry for entry in tgt_entries}
        tgt_by_key: Dict[Tuple, List[Dict[str, object]]] = defaultdict(list)
        for entry in tgt_entries:
            tgt_by_key[entry["key"]].append(entry)
        for candidate_list in tgt_by_key.values():
            candidate_list.sort(key=lambda e: str(e.get("name") or ""))

        used_tgt_names: Set[str] = set()
        for src_entry in sorted(src_entries, key=lambda e: str(e.get("name") or "")):
            src_name_u = str(src_entry.get("name") or "")
            if not src_name_u:
                continue
            matched: Optional[Dict[str, object]] = None
            direct = tgt_by_name.get(src_name_u)
            if direct and direct.get("name") not in used_tgt_names:
                matched = direct
            if matched is None:
                candidates = [
                    item for item in tgt_by_key.get(src_entry.get("key"), [])
                    if item.get("name") not in used_tgt_names
                ]
                if candidates:
                    matched = candidates[0]
            if not matched:
                continue

            tgt_name_u = str(matched.get("name") or "")
            if not tgt_name_u:
                continue
            used_tgt_names.add(tgt_name_u)

            src_status = str(src_entry.get("status") or "UNKNOWN")
            tgt_status = str(matched.get("status") or "UNKNOWN")
            src_validated = str(src_entry.get("validated") or "UNKNOWN")
            tgt_validated = str(matched.get("validated") or "UNKNOWN")
            status_diff = src_status != tgt_status
            validated_diff = (
                sync_mode == "full"
                and src_status == "ENABLED"
                and tgt_status == "ENABLED"
                and src_validated != tgt_validated
            )
            if not status_diff and not validated_diff:
                continue

            drift_fields: List[str] = []
            if status_diff:
                drift_fields.append("ENABLED")
            if validated_diff:
                drift_fields.append("VALIDATED")

            detail_parts = [",".join(drift_fields) if drift_fields else "-"]
            if src_name_u != tgt_name_u:
                detail_parts.append(f"MATCH={src_name_u}->{tgt_name_u}")
            action_sql = _build_constraint_status_drift_action_sql(
                tgt_schema,
                tgt_table,
                tgt_name_u,
                src_status,
                tgt_status,
                src_validated,
                tgt_validated,
                sync_mode
            )
            rows.append(
                ConstraintStatusDriftRow(
                    table_full=f"{tgt_schema}.{tgt_table}",
                    constraint_type=str(src_entry.get("type") or ""),
                    src_constraint=src_name_u,
                    tgt_constraint=tgt_name_u,
                    src_status=src_status,
                    tgt_status=tgt_status,
                    src_validated=src_validated,
                    tgt_validated=tgt_validated,
                    detail="; ".join(part for part in detail_parts if part),
                    action_sql=action_sql or "-"
                )
            )
    return rows


def build_blocked_dependency_map(
    dependency_graph: Optional[DependencyGraph],
    unsupported_nodes: Set[DependencyNode],
    source_objects: Optional[SourceObjectMap] = None,
    object_parent_map: Optional["ObjectParentMap"] = None
) -> Dict[DependencyNode, Set[DependencyNode]]:
    """
    从不支持对象集合出发，沿依赖反向传播，构建被阻断的对象映射。
    """
    if not dependency_graph or not unsupported_nodes:
        return {}

    reverse_graph: Dict[DependencyNode, Set[DependencyNode]] = defaultdict(set)
    for dep_node, refs in dependency_graph.items():
        for ref_node in refs:
            reverse_graph[ref_node].add(dep_node)

    if object_parent_map:
        for dep_full, parent_full in object_parent_map.items():
            if not dep_full or not parent_full:
                continue
            dep_full_u = dep_full.upper()
            parent_full_u = parent_full.upper()
            dep_types = set()
            if source_objects:
                dep_types = {t.upper() for t in source_objects.get(dep_full_u, set()) if t}
            if not dep_types:
                dep_types = {"TRIGGER"}
            for dep_type in dep_types:
                reverse_graph[(parent_full_u, "TABLE")].add((dep_full_u, dep_type))

    blocked_by: Dict[DependencyNode, Set[DependencyNode]] = defaultdict(set)
    queue = deque(unsupported_nodes)
    visited: Set[DependencyNode] = set(unsupported_nodes)

    while queue:
        node = queue.popleft()
        for dep in reverse_graph.get(node, set()):
            if dep in unsupported_nodes:
                continue
            blocked_by[dep].add(node)
            if dep not in visited:
                visited.add(dep)
                queue.append(dep)

    return dict(blocked_by)


def classify_missing_objects(
    ora_cfg: OraConfig,
    settings: Dict,
    tv_results: ReportResults,
    extra_results: ExtraCheckResults,
    oracle_meta: OracleMetadata,
    ob_meta: ObMetadata,
    full_object_mapping: FullObjectMapping,
    source_objects: SourceObjectMap,
    dependency_graph: Optional[DependencyGraph],
    object_parent_map: Optional["ObjectParentMap"],
    table_target_map: Dict[Tuple[str, str], Tuple[str, str]],
    synonym_meta_map: Dict[Tuple[str, str], SynonymMeta]
) -> SupportClassificationResult:
    """
    对缺失对象进行支持性分类，并输出支持/不支持/被阻断的统计与明细。
    """
    support_state_map: Dict[Tuple[str, str], ObjectSupportReportRow] = {}
    missing_detail_rows: List[ObjectSupportReportRow] = []
    unsupported_rows: List[ObjectSupportReportRow] = []
    missing_support_counts: Dict[str, Dict[str, int]] = defaultdict(lambda: {"supported": 0, "unsupported": 0, "blocked": 0})
    extra_blocked_counts: Dict[str, int] = defaultdict(int)
    unsupported_table_keys: Set[Tuple[str, str]] = set()
    unsupported_view_keys: Set[Tuple[str, str]] = set()
    view_constraint_cleaned_rows: List[ViewConstraintReportRow] = []
    view_constraint_uncleanable_rows: List[ViewConstraintReportRow] = []

    view_compat_map: Dict[Tuple[str, str], ViewCompatResult] = {}
    invalid_nodes: Set[DependencyNode] = set()
    invalid_full_types: Dict[str, Set[str]] = defaultdict(set)

    view_rules = settings.get("view_compat_rules") or {}
    dblink_policy = settings.get("view_dblink_policy", "block")
    view_constraint_mode = settings.get("view_constraint_cleanup", "auto")

    missing_items = tv_results.get("missing", []) or []
    missing_views: List[Tuple[str, str]] = []
    for obj_type, _tgt_name, src_name in missing_items:
        if (obj_type or "").upper() != "VIEW":
            continue
        if "." not in src_name:
            continue
        src_schema, src_obj = src_name.split(".", 1)
        missing_views.append((src_schema.upper(), src_obj.upper()))

    view_ddl_map: Dict[Tuple[str, str], str] = {}
    if missing_views:
        view_ddl_map = oracle_get_views_ddl_batch(ora_cfg, missing_views)
        missing_ddl_keys = [key for key in missing_views if key not in view_ddl_map]
        if missing_ddl_keys:
            try:
                with oracledb.connect(
                    user=ora_cfg['user'],
                    password=ora_cfg['password'],
                    dsn=ora_cfg['dsn']
                ) as ora_conn:
                    for schema, view_name in missing_ddl_keys:
                        view_text = oracle_get_view_text(ora_conn, schema, view_name)
                        if view_text:
                            ddl = build_view_ddl_from_text(schema, view_name, *view_text)
                            if ddl:
                                view_ddl_map[(schema, view_name)] = ddl
            except Exception as exc:
                log.warning("[VIEW] 读取 DBA_VIEWS 失败，将跳过部分视图兼容性分析: %s", exc)

    for schema, view_name in missing_views:
        ddl = view_ddl_map.get((schema, view_name))
        cleanup_result = apply_view_constraint_cleanup(ddl or "", view_constraint_mode)
        ddl_for_analysis = cleanup_result.cleaned_ddl if ddl else ddl
        compat = analyze_view_compatibility(
            ddl_for_analysis,
            view_rules,
            dblink_policy
        )
        if cleanup_result.action == VIEW_CONSTRAINT_ACTION_CLEANED:
            compat = compat._replace(
                cleaned_ddl=compat.cleaned_ddl,
                rewrite_notes=compat.rewrite_notes + ["VIEW_CONSTRAINT_CLEANED"]
            )
            view_constraint_cleaned_rows.append(
                ViewConstraintReportRow(
                    view_full=f"{schema.upper()}.{view_name.upper()}",
                    mode=view_constraint_mode,
                    action=cleanup_result.action,
                    reason=cleanup_result.reason,
                    constraints=cleanup_result.detail or "; ".join(cleanup_result.constraints)
                )
            )
        elif cleanup_result.action == VIEW_CONSTRAINT_ACTION_UNCLEANABLE:
            view_constraint_uncleanable_rows.append(
                ViewConstraintReportRow(
                    view_full=f"{schema.upper()}.{view_name.upper()}",
                    mode=view_constraint_mode,
                    action=cleanup_result.action,
                    reason=cleanup_result.reason,
                    constraints=cleanup_result.detail or "; ".join(cleanup_result.constraints)
                )
            )
            if compat.support_state == SUPPORT_STATE_SUPPORTED:
                compat = compat._replace(
                    support_state=SUPPORT_STATE_UNSUPPORTED,
                    reason_code="VIEW_CONSTRAINT_UNCLEANABLE",
                    reason="视图列清单约束无法清洗",
                    detail=cleanup_result.detail or "VIEW_CONSTRAINT"
                )
        view_compat_map[(schema, view_name)] = compat

    source_objects = source_objects or {}

    def _is_x_dollar_ref(ref_full: str) -> bool:
        if not ref_full:
            return False
        obj = ref_full.split('.', 1)[1] if '.' in ref_full else ref_full
        return obj.upper().startswith("X$")

    def _collect_view_x_refs(schema: str, view_name: str) -> Set[str]:
        refs: Set[str] = set()
        view_full = f"{schema.upper()}.{view_name.upper()}"
        if dependency_graph:
            for ref_full, _ref_type in dependency_graph.get((view_full, "VIEW"), set()):
                if ref_full:
                    refs.add(ref_full.upper())
        elif view_ddl_map:
            ddl = view_ddl_map.get((schema, view_name))
            if ddl:
                deps = extract_view_dependencies(ddl, default_schema=schema)
                refs.update({d.upper() for d in deps})
        return {ref for ref in refs if _is_x_dollar_ref(ref)}

    if view_compat_map:
        for (schema, view_name), compat in list(view_compat_map.items()):
            x_refs = _collect_view_x_refs(schema, view_name)
            if not x_refs:
                continue
            blocked = {ref for ref in x_refs if ref.upper() not in source_objects}
            if blocked:
                if compat.support_state == SUPPORT_STATE_SUPPORTED or compat.reason_code == "VIEW_X$":
                    compat = compat._replace(
                        support_state=SUPPORT_STATE_UNSUPPORTED,
                        reason_code="VIEW_X$",
                        reason="引用 X$ 系统表，OB 不支持",
                        detail=",".join(sorted(blocked))
                    )
            else:
                if compat.reason_code == "VIEW_X$":
                    compat = compat._replace(
                        support_state=SUPPORT_STATE_SUPPORTED,
                        reason_code="",
                        reason="",
                        detail=""
                    )
            view_compat_map[(schema, view_name)] = compat

    for (owner, name, obj_type), status in (oracle_meta.object_statuses or {}).items():
        obj_type_u = (obj_type or "").upper()
        if obj_type_u not in INVALID_STATUS_TYPES:
            continue
        if normalize_object_status(status) != "INVALID":
            continue
        full = f"{(owner or '').upper()}.{(name or '').upper()}"
        if "." not in full:
            continue
        invalid_nodes.add((full, obj_type_u))
        invalid_full_types[full].add(obj_type_u)

    unsupported_nodes: Set[DependencyNode] = set()
    unsupported_table_map: Dict[str, Tuple[str, str, str]] = {}
    for (schema, table), entries in (oracle_meta.blacklist_tables or {}).items():
        schema_u = (schema or "").upper()
        table_u = (table or "").upper()
        if not schema_u or not table_u:
            continue
        full = f"{schema_u}.{table_u}"
        if is_long_only_blacklist(entries):
            tgt_schema_u, tgt_table_u = table_target_map.get((schema_u, table_u), (schema_u, table_u))
            tgt_full = f"{tgt_schema_u}.{tgt_table_u}"
            if ob_meta and ob_meta.objects_by_type and "TABLE" in (ob_meta.objects_by_type or {}):
                if tgt_full in ob_meta.objects_by_type.get("TABLE", set()):
                    log.info("[BLACKLIST] LONG 表 %s 目标端存在，依赖不再阻断。", full)
                else:
                    log.info("[BLACKLIST] LONG 表 %s 目标端缺失，仍不阻断依赖。", full)
            else:
                log.info("[BLACKLIST] LONG 表 %s 未能确认目标端存在性，默认不阻断依赖。", full)
            continue

        black_type, reason, detail = summarize_blacklist_entries(entries)
        unsupported_nodes.add((full, "TABLE"))
        unsupported_table_map[full] = (black_type, reason, detail)
        unsupported_table_keys.add((schema_u, table_u))

    for (schema, view_name), compat in view_compat_map.items():
        if compat.support_state != SUPPORT_STATE_SUPPORTED:
            full = f"{schema.upper()}.{view_name.upper()}"
            unsupported_nodes.add((full, "VIEW"))
            unsupported_view_keys.add((schema.upper(), view_name.upper()))

    block_source_nodes = set(unsupported_nodes)
    if invalid_nodes:
        block_source_nodes.update(invalid_nodes)

    blocked_by_map = build_blocked_dependency_map(
        dependency_graph,
        block_source_nodes,
        source_objects=source_objects,
        object_parent_map=object_parent_map
    )

    root_cause_cache: Dict[DependencyNode, str] = {}

    def _root_cause_label(node: DependencyNode) -> str:
        full, obj_type = node
        if node in invalid_nodes:
            return f"{full}(SOURCE_INVALID)"
        obj_type_u = (obj_type or "").upper()
        if obj_type_u == "TABLE" and full in unsupported_table_map:
            black_type = (unsupported_table_map.get(full) or ("", "", ""))[0] or "BLACKLIST"
            return f"{full}({black_type})"
        if obj_type_u == "VIEW":
            parsed = parse_full_object_name(full)
            if parsed:
                key = (parsed[0].upper(), parsed[1].upper())
                compat = view_compat_map.get(key)
                if compat and compat.support_state != SUPPORT_STATE_SUPPORTED:
                    reason_code = compat.reason_code or "VIEW_UNSUPPORTED"
                    return f"{full}({reason_code})"
        return f"{full}(UNSUPPORTED)"

    def _resolve_root_cause(node: DependencyNode) -> str:
        if node in root_cause_cache:
            return root_cause_cache[node]
        if node in block_source_nodes:
            root_cause_cache[node] = _root_cause_label(node)
            return root_cause_cache[node]
        blocked_refs = blocked_by_map.get(node)
        if not blocked_refs:
            root_cause_cache[node] = ""
            return ""
        # Prefer stable ordering for deterministic output
        for ref in sorted(blocked_refs, key=lambda n: (n[0], n[1])):
            if ref in block_source_nodes:
                root_cause_cache[node] = _root_cause_label(ref)
                return root_cause_cache[node]
            rc = _resolve_root_cause(ref)
            if rc:
                root_cause_cache[node] = rc
                return rc
        root_cause_cache[node] = ""
        return ""

    for obj_type, tgt_name, src_name in missing_items:
        obj_type_u = (obj_type or "").upper()
        if "." not in src_name or "." not in tgt_name:
            continue
        src_full = src_name.upper()
        tgt_full = tgt_name.upper()
        src_schema, src_obj = src_full.split(".", 1)

        support_state = SUPPORT_STATE_SUPPORTED
        reason_code = ""
        reason = ""
        dependency = ""
        action = "FIXUP"
        detail = ""
        root_cause = ""

        if (src_full, obj_type_u) in invalid_nodes:
            support_state = SUPPORT_STATE_BLOCKED
            reason_code = "SOURCE_INVALID"
            reason = "源端对象无效"
            dependency = "-"
            action = "先修复源端"
        elif obj_type_u == "TABLE":
            entries = oracle_meta.blacklist_tables.get((src_schema, src_obj))
            if entries:
                if not is_long_only_blacklist(entries):
                    black_type, reason, detail = summarize_blacklist_entries(entries)
                    support_state = SUPPORT_STATE_UNSUPPORTED
                    reason_code = f"BLACKLIST_{black_type}" if black_type else "BLACKLIST"
                    dependency = "-"
                    action = "改造/不迁移"
        elif obj_type_u == "VIEW":
            compat = view_compat_map.get((src_schema, src_obj))
            if compat and compat.support_state != SUPPORT_STATE_SUPPORTED:
                support_state = compat.support_state
                reason_code = compat.reason_code
                reason = compat.reason
                detail = compat.detail
                dependency = "-"
                action = "改造/授权"
        elif obj_type_u == "SYNONYM":
            syn_meta = synonym_meta_map.get((src_schema, src_obj))
            if syn_meta and syn_meta.table_owner and syn_meta.table_name:
                ref_full = f"{syn_meta.table_owner.upper()}.{syn_meta.table_name.upper()}"
                if ref_full in unsupported_table_map or (ref_full, "VIEW") in unsupported_nodes:
                    support_state = SUPPORT_STATE_BLOCKED
                    reason_code = "DEPENDENCY_UNSUPPORTED"
                    reason = "同义词指向不支持对象"
                    dependency = ref_full
                    action = "先改造依赖对象"
                elif ref_full in invalid_full_types:
                    support_state = SUPPORT_STATE_BLOCKED
                    reason_code = "DEPENDENCY_INVALID"
                    reason = "同义词指向无效对象"
                    dependency = ref_full
                    action = "先修复依赖对象"

        if support_state == SUPPORT_STATE_SUPPORTED:
            node = (src_full, obj_type_u)
            blocked_refs = blocked_by_map.get(node)
            if blocked_refs:
                support_state = SUPPORT_STATE_BLOCKED
                if any(ref in invalid_nodes for ref in blocked_refs):
                    reason_code = "DEPENDENCY_INVALID"
                    reason = "依赖源端 INVALID 对象"
                    action = "先修复依赖对象"
                else:
                    reason_code = "DEPENDENCY_UNSUPPORTED"
                    reason = "依赖不支持对象"
                    action = "先改造依赖对象"
                dependency = ",".join(sorted({n[0] for n in blocked_refs}))

        if support_state == SUPPORT_STATE_UNSUPPORTED:
            root_cause = _root_cause_label((src_full, obj_type_u))
        elif support_state == SUPPORT_STATE_BLOCKED:
            root_cause = _resolve_root_cause((src_full, obj_type_u))
            if not root_cause and dependency and dependency != "-":
                dep_full = dependency.split(",", 1)[0].upper()
                root_cause = (
                    _resolve_root_cause((dep_full, "TABLE"))
                    or _resolve_root_cause((dep_full, "VIEW"))
                    or f"{dep_full}(DEPENDENCY)"
                )

        row = ObjectSupportReportRow(
            obj_type=obj_type_u,
            src_full=src_full,
            tgt_full=tgt_full,
            support_state=support_state,
            reason_code=reason_code or ("-" if support_state == SUPPORT_STATE_SUPPORTED else ""),
            reason=reason or ("-" if support_state == SUPPORT_STATE_SUPPORTED else ""),
            dependency=dependency or ("-" if support_state == SUPPORT_STATE_SUPPORTED else ""),
            action=action,
            detail=detail or "-",
            root_cause=root_cause or "-"
        )
        support_state_map[(obj_type_u, src_full)] = row
        missing_detail_rows.append(row)
        if support_state == SUPPORT_STATE_UNSUPPORTED:
            missing_support_counts[obj_type_u]["unsupported"] += 1
            unsupported_rows.append(row)
        elif support_state == SUPPORT_STATE_BLOCKED:
            missing_support_counts[obj_type_u]["blocked"] += 1
            unsupported_rows.append(row)
        else:
            missing_support_counts[obj_type_u]["supported"] += 1

    # 扩展对象：索引/约束/触发器被不支持表阻断时的明细统计
    table_map: Dict[str, str] = {}
    for (src_schema, src_table), (tgt_schema, tgt_table) in table_target_map.items():
        table_map[f"{tgt_schema.upper()}.{tgt_table.upper()}"] = f"{src_schema.upper()}.{src_table.upper()}"

    for item in extra_results.get('index_mismatched', []):
        table_str = item.table.split()[0]
        if '.' not in table_str:
            continue
        src_name = table_map.get(table_str)
        if not src_name or '.' not in src_name:
            continue
        src_schema, src_table = src_name.split('.', 1)
        if (src_schema.upper(), src_table.upper()) not in unsupported_table_keys:
            continue
        tgt_schema, _ = table_str.split('.', 1)
        root_cause = _root_cause_label((f"{src_schema.upper()}.{src_table.upper()}", "TABLE"))
        for idx_name in sorted(item.missing_indexes):
            row = ObjectSupportReportRow(
                obj_type="INDEX",
                src_full=f"{src_schema.upper()}.{idx_name.upper()}",
                tgt_full=f"{tgt_schema.upper()}.{idx_name.upper()}",
                support_state=SUPPORT_STATE_BLOCKED,
                reason_code="DEPENDENCY_UNSUPPORTED",
                reason="依赖不支持表",
                dependency=table_str.upper(),
                action="先改造依赖表",
                detail="INDEX",
                root_cause=root_cause
            )
            unsupported_rows.append(row)
            extra_blocked_counts["INDEX"] += 1

    for item in extra_results.get('constraint_mismatched', []):
        table_str = item.table.split()[0]
        if '.' not in table_str:
            continue
        src_name = table_map.get(table_str)
        if not src_name or '.' not in src_name:
            continue
        src_schema, src_table = src_name.split('.', 1)
        if (src_schema.upper(), src_table.upper()) not in unsupported_table_keys:
            continue
        tgt_schema, _ = table_str.split('.', 1)
        root_cause = _root_cause_label((f"{src_schema.upper()}.{src_table.upper()}", "TABLE"))
        for cons_name in sorted(item.missing_constraints):
            row = ObjectSupportReportRow(
                obj_type="CONSTRAINT",
                src_full=f"{src_schema.upper()}.{cons_name.upper()}",
                tgt_full=f"{tgt_schema.upper()}.{cons_name.upper()}",
                support_state=SUPPORT_STATE_BLOCKED,
                reason_code="DEPENDENCY_UNSUPPORTED",
                reason="依赖不支持表",
                dependency=table_str.upper(),
                action="先改造依赖表",
                detail="CONSTRAINT",
                root_cause=root_cause
            )
            unsupported_rows.append(row)
            extra_blocked_counts["CONSTRAINT"] += 1

    def _iter_missing_trigger_full_pairs(
        item: TriggerMismatch,
        src_schema: str,
        tgt_schema: str
    ) -> Iterable[Tuple[str, str]]:
        """
        统一输出缺失触发器的 (SRC_FULL, TGT_FULL)：
        - 优先使用 compare 阶段产出的 missing_mappings
        - 回退到 missing_triggers 时兼容 OWNER.NAME 与 NAME 两种格式
        """
        if item.missing_mappings:
            for src_full, tgt_full in item.missing_mappings:
                src_u = (src_full or "").strip().upper()
                tgt_u = (tgt_full or "").strip().upper()
                if src_u and tgt_u and "." in src_u and "." in tgt_u:
                    yield src_u, tgt_u
            return

        for trg_name in sorted(item.missing_triggers or set()):
            trg_owner_u, trg_name_u = normalize_trigger_identity(trg_name, None, src_schema)
            if not trg_name_u:
                continue
            src_owner_u = (trg_owner_u or src_schema).upper()
            src_full = f"{src_owner_u}.{trg_name_u}"
            mapped = get_mapped_target(full_object_mapping, src_full, 'TRIGGER')
            if mapped and "." in mapped:
                tgt_full = mapped.upper()
            else:
                tgt_owner_u = src_owner_u or tgt_schema.upper()
                tgt_full = f"{tgt_owner_u}.{trg_name_u}"
            yield src_full, tgt_full

    for item in extra_results.get('trigger_mismatched', []):
        table_str = item.table.split()[0]
        if '.' not in table_str:
            continue
        src_name = table_map.get(table_str)
        if not src_name or '.' not in src_name:
            continue
        src_schema, src_table = src_name.split('.', 1)
        if (src_schema.upper(), src_table.upper()) not in unsupported_table_keys:
            continue
        tgt_schema, _ = table_str.split('.', 1)
        root_cause = _root_cause_label((f"{src_schema.upper()}.{src_table.upper()}", "TABLE"))
        for src_full, tgt_full in _iter_missing_trigger_full_pairs(item, src_schema, tgt_schema):
            row = ObjectSupportReportRow(
                obj_type="TRIGGER",
                src_full=src_full,
                tgt_full=tgt_full,
                support_state=SUPPORT_STATE_BLOCKED,
                reason_code="DEPENDENCY_UNSUPPORTED",
                reason="依赖不支持表",
                dependency=table_str.upper(),
                action="先改造依赖表",
                detail="TRIGGER",
                root_cause=root_cause
            )
            unsupported_rows.append(row)
            extra_blocked_counts["TRIGGER"] += 1

    extra_missing_rows: List[ObjectSupportReportRow] = []

    def _add_extra_missing_row(
        obj_type: str,
        src_full: str,
        tgt_full: str,
        dependency: str,
        detail: str
    ) -> None:
        extra_missing_rows.append(
            ObjectSupportReportRow(
                obj_type=obj_type,
                src_full=src_full,
                tgt_full=tgt_full,
                support_state=SUPPORT_STATE_SUPPORTED,
                reason_code="-",
                reason="-",
                dependency=dependency or "-",
                action="FIXUP",
                detail=detail or "-",
                root_cause="-"
            )
        )

    for item in extra_results.get('index_mismatched', []):
        table_str = item.table.split()[0]
        if '.' not in table_str:
            continue
        src_name = table_map.get(table_str)
        if not src_name or '.' not in src_name:
            continue
        src_schema, src_table = src_name.split('.', 1)
        if (src_schema.upper(), src_table.upper()) in unsupported_table_keys:
            continue
        tgt_schema, _ = table_str.split('.', 1)
        for idx_name in sorted(item.missing_indexes):
            _add_extra_missing_row(
                "INDEX",
                f"{src_schema.upper()}.{idx_name.upper()}",
                f"{tgt_schema.upper()}.{idx_name.upper()}",
                table_str.upper(),
                "INDEX"
            )

    for item in extra_results.get('constraint_mismatched', []):
        table_str = item.table.split()[0]
        if '.' not in table_str:
            continue
        src_name = table_map.get(table_str)
        if not src_name or '.' not in src_name:
            continue
        src_schema, src_table = src_name.split('.', 1)
        if (src_schema.upper(), src_table.upper()) in unsupported_table_keys:
            continue
        tgt_schema, _ = table_str.split('.', 1)
        for cons_name in sorted(item.missing_constraints):
            _add_extra_missing_row(
                "CONSTRAINT",
                f"{src_schema.upper()}.{cons_name.upper()}",
                f"{tgt_schema.upper()}.{cons_name.upper()}",
                table_str.upper(),
                "CONSTRAINT"
            )

    for item in extra_results.get('trigger_mismatched', []):
        table_str = item.table.split()[0]
        if '.' not in table_str:
            continue
        src_name = table_map.get(table_str)
        if not src_name or '.' not in src_name:
            continue
        src_schema, src_table = src_name.split('.', 1)
        if (src_schema.upper(), src_table.upper()) in unsupported_table_keys:
            continue
        tgt_schema, _ = table_str.split('.', 1)
        for src_full, tgt_full in _iter_missing_trigger_full_pairs(item, src_schema, tgt_schema):
            _add_extra_missing_row(
                "TRIGGER",
                src_full,
                tgt_full,
                table_str.upper(),
                "TRIGGER"
            )

    for item in extra_results.get('sequence_mismatched', []):
        src_schema = (item.src_schema or "").upper()
        tgt_schema = (item.tgt_schema or "").upper()
        for seq_name in sorted(item.missing_sequences):
            seq_u = (seq_name or "").upper()
            if not seq_u:
                continue
            _add_extra_missing_row(
                "SEQUENCE",
                f"{src_schema}.{seq_u}" if src_schema else seq_u,
                f"{tgt_schema}.{seq_u}" if tgt_schema else seq_u,
                "-",
                "SEQUENCE"
            )

    return SupportClassificationResult(
        support_state_map=support_state_map,
        missing_detail_rows=missing_detail_rows,
        unsupported_rows=unsupported_rows,
        extra_missing_rows=extra_missing_rows,
        missing_support_counts=dict(missing_support_counts),
        extra_blocked_counts=dict(extra_blocked_counts),
        unsupported_table_keys=unsupported_table_keys,
        unsupported_view_keys=unsupported_view_keys,
        view_compat_map=view_compat_map,
        view_constraint_cleaned_rows=view_constraint_cleaned_rows,
        view_constraint_uncleanable_rows=view_constraint_uncleanable_rows
    )


def init_oracle_client_from_settings(settings: Dict) -> None:
    """根据配置初始化 Oracle Thick Mode 并提示环境变量设置。"""
    client_dir = settings.get('oracle_client_lib_dir', '').strip()
    if not client_dir:
        log.error("严重错误: 未在 [SETTINGS] 中配置 oracle_client_lib_dir。")
        log.error("请在 config.ini 中添加例如: oracle_client_lib_dir = /home/user/instantclient_19_28")
        abort_run()

    client_path = Path(client_dir).expanduser()
    if not client_path.exists():
        log.error(f"严重错误: 指定的 Oracle Instant Client 目录不存在: {client_path}")
        abort_run()

    ld_path = os.environ.get('LD_LIBRARY_PATH') or '<未设置>'
    log.info(f"准备使用 Oracle Instant Client 目录: {client_path}")
    log.info("如遇 libnnz19.so 等库缺失，请先执行:")
    log.info(f"  export LD_LIBRARY_PATH=\"{client_path}:${{LD_LIBRARY_PATH}}\"")
    log.info(f"当前 LD_LIBRARY_PATH: {ld_path}")

    try:
        oracledb.init_oracle_client(lib_dir=str(client_path))
    except Exception as exc:
        log.error("严重错误: Oracle Thick Mode 初始化失败。")
        log.error("请确认 instant client 路径和 LD_LIBRARY_PATH 设置正确。")
        log.error(f"错误详情: {exc}")
        abort_run()


def get_source_objects(
    ora_cfg: OraConfig,
    schemas_list: List[str],
    object_types: Optional[Set[str]] = None,
    synonym_check_scope: str = "all"
) -> SourceObjectMap:
    """
    从 Oracle 源端获取所有需要纳入 remap/依赖分析的对象：
      TABLE / VIEW / MATERIALIZED VIEW / PROCEDURE / FUNCTION / PACKAGE / PACKAGE BODY /
      SYNONYM / JOB / SCHEDULE / TYPE / TYPE BODY / TRIGGER / SEQUENCE / INDEX
    object_types: 可选的类型过滤集合（只查询指定类型）
    """
    log.info(f"正在连接 Oracle 源端: {ora_cfg['dsn']}...")

    enabled_types = {t.upper() for t in (object_types or set(ALL_TRACKED_OBJECT_TYPES))}
    enabled_types &= set(ALL_TRACKED_OBJECT_TYPES)
    include_synonyms = 'SYNONYM' in enabled_types
    synonym_scope = normalize_synonym_check_scope(synonym_check_scope)
    object_types_for_objects = set(enabled_types)
    if include_synonyms:
        object_types_for_objects.discard('SYNONYM')
    if not object_types_for_objects and not include_synonyms:
        log.warning("未启用任何可管理对象类型，源端对象列表为空。")
        return {}
    object_types_clause = ",".join(f"'{obj}'" for obj in sorted(object_types_for_objects)) if object_types_for_objects else ""

    source_objects: SourceObjectMap = defaultdict(set)
    mview_pairs: Set[Tuple[str, str]] = set()
    table_pairs: Set[Tuple[str, str]] = set()
    skipped_iot = 0
    added_synonyms = 0
    added_public_synonyms = 0

    try:
        with oracledb.connect(
            user=ora_cfg['user'],
            password=ora_cfg['password'],
            dsn=ora_cfg['dsn']
        ) as connection:
            log.info("Oracle 连接成功。正在查询源对象列表...")
            if object_types_for_objects:
                sql_tpl = """
                    SELECT OWNER, OBJECT_NAME, OBJECT_TYPE
                    FROM DBA_OBJECTS
                    WHERE OWNER IN ({placeholders})
                      AND OBJECT_TYPE IN (
                          {object_types_clause}
                      )
                """
                with connection.cursor() as cursor:
                    for placeholders, chunk in iter_in_chunks(schemas_list):
                        sql = sql_tpl.format(
                            placeholders=placeholders,
                            object_types_clause=object_types_clause
                        )
                        cursor.execute(sql, chunk)
                        for row in cursor:
                            owner = (row[0] or '').strip().upper()
                            obj_name = (row[1] or '').strip().upper()
                            obj_type = (row[2] or '').strip().upper()
                            if not owner or not obj_name or not obj_type:
                                continue
                            if obj_name.startswith("SYS_IOT_OVER_"):
                                skipped_iot += 1
                                continue
                            full_name = f"{owner}.{obj_name}"
                            source_objects[full_name].add(obj_type)
            if include_synonyms:
                if synonym_scope == "public_only":
                    synonym_owners = ["PUBLIC"]
                else:
                    synonym_owners = sorted(set(s.upper() for s in schemas_list) | {"PUBLIC"})
                target_owners = sorted({s.upper() for s in schemas_list})
                if target_owners:
                    synonym_chunks = chunk_list(synonym_owners, ORACLE_IN_BATCH_SIZE)
                    target_chunks = chunk_list(target_owners, ORACLE_IN_BATCH_SIZE)
                    sql_tpl = """
                        SELECT OWNER, SYNONYM_NAME, TABLE_OWNER, TABLE_NAME
                        FROM DBA_SYNONYMS
                        WHERE OWNER IN ({owner_ph})
                          AND TABLE_OWNER IN ({target_ph})
                          AND TABLE_NAME IS NOT NULL
                    """
                    with connection.cursor() as cursor:
                        for owner_chunk in synonym_chunks:
                            owner_ph = build_bind_placeholders(len(owner_chunk))
                            for target_chunk in target_chunks:
                                target_ph = build_bind_placeholders(len(target_chunk), offset=len(owner_chunk))
                                sql = sql_tpl.format(owner_ph=owner_ph, target_ph=target_ph)
                                cursor.execute(sql, owner_chunk + target_chunk)
                                for row in cursor:
                                    owner = (row[0] or '').strip().upper()
                                    syn_name = (row[1] or '').strip().upper()
                                    table_owner = (row[2] or '').strip().upper()
                                    table_name = (row[3] or '').strip().upper()
                                    if not owner or not syn_name or not table_owner or not table_name:
                                        continue
                                    full_name = f"{owner}.{syn_name}"
                                    source_objects[full_name].add('SYNONYM')
                                    if owner == 'PUBLIC':
                                        added_public_synonyms += 1
                                    else:
                                        added_synonyms += 1
            # 精确认定物化视图集合，避免误删真实表
            with connection.cursor() as cursor:
                for placeholders, chunk in iter_in_chunks(schemas_list):
                    cursor.execute(
                        f"SELECT OWNER, MVIEW_NAME FROM DBA_MVIEWS WHERE OWNER IN ({placeholders})",
                        chunk
                    )
                    for row in cursor:
                        owner = (row[0] or '').strip().upper()
                        name = (row[1] or '').strip().upper()
                        if owner and name:
                            mview_pairs.add((owner, name))
            with connection.cursor() as cursor:
                for placeholders, chunk in iter_in_chunks(schemas_list):
                    cursor.execute(
                        f"SELECT OWNER, TABLE_NAME FROM DBA_TABLES WHERE OWNER IN ({placeholders})",
                        chunk
                    )
                    for row in cursor:
                        owner = (row[0] or '').strip().upper()
                        name = (row[1] or '').strip().upper()
                        if owner and name:
                            if name.startswith("SYS_IOT_OVER_"):
                                skipped_iot += 1
                                continue
                            table_pairs.add((owner, name))
    except oracledb.Error as e:
        log.error(f"严重错误: 连接或查询 Oracle 失败: {e}")
        abort_run()

    # Materialized View 在 DBA_OBJECTS 中通常会同时作为 TABLE 出现，去重以避免误将 MV 当成 TABLE 校验/抽取。
    mview_dedup = 0
    mview_table_keep = 0
    pure_tables = table_pairs - mview_pairs  # DBA_TABLES 也包含 MVIEW，这里只保留真实 TABLE
    for full_name, types in source_objects.items():
        if 'MATERIALIZED VIEW' in types and 'TABLE' in types:
            try:
                owner, name = full_name.split('.', 1)
            except ValueError:
                continue
            key = (owner.upper(), name.upper())
            # 只有确定该对象存在于 DBA_MVIEWS 且不在“纯表”列表时，才移除 TABLE 标记
            if key in mview_pairs and key not in pure_tables:
                types.discard('TABLE')
                mview_dedup += 1
            elif key in pure_tables:
                mview_table_keep += 1
                log.warning(
                    "检测到同名 TABLE 与 MATERIALIZED VIEW，保留 TABLE 校验: %s",
                    full_name
                )
    if mview_dedup:
        log.info(
            "检测到 %d 个 MATERIALIZED VIEW 同时出现在 TABLE 列表中，已按 MVIEW 处理并移除重复 TABLE 类型。",
            mview_dedup
        )

    total_objects = sum(len(types) for types in source_objects.values())
    log.info(
        "从 Oracle 成功获取 %d 个受管对象 (包含主对象与扩展对象)。",
        total_objects
    )
    if include_synonyms:
        log.info(
            "已纳入同义词 %d 个（含 PUBLIC %d 个），仅保留指向 source_schemas 的同义词。",
            added_synonyms + added_public_synonyms,
            added_public_synonyms
        )
    if skipped_iot:
        log.info("已跳过 %d 个 SYS_IOT_OVER_* IOT 表，不参与对比或修补脚本生成。", skipped_iot)
    return dict(source_objects)


# 依附对象到父表的映射类型（触发器/同义词等需要跟随父表 schema）
ObjectParentMap = Dict[str, str]  # {SCHEMA.OBJECT_NAME: SCHEMA.TABLE_NAME}


def get_object_parent_tables(
    ora_cfg: OraConfig,
    schemas_list: List[str],
    enabled_object_types: Optional[Set[str]] = None
) -> ObjectParentMap:
    """
    获取依附对象（TRIGGER/SYNONYM/INDEX/CONSTRAINT 等）所属的父表。
    返回 {SCHEMA.OBJECT_NAME: SCHEMA.TABLE_NAME} 映射：
      - INDEX/CONSTRAINT/SEQUENCE 等依附对象跟随父表 schema
      - TRIGGER 仅用于依赖推导（触发器自身 schema 不随父表 remap）
    """
    parent_map: ObjectParentMap = {}
    enabled_types = {t.upper() for t in (enabled_object_types or set(ALL_TRACKED_OBJECT_TYPES))}
    include_triggers = 'TRIGGER' in enabled_types
    include_synonyms = 'SYNONYM' in enabled_types
    include_indexes = 'INDEX' in enabled_types
    include_constraints = 'CONSTRAINT' in enabled_types
    
    try:
        with oracledb.connect(
            user=ora_cfg['user'],
            password=ora_cfg['password'],
            dsn=ora_cfg['dsn']
        ) as connection:
            # 获取触发器所属的表
            if include_triggers:
                sql_tpl = """
                    SELECT OWNER, TRIGGER_NAME, TABLE_OWNER, TABLE_NAME
                    FROM DBA_TRIGGERS
                    WHERE OWNER IN ({placeholders})
                      AND TABLE_NAME IS NOT NULL
                      AND BASE_OBJECT_TYPE IN ('TABLE', 'VIEW')
                """
                with connection.cursor() as cursor:
                    for placeholders, chunk in iter_in_chunks(schemas_list):
                        sql = sql_tpl.format(placeholders=placeholders)
                        cursor.execute(sql, chunk)
                        for row in cursor:
                            owner = (row[0] or '').strip().upper()
                            trigger_name = (row[1] or '').strip().upper()
                            table_owner = (row[2] or '').strip().upper()
                            table_name = (row[3] or '').strip().upper()
                            if owner and trigger_name and table_owner and table_name:
                                trigger_key = f"{owner}.{trigger_name}"
                                table_key = f"{table_owner}.{table_name}"
                                parent_map[trigger_key] = table_key

            # 获取同义词指向的表/视图，使同义词也能跟随父表 schema
            if include_synonyms:
                sql_tpl = """
                    SELECT OWNER, SYNONYM_NAME, TABLE_OWNER, TABLE_NAME
                    FROM DBA_SYNONYMS
                    WHERE OWNER IN ({placeholders})
                      AND TABLE_OWNER IS NOT NULL
                      AND TABLE_NAME IS NOT NULL
                """
                with connection.cursor() as cursor:
                    for placeholders, chunk in iter_in_chunks(schemas_list):
                        sql = sql_tpl.format(placeholders=placeholders)
                        cursor.execute(sql, chunk)
                        for row in cursor:
                            owner = (row[0] or '').strip().upper()
                            syn_name = (row[1] or '').strip().upper()
                            table_owner = (row[2] or '').strip().upper()
                            table_name = (row[3] or '').strip().upper()
                            if owner and syn_name and table_owner and table_name:
                                syn_key = f"{owner}.{syn_name}"
                                table_key = f"{table_owner}.{table_name}"
                                parent_map[syn_key] = table_key

            # 获取索引所属的表
            if include_indexes:
                sql_tpl = """
                    SELECT OWNER, INDEX_NAME, TABLE_OWNER, TABLE_NAME
                    FROM DBA_INDEXES
                    WHERE OWNER IN ({placeholders})
                      AND TABLE_OWNER IS NOT NULL
                      AND TABLE_NAME IS NOT NULL
                """
                with connection.cursor() as cursor:
                    for placeholders, chunk in iter_in_chunks(schemas_list):
                        sql = sql_tpl.format(placeholders=placeholders)
                        cursor.execute(sql, chunk)
                        for row in cursor:
                            owner = (row[0] or '').strip().upper()
                            index_name = (row[1] or '').strip().upper()
                            table_owner = (row[2] or '').strip().upper()
                            table_name = (row[3] or '').strip().upper()
                            if owner and index_name and table_owner and table_name:
                                index_key = f"{owner}.{index_name}"
                                table_key = f"{table_owner}.{table_name}"
                                parent_map[index_key] = table_key

            # 获取约束所属的表
            if include_constraints:
                sql_tpl = """
                    SELECT OWNER, CONSTRAINT_NAME, TABLE_NAME
                    FROM DBA_CONSTRAINTS
                    WHERE OWNER IN ({placeholders})
                      AND TABLE_NAME IS NOT NULL
                """
                with connection.cursor() as cursor:
                    for placeholders, chunk in iter_in_chunks(schemas_list):
                        sql = sql_tpl.format(placeholders=placeholders)
                        cursor.execute(sql, chunk)
                        for row in cursor:
                            owner = (row[0] or '').strip().upper()
                            cons_name = (row[1] or '').strip().upper()
                            table_name = (row[2] or '').strip().upper()
                            if owner and cons_name and table_name:
                                cons_key = f"{owner}.{cons_name}"
                                table_key = f"{owner}.{table_name}"
                                parent_map[cons_key] = table_key

            log.info("已获取 %d 个依附对象的父表映射（触发器/同义词/索引/约束）。", len(parent_map))
    except oracledb.Error as e:
        log.warning(f"获取依附对象父表映射失败: {e}")
    
    return parent_map


def load_synonym_metadata(
    ora_cfg: OraConfig,
    schemas_list: List[str],
    allowed_target_schemas: Optional[List[str]] = None
) -> Dict[Tuple[str, str], SynonymMeta]:
    """
    快速读取同义词定义，避免逐个 DBMS_METADATA 调用。
    返回 {(OWNER, SYNONYM_NAME): SynonymMeta}
    allowed_target_schemas: 仅保留指向这些 schema 的同义词（用于过滤系统 PUBLIC 同义词等无关对象）。
    """
    if not schemas_list:
        return {}

    allowed_targets = {s.upper() for s in (allowed_target_schemas or [])}
    owners = sorted(set(s.upper() for s in schemas_list) | {"PUBLIC"})
    sql_tpl = """
        SELECT OWNER, SYNONYM_NAME, TABLE_OWNER, TABLE_NAME, DB_LINK
        FROM DBA_SYNONYMS
        WHERE OWNER IN ({owner_ph})
          AND TABLE_OWNER IS NOT NULL
          AND TABLE_NAME IS NOT NULL
          {target_filter}
    """

    result: Dict[Tuple[str, str], SynonymMeta] = {}
    skipped_public = 0
    try:
        with oracledb.connect(
            user=ora_cfg['user'],
            password=ora_cfg['password'],
            dsn=ora_cfg['dsn']
        ) as connection:
            with connection.cursor() as cursor:
                owner_chunks = chunk_list(owners, ORACLE_IN_BATCH_SIZE)
                target_chunks = chunk_list(sorted(allowed_targets), ORACLE_IN_BATCH_SIZE) if allowed_targets else []
                if not allowed_targets:
                    for owner_chunk in owner_chunks:
                        owner_ph = build_bind_placeholders(len(owner_chunk))
                        sql = sql_tpl.format(owner_ph=owner_ph, target_filter="")
                        cursor.execute(sql, owner_chunk)
                        for row in cursor:
                            owner = (row[0] or '').strip().upper()
                            name = (row[1] or '').strip().upper()
                            table_owner = (row[2] or '').strip().upper()
                            table_name = (row[3] or '').strip().upper()
                            db_link = (row[4] or '').strip().upper() if row[4] else None
                            if not owner or not name or not table_name:
                                continue
                            key = (owner, name)
                            result[key] = SynonymMeta(
                                owner=owner,
                                name=name,
                                table_owner=table_owner,
                                table_name=table_name,
                                db_link=db_link
                            )
                else:
                    for owner_chunk in owner_chunks:
                        owner_ph = build_bind_placeholders(len(owner_chunk))
                        for target_chunk in target_chunks:
                            target_ph = build_bind_placeholders(len(target_chunk), offset=len(owner_chunk))
                            target_filter = f"AND TABLE_OWNER IN ({target_ph})"
                            sql = sql_tpl.format(owner_ph=owner_ph, target_filter=target_filter)
                            cursor.execute(sql, owner_chunk + target_chunk)
                            for row in cursor:
                                owner = (row[0] or '').strip().upper()
                                name = (row[1] or '').strip().upper()
                                table_owner = (row[2] or '').strip().upper()
                                table_name = (row[3] or '').strip().upper()
                                db_link = (row[4] or '').strip().upper() if row[4] else None
                                if not owner or not name or not table_name:
                                    continue
                                if owner == 'PUBLIC' and table_owner and table_owner.upper() not in allowed_targets:
                                    skipped_public += 1
                                    continue
                                key = (owner, name)
                                result[key] = SynonymMeta(
                                    owner=owner,
                                    name=name,
                                    table_owner=table_owner,
                                    table_name=table_name,
                                    db_link=db_link
                                )
    except oracledb.Error as exc:
        log.warning("读取同义词元数据失败，将回退 DBMS_METADATA：%s", exc)

    target_hint = ",".join(sorted(allowed_targets)) if allowed_targets else "<ALL>"
    log.info(
        "已缓存 %d 个同义词元数据（OWNER IN %s，TABLE_OWNER IN %s）。",
        len(result),
        ",".join(owners),
        target_hint
    )
    return result


def validate_remap_rules(
    remap_rules: RemapRules,
    source_objects: SourceObjectMap,
    remap_file_path: Optional[str] = None
) -> List[str]:
    """检查 remap 规则中的源对象是否存在于 Oracle source_objects 中，并清洗无效条目。"""
    log.info("正在验证 Remap 规则...")
    remap_keys = set(remap_rules.keys())
    source_keys = set(source_objects.keys())
    body_aliases = {
        f"{name} BODY"
        for name, obj_types in source_objects.items()
        if any(obj_type.upper() in ('PACKAGE BODY', 'TYPE BODY') for obj_type in obj_types)
    }
    source_keys_with_alias = source_keys | body_aliases

    extraneous_keys = sorted(list(remap_keys - source_keys_with_alias))

    if extraneous_keys:
        log.warning(f"  [规则警告] 在 remap_rules.txt 中发现了 {len(extraneous_keys)} 个无效的源对象。")
        log.warning("  (这些对象在源端 Oracle (config.ini 中配置的 schema) 中未找到)")
        for key in extraneous_keys:
            log.warning(f"    - 无效条目: {key}")
        # 将无效规则另存，不修改原始 remap 文件
        if remap_file_path:
            remap_path = Path(remap_file_path).expanduser()
            try:
                raw_lines = remap_path.read_text(encoding="utf-8").splitlines()
            except OSError as exc:
                log.warning("  [规则警告] 无法读取 remap 文件以清洗无效条目: %s", exc)
            else:
                removed: List[str] = []
                extra_set = set(extraneous_keys)
                for line in raw_lines:
                    stripped = line.strip()
                    if not stripped or stripped.startswith("#") or "=" not in stripped:
                        continue
                    src_part = stripped.split("=", 1)[0].strip().upper()
                    if src_part in extra_set:
                        removed.append(line)

                if removed:
                    invalid_path = remap_path.with_name(
                        f"{remap_path.stem}_invalid{remap_path.suffix or '.txt'}"
                    )
                    try:
                        invalid_path.write_text("\n".join(removed) + "\n", encoding="utf-8")
                    except OSError as exc:
                        log.warning("  [规则警告] 写入无效 remap 条目文件失败: %s", exc)
                    else:
                        log.warning(
                            "  [规则警告] 检出 %d 条无效 remap 规则并保存到: %s (原 remap_rules 未修改)",
                            len(removed),
                            invalid_path
                        )
    else:
        log.info("Remap 规则验证通过，所有规则中的源对象均存在。")

    # 从内存映射中移除无效规则，避免后续继续使用
    for key in extraneous_keys:
        remap_rules.pop(key, None)

    return extraneous_keys


def strip_body_suffix(name: str) -> str:
    text = name.rstrip()
    if text.upper().endswith(' BODY'):
        return text[:-5].rstrip()
    return text


def derive_schema_mapping_from_rules(remap_rules: RemapRules) -> Dict[str, str]:
    """
    基于 remap_rules 推导 schema 级别的映射：
      如果某个源 schema 只映射到唯一的目标 schema，则作为默认映射；
      避免多对多/多对一的模糊情况。
    """
    schema_targets: Dict[str, Set[str]] = defaultdict(set)
    for src_full, tgt_full in remap_rules.items():
        if '.' not in src_full or '.' not in tgt_full:
            continue
        src_schema, _ = src_full.split('.', 1)
        tgt_schema, _ = tgt_full.split('.', 1)
        schema_targets[src_schema.upper()].add(tgt_schema.upper())

    schema_mapping: Dict[str, str] = {}
    for src_schema, tgt_set in schema_targets.items():
        if len(tgt_set) == 1:
            schema_mapping[src_schema] = next(iter(tgt_set))
    return schema_mapping


def infer_dominant_schema_from_rules(
    remap_rules: RemapRules,
    src_schema: str,
    source_objects: Optional[SourceObjectMap] = None
) -> Optional[str]:
    """
    基于 remap_rules 中属于同一源 schema 的表映射，推导出现次数最多的目标 schema。
    在 remap_rules 只有 TABLE 映射的场景下，可用于让依附对象（如 SEQUENCE/SYNONYM）
    跟随父表的主流目标 schema。
    """
    src_schema_u = src_schema.upper()
    counts: Dict[str, int] = defaultdict(int)
    table_keys: Optional[Set[str]] = None
    explicit_tables: Set[str] = set()
    if source_objects:
        table_keys = {
            name.upper()
            for name, types in source_objects.items()
            if any(t.upper() == 'TABLE' for t in types)
        }
    for src_full, tgt_full in remap_rules.items():
        if '.' not in src_full or '.' not in tgt_full:
            continue
        src_full_u = src_full.upper()
        if table_keys is not None and src_full_u not in table_keys:
            continue
        explicit_tables.add(src_full_u)
        s_schema, _ = src_full_u.split('.', 1)
        if s_schema != src_schema_u:
            continue
        t_schema, _ = tgt_full.split('.', 1)
        counts[t_schema.upper()] += 1
    if table_keys is not None:
        # Treat tables missing in remap_rules as 1:1 mappings to their source schema.
        for src_full_u in table_keys:
            if src_full_u in explicit_tables:
                continue
            s_schema, _ = src_full_u.split('.', 1)
            if s_schema != src_schema_u:
                continue
            counts[s_schema] += 1
    if not counts:
        return None
    max_count = max(counts.values())
    candidates = [schema for schema, c in counts.items() if c == max_count]
    if len(candidates) == 1:
        return candidates[0]
    return None


def infer_sequence_target_schema_from_dependents(
    seq_full: str,
    remap_rules: RemapRules,
    source_objects: Optional[SourceObjectMap] = None,
    schema_mapping: Optional[Dict[str, str]] = None,
    object_parent_map: Optional[ObjectParentMap] = None,
    dependency_graph: Optional[DependencyGraph] = None,
    transitive_table_cache: Optional[TransitiveTableCache] = None,
    source_dependencies: Optional[SourceDependencySet] = None,
    remap_conflicts: Optional[RemapConflictMap] = None,
    sequence_remap_policy: str = "source_only",
    _path: Optional[Set[Tuple[str, str]]] = None
) -> Tuple[Optional[str], bool]:
    """
    基于引用该 SEQUENCE 的对象推导目标 schema。

    返回 (schema, conflict):
      - schema: 推导结果（仅 schema）
      - conflict: 若引用对象 remap 到多个 schema，标记冲突，避免继续回退推导
    """
    if '.' not in seq_full or not source_dependencies:
        return None, False

    seq_full_u = seq_full.upper()
    dependents: List[Tuple[str, str, str]] = []
    for dep_owner, dep_name, dep_type, ref_owner, ref_name, ref_type in source_dependencies:
        if (ref_type or "").upper() != 'SEQUENCE':
            continue
        ref_full = f"{ref_owner}.{ref_name}".upper()
        if ref_full != seq_full_u:
            continue
        dep_full = f"{dep_owner}.{dep_name}".upper()
        dep_type_u = (dep_type or "").upper()
        if dep_full and dep_type_u:
            dependents.append((dep_full, dep_type_u, dep_owner))

    if not dependents:
        return None, False

    remapped_targets: Set[str] = set()
    for dep_full, dep_type_u, dep_owner in dependents:
        if dep_type_u == 'TRIGGER' and object_parent_map:
            parent_table = object_parent_map.get(dep_full.upper())
            if parent_table:
                dep_target = resolve_remap_target(
                    parent_table,
                    'TABLE',
                    remap_rules,
                    source_objects=source_objects,
                    schema_mapping=schema_mapping,
                    object_parent_map=object_parent_map,
                    dependency_graph=dependency_graph,
                    transitive_table_cache=transitive_table_cache,
                    source_dependencies=source_dependencies,
                    remap_conflicts=remap_conflicts,
                    sequence_remap_policy=sequence_remap_policy,
                    _path=_path
                ) or parent_table
            else:
                dep_target = dep_full
        else:
            dep_target = resolve_remap_target(
                dep_full,
                dep_type_u,
                remap_rules,
                source_objects=source_objects,
                schema_mapping=schema_mapping,
                object_parent_map=object_parent_map,
                dependency_graph=dependency_graph,
                transitive_table_cache=transitive_table_cache,
                source_dependencies=source_dependencies,
                remap_conflicts=remap_conflicts,
                sequence_remap_policy=sequence_remap_policy,
                _path=_path
            ) or dep_full
        if dep_target is None:
            if remap_conflicts and (dep_full.upper(), dep_type_u) in remap_conflicts:
                return None, True
            dep_target = dep_full
        if '.' not in dep_target:
            continue
        tgt_schema = dep_target.split('.', 1)[0].upper()
        if tgt_schema != dep_owner.upper():
            remapped_targets.add(tgt_schema)

    if remapped_targets:
        if len(remapped_targets) == 1:
            inferred_schema = next(iter(remapped_targets))
            log.debug(
                "[推导] SEQUENCE %s 被 remap 对象引用，推导目标 schema: %s",
                seq_full_u, inferred_schema
            )
            return inferred_schema, False
        log.warning(
            "[推导] SEQUENCE %s 被多个 remap schema 引用 (%s)，无法自动推导，请显式配置 remap。",
            seq_full_u, sorted(remapped_targets)
        )
        return None, True

    return None, False


def infer_target_schema_from_direct_dependencies(
    src_name: str,
    obj_type: str,
    remap_rules: RemapRules,
    source_objects: Optional[SourceObjectMap] = None,
    schema_mapping: Optional[Dict[str, str]] = None,
    object_parent_map: Optional[ObjectParentMap] = None,
    dependency_graph: Optional[DependencyGraph] = None,
    transitive_table_cache: Optional[TransitiveTableCache] = None,
    source_dependencies: Optional[SourceDependencySet] = None,
    remap_conflicts: Optional[RemapConflictMap] = None,
    sequence_remap_policy: str = "source_only",
    *,
    ignore_public_synonyms: bool = True,
    _path: Optional[Set[Tuple[str, str]]] = None
) -> Tuple[Optional[str], bool]:
    """
    基于对象的“直接引用”推导目标 schema（不限于 TABLE/MVIEW）。

    Returns:
        (schema, conflict)
        - schema: 唯一目标 schema
        - conflict: 若引用对象映射到多个 schema，标记冲突
    """
    if not source_dependencies or '.' not in src_name:
        return None, False

    src_name_u = src_name.upper()
    obj_type_u = (obj_type or "").upper()
    candidate_types = {obj_type_u}
    if obj_type_u in ('PACKAGE BODY', 'TYPE BODY'):
        candidate_types.add(obj_type_u.replace(' BODY', ''))

    remapped_targets: Set[str] = set()
    for dep_owner, dep_name, dep_type, ref_owner, ref_name, ref_type in source_dependencies:
        dep_full = f"{dep_owner}.{dep_name}".upper()
        dep_type_u = (dep_type or "").upper()
        if dep_full != src_name_u or dep_type_u not in candidate_types:
            continue
        ref_owner_u = (ref_owner or "").upper()
        ref_type_u = (ref_type or "").upper()
        if ignore_public_synonyms and ref_type_u == 'SYNONYM' and ref_owner_u == 'PUBLIC':
            continue
        ref_full = f"{ref_owner}.{ref_name}".upper()
        ref_target = resolve_remap_target(
            ref_full,
            ref_type_u,
            remap_rules,
            source_objects=source_objects,
            schema_mapping=schema_mapping,
            object_parent_map=object_parent_map,
            dependency_graph=dependency_graph,
            transitive_table_cache=transitive_table_cache,
            source_dependencies=source_dependencies,
            remap_conflicts=remap_conflicts,
            sequence_remap_policy=sequence_remap_policy,
            _path=_path
        )
        if ref_target is None:
            if remap_conflicts and (ref_full.upper(), ref_type_u) in remap_conflicts:
                return None, True
            ref_target = ref_full
        if '.' not in ref_target:
            continue
        tgt_schema = ref_target.split('.', 1)[0].upper()
        if tgt_schema != ref_owner_u:
            remapped_targets.add(tgt_schema)

    if remapped_targets:
        if len(remapped_targets) == 1:
            inferred_schema = next(iter(remapped_targets))
            log.debug(
                "[推导] %s (%s) 直接引用对象映射到 %s -> 推导目标 schema: %s",
                src_name_u, obj_type_u, inferred_schema, inferred_schema
            )
            return inferred_schema, False
        log.warning(
            "[推导] %s (%s) 直接引用对象映射到多个 schema (%s)，无法自动推导，请显式配置 remap。",
            src_name_u, obj_type_u, sorted(remapped_targets)
        )
        return None, True

    return None, False


def build_dependency_graph(source_dependencies: Optional[SourceDependencySet]) -> DependencyGraph:
    """
    将源端依赖集合构建为依赖图：
      (DEP_FULL, DEP_TYPE) -> {(REF_FULL, REF_TYPE), ...}
    """
    graph: Dict[DependencyNode, Set[DependencyNode]] = defaultdict(set)
    if not source_dependencies:
        return {}
    for dep_owner, dep_name, dep_type, ref_owner, ref_name, ref_type in source_dependencies:
        dep_full = f"{dep_owner}.{dep_name}".upper()
        ref_full = f"{ref_owner}.{ref_name}".upper()
        dep_type_u = (dep_type or "").upper()
        ref_type_u = (ref_type or "").upper()
        if not dep_full or not ref_full or not dep_type_u or not ref_type_u:
            continue
        graph[(dep_full, dep_type_u)].add((ref_full, ref_type_u))
    return dict(graph)


def build_view_dependency_map(
    source_dependencies: Optional[SourceDependencySet]
) -> Dict[Tuple[str, str], Set[str]]:
    """
    预计算 VIEW -> 依赖对象集合，用于 VIEW DDL 重写的 fallback。
    返回 {(VIEW_OWNER, VIEW_NAME): {REF_OWNER.REF_NAME, ...}}
    """
    if not source_dependencies:
        return {}
    allowed_refs = {"TABLE", "VIEW", "MATERIALIZED VIEW", "SYNONYM", "FUNCTION", "PACKAGE"}
    view_map: Dict[Tuple[str, str], Set[str]] = defaultdict(set)
    for dep_owner, dep_name, dep_type, ref_owner, ref_name, ref_type in source_dependencies:
        if (dep_type or "").upper() != "VIEW":
            continue
        ref_type_u = (ref_type or "").upper()
        if ref_type_u not in allowed_refs:
            continue
        owner_u = (dep_owner or "").upper()
        name_u = (dep_name or "").upper()
        ref_owner_u = (ref_owner or "").upper()
        ref_name_u = (ref_name or "").upper()
        if not owner_u or not name_u or not ref_owner_u or not ref_name_u:
            continue
        view_map[(owner_u, name_u)].add(f"{ref_owner_u}.{ref_name_u}")
    return dict(view_map)


def precompute_transitive_table_cache(
    dependency_graph: DependencyGraph,
    *,
    object_parent_map: Optional[ObjectParentMap] = None
) -> TransitiveTableCache:
    """
    基于依赖图一次性预计算所有节点“最终引用的 TABLE/MVIEW 集合”，用于一对多 remap 推导。

    使用自底向上的单调传播（反向边 + 队列），避免每个对象重复 DFS：
      transitive(node) = direct_tables(node) ∪ ⋃ transitive(child)

    返回:
      {(OWNER.OBJ, TYPE): {TABLE_OWNER.TABLE, ...}, ...}
    """
    if not dependency_graph:
        return {}

    from collections import deque

    reverse_graph: Dict[DependencyNode, Set[DependencyNode]] = defaultdict(set)
    transitive: Dict[DependencyNode, Set[str]] = defaultdict(set)
    full_to_nodes: Dict[str, List[DependencyNode]] = defaultdict(list)

    # 先收集所有节点（包括仅被引用的节点），构建反向图
    for dep_node, refs in dependency_graph.items():
        full_to_nodes[dep_node[0].upper()].append(dep_node)
        if dep_node not in transitive:
            transitive[dep_node] = set()
        for ref_node in refs:
            reverse_graph[ref_node].add(dep_node)
            full_to_nodes[ref_node[0].upper()].append(ref_node)
            if ref_node not in transitive:
                transitive[ref_node] = set()

    # 直接引用的 TABLE/MVIEW
    for dep_node, refs in dependency_graph.items():
        for ref_full, ref_type in refs:
            ref_type_u = (ref_type or "").upper()
            if ref_type_u in ("TABLE", "MATERIALIZED VIEW"):
                transitive[dep_node].add(ref_full.upper())

    # 依附对象的父表直接视为引用
    if object_parent_map:
        for dep_full, parent_full in object_parent_map.items():
            if not parent_full or "." not in parent_full:
                continue
            parent_full_u = parent_full.upper()
            for node in full_to_nodes.get(dep_full.upper(), []):
                transitive[node].add(parent_full_u)

    # 队列传播（只在新增时继续向上游推送）
    queue = deque([n for n, tbls in transitive.items() if tbls])
    while queue:
        node = queue.popleft()
        tables_here = transitive.get(node)
        if not tables_here:
            continue
        for dep_node in reverse_graph.get(node, set()):
            existing = transitive[dep_node]
            new_tables = tables_here - existing
            if not new_tables:
                continue
            existing.update(new_tables)
            queue.append(dep_node)

    return dict(transitive)


def collect_transitive_referenced_tables(
    start_full: str,
    start_type: str,
    dependency_graph: DependencyGraph,
    *,
    object_parent_map: Optional[ObjectParentMap] = None
) -> Set[str]:
    """
    沿依赖图递归下探，收集 start 对象最终引用的 TABLE/MVIEW。
    对于 SYNONYM/TRIGGER 等依附对象，会额外通过 object_parent_map 下探到父表。
    返回集合元素为 OWNER.OBJ（大写）。
    """
    if not dependency_graph or '.' not in start_full:
        return set()

    start_node: DependencyNode = (start_full.upper(), (start_type or "").upper())
    visited: Set[DependencyNode] = set()
    tables: Set[str] = set()
    stack: List[DependencyNode] = [start_node]

    while stack:
        node = stack.pop()
        if node in visited:
            continue
        visited.add(node)
        full_name, obj_type_u = node

        if object_parent_map:
            parent = object_parent_map.get(full_name.upper())
            if parent and '.' in parent:
                tables.add(parent.upper())

        for ref_full, ref_type_u in dependency_graph.get(node, set()):
            ref_full_u = ref_full.upper()
            ref_type_upper = ref_type_u.upper()
            if ref_type_upper in ('TABLE', 'MATERIALIZED VIEW'):
                tables.add(ref_full_u)
                continue
            stack.append((ref_full_u, ref_type_upper))

    return tables


def infer_target_schema_from_dependencies(
    src_name: str,
    obj_type: str,
    remap_rules: RemapRules,
    dependency_graph: Optional[DependencyGraph] = None,
    object_parent_map: Optional[ObjectParentMap] = None,
    transitive_table_cache: Optional[TransitiveTableCache] = None
) -> Tuple[Optional[str], bool]:
    """
    基于对象的依赖关系递归推导目标 schema（用于 one-to-many 场景）。

    逻辑：
    1. 沿依赖图递归下探，收集该对象（含嵌套依赖）最终引用的 TABLE/MVIEW
    2. 对每张表取 remap 目标（未显式 remap 的表视为 1:1）
    3. 统计目标 schema 出现次数，唯一最多者作为推导结果

    Returns:
        (target_full, conflict)
        - target_full: 推导出的目标全名 "TGT_SCHEMA.OBJ"；无法推导则返回 None
        - conflict: 是否发生多 schema 冲突
    """
    if '.' not in src_name or not dependency_graph:
        return None, False

    src_name_u = src_name.upper()
    src_schema, src_obj = src_name_u.split('.', 1)

    node: DependencyNode = (src_name_u, (obj_type or "").upper())
    referenced_tables: Set[str] = set()
    if transitive_table_cache is not None:
        cached = transitive_table_cache.get(node)
        if cached:
            referenced_tables = set(cached)
    if not referenced_tables:
        referenced_tables = collect_transitive_referenced_tables(
            src_name_u,
            obj_type,
            dependency_graph,
            object_parent_map=object_parent_map
        )
    if not referenced_tables:
        return None, False

    target_schema_counts: Dict[str, int] = defaultdict(int)
    for table_full in referenced_tables:
        table_full_u = table_full.upper()
        table_target = remap_rules.get(table_full_u) or table_full_u
        if '.' in table_target:
            tgt_schema = table_target.split('.', 1)[0].upper()
        else:
            tgt_schema = table_full_u.split('.', 1)[0].upper()
        target_schema_counts[tgt_schema] += 1

    if not target_schema_counts:
        return None, False

    max_count = max(target_schema_counts.values())
    candidate_schemas = [s for s, c in target_schema_counts.items() if c == max_count]

    if len(candidate_schemas) == 1:
        inferred_schema = candidate_schemas[0]
        log.debug(
            "[推导] %s (%s) 递归引用 %d 个表/MVIEW，其中 %d 个在 %s -> 推导目标: %s.%s",
            src_name_u, obj_type, len(referenced_tables), max_count, inferred_schema, inferred_schema, src_obj
        )
        return f"{inferred_schema}.{src_obj}", False

    log.debug(
        "[推导] %s (%s) 引用的表/MVIEW 分散在多个 schema，无法推导: %s",
        src_name_u, obj_type, candidate_schemas
    )
    return None, True


def resolve_remap_target(
    src_name: str,
    obj_type: str,
    remap_rules: RemapRules,
    source_objects: Optional[SourceObjectMap] = None,
    schema_mapping: Optional[Dict[str, str]] = None,
    object_parent_map: Optional[ObjectParentMap] = None,
    dependency_graph: Optional[DependencyGraph] = None,
    transitive_table_cache: Optional[TransitiveTableCache] = None,
    source_dependencies: Optional[SourceDependencySet] = None,
    remap_conflicts: Optional[RemapConflictMap] = None,
    sequence_remap_policy: str = "source_only",
    _path: Optional[Set[Tuple[str, str]]] = None
) -> Optional[str]:
    """
    解析对象的 remap 目标：
    1. 优先查找 remap_rules 中的显式规则
    2. 对于依附对象（INDEX/CONSTRAINT/SEQUENCE/SYNONYM），使用父表的 remap 目标 schema
    3. 对于独立对象，尝试基于依赖分析推导（一对多场景）
    4. 对于其他非 TABLE 对象，尝试使用 schema_mapping（多对一、一对一场景）
    
    object_parent_map: 依附对象到父表的映射
    source_dependencies: 源端依赖关系，用于智能推导
    """
    obj_type_u = obj_type.upper()
    src_name_u = src_name.upper()
    sequence_policy = (sequence_remap_policy or "source_only").lower()
    path = _path if _path is not None else set()
    node = (src_name_u, obj_type_u)
    if node in path:
        return None
    path.add(node)
    def _record_conflict(reason: str) -> None:
        if remap_conflicts is None:
            return
        key = (src_name_u, obj_type_u)
        if key not in remap_conflicts:
            remap_conflicts[key] = reason

    try:
        candidate_keys: List[str] = [src_name]
        if obj_type_u in ('PACKAGE BODY', 'TYPE BODY'):
            candidate_keys.insert(0, f"{src_name} BODY")
        for key in candidate_keys:
            if key in remap_rules:
                tgt = remap_rules[key].strip()
                if obj_type_u in ('PACKAGE BODY', 'TYPE BODY'):
                    return strip_body_suffix(tgt)
                return tgt

        if obj_type_u == 'SEQUENCE' and sequence_policy == "source_only":
            return src_name_u

        if obj_type_u in NO_INFER_SCHEMA_TYPES:
            return src_name_u

        if obj_type_u == 'SYNONYM' and '.' in src_name:
            src_schema, src_obj = src_name.split('.', 1)
            if src_schema.upper() == 'PUBLIC':
                return f"PUBLIC.{src_obj.upper()}"

        if obj_type_u == 'SYNONYM' and '.' in src_name:
            src_schema, src_obj = src_name.split('.', 1)
            inferred_schema, conflict = infer_target_schema_from_direct_dependencies(
                src_name,
                obj_type,
                remap_rules,
                source_objects=source_objects,
                schema_mapping=schema_mapping,
                object_parent_map=object_parent_map,
                dependency_graph=dependency_graph,
                transitive_table_cache=transitive_table_cache,
                source_dependencies=source_dependencies,
                remap_conflicts=remap_conflicts,
                sequence_remap_policy=sequence_remap_policy,
                _path=path
            )
            if inferred_schema:
                return f"{inferred_schema}.{src_obj}"
            if conflict:
                _record_conflict("同义词直接依赖映射到多个 schema，无法自动推导")
                return None

        # 对于依附对象（INDEX/CONSTRAINT/SEQUENCE），使用父表的 remap 目标 schema
        # SYNONYM 保持自身 schema（避免跨 schema 同义词被错误合并到父表 schema）
        if '.' in src_name and object_parent_map and obj_type_u in ('INDEX', 'CONSTRAINT', 'SEQUENCE'):
            parent_table = object_parent_map.get(src_name.upper())
            if parent_table:
                parent_target = remap_rules.get(parent_table.upper())
                if not parent_target and schema_mapping and '.' in parent_table:
                    parent_schema, parent_obj = parent_table.split('.', 1)
                    mapped_schema = schema_mapping.get(parent_schema.upper())
                    if mapped_schema:
                        parent_target = f"{mapped_schema.upper()}.{parent_obj}"
                if not parent_target and obj_type_u == 'SYNONYM' and source_objects:
                    parent_types = {t.upper() for t in source_objects.get(parent_table.upper(), set())}
                    preferred_types = (
                        'TABLE', 'VIEW', 'MATERIALIZED VIEW', 'SEQUENCE',
                        'SYNONYM', 'FUNCTION', 'PROCEDURE', 'PACKAGE',
                        'TYPE', 'TRIGGER'
                    )
                    for parent_type in preferred_types:
                        if parent_type not in parent_types:
                            continue
                        parent_target = resolve_remap_target(
                            parent_table.upper(),
                            parent_type,
                            remap_rules,
                            source_objects=source_objects,
                            schema_mapping=schema_mapping,
                            object_parent_map=object_parent_map,
                            dependency_graph=dependency_graph,
                            transitive_table_cache=transitive_table_cache,
                            source_dependencies=source_dependencies,
                            sequence_remap_policy=sequence_policy,
                            _path=path
                        )
                        if parent_target:
                            break
                if parent_target:
                    tgt_schema = parent_target.split('.', 1)[0].upper()
                    src_obj = src_name.split('.', 1)[1]
                    return f"{tgt_schema}.{src_obj}"

        # 对于 SEQUENCE，优先根据依赖对象的 remap 结果推导
        if '.' in src_name and obj_type_u == 'SEQUENCE' and sequence_policy == "infer":
            src_schema, src_obj = src_name.split('.', 1)
            inferred_schema, conflict = infer_sequence_target_schema_from_dependents(
                src_name,
                remap_rules,
                source_objects=source_objects,
                schema_mapping=schema_mapping,
                object_parent_map=object_parent_map,
                dependency_graph=dependency_graph,
                transitive_table_cache=transitive_table_cache,
                source_dependencies=source_dependencies,
                remap_conflicts=remap_conflicts,
                sequence_remap_policy=sequence_policy,
                _path=path
            )
            if inferred_schema:
                return f"{inferred_schema}.{src_obj}"
            if conflict:
                _record_conflict("SEQUENCE 被多个 remap schema 引用，无法自动推导")
                return None

        # 已处理 SEQUENCE 的依赖推导，后续不再进行通用直接依赖推导

        # 针对 SEQUENCE / SYNONYM，在 remap_rules 仅包含 TABLE 映射时，使用该 schema 的主流目标 schema
        if '.' in src_name and obj_type_u in ('SEQUENCE', 'SYNONYM'):
            if obj_type_u == 'SEQUENCE' and sequence_policy == "source_only":
                return src_name_u
            src_schema, src_obj = src_name.split('.', 1)
            dominant_schema = infer_dominant_schema_from_rules(remap_rules, src_schema, source_objects)
            if dominant_schema:
                return f"{dominant_schema}.{src_obj}"

        # 对于独立对象，优先基于直接依赖对象的 remap 推导（不限于 TABLE/MVIEW）
        if '.' in src_name and obj_type_u not in ('TABLE', 'SEQUENCE', 'SYNONYM'):
            src_schema, src_obj = src_name.split('.', 1)
            inferred_schema, conflict = infer_target_schema_from_direct_dependencies(
                src_name,
                obj_type,
                remap_rules,
                source_objects=source_objects,
                schema_mapping=schema_mapping,
                object_parent_map=object_parent_map,
                dependency_graph=dependency_graph,
                transitive_table_cache=transitive_table_cache,
                source_dependencies=source_dependencies,
                remap_conflicts=remap_conflicts,
                sequence_remap_policy=sequence_remap_policy,
                _path=path
            )
            if inferred_schema:
                return f"{inferred_schema}.{src_obj}"
            if conflict:
                _record_conflict("直接依赖映射到多个 schema，无法自动推导")
                return None

        # 对于独立对象（VIEW/PROCEDURE/FUNCTION/PACKAGE等），尝试基于依赖分析递归推导
        if '.' in src_name and obj_type_u != 'TABLE':
            dep_graph = dependency_graph
            if dep_graph is None and source_dependencies:
                dep_graph = build_dependency_graph(source_dependencies)
            if dep_graph:
                inferred, conflict = infer_target_schema_from_dependencies(
                    src_name,
                    obj_type,
                    remap_rules,
                    dep_graph,
                    object_parent_map=object_parent_map,
                    transitive_table_cache=transitive_table_cache
                )
                if inferred:
                    return inferred
                if conflict:
                    _record_conflict("递归依赖映射到多个 schema，无法自动推导")
                    return None
            
            # 回退到schema映射推导（适用于多对一、一对一场景）
            src_schema, src_obj = src_name.split('.', 1)
            src_schema_u = src_schema.upper()
            
            if schema_mapping:
                tgt_schema = schema_mapping.get(src_schema_u)
                if tgt_schema:
                    return f"{tgt_schema}.{src_obj}"

        return None
    finally:
        path.remove(node)


def generate_master_list(
    source_objects: SourceObjectMap,
    remap_rules: RemapRules,
    enabled_primary_types: Optional[Set[str]] = None,
    schema_mapping: Optional[Dict[str, str]] = None,
    precomputed_mapping: Optional[FullObjectMapping] = None,
    object_parent_map: Optional[ObjectParentMap] = None,
    transitive_table_cache: Optional[TransitiveTableCache] = None,
    source_dependencies: Optional[SourceDependencySet] = None,
    dependency_graph: Optional[DependencyGraph] = None,
    remap_conflicts: Optional[RemapConflictMap] = None,
    sequence_remap_policy: str = "source_only"
) -> MasterCheckList:
    """
    生成“最终校验清单”并检测 "多对一" 映射。
    仅保留 PRIMARY_OBJECT_TYPES 中的主对象，用于主校验。
    """
    log.info("正在生成主校验清单 (应用 Remap 规则)...")
    master_list: MasterCheckList = []

    target_tracker: Dict[Tuple[str, str], str] = {}

    allowed_primary = enabled_primary_types or set(PRIMARY_OBJECT_TYPES)

    for src_name, obj_types in source_objects.items():
        src_name_u = src_name.upper()
        for obj_type in sorted(obj_types):
            obj_type_u = obj_type.upper()
            if obj_type_u not in allowed_primary:
                continue
            if remap_conflicts and (src_name_u, obj_type_u) in remap_conflicts:
                continue

            if precomputed_mapping and src_name_u in precomputed_mapping:
                tgt_name = precomputed_mapping[src_name_u].get(obj_type_u, src_name_u)
            else:
                tgt_name = resolve_remap_target(
                    src_name_u,
                    obj_type_u,
                    remap_rules,
                    source_objects=source_objects,
                    schema_mapping=schema_mapping,
                    object_parent_map=object_parent_map,
                    dependency_graph=dependency_graph,
                    transitive_table_cache=transitive_table_cache,
                    source_dependencies=source_dependencies,
                    remap_conflicts=remap_conflicts,
                    sequence_remap_policy=sequence_remap_policy
                )
                if remap_conflicts and (src_name_u, obj_type_u) in remap_conflicts:
                    continue
                tgt_name = tgt_name or src_name_u
            tgt_name_u = tgt_name.upper()

            key = (tgt_name_u, obj_type_u)
            if key in target_tracker:
                existing_src = target_tracker[key]
                if existing_src != src_name_u:
                    log.warning(
                        "检测到多对一映射: 目标 %s (类型 %s) 已由 %s 映射，当前 %s 将回退为 1:1 映射。",
                        tgt_name_u, obj_type_u, existing_src, src_name_u
                    )
                    tgt_name_u = src_name_u
                    tgt_name = src_name_u
                    key = (tgt_name_u, obj_type_u)

            target_tracker[key] = src_name_u
            master_list.append((src_name_u, tgt_name_u, obj_type_u))

    log.info(f"主校验清单生成完毕，共 {len(master_list)} 个待校验项。")
    return master_list


def build_full_object_mapping(
    source_objects: SourceObjectMap,
    remap_rules: RemapRules,
    schema_mapping: Optional[Dict[str, str]] = None,
    object_parent_map: Optional[ObjectParentMap] = None,
    transitive_table_cache: Optional[TransitiveTableCache] = None,
    source_dependencies: Optional[SourceDependencySet] = None,
    dependency_graph: Optional[DependencyGraph] = None,
    enabled_types: Optional[Set[str]] = None,
    remap_conflicts: Optional[RemapConflictMap] = None,
    sequence_remap_policy: str = "source_only"
) -> FullObjectMapping:
    """
    为所有受管对象建立映射 (源 -> 目标)。
    返回 {'SRC.OBJ': {'TYPE': 'TGT.OBJ'}}
    
    object_parent_map: 依附对象到父表的映射，用于 one-to-many schema 拆分场景
    source_dependencies: 源端依赖关系，用于智能推导目标schema
    enabled_types: 若提供，仅处理这些对象类型
    """
    def _enforce_paired_objects_same_target(
        src_full: str,
        type_map: Dict[str, str]
    ) -> None:
        """
        PACKAGE/PACKAGE BODY、TYPE/TYPE BODY 必须保持同 schema 同名。
        若推导结果不一致：
          - 若 PACKAGE/TYPE 有显式 remap 规则，优先显式规则；
          - 否则若 BODY 有显式规则，优先 BODY；
          - 否则优先 BODY 的推导（通常更依赖真实表）。
        """

        def _fix_pair(primary: str, body: str) -> None:
            if primary not in type_map or body not in type_map:
                return
            prim_tgt = type_map.get(primary)
            body_tgt = type_map.get(body)
            if not prim_tgt or not body_tgt or prim_tgt.upper() == body_tgt.upper():
                return

            explicit_primary = src_full in remap_rules
            explicit_body = explicit_primary or f"{src_full} BODY" in remap_rules

            if explicit_primary:
                chosen = prim_tgt
            elif explicit_body:
                chosen = body_tgt
            else:
                chosen = body_tgt or prim_tgt

            log.warning(
                "检测到 %s/%s 推导目标不一致 (%s vs %s)，已强制统一为 %s。",
                primary, body, prim_tgt, body_tgt, chosen
            )
            type_map[primary] = chosen.upper()
            type_map[body] = chosen.upper()

        _fix_pair("PACKAGE", "PACKAGE BODY")
        _fix_pair("TYPE", "TYPE BODY")

    mapping: FullObjectMapping = {}
    target_tracker: Dict[Tuple[str, str], str] = {}

    enabled_types_u = {t.upper() for t in enabled_types} if enabled_types else None
    conflict_map = remap_conflicts

    for src_name, obj_types in source_objects.items():
        src_name_u = src_name.upper()
        local_map: Dict[str, str] = {}

        # 先为该对象的所有类型计算目标
        for obj_type in sorted(obj_types):
            obj_type_u = obj_type.upper()
            if enabled_types_u and obj_type_u not in enabled_types_u:
                continue
            tgt_name = resolve_remap_target(
                src_name_u,
                obj_type_u,
                remap_rules,
                source_objects=source_objects,
                schema_mapping=schema_mapping,
                object_parent_map=object_parent_map,
                dependency_graph=dependency_graph,
                transitive_table_cache=transitive_table_cache,
                source_dependencies=source_dependencies,
                remap_conflicts=conflict_map,
                sequence_remap_policy=sequence_remap_policy
            )
            if tgt_name is None:
                if conflict_map and (src_name_u, obj_type_u) in conflict_map:
                    continue
                tgt_name = src_name_u
            local_map[obj_type_u] = tgt_name.upper()

        if not local_map:
            continue

        # 强制配对对象统一目标
        _enforce_paired_objects_same_target(src_name_u, local_map)

        # 多对一冲突检测：配对对象需整体回退
        paired_groups = [("PACKAGE", "PACKAGE BODY"), ("TYPE", "TYPE BODY")]
        paired_handled: Set[str] = set()
        for primary, body in paired_groups:
            if primary not in local_map and body not in local_map:
                continue
            paired_handled.update([primary, body])
            tgt = local_map.get(primary) or local_map.get(body) or src_name_u

            conflict_src: Optional[str] = None
            for t in (primary, body):
                if t not in local_map:
                    continue
                key = (tgt, t)
                existing_src = target_tracker.get(key)
                if existing_src and existing_src != src_name_u:
                    conflict_src = existing_src
                    break

            if conflict_src:
                log.warning(
                    "检测到多对一映射: 目标 %s (类型 %s/%s) 已由 %s 映射，当前 %s 整体回退为 1:1。",
                    tgt, primary, body, conflict_src, src_name_u
                )
                tgt = src_name_u
                for t in (primary, body):
                    if t in local_map:
                        local_map[t] = tgt

            for t in (primary, body):
                if t in local_map:
                    target_tracker[(local_map[t], t)] = src_name_u

        # 其余类型按单独规则检测
        for obj_type_u, tgt_name_u in local_map.items():
            if obj_type_u in paired_handled:
                continue
            key = (tgt_name_u, obj_type_u)
            existing_src = target_tracker.get(key)
            if existing_src and existing_src != src_name_u:
                log.warning(
                    "检测到多对一映射: 目标 %s (类型 %s) 已由 %s 映射，当前 %s 回退为 1:1 映射。",
                    tgt_name_u, obj_type_u, existing_src, src_name_u
                )
                tgt_name_u = src_name_u
                local_map[obj_type_u] = tgt_name_u
            target_tracker[key] = src_name_u

        mapping[src_name_u] = local_map

    return mapping


def get_mapped_target(
    full_object_mapping: FullObjectMapping,
    src_full_name: str,
    obj_type: str
) -> Optional[str]:
    src_key = src_full_name.upper()
    obj_type_u = obj_type.upper()
    type_map = full_object_mapping.get(src_key)
    if not type_map:
        return None
    return type_map.get(obj_type_u)


def find_mapped_target_any_type(
    full_object_mapping: FullObjectMapping,
    src_full_name: str,
    preferred_types: Optional[Tuple[str, ...]] = None
) -> Optional[str]:
    """
    在不知道对象类型的情况下，根据可选的类型优先级查找映射的目标名。
    先按 preferred_types 顺序查找，找不到再回退到该源对象的任意映射值。
    """
    preferred_types = preferred_types or ()
    src_key = src_full_name.upper()
    for obj_type in preferred_types:
        mapped = get_mapped_target(full_object_mapping, src_key, obj_type)
        if mapped:
            return mapped
    type_map = full_object_mapping.get(src_key)
    if not type_map:
        return None
    # 回退时取确定性的最小值，避免 set/插入顺序导致的随机性
    return sorted(type_map.values())[0] if type_map else None


def ensure_mapping_entry(
    full_object_mapping: FullObjectMapping,
    src_full_name: str,
    obj_type: str,
    tgt_full_name: str
) -> None:
    src_key = src_full_name.upper()
    obj_type_u = obj_type.upper()
    tgt_full = tgt_full_name.upper()
    full_object_mapping.setdefault(src_key, {})[obj_type_u] = tgt_full


def find_source_by_target(
    full_object_mapping: FullObjectMapping,
    tgt_full_name: str,
    obj_type: str
) -> Optional[str]:
    obj_type_u = obj_type.upper()
    tgt_u = tgt_full_name.upper()
    for src_name, type_map in full_object_mapping.items():
        target = type_map.get(obj_type_u)
        if target and target.upper() == tgt_u:
            return src_name
    return None


def collect_table_pairs(master_list: MasterCheckList, use_target: bool = False) -> Set[Tuple[str, str]]:
    """
    提取 master_list 中的 (schema, table) 集合。
    use_target=True 时基于目标端表名，否则使用源端。
    """
    pairs: Set[Tuple[str, str]] = set()
    for src_name, tgt_name, obj_type in master_list:
        if obj_type.upper() != 'TABLE':
            continue
        name = tgt_name if use_target else src_name
        if '.' not in name:
            continue
        schema, table = name.split('.', 1)
        pairs.add((schema.upper(), table.upper()))
    return pairs


def build_table_target_map(master_list: MasterCheckList) -> Dict[Tuple[str, str], Tuple[str, str]]:
    """
    基于 master_list 构造源表 -> 目标表映射。
    """
    mapping: Dict[Tuple[str, str], Tuple[str, str]] = {}
    for src_name, tgt_name, obj_type in master_list:
        if obj_type.upper() != 'TABLE':
            continue
        src_key = parse_full_object_name(src_name)
        tgt_key = parse_full_object_name(tgt_name)
        if src_key and tgt_key:
            mapping[src_key] = tgt_key
    return mapping


def filter_trigger_results_for_unsupported_tables(
    extra_results: ExtraCheckResults,
    unsupported_table_keys: Optional[Set[Tuple[str, str]]],
    table_target_map: Optional[Dict[Tuple[str, str], Tuple[str, str]]]
) -> ExtraCheckResults:
    """
    将黑名单 TABLE 相关的触发器差异从扩展校验结果中剔除，避免误报。
    """
    if not extra_results or not unsupported_table_keys or not table_target_map:
        return extra_results

    unsupported_keys = {(s.upper(), t.upper()) for s, t in unsupported_table_keys}
    table_map = {
        f"{tgt_schema.upper()}.{tgt_table.upper()}": (src_schema.upper(), src_table.upper())
        for (src_schema, src_table), (tgt_schema, tgt_table) in table_target_map.items()
    }

    def _is_unsupported_table(table_name: str) -> bool:
        if not table_name:
            return False
        table_key = table_name.split()[0].upper()
        src_key = table_map.get(table_key)
        return bool(src_key and src_key in unsupported_keys)

    filtered = dict(extra_results)
    filtered["trigger_mismatched"] = [
        item for item in (extra_results.get("trigger_mismatched", []) or [])
        if not _is_unsupported_table(item.table)
    ]
    filtered["trigger_ok"] = [
        name for name in (extra_results.get("trigger_ok", []) or [])
        if not _is_unsupported_table(name)
    ]
    return filtered


def apply_noise_suppression(
    tv_results: Optional[ReportResults],
    extra_results: Optional[ExtraCheckResults],
    comment_results: Optional[Dict[str, object]]
) -> NoiseSuppressionResult:
    """
    基于确定性的系统生成特征做降噪分类，仅影响报告与修补输出，不改变对比逻辑。
    """
    suppressed_details: List[NoiseSuppressedDetail] = []

    filtered_tv: ReportResults = dict(tv_results or {})
    filtered_extra: ExtraCheckResults = dict(extra_results or {})
    filtered_comments: Dict[str, object] = dict(comment_results or {})

    def _split_noise_columns(columns: Set[str]) -> Tuple[Set[str], Dict[str, Set[str]]]:
        kept: Set[str] = set()
        noise_map: Dict[str, Set[str]] = defaultdict(set)
        for col in columns or set():
            reason = classify_noise_column(col)
            if reason:
                name_norm = normalize_identifier_name(col) or str(col)
                noise_map[reason].add(name_norm)
            else:
                kept.add(col)
        return kept, noise_map

    def _record_noise_details(
        category: str,
        scope: str,
        detail: str,
        noise_map: Dict[str, Set[str]]
    ) -> None:
        for reason, cols in sorted(noise_map.items()):
            if not cols:
                continue
            suppressed_details.append(NoiseSuppressedDetail(
                category=category,
                scope=scope,
                reason=reason,
                identifiers=",".join(sorted(cols)),
                detail=detail
            ))

    def _filter_detail_lines(lines: List[str], names: Set[str]) -> List[str]:
        if not lines or not names:
            return lines
        names_u = {normalize_identifier_name(name) for name in names if name}
        if not names_u:
            return lines
        filtered: List[str] = []
        for line in lines:
            line_u = (line or "").upper()
            if any(name in line_u for name in names_u):
                continue
            filtered.append(line)
        return filtered

    # TABLE 列差异降噪
    ok_items = list(filtered_tv.get("ok", []) or [])
    mismatched_items = list(filtered_tv.get("mismatched", []) or [])
    filtered_mismatched: List[Tuple[str, str, Set[str], Set[str], List[Tuple[str, int, int, int, str]], List[Tuple[str, str, str, str]]]] = []
    for obj_type, tgt_name, missing, extra, length_mismatches, type_mismatches in mismatched_items:
        if (obj_type or "").upper() != "TABLE":
            filtered_mismatched.append((obj_type, tgt_name, missing, extra, length_mismatches, type_mismatches))
            continue
        if "获取失败" in (tgt_name or ""):
            filtered_mismatched.append((obj_type, tgt_name, missing, extra, length_mismatches, type_mismatches))
            continue
        kept_missing, missing_noise = _split_noise_columns(set(missing))
        kept_extra, extra_noise = _split_noise_columns(set(extra))
        if missing_noise:
            _record_noise_details("TABLE_COLUMN", tgt_name, "MISSING_COLS", missing_noise)
        if extra_noise:
            _record_noise_details("TABLE_COLUMN", tgt_name, "EXTRA_COLS", extra_noise)
        if not kept_missing and not kept_extra and not length_mismatches and not type_mismatches:
            ok_entry = (obj_type, tgt_name)
            if ok_entry not in ok_items:
                ok_items.append(ok_entry)
            continue
        filtered_mismatched.append((
            obj_type,
            tgt_name,
            kept_missing,
            kept_extra,
            length_mismatches,
            type_mismatches
        ))
    filtered_tv["ok"] = ok_items
    filtered_tv["mismatched"] = filtered_mismatched

    # 注释差异降噪
    comment_ok = list(filtered_comments.get("ok", []) or [])
    comment_mismatched = list(filtered_comments.get("mismatched", []) or [])
    filtered_comment_mismatched: List[CommentMismatch] = []
    for item in comment_mismatched:
        kept_missing, missing_noise = _split_noise_columns(set(item.missing_columns))
        kept_extra, extra_noise = _split_noise_columns(set(item.extra_columns))
        if missing_noise:
            _record_noise_details("COMMENT_COLUMN", item.table, "MISSING_COLS", missing_noise)
        if extra_noise:
            _record_noise_details("COMMENT_COLUMN", item.table, "EXTRA_COLS", extra_noise)
        kept_diffs: List[Tuple[str, str, str]] = []
        diff_noise: Dict[str, Set[str]] = defaultdict(set)
        for col, src_cmt, tgt_cmt in item.column_comment_diffs:
            reason = classify_noise_column(col)
            if reason:
                diff_noise[reason].add(normalize_identifier_name(col) or str(col))
            else:
                kept_diffs.append((col, src_cmt, tgt_cmt))
        if diff_noise:
            _record_noise_details("COMMENT_COLUMN", item.table, "COMMENT_DIFF", diff_noise)
        if item.table_comment or kept_missing or kept_extra or kept_diffs:
            filtered_comment_mismatched.append(CommentMismatch(
                table=item.table,
                table_comment=item.table_comment,
                column_comment_diffs=kept_diffs,
                missing_columns=kept_missing,
                extra_columns=kept_extra
            ))
        else:
            if item.table not in comment_ok:
                comment_ok.append(item.table)
    filtered_comments["ok"] = comment_ok
    filtered_comments["mismatched"] = filtered_comment_mismatched

    # 扩展对象降噪
    index_ok = list(filtered_extra.get("index_ok", []) or [])
    index_mismatched = list(filtered_extra.get("index_mismatched", []) or [])
    filtered_index_mismatched: List[IndexMismatch] = []
    for item in index_mismatched:
        missing_suppressed = {name for name in item.missing_indexes if is_oms_rowid_index_name(name)}
        extra_suppressed = {name for name in item.extra_indexes if is_oms_rowid_index_name(name)}
        if missing_suppressed:
            _record_noise_details(
                "INDEX",
                item.table,
                "MISSING_INDEX",
                {NOISE_REASON_OMS_ROWID_INDEX: {normalize_identifier_name(n) or str(n) for n in missing_suppressed}}
            )
        if extra_suppressed:
            _record_noise_details(
                "INDEX",
                item.table,
                "EXTRA_INDEX",
                {NOISE_REASON_OMS_ROWID_INDEX: {normalize_identifier_name(n) or str(n) for n in extra_suppressed}}
            )
        new_missing = set(item.missing_indexes) - missing_suppressed
        new_extra = set(item.extra_indexes) - extra_suppressed
        new_detail = _filter_detail_lines(item.detail_mismatch, missing_suppressed | extra_suppressed)
        if not new_missing and not new_extra and not new_detail:
            if item.table not in index_ok:
                index_ok.append(item.table)
            continue
        filtered_index_mismatched.append(IndexMismatch(
            table=item.table,
            missing_indexes=new_missing,
            extra_indexes=new_extra,
            detail_mismatch=new_detail
        ))
    filtered_extra["index_ok"] = index_ok
    filtered_extra["index_mismatched"] = filtered_index_mismatched

    constraint_ok = list(filtered_extra.get("constraint_ok", []) or [])
    constraint_mismatched = list(filtered_extra.get("constraint_mismatched", []) or [])
    filtered_constraint_mismatched: List[ConstraintMismatch] = []
    for item in constraint_mismatched:
        missing_suppressed = {name for name in item.missing_constraints if is_ob_notnull_constraint(name)}
        extra_suppressed = {name for name in item.extra_constraints if is_ob_notnull_constraint(name)}
        if missing_suppressed:
            _record_noise_details(
                "CONSTRAINT",
                item.table,
                "MISSING_CONSTRAINT",
                {NOISE_REASON_OBNOTNULL_CONSTRAINT: {
                    normalize_identifier_name(extract_constraint_name(n)) or str(n)
                    for n in missing_suppressed
                }}
            )
        if extra_suppressed:
            _record_noise_details(
                "CONSTRAINT",
                item.table,
                "EXTRA_CONSTRAINT",
                {NOISE_REASON_OBNOTNULL_CONSTRAINT: {
                    normalize_identifier_name(extract_constraint_name(n)) or str(n)
                    for n in extra_suppressed
                }}
            )
        new_missing = set(item.missing_constraints) - missing_suppressed
        new_extra = set(item.extra_constraints) - extra_suppressed
        new_detail = _filter_detail_lines(item.detail_mismatch, missing_suppressed | extra_suppressed)
        if not new_missing and not new_extra and not new_detail:
            if item.table not in constraint_ok:
                constraint_ok.append(item.table)
            continue
        filtered_constraint_mismatched.append(ConstraintMismatch(
            table=item.table,
            missing_constraints=new_missing,
            extra_constraints=new_extra,
            detail_mismatch=new_detail,
            downgraded_pk_constraints=item.downgraded_pk_constraints
        ))
    filtered_extra["constraint_ok"] = constraint_ok
    filtered_extra["constraint_mismatched"] = filtered_constraint_mismatched

    sequence_ok = list(filtered_extra.get("sequence_ok", []) or [])
    sequence_mismatched = list(filtered_extra.get("sequence_mismatched", []) or [])
    filtered_sequence_mismatched: List[SequenceMismatch] = []
    for item in sequence_mismatched:
        mapping_label = f"{item.src_schema}->{item.tgt_schema}"
        missing_suppressed = {name for name in item.missing_sequences if is_auto_sequence_name(name)}
        extra_suppressed = {name for name in item.extra_sequences if is_auto_sequence_name(name)}
        if missing_suppressed:
            _record_noise_details(
                "SEQUENCE",
                mapping_label,
                "MISSING_SEQUENCE",
                {NOISE_REASON_AUTO_SEQUENCE: {normalize_identifier_name(n) or str(n) for n in missing_suppressed}}
            )
        if extra_suppressed:
            _record_noise_details(
                "SEQUENCE",
                mapping_label,
                "EXTRA_SEQUENCE",
                {NOISE_REASON_AUTO_SEQUENCE: {normalize_identifier_name(n) or str(n) for n in extra_suppressed}}
            )
        new_missing = set(item.missing_sequences) - missing_suppressed
        new_extra = set(item.extra_sequences) - extra_suppressed
        new_mappings: List[Tuple[str, str]] = []
        for src_full, tgt_full in item.missing_mappings or []:
            src_name = src_full.split(".", 1)[1] if "." in src_full else src_full
            tgt_name = tgt_full.split(".", 1)[1] if "." in tgt_full else tgt_full
            if is_auto_sequence_name(src_name) or is_auto_sequence_name(tgt_name):
                continue
            new_mappings.append((src_full, tgt_full))
        if not new_missing and not new_extra and not item.detail_mismatch:
            if mapping_label not in sequence_ok:
                sequence_ok.append(mapping_label)
            continue
        filtered_sequence_mismatched.append(SequenceMismatch(
            src_schema=item.src_schema,
            tgt_schema=item.tgt_schema,
            missing_sequences=new_missing,
            extra_sequences=new_extra,
            note=item.note,
            missing_mappings=new_mappings,
            detail_mismatch=item.detail_mismatch
        ))
    filtered_extra["sequence_ok"] = sequence_ok
    filtered_extra["sequence_mismatched"] = filtered_sequence_mismatched

    return NoiseSuppressionResult(
        tv_results=filtered_tv,
        extra_results=filtered_extra,
        comment_results=filtered_comments,
        suppressed_details=suppressed_details
    )


def normalize_extra_results_names(extra_results: ExtraCheckResults) -> ExtraCheckResults:
    if not extra_results:
        return extra_results

    def _norm_set(values: Optional[Iterable[object]]) -> Set[str]:
        if not values:
            return set()
        normalized: Set[str] = set()
        for value in values:
            name = extract_name_value(value)
            if name:
                normalized.add(name)
        return normalized

    def _norm_list(values: Optional[Iterable[object]]) -> List[str]:
        if not values:
            return []
        normalized: List[str] = []
        for value in values:
            name = extract_name_value(value)
            if name:
                normalized.append(name)
        return normalized

    normalized = dict(extra_results)
    normalized["index_ok"] = _norm_list(normalized.get("index_ok"))
    normalized["constraint_ok"] = _norm_list(normalized.get("constraint_ok"))
    normalized["sequence_ok"] = _norm_list(normalized.get("sequence_ok"))
    normalized["trigger_ok"] = _norm_list(normalized.get("trigger_ok"))

    index_items = []
    for item in normalized.get("index_mismatched", []) or []:
        index_items.append(item._replace(
            missing_indexes=_norm_set(item.missing_indexes),
            extra_indexes=_norm_set(item.extra_indexes)
        ))
    normalized["index_mismatched"] = index_items

    constraint_items = []
    for item in normalized.get("constraint_mismatched", []) or []:
        constraint_items.append(item._replace(
            missing_constraints=_norm_set(item.missing_constraints),
            extra_constraints=_norm_set(item.extra_constraints),
            downgraded_pk_constraints=_norm_set(item.downgraded_pk_constraints)
        ))
    normalized["constraint_mismatched"] = constraint_items

    sequence_items = []
    for item in normalized.get("sequence_mismatched", []) or []:
        sequence_items.append(item._replace(
            missing_sequences=_norm_set(item.missing_sequences),
            extra_sequences=_norm_set(item.extra_sequences)
        ))
    normalized["sequence_mismatched"] = sequence_items

    trigger_items = []
    for item in normalized.get("trigger_mismatched", []) or []:
        trigger_items.append(item._replace(
            missing_triggers=_norm_set(item.missing_triggers),
            extra_triggers=_norm_set(item.extra_triggers)
        ))
    normalized["trigger_mismatched"] = trigger_items

    return normalized


def classify_unsupported_check_constraints(
    extra_results: ExtraCheckResults,
    oracle_meta: OracleMetadata,
    table_target_map: Optional[Dict[Tuple[str, str], Tuple[str, str]]]
) -> List[ConstraintUnsupportedDetail]:
    """
    标记 Oracle->OB 不支持的约束（含 DEFERRABLE），并从缺失列表中移除。
    """
    if not extra_results:
        return []
    constraint_items = list(extra_results.get("constraint_mismatched", []) or [])
    if not constraint_items or not table_target_map:
        return []

    table_map = {
        f"{tgt_schema.upper()}.{tgt_table.upper()}": (src_schema.upper(), src_table.upper())
        for (src_schema, src_table), (tgt_schema, tgt_table) in table_target_map.items()
    }
    unsupported_rows: List[ConstraintUnsupportedDetail] = []
    updated_mismatches: List[ConstraintMismatch] = []
    ok_tables: Set[str] = set(extra_results.get("constraint_ok", []) or [])
    ref_lookup: Dict[Tuple[str, str], Tuple[str, str]] = {}
    for (owner, table), cons_map in (oracle_meta.constraints or {}).items():
        for cons_name, info in cons_map.items():
            ctype = (info.get("type") or "").upper()
            if ctype in ("P", "U"):
                ref_lookup[(owner.upper(), cons_name.upper())] = (owner.upper(), table.upper())

    def _filter_detail_lines(lines: List[str], names: Set[str]) -> List[str]:
        if not lines or not names:
            return lines
        names_u = {n.upper() for n in names}
        filtered: List[str] = []
        for line in lines:
            line_u = line.upper()
            if any(name in line_u for name in names_u):
                continue
            filtered.append(line)
        return filtered

    for item in constraint_items:
        table_str = (item.table or "").split()[0].upper()
        src_pair = table_map.get(table_str)
        if not src_pair:
            updated_mismatches.append(item)
            continue
        src_schema, src_table = src_pair
        cons_map = oracle_meta.constraints.get((src_schema, src_table), {})
        unsupported_names: Set[str] = set()
        for cons_name in item.missing_constraints:
            cons_meta = cons_map.get(cons_name.upper()) or cons_map.get(cons_name)
            if not cons_meta:
                continue
            ctype = (cons_meta.get("type") or "").upper()
            if ctype == "C" and is_system_notnull_check(cons_name, cons_meta.get("search_condition")):
                continue
            if ctype == "R":
                ref_owner = (cons_meta.get("ref_table_owner") or "").upper()
                ref_table = (cons_meta.get("ref_table_name") or "").upper()
                if not ref_owner or not ref_table:
                    r_owner = (cons_meta.get("r_owner") or "").upper()
                    r_cons = (cons_meta.get("r_constraint") or "").upper()
                    if r_owner and r_cons:
                        ref_pair = ref_lookup.get((r_owner, r_cons))
                        if ref_pair:
                            ref_owner, ref_table = ref_pair
                if ref_owner and ref_table and ref_owner == src_schema and ref_table == src_table:
                    unsupported_names.add(cons_name)
                    unsupported_rows.append(ConstraintUnsupportedDetail(
                        table_full=f"{src_schema}.{src_table}",
                        constraint_name=cons_name.upper(),
                        search_condition="-",
                        reason_code="FK_SELF_REF",
                        reason="自引用外键，OceanBase 不支持。",
                        ob_error_hint="ORA-00600(-5317)"
                    ))
                    continue
            reason = classify_unsupported_constraint(cons_meta)
            if not reason:
                continue
            reason_code, reason_text, ob_hint = reason
            unsupported_names.add(cons_name)
            unsupported_rows.append(ConstraintUnsupportedDetail(
                table_full=f"{src_schema}.{src_table}",
                constraint_name=cons_name.upper(),
                search_condition=str(cons_meta.get("search_condition") or "-") if ctype == "C" else "-",
                reason_code=reason_code,
                reason=reason_text,
                ob_error_hint=ob_hint
            ))

        if not unsupported_names:
            updated_mismatches.append(item)
            continue

        new_missing = set(item.missing_constraints) - unsupported_names
        new_extra = set(item.extra_constraints) - unsupported_names
        new_detail = _filter_detail_lines(item.detail_mismatch, unsupported_names)
        if not new_missing and not new_extra and not new_detail:
            ok_tables.add(item.table)
            continue

        updated_mismatches.append(ConstraintMismatch(
            table=item.table,
            missing_constraints=new_missing,
            extra_constraints=new_extra,
            detail_mismatch=new_detail,
            downgraded_pk_constraints=item.downgraded_pk_constraints
        ))

    extra_results["constraint_mismatched"] = updated_mismatches
    extra_results["constraint_ok"] = sorted(ok_tables)
    return unsupported_rows


def classify_unsupported_indexes(
    extra_results: ExtraCheckResults,
    oracle_meta: OracleMetadata,
    table_target_map: Optional[Dict[Tuple[str, str], Tuple[str, str]]]
) -> List[IndexUnsupportedDetail]:
    """
    标记 Oracle->OB 不支持的索引（如 DESC），并从缺失列表中移除。
    """
    if not extra_results:
        return []
    index_items = list(extra_results.get("index_mismatched", []) or [])
    if not index_items or not table_target_map:
        return []

    table_map = {
        f"{tgt_schema.upper()}.{tgt_table.upper()}": (src_schema.upper(), src_table.upper())
        for (src_schema, src_table), (tgt_schema, tgt_table) in table_target_map.items()
    }
    unsupported_rows: List[IndexUnsupportedDetail] = []
    updated_mismatches: List[IndexMismatch] = []
    ok_tables: Set[str] = set(extra_results.get("index_ok", []) or [])

    def _extract_cols_from_detail(line: str) -> Optional[Tuple[str, ...]]:
        match = re.search(r"索引列\s*\[(.*?)\]", line)
        if not match:
            return None
        raw = match.group(1)
        if not raw:
            return tuple()
        parts = [p.strip().strip("'\"") for p in raw.split(",") if p.strip()]
        return tuple(parts)

    def _filter_detail_lines(lines: List[str], unsupported_cols: Set[Tuple[str, ...]]) -> List[str]:
        if not lines or not unsupported_cols:
            return lines
        filtered: List[str] = []
        for line in lines:
            if "在目标端未找到" in line and "索引列" in line:
                cols = _extract_cols_from_detail(line)
                if cols and cols in unsupported_cols:
                    continue
            filtered.append(line)
        return filtered

    for item in index_items:
        table_str = (item.table or "").split()[0].upper()
        src_pair = table_map.get(table_str)
        if not src_pair:
            updated_mismatches.append(item)
            continue
        src_schema, src_table = src_pair
        idx_map = oracle_meta.indexes.get((src_schema, src_table), {}) or {}
        unsupported_names: Set[str] = set()
        unsupported_cols: Set[Tuple[str, ...]] = set()
        for idx_name in item.missing_indexes:
            idx_meta = idx_map.get(idx_name.upper()) or idx_map.get(idx_name)
            if not idx_meta:
                continue
            if not index_has_desc(idx_meta):
                continue
            cols = normalize_index_columns(
                idx_meta.get("columns") or [],
                idx_meta.get("expressions") or {}
            )
            unsupported_names.add(idx_name)
            unsupported_cols.add(cols)
            unsupported_rows.append(IndexUnsupportedDetail(
                table_full=f"{src_schema}.{src_table}",
                index_name=idx_name.upper(),
                columns=",".join(cols) if cols else "-",
                reason_code="INDEX_DESC",
                reason="索引包含 DESC 列，OceanBase 不支持。",
                ob_error_hint="ORA-00900"
            ))

        if not unsupported_names:
            updated_mismatches.append(item)
            continue

        new_missing = set(item.missing_indexes) - unsupported_names
        new_detail = _filter_detail_lines(item.detail_mismatch, unsupported_cols)
        if not new_missing and not item.extra_indexes and not new_detail:
            ok_tables.add(item.table)
            continue

        updated_mismatches.append(IndexMismatch(
            table=item.table,
            missing_indexes=new_missing,
            extra_indexes=item.extra_indexes,
            detail_mismatch=new_detail
        ))

    extra_results["index_mismatched"] = updated_mismatches
    extra_results["index_ok"] = sorted(ok_tables)
    return unsupported_rows


def build_schema_mapping(master_list: MasterCheckList) -> Dict[str, str]:
    """
    基于 master_list 中 TABLE 映射，推导 schema 映射：
      如果同一 src_schema 只映射到唯一一个 tgt_schema，则使用该映射；
      否则 (映射到多个目标 schema)，退回 src_schema 本身 (1:1)。
    """
    mapping_tmp: Dict[str, Set[str]] = {}
    for src_name, tgt_name, obj_type in master_list:
        if obj_type.upper() != 'TABLE':
            continue
        try:
            src_schema, _ = src_name.split('.', 1)
            tgt_schema, _ = tgt_name.split('.', 1)
        except ValueError:
            continue
        mapping_tmp.setdefault(src_schema.upper(), set()).add(tgt_schema.upper())

    final_mapping: Dict[str, str] = {}
    one_to_many_schemas: List[Tuple[str, Set[str]]] = []
    
    for src_schema, tgt_set in mapping_tmp.items():
        if len(tgt_set) == 1:
            final_mapping[src_schema] = next(iter(tgt_set))
        else:
            # 一对多映射：源schema的表分散到多个目标schema
            final_mapping[src_schema] = src_schema
            one_to_many_schemas.append((src_schema, tgt_set))
    
    # 输出schema映射摘要
    if final_mapping:
        log.info("Schema映射推导完成，共 %d 个源schema:", len(final_mapping))
        for src_s, tgt_s in sorted(final_mapping.items()):
            if src_s == tgt_s:
                log.info("  %s -> %s (1:1或一对多场景)", src_s, tgt_s)
            else:
                log.info("  %s -> %s", src_s, tgt_s)
    
    # 提示：一对多场景下的推导策略
    if one_to_many_schemas:
        log_subsection("一对多 schema 映射场景")
        log.info("检测到一对多 schema 映射场景（源schema的表分散到多个目标schema）：")
        for src_schema, tgt_set in one_to_many_schemas:
            log.info("  %s -> %s", src_schema, sorted(tgt_set))
        log.info("推导策略：")
        log.info("  1. 独立对象（VIEW/PROCEDURE/FUNCTION/PACKAGE等）：")
        log.info("     - 优先通过依赖分析推导（分析对象引用的表，选择出现最多的目标schema）")
        log.info("     - 如果依赖推导失败，需要在 remap_rules.txt 中显式指定")
        log.info("  2. 依附对象（INDEX/CONSTRAINT/SEQUENCE）：")
        log.info("     - 自动跟随父表的 schema，无需显式指定")
        log.info("  3. TRIGGER：")
        log.info("     - 默认保持源 schema，除非在 remap_rules.txt 中显式指定")

    return final_mapping


def compute_schema_coverage(
    configured_source_schemas: List[str],
    source_objects: SourceObjectMap,
    expected_target_schemas: Set[str],
    ob_meta: ObMetadata
) -> Dict[str, List[str]]:
    """
    计算 schema 层面的覆盖情况：
      - 源端配置了但未在元数据中找到对象的 schema
      说明：目标端可能是“超集”，因此不检查“额外 schema”或“目标缺失 schema”。
    """
    cfg_src_set = {s.upper() for s in configured_source_schemas}
    src_seen = {name.split('.', 1)[0].upper() for name in source_objects.keys() if '.' in name}
    source_missing = sorted(cfg_src_set - src_seen)

    expected_tgt_set = {s.upper() for s in expected_target_schemas}
    tgt_seen: Set[str] = set()
    for type_set in ob_meta.objects_by_type.values():
        for full_name in type_set:
            if '.' in full_name:
                tgt_seen.add(full_name.split('.', 1)[0].upper())

    target_missing = sorted(expected_tgt_set - tgt_seen)
    target_extra = sorted(tgt_seen - expected_tgt_set)
    hints: List[str] = []
    if target_missing:
        hints.append(
            f"目标端缺失 schema: {', '.join(target_missing)}（可能未创建或权限不足）"
        )

    return {
        "source_missing": source_missing,
        "target_missing": target_missing,
        "target_extra": target_extra,
        "target_missing_schema_hint": hints
    }


def compute_object_counts(
    full_object_mapping: FullObjectMapping,
    ob_meta: ObMetadata,
    oracle_meta: OracleMetadata,
    monitored_types: Tuple[str, ...] = OBJECT_COUNT_TYPES
) -> ObjectCountSummary:
    """
    基于“期望对象集合”统计各类型的：源端数量、目标端命中数量、缺失数量、额外数量。
    目标端数量仅统计“期望对象”的命中数，避免“缺 1 张表 + 额外 1 张表”被误判为数量一致。
    """
    expected_by_type: Dict[str, Set[str]] = {t.upper(): set() for t in monitored_types}
    for _src, type_map in full_object_mapping.items():
        for obj_type, tgt_name in type_map.items():
            obj_type_u = obj_type.upper()
            if obj_type_u not in expected_by_type:
                continue
            expected_by_type[obj_type_u].add(tgt_name.upper())

    invalid_targets_by_type: Dict[str, Set[str]] = defaultdict(set)
    for src_full, type_map in full_object_mapping.items():
        if "." not in src_full:
            continue
        src_schema, src_obj = src_full.split(".", 1)
        src_schema_u = src_schema.upper()
        src_obj_u = src_obj.upper()
        for obj_type in PACKAGE_OBJECT_TYPES:
            tgt_name = type_map.get(obj_type)
            if not tgt_name:
                continue
            status = oracle_meta.object_statuses.get((src_schema_u, src_obj_u, obj_type))
            if normalize_object_status(status) == "INVALID":
                invalid_targets_by_type[obj_type].add(tgt_name.upper())

    actual_by_type: Dict[str, Set[str]] = {
        t.upper(): {name.upper() for name in ob_meta.objects_by_type.get(t.upper(), set())}
        for t in monitored_types if t != 'CONSTRAINT'
    }

    summary: ObjectCountSummary = {
        "oracle": {},
        "oceanbase": {},
        "missing": {},
        "extra": {}
    }

    issue_types: List[str] = []
    for obj_type in monitored_types:
        obj_type_u = obj_type.upper()
        
        if obj_type_u == 'CONSTRAINT':
            # Constraints are not in DBA_OBJECTS, so they need special handling
            expected_set = {
                cons_name
                for cons_map in oracle_meta.constraints.values()
                for cons_name in cons_map
            }
            actual_set = {
                cons_name
                for cons_map in ob_meta.constraints.values()
                for cons_name in cons_map
            }
        else:
            expected_set = expected_by_type.get(obj_type_u, set())
            actual_set = actual_by_type.get(obj_type_u, set())
            ignored_due_source_invalid = invalid_targets_by_type.get(obj_type_u, set())
            if ignored_due_source_invalid:
                # 源端 INVALID 的 PACKAGE/PACKAGE BODY 不参与“缺失/多余”数量统计：
                # 需同时从 expected 和 actual 移除，避免被误计为 extra。
                expected_set = expected_set - ignored_due_source_invalid
                actual_set = actual_set - ignored_due_source_invalid

        # For constraints and indexes, names can be system-generated. A simple name comparison is not enough.
        # This count is a rough estimation. Final report counts are reconciled later from semantic mismatch details
        # (see reconcile_object_counts_summary), so we avoid raising early "issue_types" noise from this stage.
        # Here we count based on what's found in meta, not remapped names, for simplicity.
        if obj_type_u in ('CONSTRAINT', 'INDEX'):
            if obj_type_u == 'CONSTRAINT':
                def _count_pkukfk(cons_maps: Dict[Tuple[str, str], Dict[str, Dict]]) -> int:
                    cnt = 0
                    for cons_map in cons_maps.values():
                        for info in cons_map.values():
                            ctype = (info.get("type") or "").upper()
                            if ctype in ('P', 'U', 'R', 'C'):
                                cnt += 1
                    return cnt

                src_count = _count_pkukfk(oracle_meta.constraints)
                tgt_count = _count_pkukfk(ob_meta.constraints)
            else: # INDEX
                src_count = sum(len(v) for v in oracle_meta.indexes.values())
                tgt_count = sum(len(v) for v in ob_meta.indexes.values())
            
            summary["oracle"][obj_type_u] = src_count
            summary["oceanbase"][obj_type_u] = tgt_count
            summary["missing"][obj_type_u] = max(0, src_count - tgt_count)
            summary["extra"][obj_type_u] = max(0, tgt_count - src_count)
            # NOTE: do not add INDEX/CONSTRAINT to early issue_types warning list.
            continue

        matched = expected_set & actual_set
        missing_set = expected_set - actual_set
        extra_set = actual_set - expected_set

        summary["oracle"][obj_type_u] = len(expected_set)
        summary["oceanbase"][obj_type_u] = len(matched)
        summary["missing"][obj_type_u] = len(missing_set)
        summary["extra"][obj_type_u] = len(extra_set)

        if missing_set or extra_set:
            issue_types.append(obj_type_u)

    if issue_types:
        log.warning(
            "检查汇总: 基于 remap 后的期望集合，以下类型存在缺失或多余对象: %s",
            ", ".join(issue_types)
        )
    else:
        log.info("检查汇总: 所有关注对象类型的数量与期望一致（不计入额外对象）。")

    return summary


def reconcile_object_counts_summary(
    object_counts_summary: Optional[ObjectCountSummary],
    tv_results: Optional[ReportResults],
    extra_results: Optional[ExtraCheckResults],
    package_results: Optional[PackageCompareResults]
) -> Optional[ObjectCountSummary]:
    """
    将“检查汇总”口径与最终明细结果对齐。

    说明：
      - compute_object_counts 在 INDEX/CONSTRAINT 上是基于元数据总量的近似统计，
        容易与语义比对结果（按列集/表达式/规则匹配）产生偏差。
      - 降噪/过滤后，tv_results / extra_results 会变化，需同步刷新汇总。
    """
    if not object_counts_summary:
        return object_counts_summary

    summary: ObjectCountSummary = {
        "oracle": dict(object_counts_summary.get("oracle", {})),
        "oceanbase": dict(object_counts_summary.get("oceanbase", {})),
        "missing": dict(object_counts_summary.get("missing", {})),
        "extra": dict(object_counts_summary.get("extra", {})),
    }

    def _set_counts(obj_type: str, missing_cnt: int, extra_cnt: int, *, keep_extra: bool = False) -> None:
        obj_type_u = (obj_type or "").upper()
        if not obj_type_u:
            return
        missing_val = max(0, int(missing_cnt or 0))
        extra_val = int(summary["extra"].get(obj_type_u, 0) or 0) if keep_extra else max(0, int(extra_cnt or 0))
        oracle_val = max(0, int(summary["oracle"].get(obj_type_u, 0) or 0))
        summary["missing"][obj_type_u] = missing_val
        summary["extra"][obj_type_u] = extra_val
        summary["oceanbase"][obj_type_u] = max(0, oracle_val - missing_val)

    # 1) 主对象口径：按最终 tv_results 统计（已包含降噪后的结果）
    missing_by_type: Dict[str, int] = defaultdict(int)
    extra_by_type: Dict[str, int] = defaultdict(int)
    if tv_results:
        for obj_type, _tgt_name, _src_name in (tv_results.get("missing", []) or []):
            missing_by_type[(obj_type or "").upper()] += 1
        for obj_type, _tgt_name in (tv_results.get("extra_targets", []) or []):
            extra_by_type[(obj_type or "").upper()] += 1
    for obj_type in set(missing_by_type) | set(extra_by_type):
        _set_counts(obj_type, missing_by_type.get(obj_type, 0), extra_by_type.get(obj_type, 0))

    # 2) PACKAGE / PACKAGE BODY：缺失按 package compare，extra 保留原统计（无专门extra明细）
    if package_results:
        pkg_missing: Dict[str, int] = defaultdict(int)
        for row in (package_results.get("rows", []) or []):
            if (row.result or "").upper() == "MISSING_TARGET":
                pkg_missing[(row.obj_type or "").upper()] += 1
        for obj_type in PACKAGE_OBJECT_TYPES:
            if obj_type in summary["oracle"]:
                _set_counts(obj_type, pkg_missing.get(obj_type, 0), 0, keep_extra=True)

    # 3) 扩展对象：按最终 extra_results 语义结果覆盖（而不是总量估算）
    if extra_results:
        index_missing = sum(len(item.missing_indexes or set()) for item in (extra_results.get("index_mismatched", []) or []))
        index_extra = sum(len(item.extra_indexes or set()) for item in (extra_results.get("index_mismatched", []) or []))
        _set_counts("INDEX", index_missing, index_extra)

        constraint_missing = sum(
            len(item.missing_constraints or set()) for item in (extra_results.get("constraint_mismatched", []) or [])
        )
        constraint_extra = sum(
            len(item.extra_constraints or set()) for item in (extra_results.get("constraint_mismatched", []) or [])
        )
        _set_counts("CONSTRAINT", constraint_missing, constraint_extra)

        sequence_missing = sum(
            len(item.missing_sequences or set()) for item in (extra_results.get("sequence_mismatched", []) or [])
        )
        sequence_extra = sum(
            len(item.extra_sequences or set()) for item in (extra_results.get("sequence_mismatched", []) or [])
        )
        _set_counts("SEQUENCE", sequence_missing, sequence_extra)

        trigger_missing = sum(
            len(item.missing_triggers or set()) for item in (extra_results.get("trigger_mismatched", []) or [])
        )
        trigger_extra = sum(
            len(item.extra_triggers or set()) for item in (extra_results.get("trigger_mismatched", []) or [])
        )
        _set_counts("TRIGGER", trigger_missing, trigger_extra)

    return summary




# ====================== obclient + 一次性元数据转储 ======================

def obclient_run_sql(ob_cfg: ObConfig, sql_query: str, timeout: Optional[int] = None) -> Tuple[bool, str, str]:
    """运行 obclient CLI 命令并返回 (Success, stdout, stderr)，带 timeout。"""
    timeout_val = OBC_TIMEOUT if timeout is None else max(1, int(timeout))
    sql_payload = (sql_query or "").strip()
    if sql_payload and not sql_payload.endswith(";"):
        sql_payload += ";"
    if sql_payload:
        sql_payload += "\n"
    command_args = [
        ob_cfg['executable'],
        '-h', ob_cfg['host'],
        '-P', ob_cfg['port'],
        '-u', ob_cfg['user_string'],
        '-p' + ob_cfg['password'],
        '-ss',  # Silent 模式
    ]

    def _extract_obclient_error(text: str) -> str:
        if not text:
            return ""
        for line in text.splitlines():
            line_clean = line.strip()
            if not line_clean:
                continue
            if re.search(r"^(warning|警告)\b", line_clean, flags=re.IGNORECASE):
                continue
            # 仅当错误出现在行首时视为执行失败，避免 DBA_ERRORS TEXT 列中的 ORA-XXXX 被误判。
            if re.search(r"^(ORA-\d{5}|OB-\d+)\b", line_clean, flags=re.IGNORECASE):
                return line_clean
            if re.search(r"^ERROR(\s+\d+|\b)", line_clean, flags=re.IGNORECASE):
                return line_clean
        return ""

    try:
        result = subprocess.run(
            command_args,
            input=sql_payload,
            capture_output=True,
            text=True,
            encoding='utf-8',
            errors='ignore',
            timeout=timeout_val
        )

        stdout_clean = (result.stdout or "").strip()
        stderr_clean = (result.stderr or "").strip()

        if result.returncode != 0:
            err_line = _extract_obclient_error(stderr_clean) or _extract_obclient_error(stdout_clean)
            err_text = err_line or stderr_clean or stdout_clean or f"obclient return code {result.returncode}"
            log.error("  [OBClient 错误] SQL: %s | 错误: %s", sql_query.strip(), err_text)
            return False, stdout_clean, err_text
        err_line = _extract_obclient_error(stderr_clean) or _extract_obclient_error(stdout_clean)
        if err_line:
            log.error("  [OBClient 错误] SQL: %s | 错误: %s", sql_query.strip(), err_line)
            return False, stdout_clean, err_line
        if stderr_clean:
            if re.search(r"warning|警告", stderr_clean, flags=re.IGNORECASE):
                log.debug("  [OBClient 警告] SQL: %s | 警告: %s", sql_query.strip(), stderr_clean)
            else:
                log.warning("  [OBClient STDERR] SQL: %s | 输出: %s", sql_query.strip(), stderr_clean)

        return True, stdout_clean, ""

    except subprocess.TimeoutExpired:
        log.error(f"严重错误: obclient 执行超时 (>{timeout_val} 秒)。请检查网络/OB 状态或调大 obclient_timeout。")
        return False, "", "TimeoutExpired"
    except FileNotFoundError:
        log.error(f"严重错误: 未找到 obclient 可执行文件: {ob_cfg['executable']}")
        log.error("请检查 config.ini 中的 [OCEANBASE_TARGET] -> executable 路径。")
        abort_run()
    except Exception as e:
        log.error(f"严重错误: 执行 subprocess 时发生未知错误: {e}")
        return False, "", str(e)


def obclient_run_sql_commit(
    ob_cfg: ObConfig,
    sql_query: str,
    timeout: Optional[int] = None
) -> Tuple[bool, str, str]:
    """
    执行 DML 后显式 COMMIT（用于 report_db 写库等场景）。
    """
    if not sql_query:
        return False, "", "Empty SQL"
    sql_payload = sql_query.strip()
    if not sql_payload.endswith(";"):
        sql_payload += ";"
    sql_payload += "\nCOMMIT;"
    return obclient_run_sql(ob_cfg, sql_payload, timeout=timeout)


def obclient_query_by_owner_chunks(
    ob_cfg: ObConfig,
    sql_tpl: str,
    owners: List[str],
    *,
    chunk_size: int = ORACLE_IN_BATCH_SIZE
) -> Tuple[bool, List[str], str]:
    """
    将 OWNER IN (...) 拆分为多个 chunk 运行，避免 IN 列表过长或超过 1000 限制。
    sql_tpl 需包含 {owners_in} 占位符。
    返回 (ok, lines, err)。
    """
    if not owners:
        return True, [], ""

    def _quote_owner(value: str) -> str:
        return "'" + str(value).replace("'", "''") + "'"

    lines: List[str] = []
    for chunk in chunk_list(owners, chunk_size):
        owners_in = ",".join(_quote_owner(s) for s in chunk)
        sql = sql_tpl.format(owners_in=owners_in)
        ok, out, err = obclient_run_sql(ob_cfg, sql)
        if not ok:
            return False, [], err
        if out:
            lines.extend(out.splitlines())
    return True, lines, ""


def ob_has_dba_column(
    ob_cfg: ObConfig,
    table_name: str,
    column_name: str,
    owner: str = "SYS"
) -> bool:
    table_u = (table_name or "").upper()
    column_u = (column_name or "").upper()
    owner_u = (owner or "SYS").upper()
    if not table_u or not column_u or not owner_u:
        return False
    def _quote_literal(value: str) -> str:
        return "'" + str(value).replace("'", "''") + "'"

    sql = (
        "SELECT 1 FROM DBA_TAB_COLUMNS "
        f"WHERE OWNER={_quote_literal(owner_u)} AND TABLE_NAME={_quote_literal(table_u)} "
        f"AND COLUMN_NAME={_quote_literal(column_u)} "
        "AND ROWNUM = 1"
    )
    ok, out, err = obclient_run_sql(ob_cfg, sql)
    if not ok:
        log.warning("读取 OB DBA_TAB_COLUMNS 失败，无法判断字段存在性: %s", err)
        return False
    return bool((out or "").strip())


def obclient_query_by_owner_pairs(
    ob_cfg: ObConfig,
    sql_tpl: str,
    owners: List[str],
    ref_owners: List[str],
    *,
    chunk_size: int = ORACLE_IN_BATCH_SIZE
) -> Tuple[bool, List[str], str]:
    """
    双 OWNER 列表的嵌套分块查询，用于依赖等需要 owner/ref_owner 双过滤的场景。
    sql_tpl 需包含 {owners_in} 与 {ref_owners_in} 占位符。
    """
    if not owners or not ref_owners:
        return True, [], ""

    def _quote_owner(value: str) -> str:
        return "'" + str(value).replace("'", "''") + "'"

    lines: List[str] = []
    owner_chunks = chunk_list(owners, chunk_size)
    ref_chunks = chunk_list(ref_owners, chunk_size)
    for owner_chunk in owner_chunks:
        owners_in = ",".join(_quote_owner(s) for s in owner_chunk)
        for ref_chunk in ref_chunks:
            ref_in = ",".join(_quote_owner(s) for s in ref_chunk)
            sql = sql_tpl.format(owners_in=owners_in, ref_owners_in=ref_in)
            ok, out, err = obclient_run_sql(ob_cfg, sql)
            if not ok:
                return False, [], err
            if out:
                lines.extend(out.splitlines())
    return True, lines, ""


def dump_ob_metadata(
    ob_cfg: ObConfig,
    target_schemas: Set[str],
    tracked_object_types: Optional[Set[str]] = None,
    synonym_check_scope: str = "all",
    include_tab_columns: bool = True,
    include_column_order: bool = False,
    include_indexes: bool = True,
    include_constraints: bool = True,
    include_triggers: bool = True,
    include_sequences: bool = True,
    include_comments: bool = True,
    include_roles: bool = False,
    target_table_pairs: Optional[Set[Tuple[str, str]]] = None
) -> ObMetadata:
    """
    一次性从 OceanBase dump 所有需要的元数据，返回 ObMetadata。
    如果任何关键视图查询失败，则视为致命错误并退出。
    """
    if not target_schemas:
        log.warning("目标 schema 集合为空，OB 元数据转储将返回空结构。")
        return ObMetadata(
            objects_by_type={},
            tab_columns={},
            invisible_column_supported=False,
            identity_column_supported=False,
            default_on_null_supported=False,
            indexes={},
            constraints={},
            triggers={},
            sequences={},
            sequence_attrs={},
            roles=set(),
            table_comments={},
            column_comments={},
            comments_complete=False,
            object_statuses={},
            package_errors={},
            package_errors_complete=False,
            partition_key_columns={},
            constraint_deferrable_supported=False
        )

    synonym_scope = normalize_synonym_check_scope(synonym_check_scope)
    ob_default_char_used = "B"
    if include_tab_columns:
        ok, out, err = obclient_run_sql(
            ob_cfg,
            "SELECT value FROM nls_database_parameters WHERE parameter='NLS_LENGTH_SEMANTICS'"
        )
        if ok and out:
            nls_val = out.splitlines()[0].strip().upper()
            if nls_val.startswith("CHAR"):
                ob_default_char_used = "C"
            elif nls_val.startswith("BYTE"):
                ob_default_char_used = "B"
        else:
            log.warning(
                "读取 OB NLS_LENGTH_SEMANTICS 失败，将按 BYTE 语义处理: %s",
                err or "unknown"
            )

    owners_in_list = sorted(target_schemas)
    owners_in_objects_list = list(owners_in_list)
    if 'PUBLIC' in target_schemas and '__PUBLIC' not in owners_in_objects_list:
        owners_in_objects_list.append('__PUBLIC')

    constraint_deferrable_supported = False

    # --- 1. DBA_OBJECTS ---
    objects_by_type: Dict[str, Set[str]] = {}
    object_statuses: Dict[Tuple[str, str, str], str] = {}
    object_types_filter = tracked_object_types or set(ALL_TRACKED_OBJECT_TYPES)
    if not object_types_filter:
        object_types_filter = {'TABLE'}
    object_types_clause = ",".join(f"'{obj}'" for obj in sorted(object_types_filter))

    sql_tpl = f"""
        SELECT OWNER, OBJECT_NAME, OBJECT_TYPE, STATUS
        FROM DBA_OBJECTS
        WHERE UPPER(OWNER) IN ({{owners_in}})
          AND OBJECT_TYPE IN (
              {object_types_clause}
          )
    """
    ok, lines, err = obclient_query_by_owner_chunks(
        ob_cfg,
        sql_tpl,
        sorted(owners_in_objects_list)
    )
    if not ok:
        log.error("无法从 OB 读取 DBA_OBJECTS，程序退出。")
        abort_run()

    if lines:
        for line in lines:
            parts = line.split('\t')
            if len(parts) < 3:
                continue
            owner = parts[0].strip().upper()
            name = parts[1].strip().upper()
            obj_type = parts[2].strip().upper()
            status = parts[3].strip().upper() if len(parts) > 3 else "UNKNOWN"
            if obj_type == 'SYNONYM' and owner == '__PUBLIC':
                owner = 'PUBLIC'
            if obj_type == 'SYNONYM' and synonym_scope == 'public_only' and owner != 'PUBLIC':
                continue
            full = f"{owner}.{name}"
            objects_by_type.setdefault(obj_type, set()).add(full)
            object_statuses[(owner, name, obj_type)] = status or "UNKNOWN"

    # 补充 DBA_TYPES (部分 OB 环境中 TYPE 不出现在 DBA_OBJECTS)
    # 注意：DBA_TYPES.TYPECODE=OBJECT 仅表示对象类型，本身不代表存在 TYPE BODY，
    # 过去直接据此推断 TYPE BODY 会导致误报，因此这里只补 TYPE。
    if 'TYPE' in object_types_filter or 'TYPE BODY' in object_types_filter:
        sql_types_tpl = """
            SELECT OWNER, TYPE_NAME, TYPECODE
            FROM DBA_TYPES
            WHERE OWNER IN ({owners_in})
        """
        ok, lines, err = obclient_query_by_owner_chunks(ob_cfg, sql_types_tpl, owners_in_list)
        if not ok:
            log.warning("读取 DBA_TYPES 失败，TYPE 检查可能不完整: %s", err)
        elif lines:
            for line in lines:
                parts = line.split('\t')
                if len(parts) < 2:
                    continue
                owner, name = parts[0].strip().upper(), parts[1].strip().upper()
                full = f"{owner}.{name}"
                objects_by_type.setdefault('TYPE', set()).add(full)

    # 仅在显式启用 TYPE BODY 检查时，通过 DBA_SOURCE 探测真实 TYPE BODY
    if 'TYPE BODY' in object_types_filter:
        sql_type_body_tpl = """
            SELECT OWNER, NAME
            FROM DBA_SOURCE
            WHERE OWNER IN ({owners_in})
              AND TYPE = 'TYPE BODY'
            GROUP BY OWNER, NAME
        """
        ok, lines, err = obclient_query_by_owner_chunks(ob_cfg, sql_type_body_tpl, owners_in_list)
        if not ok:
            log.warning("读取 DBA_SOURCE(TYPE BODY) 失败，将不补充 TYPE BODY 元数据: %s", err)
        elif lines:
            for line in lines:
                parts = line.split('\t')
                if len(parts) < 2:
                    continue
                owner, name = parts[0].strip().upper(), parts[1].strip().upper()
                full = f"{owner}.{name}"
                objects_by_type.setdefault('TYPE BODY', set()).add(full)

    package_errors: Dict[Tuple[str, str, str], PackageErrorInfo] = {}
    package_errors_complete = True
    if 'PACKAGE' in object_types_filter or 'PACKAGE BODY' in object_types_filter:
        sql_pkg_err_tpl = """
            SELECT OWNER, NAME, TYPE, LINE, POSITION, TEXT
            FROM DBA_ERRORS
            WHERE OWNER IN ({owners_in})
              AND TYPE IN ('PACKAGE', 'PACKAGE BODY')
            ORDER BY OWNER, NAME, TYPE, SEQUENCE
        """
        ok, lines, err = obclient_query_by_owner_chunks(ob_cfg, sql_pkg_err_tpl, owners_in_list)
        if not ok:
            package_errors_complete = False
            log.warning("读取 OB DBA_ERRORS 失败，包错误信息将为空: %s", err)
        elif lines:
            temp_errors: Dict[Tuple[str, str, str], Dict[str, object]] = defaultdict(
                lambda: {"count": 0, "first_error": ""}
            )
            for line in lines:
                parts = line.split('\t')
                if len(parts) < 6:
                    continue
                owner = parts[0].strip().upper()
                name = parts[1].strip().upper()
                err_type = parts[2].strip().upper()
                err_line = parts[3].strip()
                err_pos = parts[4].strip()
                err_text = normalize_error_text(parts[5])
                if not owner or not name or not err_type:
                    continue
                key = (owner, name, err_type)
                entry = temp_errors[key]
                entry["count"] = int(entry["count"]) + 1
                if not entry["first_error"]:
                    prefix = f"L{err_line}:{err_pos} " if err_line or err_pos else ""
                    entry["first_error"] = f"{prefix}{err_text}".strip()
            for key, info in temp_errors.items():
                package_errors[key] = PackageErrorInfo(
                    count=int(info.get("count") or 0),
                    first_error=str(info.get("first_error") or "")
                )

    # --- 2. DBA_TAB_COLUMNS ---
    tab_cols_view = "DBA_TAB_COLUMNS"
    support_hidden_col = False
    support_virtual_col = False
    support_identity_col = False
    support_default_on_null = False
    support_invisible_col = False
    support_column_id = False
    if include_tab_columns:
        def _probe_ob_dict_col(col_name: str, view_name: str) -> bool:
            sql = (
                "SELECT COUNT(*) FROM DBA_TAB_COLUMNS "
                f"WHERE OWNER='SYS' AND TABLE_NAME='{view_name}' "
                f"AND COLUMN_NAME='{col_name}'"
            )
            ok, out, err = obclient_run_sql(ob_cfg, sql)
            if not ok:
                log.info("OB 字典列探测失败 %s.%s: %s", view_name, col_name, err)
                return False
            if not out:
                return False
            match = re.search(r"\d+", out)
            return bool(match and int(match.group(0)) > 0)

        optional_cols = (
            "HIDDEN_COLUMN",
            "VIRTUAL_COLUMN",
            "IDENTITY_COLUMN",
            "DEFAULT_ON_NULL",
            "INVISIBLE_COLUMN"
        )
        if include_column_order:
            optional_cols = optional_cols + ("COLUMN_ID",)
        support_by_view: Dict[str, Dict[str, bool]] = {
            "DBA_TAB_COLUMNS": {col: False for col in optional_cols},
            "DBA_TAB_COLS": {col: False for col in optional_cols}
        }
        for view_name in ("DBA_TAB_COLUMNS", "DBA_TAB_COLS"):
            for col_name in optional_cols:
                support_by_view[view_name][col_name] = _probe_ob_dict_col(col_name, view_name)
        tab_cols_view, support_cols, missing_cols = select_tab_columns_view(
            "DBA_TAB_COLUMNS",
            support_by_view["DBA_TAB_COLUMNS"],
            "DBA_TAB_COLS",
            support_by_view["DBA_TAB_COLS"]
        )
        support_hidden_col = bool(support_cols.get("HIDDEN_COLUMN"))
        support_virtual_col = bool(support_cols.get("VIRTUAL_COLUMN"))
        support_identity_col = bool(support_cols.get("IDENTITY_COLUMN"))
        support_default_on_null = bool(support_cols.get("DEFAULT_ON_NULL"))
        support_invisible_col = bool(support_cols.get("INVISIBLE_COLUMN"))
        if include_column_order:
            support_column_id = bool(support_cols.get("COLUMN_ID"))
        if tab_cols_view != "DBA_TAB_COLUMNS" and missing_cols:
            log.info(
                "OB DBA_TAB_COLUMNS 缺少列元数据(%s)，切换为 %s 读取列信息。",
                ",".join(missing_cols),
                tab_cols_view
            )

    hidden_col = ", HIDDEN_COLUMN" if support_hidden_col else ""
    virtual_col = ", VIRTUAL_COLUMN" if support_virtual_col else ""
    identity_col = ", IDENTITY_COLUMN" if support_identity_col else ""
    default_on_null_col = ", DEFAULT_ON_NULL" if support_default_on_null else ""
    invisible_col = ", INVISIBLE_COLUMN" if support_invisible_col else ""
    column_id_col = ", COLUMN_ID" if support_column_id else ""
    sql_cols_ext_tpl = f"""
        SELECT OWNER, TABLE_NAME, COLUMN_NAME, DATA_TYPE,
               CHAR_LENGTH, DATA_LENGTH, DATA_PRECISION, DATA_SCALE, CHAR_USED, NULLABLE,
               REPLACE(REPLACE(REPLACE(DATA_DEFAULT, CHR(10), ' '), CHR(13), ' '), CHR(9), ' ') AS DATA_DEFAULT
               {column_id_col}
               {hidden_col}{virtual_col}{identity_col}{default_on_null_col}{invisible_col}
        FROM {tab_cols_view}
        WHERE OWNER IN ({{owners_in}})
    """
    sql_cols_mid_tpl = f"""
        SELECT OWNER, TABLE_NAME, COLUMN_NAME, DATA_TYPE,
               CHAR_LENGTH, DATA_LENGTH, DATA_PRECISION, DATA_SCALE, NULLABLE, DATA_DEFAULT
               {column_id_col}
               {hidden_col}{virtual_col}{identity_col}{default_on_null_col}{invisible_col}
        FROM {tab_cols_view}
        WHERE OWNER IN ({{owners_in}})
    """
    sql_cols_basic_tpl = f"""
        SELECT OWNER, TABLE_NAME, COLUMN_NAME, DATA_TYPE, CHAR_LENGTH, NULLABLE, DATA_DEFAULT
               {column_id_col}
               {hidden_col}{virtual_col}{identity_col}{default_on_null_col}{invisible_col}
        FROM {tab_cols_view}
        WHERE OWNER IN ({{owners_in}})
    """

    tab_columns: Dict[Tuple[str, str], Dict[str, Dict]] = {}
    if include_tab_columns:
        parse_mode = "ext"
        ok, lines, err = obclient_query_by_owner_chunks(ob_cfg, sql_cols_ext_tpl, owners_in_list)
        if not ok:
            parse_mode = "mid"
            log.warning("读取 OB DBA_TAB_COLUMNS(含CHAR_USED)失败，将回退中间查询：%s", err)
            ok, lines, err = obclient_query_by_owner_chunks(ob_cfg, sql_cols_mid_tpl, owners_in_list)
        if not ok:
            parse_mode = "basic"
            log.warning("读取 OB DBA_TAB_COLUMNS(中间查询)失败，将回退基础查询：%s", err)
            ok, lines, err = obclient_query_by_owner_chunks(ob_cfg, sql_cols_basic_tpl, owners_in_list)
        if not ok:
            log.error("无法从 OB 读取 DBA_TAB_COLUMNS，程序退出。")
            abort_run()

        def _normalize_flag(value: str) -> Optional[bool]:
            if value is None:
                return None
            text = str(value).strip().upper()
            if not text or text == "NULL":
                return None
            if text in ("YES", "Y", "TRUE", "1"):
                return True
            if text in ("NO", "N", "FALSE", "0"):
                return False
            return None

        if lines:
            for line in lines:
                parts = line.split('\t')
                if len(parts) < 7:
                    continue
                owner = parts[0].strip().upper()
                table = parts[1].strip().upper()
                col = parts[2].strip().upper()
                dtype = parts[3].strip().upper()
                char_len = parts[4].strip()
                data_length = None
                data_precision = None
                data_scale = None
                char_used = None
                nullable = ""
                default = ""
                idx = 0
                column_id = None
                if parse_mode == "ext":
                    data_length = parts[5].strip()
                    data_precision = parts[6].strip()
                    data_scale = parts[7].strip()
                    char_used = parts[8].strip()
                    nullable = parts[9].strip()
                    default = parts[10].strip() if len(parts) > 10 else ""
                    idx = 11
                elif parse_mode == "mid":
                    data_length = parts[5].strip()
                    data_precision = parts[6].strip()
                    data_scale = parts[7].strip()
                    nullable = parts[8].strip()
                    default = parts[9].strip() if len(parts) > 9 else ""
                    idx = 10
                else:
                    nullable = parts[5].strip()
                    default = parts[6].strip() if len(parts) > 6 else ""
                    idx = 7

                if support_column_id and len(parts) > idx:
                    column_id = parse_column_id(parts[idx])
                    idx += 1

                hidden_flag = None
                virtual_flag = None
                identity_flag = None
                default_on_null_flag = None
                invisible_flag = None
                if support_hidden_col and len(parts) > idx:
                    hidden_flag = _normalize_flag(parts[idx])
                    idx += 1
                if support_virtual_col and len(parts) > idx:
                    virtual_flag = _normalize_flag(parts[idx])
                    idx += 1
                if support_identity_col and len(parts) > idx:
                    identity_flag = _normalize_flag(parts[idx])
                    idx += 1
                if support_default_on_null and len(parts) > idx:
                    default_on_null_flag = _normalize_flag(parts[idx])
                    idx += 1
                if support_invisible_col and len(parts) > idx:
                    invisible_flag = _normalize_flag(parts[idx])

                key = (owner, table)
                char_used_clean = char_used.strip().upper() if char_used else ""
                if char_used_clean in ("", "NULL"):
                    char_used_clean = ""
                if not char_used_clean:
                    if dtype in ("VARCHAR2", "VARCHAR", "CHAR"):
                        try:
                            if (
                                (data_length or "").isdigit()
                                and (char_len or "").isdigit()
                                and int(data_length) > int(char_len)
                            ):
                                char_used_clean = "C"
                        except ValueError:
                            char_used_clean = ""
                    if not char_used_clean:
                        char_used_clean = ob_default_char_used
                is_virtual = bool(virtual_flag)
                tab_columns.setdefault(key, {})[col] = {
                    "data_type": dtype,
                    "char_length": int(char_len) if char_len.isdigit() else None,
                    "data_length": int(data_length) if (data_length or "").isdigit() else None,
                    "data_precision": int(data_precision) if (data_precision or "").isdigit() else None,
                    "data_scale": int(data_scale) if (data_scale or "").isdigit() else None,
                    "char_used": char_used_clean or None,
                    "nullable": nullable,
                    "data_default": default,
                    "column_id": column_id,
                    "hidden": bool(hidden_flag) if support_hidden_col else False,
                    "virtual": is_virtual,
                    "virtual_expr": default if is_virtual else None,
                    "identity": identity_flag,
                    "default_on_null": default_on_null_flag,
                    "invisible": invisible_flag if support_invisible_col else None
                }

    identity_column_supported = False
    default_on_null_supported = False
    if include_tab_columns and tab_columns:
        identity_column_supported = support_identity_col and any(
            meta.get("identity") is not None
            for table_meta in tab_columns.values()
            for meta in table_meta.values()
        )
        default_on_null_supported = support_default_on_null and any(
            meta.get("default_on_null") is not None
            for table_meta in tab_columns.values()
            for meta in table_meta.values()
        )

    # --- 2.b 注释 (DBA_TAB_COMMENTS / DBA_COL_COMMENTS) ---
    table_comments: Dict[Tuple[str, str], Optional[str]] = {}
    column_comments: Dict[Tuple[str, str], Dict[str, Optional[str]]] = {}
    comments_complete = False
    if include_comments:
        target_pairs = target_table_pairs or set()
        if not target_pairs:
            comments_complete = True
        else:
            comment_keys = sorted(f"{owner}.{table}" for owner, table in target_pairs)
            comments_complete = True

            for chunk in chunk_list(comment_keys, COMMENT_BATCH_SIZE):
                key_clause = ",".join(f"'{val}'" for val in chunk)
                sql_tab_cmt = f"""
                    SELECT OWNER, TABLE_NAME,
                           REPLACE(REPLACE(REPLACE(COMMENTS, CHR(10), ' '), CHR(13), ' '), CHR(9), ' ') AS COMMENTS
                    FROM DBA_TAB_COMMENTS
                    WHERE OWNER||'.'||TABLE_NAME IN ({key_clause})
                """
                ok, out, err = obclient_run_sql(ob_cfg, sql_tab_cmt)
                if not ok:
                    log.warning("无法从 OB 读取 DBA_TAB_COMMENTS，注释比对将跳过：%s", err)
                    comments_complete = False
                    break
                if out:
                    for line in out.splitlines():
                        parts = line.split('\t')
                        if len(parts) < 3:
                            continue
                        owner = parts[0].strip().upper()
                        table = parts[1].strip().upper()
                        comment = parts[2].strip() if len(parts) >= 3 else None
                        table_comments[(owner, table)] = comment

            if comments_complete:
                for chunk in chunk_list(comment_keys, COMMENT_BATCH_SIZE):
                    key_clause = ",".join(f"'{val}'" for val in chunk)
                    sql_col_cmt = f"""
                        SELECT OWNER, TABLE_NAME, COLUMN_NAME,
                               REPLACE(REPLACE(REPLACE(COMMENTS, CHR(10), ' '), CHR(13), ' '), CHR(9), ' ') AS COMMENTS
                        FROM DBA_COL_COMMENTS
                        WHERE OWNER||'.'||TABLE_NAME IN ({key_clause})
                    """
                    ok, out, err = obclient_run_sql(ob_cfg, sql_col_cmt)
                    if not ok:
                        log.warning("无法从 OB 读取 DBA_COL_COMMENTS，注释比对将跳过：%s", err)
                        comments_complete = False
                        break
                    if out:
                        for line in out.splitlines():
                            parts = line.split('\t')
                            if len(parts) < 4:
                                continue
                            owner = parts[0].strip().upper()
                            table = parts[1].strip().upper()
                            column = parts[2].strip().upper()
                            comment = parts[3].strip() if len(parts) >= 4 else None
                            column_comments.setdefault((owner, table), {})[column] = comment
            if comments_complete and target_pairs and not table_comments and not column_comments:
                log.warning("OB 端注释查询未返回任何记录，可能缺少权限，注释比对将跳过。")
                comments_complete = False

    # --- 3. DBA_INDEXES ---
    indexes: Dict[Tuple[str, str], Dict[str, Dict]] = {}
    if include_indexes:
        sql_tpl = """
            SELECT TABLE_OWNER, TABLE_NAME, INDEX_NAME, UNIQUENESS
            FROM DBA_INDEXES
            WHERE TABLE_OWNER IN ({owners_in})
        """
        ok, lines, err = obclient_query_by_owner_chunks(ob_cfg, sql_tpl, owners_in_list)
        if not ok:
            log.error("无法从 OB 读取 DBA_INDEXES，程序退出。")
            abort_run()

        if lines:
            for line in lines:
                parts = line.split('\t')
                if len(parts) < 4:
                    continue
                t_owner, t_name, idx_name, uniq = (
                    parts[0].strip().upper(),
                    parts[1].strip().upper(),
                    parts[2].strip().upper(),
                    parts[3].strip().upper()
                )
                key = (t_owner, t_name)
                indexes.setdefault(key, {})[idx_name] = {
                    "uniqueness": uniq,
                    "columns": [],
                    "expressions": {},
                    "descend": []
                }

        # --- 4. DBA_IND_COLUMNS ---
        has_descend_col = ob_has_dba_column(ob_cfg, "DBA_IND_COLUMNS", "DESCEND")
        descend_select = ", DESCEND" if has_descend_col else ""
        sql_tpl = f"""
            SELECT TABLE_OWNER, TABLE_NAME, INDEX_NAME, COLUMN_NAME, COLUMN_POSITION{descend_select}
            FROM DBA_IND_COLUMNS
            WHERE TABLE_OWNER IN ({{owners_in}})
            ORDER BY TABLE_OWNER, TABLE_NAME, INDEX_NAME, COLUMN_POSITION
        """
        ok, lines, err = obclient_query_by_owner_chunks(ob_cfg, sql_tpl, owners_in_list)
        if not ok:
            log.error("无法从 OB 读取 DBA_IND_COLUMNS，程序退出。")
            abort_run()

        if lines:
            for line in lines:
                parts = line.split('\t')
                if len(parts) < 5:
                    continue
                t_owner, t_name, idx_name, col_name = (
                    parts[0].strip().upper(),
                    parts[1].strip().upper(),
                    parts[2].strip().upper(),
                    parts[3].strip().upper()
                )
                descend_flag = parts[5].strip().upper() if has_descend_col and len(parts) > 5 else ""
                key = (t_owner, t_name)
                # 只有在 DBA_INDEXES 已有记录时才补充列，避免虚构 UNKNOWN 索引
                idx_info = indexes.get(key, {}).get(idx_name)
                if idx_info is None:
                    log.debug("索引 %s.%s.%s 未出现在 DBA_INDEXES，跳过列信息。", t_owner, t_name, idx_name)
                    continue
                idx_info["columns"].append(col_name)
                idx_info["descend"].append(descend_flag or "ASC")

        # 尝试读取函数索引表达式（可能在部分 OB 版本不可用）
        sql_expr_tpl = """
            SELECT TABLE_OWNER, TABLE_NAME, INDEX_NAME, COLUMN_POSITION, COLUMN_EXPRESSION
            FROM DBA_IND_EXPRESSIONS
            WHERE TABLE_OWNER IN ({owners_in})
            ORDER BY TABLE_OWNER, TABLE_NAME, INDEX_NAME, COLUMN_POSITION
        """
        ok, lines, err = obclient_query_by_owner_chunks(ob_cfg, sql_expr_tpl, owners_in_list)
        if ok and lines:
            for line in lines:
                parts = line.split('\t')
                if len(parts) < 5:
                    continue
                t_owner, t_name, idx_name = (
                    parts[0].strip().upper(),
                    parts[1].strip().upper(),
                    parts[2].strip().upper()
                )
                try:
                    pos = int(parts[3]) if parts[3] else None
                except (TypeError, ValueError):
                    pos = None
                expr = parts[4].strip() if len(parts) >= 5 else ""
                if not t_owner or not t_name or not idx_name or pos is None or not expr:
                    continue
                key = (t_owner, t_name)
                idx_info = indexes.get(key, {}).get(idx_name)
                if idx_info is None:
                    continue
                idx_info.setdefault("expressions", {})[pos] = expr
        elif err:
            log.info("OB 未支持 DBA_IND_EXPRESSIONS，函数索引表达式将不参与对比: %s", err)

        # 过滤 OMS_* 自动索引
        for key in list(indexes.keys()):
            pruned = {}
            for idx_name, info in indexes[key].items():
                cols = info.get("columns") or []
                if is_oms_index(idx_name, cols):
                    continue
                pruned[idx_name] = info
            if pruned:
                indexes[key] = pruned
            else:
                del indexes[key]

    # --- 5. DBA_CONSTRAINTS (P/U/R/C) ---
    constraints: Dict[Tuple[str, str], Dict[str, Dict]] = {}
    if include_constraints:
        use_search_condition_vc = ob_has_dba_column(ob_cfg, "DBA_CONSTRAINTS", "SEARCH_CONDITION_VC")
        deferrable_supported = (
            ob_has_dba_column(ob_cfg, "DBA_CONSTRAINTS", "DEFERRABLE")
            and ob_has_dba_column(ob_cfg, "DBA_CONSTRAINTS", "DEFERRED")
        )
        support_index_name = ob_has_dba_column(ob_cfg, "DBA_CONSTRAINTS", "INDEX_NAME")
        support_update_rule = ob_has_dba_column(ob_cfg, "DBA_CONSTRAINTS", "UPDATE_RULE")
        support_validated = ob_has_dba_column(ob_cfg, "DBA_CONSTRAINTS", "VALIDATED")
        constraint_deferrable_supported = deferrable_supported
        deferrable_select = ", DEFERRABLE, DEFERRED" if deferrable_supported else ""
        index_name_select = ", INDEX_NAME" if support_index_name else ""
        update_rule_select = ", UPDATE_RULE" if support_update_rule else ""
        validated_select = ", VALIDATED" if support_validated else ""

        sql_ext_tpl_vc = f"""
            SELECT OWNER, TABLE_NAME, CONSTRAINT_NAME, CONSTRAINT_TYPE, STATUS{validated_select}{index_name_select}, R_OWNER, R_CONSTRAINT_NAME,
                   DELETE_RULE{update_rule_select},
                   REPLACE(REPLACE(REPLACE(SEARCH_CONDITION_VC, CHR(10), ' '), CHR(13), ' '), CHR(9), ' ') AS SEARCH_CONDITION
                   {deferrable_select}
            FROM DBA_CONSTRAINTS
            WHERE OWNER IN ({{owners_in}})
              AND CONSTRAINT_TYPE IN ('P','U','R','C')
        """
        sql_ext_tpl = f"""
            SELECT OWNER, TABLE_NAME, CONSTRAINT_NAME, CONSTRAINT_TYPE, STATUS{validated_select}{index_name_select}, R_OWNER, R_CONSTRAINT_NAME,
                   DELETE_RULE{update_rule_select},
                   REPLACE(REPLACE(REPLACE(SEARCH_CONDITION, CHR(10), ' '), CHR(13), ' '), CHR(9), ' ') AS SEARCH_CONDITION
                   {deferrable_select}
            FROM DBA_CONSTRAINTS
            WHERE OWNER IN ({{owners_in}})
              AND CONSTRAINT_TYPE IN ('P','U','R','C')
        """
        ok = False
        lines: List[str] = []
        err = ""
        if use_search_condition_vc:
            ok, lines, err = obclient_query_by_owner_chunks(ob_cfg, sql_ext_tpl_vc, owners_in_list)
        support_fk_ref = False
        support_search_condition = False
        if ok:
            support_fk_ref = True
            support_search_condition = True
        if not ok:
            if err:
                log.warning("读取 OB DBA_CONSTRAINTS(含 SEARCH_CONDITION_VC)失败，将尝试 SEARCH_CONDITION: %s", err)
            ok, lines, err = obclient_query_by_owner_chunks(ob_cfg, sql_ext_tpl, owners_in_list)
            if ok:
                support_fk_ref = True
                support_search_condition = True
        if not ok:
            log.warning("读取 OB DBA_CONSTRAINTS(含条件)失败，将回退为中间字段：%s", err)
            sql_mid_tpl = f"""
                SELECT OWNER, TABLE_NAME, CONSTRAINT_NAME, CONSTRAINT_TYPE, STATUS{validated_select}{index_name_select}, R_OWNER, R_CONSTRAINT_NAME, DELETE_RULE{update_rule_select}
                       {deferrable_select}
                FROM DBA_CONSTRAINTS
                WHERE OWNER IN ({{owners_in}})
                  AND CONSTRAINT_TYPE IN ('P','U','R','C')
            """
            ok, lines, err = obclient_query_by_owner_chunks(ob_cfg, sql_mid_tpl, owners_in_list)
            if ok:
                support_fk_ref = True
                support_search_condition = False
        if not ok:
            log.warning("读取 OB DBA_CONSTRAINTS(含引用信息)失败，将回退为基础字段：%s", err)
            sql_tpl = f"""
                SELECT OWNER, TABLE_NAME, CONSTRAINT_NAME, CONSTRAINT_TYPE, STATUS{validated_select}{index_name_select}
                       {deferrable_select}
                FROM DBA_CONSTRAINTS
                WHERE OWNER IN ({{owners_in}})
                  AND CONSTRAINT_TYPE IN ('P','U','R','C')
            """
            ok, lines, err = obclient_query_by_owner_chunks(ob_cfg, sql_tpl, owners_in_list)
            if not ok:
                log.error("无法从 OB 读取 DBA_CONSTRAINTS，程序退出。")
                abort_run()

        if lines:
            for line in lines:
                parts = line.split('\t')
                if len(parts) < 4:
                    continue
                idx = 0
                owner = parts[idx].strip().upper()
                idx += 1
                table = parts[idx].strip().upper()
                idx += 1
                cons_name = parts[idx].strip().upper()
                idx += 1
                ctype = parts[idx].strip().upper()
                idx += 1
                cons_status = parts[idx].strip().upper() if len(parts) > idx else None
                idx += 1
                validated = None
                if support_validated and len(parts) > idx:
                    validated = parts[idx].strip().upper()
                    idx += 1
                index_name = None
                if support_index_name and len(parts) > idx:
                    index_name = parts[idx].strip().upper()
                    idx += 1
                r_owner = r_cons = delete_rule = update_rule = None
                if support_fk_ref:
                    r_owner = parts[idx].strip().upper() if len(parts) > idx else None
                    idx += 1
                    r_cons = parts[idx].strip().upper() if len(parts) > idx else None
                    idx += 1
                    delete_rule = parts[idx].strip().upper() if len(parts) > idx else None
                    idx += 1
                    if support_update_rule and len(parts) > idx:
                        update_rule = parts[idx].strip().upper() if len(parts) > idx else None
                        idx += 1
                search_condition = None
                if support_search_condition and len(parts) > idx:
                    search_condition = parts[idx].strip()
                    idx += 1
                deferrable = deferred = None
                if constraint_deferrable_supported and len(parts) > idx:
                    deferrable = parts[idx].strip() if len(parts) > idx else None
                    idx += 1
                    deferred = parts[idx].strip() if len(parts) > idx else None
                    idx += 1
                key = (owner, table)
                constraints.setdefault(key, {})[cons_name] = {
                    "type": ctype,
                    "status": cons_status,
                    "validated": validated,
                    "columns": [],
                    "index_name": index_name,
                    "r_owner": r_owner if ctype == "R" else None,
                    "r_constraint": r_cons if ctype == "R" else None,
                    "delete_rule": delete_rule if ctype == "R" else None,
                    "update_rule": update_rule if ctype == "R" else None,
                    "search_condition": search_condition if ctype == "C" else None,
                    "deferrable": deferrable,
                    "deferred": deferred,
                }

        # --- 6. DBA_CONS_COLUMNS ---
        sql_tpl = """
            SELECT OWNER, TABLE_NAME, CONSTRAINT_NAME, COLUMN_NAME, POSITION
            FROM DBA_CONS_COLUMNS
            WHERE OWNER IN ({owners_in})
            ORDER BY OWNER, TABLE_NAME, CONSTRAINT_NAME, POSITION
        """
        ok, lines, err = obclient_query_by_owner_chunks(ob_cfg, sql_tpl, owners_in_list)
        if not ok:
            log.error("无法从 OB 读取 DBA_CONS_COLUMNS，程序退出。")
            abort_run()

        if lines:
            for line in lines:
                parts = line.split('\t')
                if len(parts) < 5:
                    continue
                owner, table, cons_name, col_name = (
                    parts[0].strip().upper(),
                    parts[1].strip().upper(),
                    parts[2].strip().upper(),
                    parts[3].strip().upper()
                )
                key = (owner, table)
                if key not in constraints:
                    constraints[key] = {}
                if cons_name not in constraints[key]:
                    constraints[key][cons_name] = {
                        "type": "UNKNOWN",
                        "status": None,
                        "validated": None,
                        "columns": [],
                        "index_name": None,
                        "r_owner": None,
                        "r_constraint": None,
                        "delete_rule": None,
                        "update_rule": None,
                        "search_condition": None,
                        "deferrable": None,
                        "deferred": None,
                    }
                constraints[key][cons_name]["columns"].append(col_name)

        # 为外键补齐被引用表信息（若 OB 提供 R_OWNER/R_CONSTRAINT_NAME）
        if support_fk_ref:
            cons_table_lookup: Dict[Tuple[str, str], Tuple[str, str]] = {}
            for (owner, table), cons_map in constraints.items():
                for cons_name, info in cons_map.items():
                    ctype = (info.get("type") or "").upper()
                    if ctype in ('P', 'U'):
                        cons_table_lookup[(owner, cons_name)] = (owner, table)
            for (_owner, _table), cons_map in constraints.items():
                for cons_name, info in cons_map.items():
                    ctype = (info.get("type") or "").upper()
                    if ctype != 'R':
                        continue
                    r_owner_u = (info.get("r_owner") or "").upper()
                    r_cons_u = (info.get("r_constraint") or "").upper()
                    if not r_owner_u or not r_cons_u:
                        continue
                    ref_table = cons_table_lookup.get((r_owner_u, r_cons_u))
                    if ref_table:
                        info["ref_table_owner"], info["ref_table_name"] = ref_table

        # 过滤 OceanBase 自动生成的 *_OBNOTNULL_* CHECK 约束
        if constraints:
            pruned_constraints: Dict[Tuple[str, str], Dict[str, Dict]] = {}
            removed_cnt = 0
            for key, cons_map in constraints.items():
                kept: Dict[str, Dict] = {}
                for cons_name, info in cons_map.items():
                    if is_ob_notnull_constraint(cons_name, info.get("search_condition")):
                        removed_cnt += 1
                        continue
                    kept[cons_name] = info
                if kept:
                    pruned_constraints[key] = kept
            if removed_cnt:
                log.info("[CONSTRAINT] 已忽略 %d 条 OceanBase 自动 NOT NULL 类约束（OBNOTNULL/OBCHECK）。", removed_cnt)
            constraints = pruned_constraints

    # --- 7. DBA_TRIGGERS ---
    triggers: Dict[Tuple[str, str], Dict[str, Dict]] = {}
    if include_triggers:
        sql_tpl = """
            SELECT OWNER, TABLE_OWNER, TABLE_NAME, TRIGGER_NAME, TRIGGERING_EVENT, STATUS
            FROM DBA_TRIGGERS
            WHERE TABLE_OWNER IN ({owners_in})
        """
        ok, lines, err = obclient_query_by_owner_chunks(ob_cfg, sql_tpl, owners_in_list)
        if not ok:
            log.error("无法从 OB 读取 DBA_TRIGGERS，程序退出。")
            abort_run()

        if lines:
            for line in lines:
                parts = line.split('\t')
                if len(parts) < 6:
                    continue
                trg_owner = parts[0].strip().upper()
                t_owner = parts[1].strip().upper()
                t_name = parts[2].strip().upper()
                trg_name = parts[3].strip().upper()
                ev = parts[4].strip()
                status = parts[5].strip() if len(parts) > 5 else ""
                key = (t_owner, t_name)
                trg_owner_u = trg_owner or t_owner
                trg_key = f"{trg_owner_u}.{trg_name}" if trg_owner_u and trg_name else trg_name
                triggers.setdefault(key, {})[trg_key] = {
                    "event": ev,
                    "status": status,
                    "owner": trg_owner_u,
                    "name": trg_name
                }

    # --- 8. DBA_SEQUENCES ---
    sequences: Dict[str, Set[str]] = {}
    sequence_attrs: Dict[str, Dict[str, Dict[str, object]]] = {}
    sequence_attrs: Dict[str, Dict[str, Dict[str, object]]] = {}
    partition_key_columns: Dict[Tuple[str, str], List[str]] = {}
    if include_sequences:
        sql_tpl = """
            SELECT SEQUENCE_OWNER, SEQUENCE_NAME
            FROM DBA_SEQUENCES
            WHERE SEQUENCE_OWNER IN ({owners_in})
        """
        ok, lines, err = obclient_query_by_owner_chunks(ob_cfg, sql_tpl, owners_in_list)
        if not ok:
            log.error("无法从 OB 读取 DBA_SEQUENCES，程序退出。")
            abort_run()

        if lines:
            for line in lines:
                parts = line.split('\t')
                if len(parts) < 2:
                    continue
                owner, seq_name = parts[0].strip().upper(), parts[1].strip().upper()
                sequences.setdefault(owner, set()).add(seq_name)

        sql_attr_tpl = """
            SELECT SEQUENCE_OWNER, SEQUENCE_NAME,
                   INCREMENT_BY, MIN_VALUE, MAX_VALUE, CYCLE_FLAG, ORDER_FLAG, CACHE_SIZE
            FROM DBA_SEQUENCES
            WHERE SEQUENCE_OWNER IN ({owners_in})
        """
        ok, lines, err = obclient_query_by_owner_chunks(ob_cfg, sql_attr_tpl, owners_in_list)
        if not ok:
            log.warning("读取 OB DBA_SEQUENCES(属性)失败，将跳过序列属性对比: %s", err)
        elif lines:
            for line in lines:
                parts = line.split('\t')
                if len(parts) < 8:
                    continue
                owner = parts[0].strip().upper()
                name = parts[1].strip().upper()
                if not owner or not name:
                    continue
                def _as_int(val: str) -> Optional[int]:
                    try:
                        return int(val)
                    except (TypeError, ValueError):
                        return None
                seq_info = {
                    "increment_by": _as_int(parts[2].strip()),
                    "min_value": _as_int(parts[3].strip()),
                    "max_value": _as_int(parts[4].strip()),
                    "cycle_flag": (parts[5] or "").strip().upper(),
                    "order_flag": (parts[6] or "").strip().upper(),
                    "cache_size": _as_int(parts[7].strip())
                }
                sequence_attrs.setdefault(owner, {})[name] = seq_info

    # --- 8.5. DBA_PART_KEY_COLUMNS / DBA_SUBPART_KEY_COLUMNS ---
    if include_constraints:
        target_table_pairs_u = {(o.upper(), t.upper()) for o, t in (target_table_pairs or set())}

        def _append_ob_part_col(key: Tuple[str, str], col: Optional[str]) -> None:
            if not col:
                return
            cols = partition_key_columns.setdefault(key, [])
            col_u = col.upper()
            if col_u not in cols:
                cols.append(col_u)

        def _load_ob_part_keys(view_name: str, with_object_type: bool = True) -> None:
            if with_object_type:
                sql_tpl = f"""
                    SELECT OWNER, NAME, COLUMN_NAME, COLUMN_POSITION
                    FROM {view_name}
                    WHERE OWNER IN ({{owners_in}})
                      AND OBJECT_TYPE = 'TABLE'
                    ORDER BY OWNER, NAME, COLUMN_POSITION
                """
            else:
                sql_tpl = f"""
                    SELECT OWNER, NAME, COLUMN_NAME, COLUMN_POSITION
                    FROM {view_name}
                    WHERE OWNER IN ({{owners_in}})
                    ORDER BY OWNER, NAME, COLUMN_POSITION
                """
            ok, lines, err = obclient_query_by_owner_chunks(ob_cfg, sql_tpl, owners_in_list)
            if not ok:
                if with_object_type:
                    log.warning("读取 OB %s 失败，将尝试忽略 OBJECT_TYPE: %s", view_name, err)
                    _load_ob_part_keys(view_name, with_object_type=False)
                else:
                    log.warning("读取 OB %s 失败，将跳过分区键列读取: %s", view_name, err)
                return
            if not lines:
                return
            for line in lines:
                parts = line.split('\t')
                if len(parts) < 3:
                    continue
                owner = (parts[0] or "").strip().upper()
                table = (parts[1] or "").strip().upper()
                col = (parts[2] or "").strip().upper()
                if not owner or not table or not col:
                    continue
                key = (owner, table)
                if target_table_pairs_u and key not in target_table_pairs_u:
                    continue
                _append_ob_part_col(key, col)

        _load_ob_part_keys("DBA_PART_KEY_COLUMNS")
        _load_ob_part_keys("DBA_SUBPART_KEY_COLUMNS")

    roles: Set[str] = set()
    # --- 9. DBA_ROLES ---
    if include_roles:
        sql = "SELECT ROLE FROM DBA_ROLES"
        ok, out, err = obclient_run_sql(ob_cfg, sql)
        if not ok:
            log.warning("读取 DBA_ROLES 失败，角色列表可能不完整: %s", err)
        elif out:
            for line in out.splitlines():
                role = (line or "").strip().upper()
                if role:
                    roles.add(role)

    log.info("OceanBase 元数据转储完成 (根据开关加载 DBA_OBJECTS/列/索引/约束/触发器/序列/注释)。")
    ob_meta = ObMetadata(
        objects_by_type=objects_by_type,
        tab_columns=tab_columns,
        invisible_column_supported=support_invisible_col,
        identity_column_supported=identity_column_supported,
        default_on_null_supported=default_on_null_supported,
        indexes=indexes,
        constraints=constraints,
        triggers=triggers,
        sequences=sequences,
        sequence_attrs=sequence_attrs,
        roles=roles,
        table_comments=table_comments,
        column_comments=column_comments,
        comments_complete=comments_complete,
        object_statuses=object_statuses,
        package_errors=package_errors,
        package_errors_complete=package_errors_complete,
        partition_key_columns=partition_key_columns,
        constraint_deferrable_supported=constraint_deferrable_supported
    )
    return normalize_ob_metadata_public_owner(ob_meta)


def load_ob_supported_sys_privs(ob_cfg: ObConfig) -> Set[str]:
    """
    读取 OceanBase 支持的系统权限集合（以 DBA_SYS_PRIVS 中 SYS 拥有的权限为准）。
    """
    privs: Set[str] = set()
    sql = "SELECT PRIVILEGE FROM DBA_SYS_PRIVS WHERE GRANTEE='SYS'"
    ok, out, err = obclient_run_sql(ob_cfg, sql)
    if not ok:
        log.warning("读取 OceanBase DBA_SYS_PRIVS 失败，系统权限过滤将基于 Oracle 合法性校验: %s", err)
        return set()
    if out:
        for line in out.splitlines():
            name = (line or "").strip().upper()
            if name:
                privs.add(name)
    return privs


def load_ob_roles(ob_cfg: ObConfig) -> Optional[Set[str]]:
    """
    读取 OceanBase 侧角色列表，用于避免重复 CREATE ROLE。
    """
    roles: Set[str] = set()
    sql = "SELECT ROLE FROM DBA_ROLES"
    ok, out, err = obclient_run_sql(ob_cfg, sql)
    if not ok:
        log.warning("读取 OceanBase DBA_ROLES 失败，将无法避免重复 CREATE ROLE: %s", err)
        return None
    if out:
        for line in out.splitlines():
            role = (line or "").strip().upper()
            if role:
                roles.add(role)
    if not roles:
        log.warning("OB 端角色列表为空，授权 grantee 角色过滤可能不完整。")
    return roles


def load_ob_users(ob_cfg: ObConfig) -> Optional[Set[str]]:
    """
    读取 OceanBase 侧用户列表，用于过滤授权目标。
    优先使用 DBA_USERS，失败时回退 ALL_USERS。
    """
    users: Set[str] = set()
    sql = "SELECT USERNAME FROM DBA_USERS"
    ok, out, err = obclient_run_sql(ob_cfg, sql)
    if not ok:
        log.warning("读取 OceanBase DBA_USERS 失败，将尝试 ALL_USERS: %s", err)
        sql = "SELECT USERNAME FROM ALL_USERS"
        ok, out, err = obclient_run_sql(ob_cfg, sql)
        if not ok:
            log.warning("读取 OceanBase ALL_USERS 失败，将跳过授权目标用户过滤: %s", err)
            return None
    if out:
        for line in out.splitlines():
            name = (line or "").strip().upper()
            if name:
                users.add(name)
    if not users:
        log.warning("OB 端用户列表为空，授权 grantee 用户过滤可能不完整。")
    return users


def load_ob_grant_catalog(
    ob_cfg: ObConfig,
    grantees: Set[str]
) -> Optional[ObGrantCatalog]:
    """
    读取 OceanBase 端权限目录，用于缺失授权计算。
    仅检查直接授权（DBA_TAB_PRIVS / DBA_SYS_PRIVS / DBA_ROLE_PRIVS）。
    """
    if not grantees:
        return ObGrantCatalog(set(), set(), set(), set(), set(), set())

    def _sql_list(vals: List[str]) -> str:
        safe_vals = [v.replace("'", "''") for v in vals if v]
        return ",".join(f"'{v}'" for v in safe_vals)

    object_privs: Set[Tuple[str, str, str]] = set()
    object_privs_grantable: Set[Tuple[str, str, str]] = set()
    sys_privs: Set[Tuple[str, str]] = set()
    sys_privs_admin: Set[Tuple[str, str]] = set()
    role_privs: Set[Tuple[str, str]] = set()
    role_privs_admin: Set[Tuple[str, str]] = set()

    try:
        for chunk in chunk_list(sorted(grantees), 900):
            grantee_list = _sql_list([g.upper() for g in chunk if g])
            if not grantee_list:
                continue

            tab_sql = textwrap.dedent(f"""
                SELECT GRANTEE, PRIVILEGE, OWNER, TABLE_NAME, GRANTABLE
                FROM DBA_TAB_PRIVS
                WHERE GRANTEE IN ({grantee_list})
            """).strip()
            ok, out, err = obclient_run_sql(ob_cfg, tab_sql)
            if not ok:
                log.warning("[GRANT_MISS] 读取 DBA_TAB_PRIVS 失败: %s", err)
                return None
            if out:
                for line in out.splitlines():
                    parts = line.split('\t')
                    if len(parts) < 5:
                        continue
                    grantee = (parts[0] or "").strip().upper()
                    priv = (parts[1] or "").strip().upper()
                    owner = (parts[2] or "").strip().upper()
                    name = (parts[3] or "").strip().upper()
                    grantable = (parts[4] or "").strip().upper() == "YES"
                    if not grantee or not priv or not owner or not name:
                        continue
                    key = (grantee, priv, f"{owner}.{name}")
                    object_privs.add(key)
                    if grantable:
                        object_privs_grantable.add(key)

            sys_sql = textwrap.dedent(f"""
                SELECT GRANTEE, PRIVILEGE, ADMIN_OPTION
                FROM DBA_SYS_PRIVS
                WHERE GRANTEE IN ({grantee_list})
            """).strip()
            ok, out, err = obclient_run_sql(ob_cfg, sys_sql)
            if not ok:
                log.warning("[GRANT_MISS] 读取 DBA_SYS_PRIVS 失败: %s", err)
                return None
            if out:
                for line in out.splitlines():
                    parts = line.split('\t')
                    if len(parts) < 3:
                        continue
                    grantee = (parts[0] or "").strip().upper()
                    priv = (parts[1] or "").strip().upper()
                    admin = (parts[2] or "").strip().upper() == "YES"
                    if not grantee or not priv:
                        continue
                    key = (grantee, priv)
                    sys_privs.add(key)
                    if admin:
                        sys_privs_admin.add(key)

            role_sql = textwrap.dedent(f"""
                SELECT GRANTEE, GRANTED_ROLE, ADMIN_OPTION
                FROM DBA_ROLE_PRIVS
                WHERE GRANTEE IN ({grantee_list})
            """).strip()
            ok, out, err = obclient_run_sql(ob_cfg, role_sql)
            if not ok:
                log.warning("[GRANT_MISS] 读取 DBA_ROLE_PRIVS 失败: %s", err)
                return None
            if out:
                for line in out.splitlines():
                    parts = line.split('\t')
                    if len(parts) < 3:
                        continue
                    grantee = (parts[0] or "").strip().upper()
                    role = (parts[1] or "").strip().upper()
                    admin = (parts[2] or "").strip().upper() == "YES"
                    if not grantee or not role:
                        continue
                    key = (grantee, role)
                    role_privs.add(key)
                    if admin:
                        role_privs_admin.add(key)
    except Exception as exc:  # pragma: no cover
        log.warning("[GRANT_MISS] 读取 OB 权限目录失败: %s", exc)
        return None

    return ObGrantCatalog(
        object_privs=object_privs,
        object_privs_grantable=object_privs_grantable,
        sys_privs=sys_privs,
        sys_privs_admin=sys_privs_admin,
        role_privs=role_privs,
        role_privs_admin=role_privs_admin
    )


# ====================== Oracle 侧辅助函数 ======================

def load_oracle_role_privileges(
    ora_conn,
    base_grantees: Set[str]
) -> Tuple[List[OracleRolePrivilege], Set[str]]:
    """
    读取 DBA_ROLE_PRIVS，并递归展开角色授予链路。
    返回 (role_grants, discovered_roles)。
    """
    role_grants: List[OracleRolePrivilege] = []
    discovered_roles: Set[str] = set()
    pending: Set[str] = {g.upper() for g in base_grantees if g}
    seen: Set[str] = set()

    if not pending:
        return role_grants, discovered_roles

    while pending:
        batch = sorted(pending - seen)
        if not batch:
            break
        seen.update(batch)
        sql_tpl = """
            SELECT GRANTEE, GRANTED_ROLE, ADMIN_OPTION
            FROM DBA_ROLE_PRIVS
            WHERE GRANTEE IN ({placeholders})
        """
        with ora_conn.cursor() as cursor:
            for placeholders, chunk in iter_in_chunks(batch):
                sql = sql_tpl.format(placeholders=placeholders)
                cursor.execute(sql, chunk)
                for row in cursor:
                    grantee = (row[0] or "").strip().upper()
                    role = (row[1] or "").strip().upper()
                    admin_opt = (row[2] or "").strip().upper() == "YES"
                    if not grantee or not role:
                        continue
                    role_grants.append(OracleRolePrivilege(grantee, role, admin_opt))
                    if role not in discovered_roles:
                        discovered_roles.add(role)
                        if role not in seen:
                            pending.add(role)

    return role_grants, discovered_roles


def load_oracle_roles(ora_conn) -> Dict[str, OracleRoleInfo]:
    """
    读取 DBA_ROLES，用于生成 CREATE ROLE DDL。
    """
    roles: Dict[str, OracleRoleInfo] = {}

    def _supports_oracle_maintained() -> bool:
        try:
            with ora_conn.cursor() as cursor:
                cursor.execute("""
                    SELECT COUNT(*)
                    FROM DBA_TAB_COLUMNS
                    WHERE OWNER = 'SYS'
                      AND TABLE_NAME = 'DBA_ROLES'
                      AND COLUMN_NAME = 'ORACLE_MAINTAINED'
                """)
                row = cursor.fetchone()
                return bool(row and row[0] and int(row[0]) > 0)
        except oracledb.Error:
            return False

    try:
        has_om = _supports_oracle_maintained()
        select_cols = "ROLE, AUTHENTICATION_TYPE, PASSWORD_REQUIRED"
        if has_om:
            select_cols += ", ORACLE_MAINTAINED"
        sql = f"SELECT {select_cols} FROM DBA_ROLES"
        with ora_conn.cursor() as cursor:
            cursor.execute(sql)
            for row in cursor:
                role = (row[0] or "").strip().upper()
                if not role:
                    continue
                auth_type = (row[1] or "").strip().upper()
                pwd_required = (row[2] or "").strip().upper() == "YES"
                oracle_maintained: Optional[bool] = None
                if has_om and len(row) > 3:
                    oracle_maintained = (row[3] or "").strip().upper() == "Y"
                roles[role] = OracleRoleInfo(
                    role=role,
                    authentication_type=auth_type,
                    password_required=pwd_required,
                    oracle_maintained=oracle_maintained
                )
    except oracledb.Error as exc:
        log.warning("读取 DBA_ROLES 失败，角色 DDL 将仅基于授权引用推断: %s", exc)
        return {}

    return roles


def load_oracle_system_privilege_map(ora_conn) -> Set[str]:
    """
    读取 SYSTEM_PRIVILEGE_MAP，提供 Oracle 侧系统权限全集。
    """
    privs: Set[str] = set()
    try:
        with ora_conn.cursor() as cursor:
            cursor.execute("SELECT NAME FROM SYSTEM_PRIVILEGE_MAP")
            for row in cursor:
                name = (row[0] or "").strip().upper()
                if name:
                    privs.add(name)
    except oracledb.Error as exc:
        log.warning("读取 SYSTEM_PRIVILEGE_MAP 失败，将跳过 Oracle 系统权限合法性校验: %s", exc)
    return privs


def load_oracle_table_privilege_map(ora_conn) -> Set[str]:
    """
    读取 TABLE_PRIVILEGE_MAP，提供 Oracle 侧对象权限全集。
    """
    privs: Set[str] = set()
    try:
        with ora_conn.cursor() as cursor:
            cursor.execute("SELECT NAME FROM TABLE_PRIVILEGE_MAP")
            for row in cursor:
                name = (row[0] or "").strip().upper()
                if name:
                    privs.add(name)
    except oracledb.Error as exc:
        log.warning("读取 TABLE_PRIVILEGE_MAP 失败，将跳过 Oracle 对象权限合法性校验: %s", exc)
    return privs


def load_oracle_sys_privileges(
    ora_conn,
    grantees: Set[str]
) -> List[OracleSysPrivilege]:
    """
    读取 DBA_SYS_PRIVS。
    """
    sys_privs: List[OracleSysPrivilege] = []
    if not grantees:
        return sys_privs

    for chunk in chunk_list(sorted(grantees), ORACLE_IN_BATCH_SIZE):
        placeholders = ",".join(f":{i+1}" for i in range(len(chunk)))
        sql = f"""
            SELECT GRANTEE, PRIVILEGE, ADMIN_OPTION
            FROM DBA_SYS_PRIVS
            WHERE GRANTEE IN ({placeholders})
        """
        with ora_conn.cursor() as cursor:
            cursor.execute(sql, chunk)
            for row in cursor:
                grantee = (row[0] or "").strip().upper()
                privilege = (row[1] or "").strip().upper()
                admin_opt = (row[2] or "").strip().upper() == "YES"
                if not grantee or not privilege:
                    continue
                sys_privs.append(OracleSysPrivilege(grantee, privilege, admin_opt))

    return sys_privs


def load_oracle_tab_privileges(
    ora_conn,
    owners: Set[str],
    grantees: Set[str],
    scope: str = "owner"
) -> List[OracleObjectPrivilege]:
    """
    读取 DBA_TAB_PRIVS，按 OWNER 或 OWNER+GRANTEE 范围过滤后去重。
    """
    def _supports_type_column() -> bool:
        try:
            with ora_conn.cursor() as cursor:
                cursor.execute("""
                    SELECT COUNT(*)
                    FROM DBA_TAB_COLUMNS
                    WHERE OWNER = 'SYS'
                      AND TABLE_NAME = 'DBA_TAB_PRIVS'
                      AND COLUMN_NAME = 'TYPE'
                """)
                row = cursor.fetchone()
                return bool(row and row[0] and int(row[0]) > 0)
        except oracledb.Error:
            return False

    def _load_for(column: str, values: Set[str], has_type: bool) -> List[OracleObjectPrivilege]:
        results: List[OracleObjectPrivilege] = []
        if not values:
            return results
        select_cols = "GRANTEE, OWNER, TABLE_NAME, PRIVILEGE, GRANTABLE"
        if has_type:
            select_cols += ", TYPE"
        for chunk in chunk_list(sorted(values), ORACLE_IN_BATCH_SIZE):
            placeholders = ",".join(f":{i+1}" for i in range(len(chunk)))
            sql = f"""
                SELECT {select_cols}
                FROM DBA_TAB_PRIVS
                WHERE {column} IN ({placeholders})
            """
            with ora_conn.cursor() as cursor:
                cursor.execute(sql, chunk)
                for row in cursor:
                    grantee = (row[0] or "").strip().upper()
                    owner = (row[1] or "").strip().upper()
                    obj_name = (row[2] or "").strip().upper()
                    privilege = (row[3] or "").strip().upper()
                    grantable = (row[4] or "").strip().upper() == "YES"
                    obj_type = ""
                    if has_type and len(row) > 5:
                        obj_type = (row[5] or "").strip().upper()
                    if not grantee or not owner or not obj_name or not privilege:
                        continue
                    results.append(OracleObjectPrivilege(
                        grantee=grantee,
                        owner=owner,
                        object_name=obj_name,
                        object_type=obj_type,
                        privilege=privilege,
                        grantable=grantable
                    ))
        return results

    has_type = _supports_type_column()
    scope_u = (scope or "owner").strip().lower()
    if scope_u not in ("owner", "owner_or_grantee"):
        log.warning("未知 grant_tab_privs_scope=%s，回退为 owner。", scope)
        scope_u = "owner"

    results: List[OracleObjectPrivilege] = []
    results.extend(_load_for("OWNER", owners, has_type))
    if scope_u == "owner_or_grantee":
        results.extend(_load_for("GRANTEE", grantees, has_type))

    deduped: Dict[Tuple[str, str, str, str, str, bool], OracleObjectPrivilege] = {}
    for item in results:
        key = (
            item.grantee.upper(),
            item.owner.upper(),
            item.object_name.upper(),
            (item.object_type or "").upper(),
            item.privilege.upper(),
            bool(item.grantable)
        )
        deduped.setdefault(key, item)

    return list(deduped.values())

def dump_oracle_metadata(
    ora_cfg: OraConfig,
    master_list: MasterCheckList,
    settings: Dict,
    include_indexes: bool = True,
    include_constraints: bool = True,
    include_triggers: bool = True,
    include_sequences: bool = True,
    include_comments: bool = True,
    include_blacklist: bool = True,
    include_privileges: bool = True,
    include_interval_partitions: bool = False
) -> OracleMetadata:
    """
    预先加载 Oracle 端所需的所有元数据，避免在校验/修补阶段频繁查询。
    """
    table_pairs: Set[Tuple[str, str]] = collect_table_pairs(master_list)
    owner_set: Set[str] = {schema for schema, _ in table_pairs}

    owners = sorted(owner_set)
    seq_owners = sorted({s.upper() for s in settings.get('source_schemas_list', [])})
    source_schema_set: Set[str] = {s.upper() for s in settings.get('source_schemas_list', []) if s}
    privilege_owners: Set[str] = set(source_schema_set)

    if not owners and not seq_owners:
        log.warning("未检测到需要加载的 Oracle schema，返回空元数据。")
        return OracleMetadata(
            table_columns={},
            invisible_column_supported=False,
            identity_column_supported=False,
            default_on_null_supported=False,
            indexes={},
            constraints={},
            triggers={},
            sequences={},
            sequence_attrs={},
            table_comments={},
            column_comments={},
            comments_complete=False,
            blacklist_tables={},
            object_privileges=[],
            sys_privileges=[],
            role_privileges=[],
            role_metadata={},
            system_privilege_map=set(),
            table_privilege_map=set(),
            object_statuses={},
            package_errors={},
            package_errors_complete=False,
            partition_key_columns={},
            interval_partitions={}
        )

    log.info("正在批量加载 Oracle 元数据 (DBA_TAB_COLUMNS/DBA_INDEXES/DBA_CONSTRAINTS/DBA_TRIGGERS/DBA_SEQUENCES)...")
    table_columns: Dict[Tuple[str, str], Dict[str, Dict]] = {}
    indexes: Dict[Tuple[str, str], Dict[str, Dict]] = {}
    constraints: Dict[Tuple[str, str], Dict[str, Dict]] = {}
    triggers: Dict[Tuple[str, str], Dict[str, Dict]] = {}
    sequences: Dict[str, Set[str]] = {}
    sequence_attrs: Dict[str, Dict[str, Dict]] = {}
    roles: Set[str] = set()
    table_comments: Dict[Tuple[str, str], Optional[str]] = {}
    column_comments: Dict[Tuple[str, str], Dict[str, Optional[str]]] = {}
    comments_complete = False
    blacklist_tables: BlacklistTableMap = {}
    object_privileges: List[OracleObjectPrivilege] = []
    sys_privileges: List[OracleSysPrivilege] = []
    role_privileges: List[OracleRolePrivilege] = []
    role_metadata: Dict[str, OracleRoleInfo] = {}
    system_privilege_map: Set[str] = set()
    table_privilege_map: Set[str] = set()
    object_statuses: Dict[Tuple[str, str, str], str] = {}
    package_errors: Dict[Tuple[str, str, str], PackageErrorInfo] = {}
    package_errors_complete = True
    partition_key_columns: Dict[Tuple[str, str], List[str]] = {}
    interval_partitions: Dict[Tuple[str, str], IntervalPartitionInfo] = {}
    invisible_column_supported = False
    identity_column_supported = False
    default_on_null_supported = False
    support_identity_col = False
    support_default_on_null = False

    def _safe_upper(value: Optional[str]) -> Optional[str]:
        if value is None:
            return None
        try:
            return value.upper()
        except AttributeError:
            return str(value).upper()

    try:
        with oracledb.connect(
            user=ora_cfg['user'],
            password=ora_cfg['password'],
            dsn=ora_cfg['dsn']
        ) as ora_conn:
            owner_chunks = chunk_list(owners, ORACLE_IN_BATCH_SIZE)
            seq_owner_chunks = chunk_list(seq_owners, ORACLE_IN_BATCH_SIZE)
            need_package_status = any(
                (obj_type or "").upper() in PACKAGE_OBJECT_TYPES
                for _, _, obj_type in master_list
            )
            invalid_status_types = {
                (obj_type or "").upper()
                for _, _, obj_type in master_list
                if (obj_type or "").upper() in INVALID_STATUS_TYPES
            }
            include_partition_keys = include_constraints or include_interval_partitions
            include_column_order = bool(settings.get("enable_column_order_check", False))
            if owners or need_package_status:
                # 检测列元数据视图支持情况（DBA_TAB_COLUMNS / DBA_TAB_COLS）
                support_hidden_col = False
                support_virtual_col = False
                support_identity_col = False
                support_default_on_null = False
                support_invisible_col = False
                support_column_id = False
                tab_cols_view = "DBA_TAB_COLUMNS"
                optional_cols = (
                    "HIDDEN_COLUMN",
                    "VIRTUAL_COLUMN",
                    "IDENTITY_COLUMN",
                    "DEFAULT_ON_NULL",
                    "INVISIBLE_COLUMN"
                )
                if include_column_order:
                    optional_cols = optional_cols + ("COLUMN_ID",)
                support_by_view: Dict[str, Dict[str, bool]] = {
                    "DBA_TAB_COLUMNS": {col: False for col in optional_cols},
                    "DBA_TAB_COLS": {col: False for col in optional_cols}
                }

                def _probe_dict_col(cursor, view_name: str, col_name: str) -> bool:
                    cursor.execute("""
                        SELECT COUNT(*)
                        FROM DBA_TAB_COLUMNS
                        WHERE OWNER = 'SYS'
                          AND TABLE_NAME = :view_name
                          AND COLUMN_NAME = :col_name
                    """, view_name=view_name, col_name=col_name)
                    count_row = cursor.fetchone()
                    return bool(count_row and count_row[0] and int(count_row[0]) > 0)

                try:
                    with ora_conn.cursor() as cursor:
                        for view_name in ("DBA_TAB_COLUMNS", "DBA_TAB_COLS"):
                            for col_name in optional_cols:
                                support_by_view[view_name][col_name] = _probe_dict_col(
                                    cursor,
                                    view_name,
                                    col_name
                                )
                    tab_cols_view, support_cols, missing_cols = select_tab_columns_view(
                        "DBA_TAB_COLUMNS",
                        support_by_view["DBA_TAB_COLUMNS"],
                        "DBA_TAB_COLS",
                        support_by_view["DBA_TAB_COLS"]
                    )
                    support_hidden_col = bool(support_cols.get("HIDDEN_COLUMN"))
                    support_virtual_col = bool(support_cols.get("VIRTUAL_COLUMN"))
                    support_identity_col = bool(support_cols.get("IDENTITY_COLUMN"))
                    support_default_on_null = bool(support_cols.get("DEFAULT_ON_NULL"))
                    support_invisible_col = bool(support_cols.get("INVISIBLE_COLUMN"))
                    if include_column_order:
                        support_column_id = bool(support_cols.get("COLUMN_ID"))
                    if tab_cols_view != "DBA_TAB_COLUMNS" and missing_cols:
                        log.info(
                            "[CHECK] DBA_TAB_COLUMNS 缺少列元数据(%s)，切换为 %s 读取列信息。",
                            ",".join(missing_cols),
                            tab_cols_view
                        )
                except oracledb.Error as e:
                    log.info(
                        "无法探测 DBA_TAB_COLUMNS/DBA_TAB_COLS 支持，默认不读取列扩展标记：%s",
                        e
                    )
                    support_hidden_col = False
                    support_virtual_col = False
                    support_identity_col = False
                    support_default_on_null = False
                    support_invisible_col = False
                    support_column_id = False
                    tab_cols_view = "DBA_TAB_COLUMNS"

                invisible_column_supported = support_invisible_col

                # 列定义
                def _load_ora_tab_columns_sql(
                    include_hidden: bool,
                    include_virtual: bool,
                    include_identity: bool,
                    include_default_on_null: bool,
                    include_invisible: bool,
                    include_column_id: bool,
                    view_name: str
                ) -> str:
                    select_cols = [
                        "OWNER", "TABLE_NAME", "COLUMN_NAME", "DATA_TYPE",
                        "DATA_LENGTH", "DATA_PRECISION", "DATA_SCALE",
                        "NULLABLE", "DATA_DEFAULT", "CHAR_USED", "CHAR_LENGTH"
                    ]
                    if include_column_id:
                        select_cols.append("COLUMN_ID")
                    if include_hidden:
                        select_cols.append("NVL(TO_CHAR(HIDDEN_COLUMN),'NO') AS HIDDEN_COLUMN")
                    if include_virtual:
                        select_cols.append("NVL(TO_CHAR(VIRTUAL_COLUMN),'NO') AS VIRTUAL_COLUMN")
                    if include_identity:
                        select_cols.append("NVL(TO_CHAR(IDENTITY_COLUMN),'NO') AS IDENTITY_COLUMN")
                    if include_default_on_null:
                        select_cols.append("NVL(TO_CHAR(DEFAULT_ON_NULL),'NO') AS DEFAULT_ON_NULL")
                    if include_invisible:
                        select_cols.append("NVL(TO_CHAR(INVISIBLE_COLUMN),'NO') AS INVISIBLE_COLUMN")
                    return f"""
                        SELECT {", ".join(select_cols)}
                        FROM {view_name}
                        WHERE OWNER IN ({{owners_clause}})
                    """

                def _parse_tab_column_row(
                    row,
                    include_hidden: bool,
                    include_virtual: bool,
                    include_identity: bool,
                    include_default_on_null: bool,
                    include_invisible: bool,
                    include_column_id: bool
                ) -> Dict:
                    idx = 11
                    column_id = None
                    if include_column_id and len(row) > idx:
                        column_id = parse_column_id(row[idx])
                        idx += 1
                    hidden_flag = False
                    virtual_flag = False
                    identity_flag = None
                    default_on_null_flag = None
                    invisible_flag = None
                    if include_hidden and len(row) > idx:
                        hidden_flag = (row[idx] or "").strip().upper() == "YES"
                        idx += 1
                    if include_virtual and len(row) > idx:
                        virtual_flag = (row[idx] or "").strip().upper() == "YES"
                        idx += 1
                    if include_identity and len(row) > idx:
                        identity_flag = (row[idx] or "").strip().upper() == "YES"
                        idx += 1
                    if include_default_on_null and len(row) > idx:
                        default_on_null_flag = (row[idx] or "").strip().upper() == "YES"
                        idx += 1
                    if include_invisible and len(row) > idx:
                        invisible_flag = (row[idx] or "").strip().upper() == "YES"
                    return {
                        "data_type": row[3],
                        "data_length": row[4],
                        "data_precision": row[5],
                        "data_scale": row[6],
                        "nullable": row[7],
                        "data_default": row[8],
                        "char_used": row[9],
                        "char_length": row[10],
                        "column_id": column_id,
                        "hidden": hidden_flag if include_hidden else False,
                        "virtual": virtual_flag,
                        "virtual_expr": row[8] if virtual_flag else None,
                        "identity": identity_flag,
                        "default_on_null": default_on_null_flag,
                        "invisible": invisible_flag if include_invisible else None
                    }

                try:
                    sql_tpl = _load_ora_tab_columns_sql(
                        include_hidden=support_hidden_col,
                        include_virtual=support_virtual_col,
                        include_identity=support_identity_col,
                        include_default_on_null=support_default_on_null,
                        include_invisible=support_invisible_col,
                        include_column_id=include_column_order and support_column_id,
                        view_name=tab_cols_view
                    )
                    with ora_conn.cursor() as cursor:
                        for owner_chunk in owner_chunks:
                            owners_clause = build_bind_placeholders(len(owner_chunk))
                            sql = sql_tpl.format(owners_clause=owners_clause)
                            cursor.execute(sql, owner_chunk)
                            for row in cursor:
                                owner = _safe_upper(row[0])
                                table = _safe_upper(row[1])
                                col = _safe_upper(row[2])
                                if not owner or not table or not col:
                                    continue
                                key = (owner, table)
                                if key not in table_pairs:
                                    continue
                                table_columns.setdefault(key, {})[col] = _parse_tab_column_row(
                                    row,
                                    support_hidden_col,
                                    support_virtual_col,
                                    support_identity_col,
                                    support_default_on_null,
                                    support_invisible_col,
                                    include_column_order and support_column_id
                                )
                except oracledb.Error as e:
                    if support_hidden_col or support_virtual_col or support_identity_col or support_default_on_null or support_invisible_col:
                        log.info("读取 %s(含 hidden/virtual) 失败，尝试降级：%s", tab_cols_view, e)
                        support_hidden_col = False
                        support_virtual_col = False
                        support_identity_col = False
                        support_default_on_null = False
                        support_invisible_col = False
                        support_column_id = False
                        if tab_cols_view != "DBA_TAB_COLUMNS":
                            log.info("回退到 DBA_TAB_COLUMNS 读取列元数据。")
                            tab_cols_view = "DBA_TAB_COLUMNS"
                        sql_tpl = _load_ora_tab_columns_sql(
                            include_hidden=False,
                            include_virtual=False,
                            include_identity=False,
                            include_default_on_null=False,
                            include_invisible=False,
                            include_column_id=False,
                            view_name=tab_cols_view
                        )
                        with ora_conn.cursor() as cursor:
                            for owner_chunk in owner_chunks:
                                owners_clause = build_bind_placeholders(len(owner_chunk))
                                sql = sql_tpl.format(owners_clause=owners_clause)
                                cursor.execute(sql, owner_chunk)
                                for row in cursor:
                                    owner = _safe_upper(row[0])
                                    table = _safe_upper(row[1])
                                    col = _safe_upper(row[2])
                                    if not owner or not table or not col:
                                        continue
                                key = (owner, table)
                                if key not in table_pairs:
                                    continue
                                table_columns.setdefault(key, {})[col] = _parse_tab_column_row(
                                    row,
                                    False,
                                    False,
                                    False,
                                    False,
                                    False,
                                    False
                                )
                    else:
                        raise

                # 索引
                if include_indexes:
                    sql_idx_tpl = """
                        SELECT TABLE_OWNER, TABLE_NAME, INDEX_NAME, UNIQUENESS
                        FROM DBA_INDEXES
                        WHERE TABLE_OWNER IN ({owners_clause})
                    """
                    with ora_conn.cursor() as cursor:
                        for owner_chunk in owner_chunks:
                            owners_clause = build_bind_placeholders(len(owner_chunk))
                            sql_idx = sql_idx_tpl.format(owners_clause=owners_clause)
                            cursor.execute(sql_idx, owner_chunk)
                            for row in cursor:
                                owner = _safe_upper(row[0])
                                table = _safe_upper(row[1])
                                if not owner or not table:
                                    continue
                                key = (owner, table)
                                if key not in table_pairs:
                                    continue
                                idx_name = _safe_upper(row[2])
                                if not idx_name:
                                    continue
                                indexes.setdefault(key, {})[idx_name] = {
                                    "uniqueness": (row[3] or "").upper(),
                                    "columns": [],
                                    "expressions": {},
                                    "descend": []
                                }

                    sql_idx_cols_tpl = """
                        SELECT TABLE_OWNER, TABLE_NAME, INDEX_NAME, COLUMN_NAME, DESCEND
                        FROM DBA_IND_COLUMNS
                        WHERE TABLE_OWNER IN ({owners_clause})
                        ORDER BY TABLE_OWNER, TABLE_NAME, INDEX_NAME, COLUMN_POSITION
                    """
                    with ora_conn.cursor() as cursor:
                        for owner_chunk in owner_chunks:
                            owners_clause = build_bind_placeholders(len(owner_chunk))
                            sql_idx_cols = sql_idx_cols_tpl.format(owners_clause=owners_clause)
                            cursor.execute(sql_idx_cols, owner_chunk)
                            for row in cursor:
                                owner = _safe_upper(row[0])
                                table = _safe_upper(row[1])
                                if not owner or not table:
                                    continue
                                key = (owner, table)
                                if key not in table_pairs:
                                    continue
                                idx_name = _safe_upper(row[2])
                                col_name = _safe_upper(row[3])
                                descend_flag = (row[4] or "").strip().upper() if len(row) > 4 else ""
                                if not idx_name or not col_name:
                                    continue
                                indexes.setdefault(key, {}).setdefault(
                                    idx_name,
                                    {"uniqueness": "UNKNOWN", "columns": [], "expressions": {}, "descend": []}
                                )["columns"].append(col_name)
                                indexes[key][idx_name]["descend"].append(descend_flag or "ASC")

                    # 读取函数索引表达式
                    sql_idx_expr_tpl = """
                        SELECT TABLE_OWNER, TABLE_NAME, INDEX_NAME, COLUMN_POSITION, COLUMN_EXPRESSION
                        FROM DBA_IND_EXPRESSIONS
                        WHERE TABLE_OWNER IN ({owners_clause})
                        ORDER BY TABLE_OWNER, TABLE_NAME, INDEX_NAME, COLUMN_POSITION
                    """
                    try:
                        with ora_conn.cursor() as cursor:
                            for owner_chunk in owner_chunks:
                                owners_clause = build_bind_placeholders(len(owner_chunk))
                                sql_idx_expr = sql_idx_expr_tpl.format(owners_clause=owners_clause)
                                cursor.execute(sql_idx_expr, owner_chunk)
                                for row in cursor:
                                    owner = _safe_upper(row[0])
                                    table = _safe_upper(row[1])
                                    if not owner or not table:
                                        continue
                                    key = (owner, table)
                                    if key not in table_pairs:
                                        continue
                                    idx_name = _safe_upper(row[2])
                                    if not idx_name:
                                        continue
                                    try:
                                        pos = int(row[3]) if row[3] is not None else None
                                    except (TypeError, ValueError):
                                        pos = None
                                    expr = str(row[4]) if row[4] is not None else None
                                    if pos is None or not expr:
                                        continue
                                    idx_info = indexes.get(key, {}).get(idx_name)
                                    if idx_info is None:
                                        continue
                                    idx_info.setdefault("expressions", {})[pos] = expr
                    except oracledb.Error as e:
                        log.info("读取 DBA_IND_EXPRESSIONS 失败，函数索引表达式将不参与对比: %s", e)

                # 约束
                if include_constraints:
                    search_condition_col = "SEARCH_CONDITION"
                    support_update_rule = False
                    try:
                        with ora_conn.cursor() as cursor:
                            cursor.execute(
                                "SELECT SEARCH_CONDITION_VC FROM DBA_CONSTRAINTS WHERE ROWNUM = 1"
                            )
                            search_condition_col = "SEARCH_CONDITION_VC"
                    except oracledb.Error:
                        search_condition_col = "SEARCH_CONDITION"
                    try:
                        with ora_conn.cursor() as cursor:
                            cursor.execute(
                                "SELECT UPDATE_RULE FROM DBA_CONSTRAINTS WHERE ROWNUM = 1"
                            )
                            support_update_rule = True
                    except oracledb.Error:
                        support_update_rule = False
                    update_rule_select = ", UPDATE_RULE" if support_update_rule else ""
                    sql_cons_tpl = f"""
                        SELECT OWNER, TABLE_NAME, CONSTRAINT_NAME, CONSTRAINT_TYPE, STATUS, VALIDATED, R_OWNER, R_CONSTRAINT_NAME,
                               DELETE_RULE{update_rule_select}, {search_condition_col}, DEFERRABLE, DEFERRED
                        FROM DBA_CONSTRAINTS
                        WHERE OWNER IN ({{owners_clause}})
                          AND CONSTRAINT_TYPE IN ('P','U','R','C')
                    """
                    with ora_conn.cursor() as cursor:
                        if search_condition_col == "SEARCH_CONDITION" and hasattr(cursor, "longfetchsize"):
                            try:
                                cursor.longfetchsize = max(int(cursor.longfetchsize or 0), 100000)
                            except (TypeError, ValueError):
                                cursor.longfetchsize = 100000
                        for owner_chunk in owner_chunks:
                            owners_clause = build_bind_placeholders(len(owner_chunk))
                            sql_cons = sql_cons_tpl.format(owners_clause=owners_clause)
                            cursor.execute(sql_cons, owner_chunk)
                            for row in cursor:
                                owner = _safe_upper(row[0])
                                table = _safe_upper(row[1])
                                if not owner or not table:
                                    continue
                                key = (owner, table)
                                if key not in table_pairs:
                                    continue
                                name = _safe_upper(row[2])
                                if not name:
                                    continue
                                cons_status = (row[4] or "").strip().upper() if len(row) > 4 else None
                                validated = (row[5] or "").strip().upper() if len(row) > 5 else None
                                update_rule = None
                                idx_base = 8
                                delete_rule = (row[idx_base] or "").strip().upper() if len(row) > idx_base else None
                                idx_base += 1
                                if support_update_rule:
                                    update_rule = (row[idx_base] or "").strip().upper() if len(row) > idx_base else None
                                    idx_base += 1
                                search_condition = str(row[idx_base]) if len(row) > idx_base and row[idx_base] is not None else None
                                idx_base += 1
                                deferrable = str(row[idx_base]) if len(row) > idx_base and row[idx_base] is not None else None
                                idx_base += 1
                                deferred = str(row[idx_base]) if len(row) > idx_base and row[idx_base] is not None else None
                                constraints.setdefault(key, {})[name] = {
                                    "type": (row[3] or "").upper(),
                                    "status": cons_status,
                                    "validated": validated,
                                    "columns": [],
                                    "index_name": None,
                                    "r_owner": _safe_upper(row[6]) if row[6] else None,
                                    "r_constraint": _safe_upper(row[7]) if row[7] else None,
                                    "delete_rule": delete_rule,
                                    "update_rule": update_rule,
                                    "search_condition": search_condition,
                                    "deferrable": deferrable,
                                    "deferred": deferred,
                                }

                    sql_cons_cols_tpl = """
                        SELECT OWNER, TABLE_NAME, CONSTRAINT_NAME, COLUMN_NAME
                        FROM DBA_CONS_COLUMNS
                        WHERE OWNER IN ({owners_clause})
                        ORDER BY OWNER, TABLE_NAME, CONSTRAINT_NAME, POSITION
                    """
                    with ora_conn.cursor() as cursor:
                        for owner_chunk in owner_chunks:
                            owners_clause = build_bind_placeholders(len(owner_chunk))
                            sql_cons_cols = sql_cons_cols_tpl.format(owners_clause=owners_clause)
                            cursor.execute(sql_cons_cols, owner_chunk)
                            for row in cursor:
                                owner = _safe_upper(row[0])
                                table = _safe_upper(row[1])
                                if not owner or not table:
                                    continue
                                key = (owner, table)
                                if key not in table_pairs:
                                    continue
                                cons_name = _safe_upper(row[2])
                                col_name = _safe_upper(row[3])
                                if not cons_name or not col_name:
                                    continue
                                constraints.setdefault(key, {}).setdefault(
                                    cons_name,
                                    {
                                        "type": "UNKNOWN",
                                        "status": None,
                                        "validated": None,
                                        "columns": [],
                                        "index_name": None,
                                        "r_owner": None,
                                        "r_constraint": None,
                                        "delete_rule": None,
                                        "update_rule": None,
                                        "search_condition": None,
                                        "deferrable": None,
                                        "deferred": None,
                                    }
                                )["columns"].append(col_name)

                    # 为外键补齐被引用表信息 (基于约束引用)
                    cons_table_lookup: Dict[Tuple[str, str], Tuple[str, str]] = {}
                    for (owner, table), cons_map in constraints.items():
                        for cons_name, info in cons_map.items():
                            ctype = (info.get("type") or "").upper()
                            if ctype in ('P', 'U'):
                                cons_table_lookup[(owner, cons_name)] = (owner, table)
                    for (owner, _), cons_map in constraints.items():
                        for cons_name, info in cons_map.items():
                            ctype = (info.get("type") or "").upper()
                            if ctype != 'R':
                                continue
                            r_owner = (info.get("r_owner") or "").upper()
                            r_cons = (info.get("r_constraint") or "").upper()
                            if not r_owner or not r_cons:
                                continue
                            ref_table = cons_table_lookup.get((r_owner, r_cons))
                            if ref_table:
                                info["ref_table_owner"], info["ref_table_name"] = ref_table

                if include_partition_keys and table_pairs:
                    def _append_partition_col(key: Tuple[str, str], col: Optional[str]) -> None:
                        if not col:
                            return
                        cols = partition_key_columns.setdefault(key, [])
                        col_u = col.upper()
                        if col_u not in cols:
                            cols.append(col_u)

                    def _load_part_keys(view_name: str) -> None:
                        sql_tpl = f"""
                            SELECT OWNER, NAME, COLUMN_NAME, COLUMN_POSITION
                            FROM {view_name}
                            WHERE OWNER IN ({{owners_clause}})
                              AND OBJECT_TYPE = 'TABLE'
                            ORDER BY OWNER, NAME, COLUMN_POSITION
                        """
                        with ora_conn.cursor() as cursor:
                            for owner_chunk in owner_chunks:
                                owners_clause = build_bind_placeholders(len(owner_chunk))
                                sql = sql_tpl.format(owners_clause=owners_clause)
                                cursor.execute(sql, owner_chunk)
                                for row in cursor:
                                    owner = _safe_upper(row[0])
                                    table = _safe_upper(row[1])
                                    col = _safe_upper(row[2]) if len(row) > 2 else None
                                    if not owner or not table or not col:
                                        continue
                                    key = (owner, table)
                                    if key not in table_pairs:
                                        continue
                                    _append_partition_col(key, col)

                    try:
                        _load_part_keys("DBA_PART_KEY_COLUMNS")
                        _load_part_keys("DBA_SUBPART_KEY_COLUMNS")
                    except oracledb.Error as e:
                        log.warning("读取 Oracle 分区键列失败，将跳过分区键处理: %s", e)

                if include_interval_partitions and table_pairs:
                    interval_candidates: Dict[Tuple[str, str], Tuple[str, str, str]] = {}
                    sql_interval_tpl = """
                        SELECT OWNER, TABLE_NAME, PARTITIONING_TYPE, SUBPARTITIONING_TYPE, INTERVAL
                        FROM DBA_PART_TABLES
                        WHERE OWNER IN ({owners_clause})
                          AND INTERVAL IS NOT NULL
                    """
                    try:
                        with ora_conn.cursor() as cursor:
                            for owner_chunk in owner_chunks:
                                owners_clause = build_bind_placeholders(len(owner_chunk))
                                sql_interval = sql_interval_tpl.format(owners_clause=owners_clause)
                                cursor.execute(sql_interval, owner_chunk)
                                for row in cursor:
                                    owner = _safe_upper(row[0])
                                    table = _safe_upper(row[1])
                                    if not owner or not table:
                                        continue
                                    key = (owner, table)
                                    if key not in table_pairs:
                                        continue
                                    interval_expr = str(row[4]).strip() if row[4] is not None else ""
                                    if not interval_expr:
                                        continue
                                    part_type = (row[2] or "").upper()
                                    subpart_type = (row[3] or "").upper()
                                    interval_candidates[key] = (interval_expr, part_type, subpart_type)
                    except oracledb.Error as e:
                        log.warning("读取 Oracle interval 分区表失败，将跳过 interval 处理: %s", e)
                        interval_candidates = {}

                    if interval_candidates:
                        part_stats: Dict[Tuple[str, str], Dict[str, object]] = {}
                        for key in interval_candidates.keys():
                            part_stats[key] = {
                                "last_pos": -1,
                                "last_name": "",
                                "last_high": "",
                                "names": set()
                            }
                        sql_part_tpl = """
                            SELECT TABLE_OWNER, TABLE_NAME, PARTITION_NAME, HIGH_VALUE, PARTITION_POSITION
                            FROM DBA_TAB_PARTITIONS
                            WHERE TABLE_OWNER IN ({owners_clause})
                            ORDER BY TABLE_OWNER, TABLE_NAME, PARTITION_POSITION
                        """
                        try:
                            with ora_conn.cursor() as cursor:
                                for owner_chunk in owner_chunks:
                                    owners_clause = build_bind_placeholders(len(owner_chunk))
                                    sql_part = sql_part_tpl.format(owners_clause=owners_clause)
                                    cursor.execute(sql_part, owner_chunk)
                                    for row in cursor:
                                        owner = _safe_upper(row[0])
                                        table = _safe_upper(row[1])
                                        if not owner or not table:
                                            continue
                                        key = (owner, table)
                                        stats = part_stats.get(key)
                                        if not stats:
                                            continue
                                        part_name = _safe_upper(row[2]) if row[2] else ""
                                        high_value = str(row[3]) if row[3] is not None else ""
                                        try:
                                            part_pos = int(row[4]) if row[4] is not None else -1
                                        except (TypeError, ValueError):
                                            part_pos = -1
                                        if part_name:
                                            stats["names"].add(part_name.upper())
                                        if part_pos >= int(stats["last_pos"]):
                                            stats["last_pos"] = part_pos
                                            stats["last_name"] = part_name or ""
                                            stats["last_high"] = high_value or ""
                        except oracledb.Error as e:
                            log.warning("读取 Oracle 分区边界失败，将跳过 interval 处理: %s", e)
                            part_stats = {}

                        for key, info in interval_candidates.items():
                            stats = part_stats.get(key)
                            if not stats:
                                continue
                            interval_partitions[key] = IntervalPartitionInfo(
                                interval_expr=info[0],
                                partitioning_type=info[1],
                                subpartitioning_type=info[2],
                                last_partition_name=str(stats.get("last_name") or ""),
                                last_high_value=str(stats.get("last_high") or ""),
                                last_partition_position=int(stats.get("last_pos") or -1),
                                partition_key_columns=partition_key_columns.get(key, []),
                                existing_partition_names=set(stats.get("names") or set())
                            )

                # 触发器
                if include_triggers:
                    sql_trg_tpl = """
                        SELECT OWNER, TABLE_OWNER, TABLE_NAME, TRIGGER_NAME, TRIGGERING_EVENT, STATUS
                        FROM DBA_TRIGGERS
                        WHERE TABLE_OWNER IN ({owners_clause})
                    """
                    with ora_conn.cursor() as cursor:
                        for owner_chunk in owner_chunks:
                            owners_clause = build_bind_placeholders(len(owner_chunk))
                            sql_trg = sql_trg_tpl.format(owners_clause=owners_clause)
                            cursor.execute(sql_trg, owner_chunk)
                            for row in cursor:
                                trg_owner = _safe_upper(row[0])
                                owner = _safe_upper(row[1])
                                table = _safe_upper(row[2])
                                if not owner or not table:
                                    continue
                                key = (owner, table)
                                if key not in table_pairs:
                                    continue
                                trg_name = _safe_upper(row[3])
                                if not trg_name:
                                    continue
                                trg_owner_u = trg_owner or owner
                                trg_key = f"{trg_owner_u}.{trg_name}" if trg_owner_u and trg_name else trg_name
                                triggers.setdefault(key, {})[trg_key] = {
                                    "event": row[4],
                                    "status": row[5],
                                    "owner": trg_owner_u,
                                    "name": trg_name
                                }

                status_types: Set[str] = set()
                if include_triggers:
                    status_types.add("TRIGGER")
                if need_package_status:
                    status_types.update({"PACKAGE", "PACKAGE BODY"})
                if invalid_status_types:
                    status_types.update(invalid_status_types)

                if status_types:
                    status_owners = sorted(source_schema_set | set(owners))
                    if status_owners:
                        status_owner_chunks = chunk_list(status_owners, ORACLE_IN_BATCH_SIZE)
                        status_types_clause = ",".join(f"'{t}'" for t in sorted(status_types))
                        sql_status_tpl = f"""
                            SELECT OWNER, OBJECT_NAME, OBJECT_TYPE, STATUS
                            FROM DBA_OBJECTS
                            WHERE OWNER IN ({{owners_clause}})
                              AND OBJECT_TYPE IN ({status_types_clause})
                        """
                        with ora_conn.cursor() as cursor:
                            for owner_chunk in status_owner_chunks:
                                owners_clause = build_bind_placeholders(len(owner_chunk))
                                sql_status = sql_status_tpl.format(owners_clause=owners_clause)
                                cursor.execute(sql_status, owner_chunk)
                                for row in cursor:
                                    owner = _safe_upper(row[0])
                                    name = _safe_upper(row[1])
                                    obj_type = _safe_upper(row[2])
                                    status = _safe_upper(row[3]) if row[3] else "UNKNOWN"
                                    if not owner or not name or not obj_type:
                                        continue
                                    object_statuses[(owner, name, obj_type)] = status or "UNKNOWN"

                if need_package_status:
                    pkg_owners = sorted(source_schema_set | set(owners))
                    if pkg_owners:
                        pkg_owner_chunks = chunk_list(pkg_owners, ORACLE_IN_BATCH_SIZE)
                        try:
                            sql_pkg_err_tpl = """
                                SELECT OWNER, NAME, TYPE, LINE, POSITION, TEXT
                                FROM DBA_ERRORS
                                WHERE OWNER IN ({owners_clause})
                                  AND TYPE IN ('PACKAGE', 'PACKAGE BODY')
                                ORDER BY OWNER, NAME, TYPE, SEQUENCE
                            """
                            temp_errors: Dict[Tuple[str, str, str], Dict[str, object]] = defaultdict(
                                lambda: {"count": 0, "first_error": ""}
                            )
                            with ora_conn.cursor() as cursor:
                                for owner_chunk in pkg_owner_chunks:
                                    owners_clause = build_bind_placeholders(len(owner_chunk))
                                    sql_pkg_err = sql_pkg_err_tpl.format(owners_clause=owners_clause)
                                    cursor.execute(sql_pkg_err, owner_chunk)
                                    for row in cursor:
                                        owner = _safe_upper(row[0])
                                        name = _safe_upper(row[1])
                                        err_type = _safe_upper(row[2])
                                        err_line = str(row[3]).strip() if row[3] is not None else ""
                                        err_pos = str(row[4]).strip() if row[4] is not None else ""
                                        err_text = normalize_error_text(row[5] if len(row) > 5 else "")
                                        if not owner or not name or not err_type:
                                            continue
                                        key = (owner, name, err_type)
                                        entry = temp_errors[key]
                                        entry["count"] = int(entry["count"]) + 1
                                        if not entry["first_error"]:
                                            prefix = f"L{err_line}:{err_pos} " if err_line or err_pos else ""
                                            entry["first_error"] = f"{prefix}{err_text}".strip()
                            for key, info in temp_errors.items():
                                package_errors[key] = PackageErrorInfo(
                                    count=int(info.get("count") or 0),
                                    first_error=str(info.get("first_error") or "")
                                )
                        except oracledb.Error as e:
                            package_errors_complete = False
                            log.warning("读取 Oracle DBA_ERRORS 失败，包错误信息将为空: %s", e)

                if include_comments:
                    if not table_pairs:
                        comments_complete = True
                    else:
                        comment_keys = sorted(f"{owner}.{table}" for owner, table in table_pairs)
                        comments_complete = True
                        try:
                            with ora_conn.cursor() as cursor:
                                for chunk in chunk_list(comment_keys, COMMENT_BATCH_SIZE):
                                    if not chunk:
                                        continue
                                    placeholders = build_bind_placeholders(len(chunk))
                                    sql_cmt = f"""
                                        SELECT OWNER, TABLE_NAME, COMMENTS
                                        FROM DBA_TAB_COMMENTS
                                        WHERE OWNER||'.'||TABLE_NAME IN ({placeholders})
                                    """
                                    cursor.execute(sql_cmt, chunk)
                                    for row in cursor:
                                        owner = _safe_upper(row[0])
                                        table = _safe_upper(row[1])
                                        if not owner or not table:
                                            continue
                                        table_comments[(owner, table)] = row[2]

                                for chunk in chunk_list(comment_keys, COMMENT_BATCH_SIZE):
                                    if not chunk:
                                        continue
                                    placeholders = build_bind_placeholders(len(chunk))
                                    sql_cmt_col = f"""
                                        SELECT OWNER, TABLE_NAME, COLUMN_NAME, COMMENTS
                                        FROM DBA_COL_COMMENTS
                                        WHERE OWNER||'.'||TABLE_NAME IN ({placeholders})
                                    """
                                    cursor.execute(sql_cmt_col, chunk)
                                    for row in cursor:
                                        owner = _safe_upper(row[0])
                                        table = _safe_upper(row[1])
                                        column = _safe_upper(row[2])
                                        if not owner or not table or not column:
                                            continue
                                        column_comments.setdefault((owner, table), {})[column] = row[3]
                        except oracledb.Error as e:
                            comments_complete = False
                            log.warning("读取 DBA_TAB_COMMENTS/DBA_COL_COMMENTS 失败，将跳过注释比对：%s", e)
                        if comments_complete and table_pairs and not table_comments and not column_comments:
                            log.warning("Oracle 端注释查询未返回任何记录，可能缺少权限，注释比对将跳过。")
                            comments_complete = False

                if include_blacklist:
                    blacklist_mode = settings.get("blacklist_mode", "auto")
                    use_table = blacklist_mode in ("auto", "table_only")
                    use_rules = blacklist_mode in ("auto", "rules_only")
                    table_loaded = False
                    rules_loaded = False
                    if blacklist_mode == "disabled":
                        log.info("黑名单过滤已关闭 (blacklist_mode=disabled)。")
                    else:
                        if use_table:
                            table_available = True
                            try:
                                total_blacklist = 0
                                with ora_conn.cursor() as cursor:
                                    sql_blacklist_count_tpl = """
                                        SELECT COUNT(*)
                                        FROM OMS_USER.TMP_BLACK_TABLE
                                        WHERE OWNER IN ({owners_clause})
                                    """
                                    for owner_chunk in owner_chunks:
                                        owners_clause = build_bind_placeholders(len(owner_chunk))
                                        sql_blacklist_count = sql_blacklist_count_tpl.format(owners_clause=owners_clause)
                                        cursor.execute(sql_blacklist_count, owner_chunk)
                                        row = cursor.fetchone()
                                        total_blacklist += int(row[0]) if row and row[0] is not None else 0
                                if total_blacklist <= 0:
                                    log.warning("未检测到 OMS_USER.TMP_BLACK_TABLE 黑名单记录（当前 schema）。")
                                else:
                                    log.info(
                                        "检测到黑名单表记录 %d 条（当前 schema），将用于过滤缺失表规则。",
                                        total_blacklist
                                    )
                            except oracledb.Error as e:
                                table_available = False
                                err_msg = str(e)
                                if any(code in err_msg for code in ("ORA-00942", "ORA-04043")):
                                    log.warning("未检测到 OMS_USER.TMP_BLACK_TABLE（将尝试规则引擎）。")
                                else:
                                    log.warning("读取 OMS_USER.TMP_BLACK_TABLE 失败：%s", e)

                            if table_available:
                                sql_blacklist_tpl = """
                                    SELECT OWNER, TABLE_NAME, DATA_TYPE, BLACK_TYPE
                                    FROM OMS_USER.TMP_BLACK_TABLE
                                    WHERE OWNER IN ({owners_clause})
                                """
                                try:
                                    with ora_conn.cursor() as cursor:
                                        for owner_chunk in owner_chunks:
                                            owners_clause = build_bind_placeholders(len(owner_chunk))
                                            sql_blacklist = sql_blacklist_tpl.format(owners_clause=owners_clause)
                                            cursor.execute(sql_blacklist, owner_chunk)
                                            for row in cursor:
                                                owner = _safe_upper(row[0])
                                                table = _safe_upper(row[1])
                                                if not owner or not table:
                                                    continue
                                                key = (owner, table)
                                                if key not in table_pairs:
                                                    continue
                                                data_type = normalize_black_data_type(row[2])
                                                black_type = normalize_black_type(row[3]) or "UNKNOWN"
                                                add_blacklist_entry(
                                                    blacklist_tables,
                                                    owner,
                                                    table,
                                                    black_type,
                                                    data_type,
                                                    source="TABLE"
                                                )
                                    table_loaded = True
                                except oracledb.Error as e:
                                    log.warning("读取 OMS_USER.TMP_BLACK_TABLE 失败：%s", e)

                        if use_rules:
                            rules_path = settings.get("blacklist_rules_path", "")
                            rules = load_blacklist_rules(rules_path)
                            if not rules:
                                log.warning("黑名单规则文件为空或不可用，将跳过规则引擎。")
                            else:
                                enable_set = settings.get("blacklist_rules_enable_set", set())
                                disable_set = settings.get("blacklist_rules_disable_set", set())
                                ob_version = settings.get("ob_version")
                                lob_max_mb = int(settings.get("blacklist_lob_max_mb", 512))
                                name_pattern_clause = settings.get("blacklist_name_pattern_clause", "")
                                rule_stats: Dict[str, int] = {}
                                skipped: Dict[str, int] = defaultdict(int)
                                for rule in rules:
                                    enabled, reason = is_blacklist_rule_enabled(
                                        rule,
                                        enable_set,
                                        disable_set,
                                        ob_version
                                    )
                                    if not enabled:
                                        skipped[reason or "disabled"] += 1
                                        continue
                                    sql_tpl = rule.sql
                                    if "{{owners_clause}}" not in sql_tpl:
                                        log.warning("黑名单规则 %s 缺少 {{owners_clause}}，已跳过。", rule.rule_id)
                                        skipped["missing_owner_clause"] += 1
                                        continue
                                    if "{{name_pattern_clause}}" in sql_tpl and not name_pattern_clause:
                                        log.warning(
                                            "黑名单规则 %s 依赖 {{name_pattern_clause}}，但未配置关键字（blacklist_name_patterns / blacklist_name_patterns_file），已跳过。",
                                            rule.rule_id
                                        )
                                        skipped["missing_name_pattern_clause"] += 1
                                        continue
                                    sql_tpl = sql_tpl.replace("{{lob_max_mb}}", str(lob_max_mb))
                                    sql_tpl = sql_tpl.replace("{{name_pattern_clause}}", name_pattern_clause)
                                    added = 0
                                    try:
                                        with ora_conn.cursor() as cursor:
                                            for owner_chunk in owner_chunks:
                                                owners_clause = build_bind_placeholders(len(owner_chunk))
                                                sql_rule = sql_tpl.replace("{{owners_clause}}", owners_clause)
                                                cursor.execute(sql_rule, owner_chunk)
                                                for row in cursor:
                                                    owner = _safe_upper(row[0]) if len(row) > 0 else None
                                                    table = _safe_upper(row[1]) if len(row) > 1 else None
                                                    if not owner or not table:
                                                        continue
                                                    key = (owner, table)
                                                    if key not in table_pairs:
                                                        continue
                                                    data_type = normalize_black_data_type(row[2]) if len(row) > 2 else ""
                                                    black_type = normalize_black_type(row[3]) if len(row) > 3 else ""
                                                    black_type = black_type or rule.black_type or "UNKNOWN"
                                                    if add_blacklist_entry(
                                                        blacklist_tables,
                                                        owner,
                                                        table,
                                                        black_type,
                                                        data_type,
                                                        source=f"RULE={rule.rule_id}"
                                                    ):
                                                        added += 1
                                        rules_loaded = True
                                        rule_stats[rule.rule_id] = added
                                    except oracledb.Error as e:
                                        log.warning("黑名单规则 %s 执行失败：%s", rule.rule_id, e)
                                        skipped["query_failed"] += 1

                                if rule_stats:
                                    total_added = sum(rule_stats.values())
                                    log.info(
                                        "黑名单规则执行完成：rules=%d, entries=%d",
                                        len(rule_stats),
                                        total_added
                                    )
                                    for rule_id, count in sorted(rule_stats.items()):
                                        if count:
                                            log.info("  - %s: %d", rule_id, count)
                                if skipped:
                                    skipped_msg = ", ".join(
                                        f"{k}={v}" for k, v in sorted(skipped.items())
                                    )
                                    log.info("黑名单规则跳过统计：%s", skipped_msg)

                        if not table_loaded and not rules_loaded:
                            log.warning("黑名单未加载（mode=%s），缺失表过滤将跳过。", blacklist_mode)

            if include_privileges:
                try:
                    base_grantees = set(source_schema_set)
                    role_privileges, role_names = load_oracle_role_privileges(ora_conn, base_grantees)
                    grantee_scope = set(base_grantees) | set(role_names) | {"PUBLIC"}
                    sys_privileges = load_oracle_sys_privileges(ora_conn, base_grantees | set(role_names))
                    object_privileges = load_oracle_tab_privileges(
                        ora_conn,
                        privilege_owners,
                        grantee_scope,
                        settings.get('grant_tab_privs_scope', 'owner')
                    )
                    role_metadata = load_oracle_roles(ora_conn)
                    system_privilege_map = load_oracle_system_privilege_map(ora_conn)
                    table_privilege_map = load_oracle_table_privilege_map(ora_conn)
                    log.info(
                        "Oracle 权限元数据加载完成：对象权限=%d, 系统权限=%d, 角色授权=%d, 角色数=%d, scope=%s",
                        len(object_privileges),
                        len(sys_privileges),
                        len(role_privileges),
                        len(role_names),
                        settings.get('grant_tab_privs_scope', 'owner')
                    )
                except oracledb.Error as e:
                    log.warning("读取 Oracle 权限元数据失败，将跳过授权生成：%s", e)
                    object_privileges = []
                    sys_privileges = []
                    role_privileges = []
                    role_metadata = {}
                    system_privilege_map = set()
                    table_privilege_map = set()

            if seq_owners and include_sequences:
                with ora_conn.cursor() as cursor:
                    sql_seq_tpl = """
                        SELECT SEQUENCE_OWNER, SEQUENCE_NAME,
                               INCREMENT_BY, MIN_VALUE, MAX_VALUE, CYCLE_FLAG, ORDER_FLAG, CACHE_SIZE
                        FROM DBA_SEQUENCES
                        WHERE SEQUENCE_OWNER IN ({owners_clause})
                    """
                    for owner_chunk in seq_owner_chunks:
                        owners_clause = build_bind_placeholders(len(owner_chunk))
                        sql_seq = sql_seq_tpl.format(owners_clause=owners_clause)
                        cursor.execute(sql_seq, owner_chunk)
                        for row in cursor:
                            owner = _safe_upper(row[0])
                            seq_name = _safe_upper(row[1])
                            if not owner or not seq_name:
                                continue
                            sequences.setdefault(owner, set()).add(seq_name)
                            def _as_int(value) -> Optional[int]:
                                try:
                                    return int(value)
                                except (TypeError, ValueError):
                                    return None
                            seq_info = {
                                "increment_by": _as_int(row[2]),
                                "min_value": _as_int(row[3]),
                                "max_value": _as_int(row[4]),
                                "cycle_flag": _safe_upper(row[5]) or "",
                                "order_flag": _safe_upper(row[6]) or "",
                                "cache_size": _as_int(row[7])
                            }
                            sequence_attrs.setdefault(owner, {})[seq_name] = seq_info

    except oracledb.Error as e:
        log.error(f"严重错误: 批量获取 Oracle 元数据失败: {e}")
        abort_run()

    log.info(
        "Oracle 元数据加载完成：列=%d, 索引表=%d, 约束表=%d, 触发器表=%d, 序列schema=%d, 注释表=%d, 黑名单表=%d",
        len(table_columns),
        len(indexes),
        len(constraints),
        len(triggers),
        len(sequences),
        len(table_comments),
        len(blacklist_tables)
    )

    if table_columns:
        identity_column_supported = support_identity_col and any(
            meta.get("identity") is not None
            for table_meta in table_columns.values()
            for meta in table_meta.values()
        )
        default_on_null_supported = support_default_on_null and any(
            meta.get("default_on_null") is not None
            for table_meta in table_columns.values()
            for meta in table_meta.values()
        )

    return OracleMetadata(
        table_columns=table_columns,
        invisible_column_supported=invisible_column_supported,
        identity_column_supported=identity_column_supported,
        default_on_null_supported=default_on_null_supported,
        indexes=indexes,
        constraints=constraints,
        triggers=triggers,
        sequences=sequences,
        sequence_attrs=sequence_attrs,
        table_comments=table_comments,
        column_comments=column_comments,
        comments_complete=comments_complete,
        blacklist_tables=blacklist_tables,
        object_privileges=object_privileges,
        sys_privileges=sys_privileges,
        role_privileges=role_privileges,
        role_metadata=role_metadata,
        system_privilege_map=system_privilege_map,
        table_privilege_map=table_privilege_map,
        object_statuses=object_statuses,
        package_errors=package_errors,
        package_errors_complete=package_errors_complete,
        partition_key_columns=partition_key_columns,
        interval_partitions=interval_partitions
    )


def load_oracle_dependencies(
    ora_cfg: OraConfig,
    schemas_list: List[str],
    object_types: Optional[Set[str]] = None,
    include_external_refs: bool = False
) -> List[DependencyRecord]:
    """
    从 Oracle 批量读取源 schema 的依赖关系（可选包含外部引用）。
    """
    if not schemas_list:
        return []

    owners = sorted({s.upper() for s in schemas_list})
    enabled_types = {t.upper() for t in (object_types or set(ALL_TRACKED_OBJECT_TYPES))}
    enabled_types &= set(ALL_TRACKED_OBJECT_TYPES)
    if not enabled_types:
        log.info("未启用依赖分析对象类型，跳过 Oracle 依赖读取。")
        return []
    types_clause = ",".join(f"'{t}'" for t in sorted(enabled_types))
    owner_chunks = chunk_list(owners, ORACLE_IN_BATCH_SIZE)
    if include_external_refs:
        sql_tpl = """
            SELECT OWNER, NAME, TYPE, REFERENCED_OWNER, REFERENCED_NAME, REFERENCED_TYPE
            FROM DBA_DEPENDENCIES
            WHERE OWNER IN ({owner_ph})
              AND TYPE IN ({types_clause})
              AND REFERENCED_TYPE IN ({types_clause})
        """
    else:
        sql_tpl = """
            SELECT OWNER, NAME, TYPE, REFERENCED_OWNER, REFERENCED_NAME, REFERENCED_TYPE
            FROM DBA_DEPENDENCIES
            WHERE OWNER IN ({owner_ph})
              AND REFERENCED_OWNER IN ({ref_ph})
              AND TYPE IN ({types_clause})
              AND REFERENCED_TYPE IN ({types_clause})
        """

    records: List[DependencyRecord] = []
    try:
        with oracledb.connect(
            user=ora_cfg['user'],
            password=ora_cfg['password'],
            dsn=ora_cfg['dsn']
        ) as connection:
            with connection.cursor() as cursor:
                if include_external_refs:
                    for owner_chunk in owner_chunks:
                        owner_ph = build_bind_placeholders(len(owner_chunk))
                        sql = sql_tpl.format(owner_ph=owner_ph, types_clause=types_clause)
                        cursor.execute(sql, owner_chunk)
                        for row in cursor:
                            owner = (row[0] or '').strip().upper()
                            name = (row[1] or '').strip().upper()
                            obj_type = (row[2] or '').strip().upper()
                            ref_owner = (row[3] or '').strip().upper()
                            ref_name = (row[4] or '').strip().upper()
                            ref_type = (row[5] or '').strip().upper()
                            if not owner or not name or not ref_owner or not ref_name:
                                continue
                            records.append(DependencyRecord(
                                owner=owner,
                                name=name,
                                object_type=obj_type,
                                referenced_owner=ref_owner,
                                referenced_name=ref_name,
                                referenced_type=ref_type
                            ))
                else:
                    for owner_chunk in owner_chunks:
                        owner_ph = build_bind_placeholders(len(owner_chunk))
                        for ref_chunk in owner_chunks:
                            ref_ph = build_bind_placeholders(len(ref_chunk), offset=len(owner_chunk))
                            sql = sql_tpl.format(owner_ph=owner_ph, ref_ph=ref_ph, types_clause=types_clause)
                            cursor.execute(sql, owner_chunk + ref_chunk)
                            for row in cursor:
                                owner = (row[0] or '').strip().upper()
                                name = (row[1] or '').strip().upper()
                                obj_type = (row[2] or '').strip().upper()
                                ref_owner = (row[3] or '').strip().upper()
                                ref_name = (row[4] or '').strip().upper()
                                ref_type = (row[5] or '').strip().upper()
                                if not owner or not name or not ref_owner or not ref_name:
                                    continue
                                records.append(DependencyRecord(
                                    owner=owner,
                                    name=name,
                                    object_type=obj_type,
                                    referenced_owner=ref_owner,
                                    referenced_name=ref_name,
                                    referenced_type=ref_type
                                ))
    except oracledb.Error as exc:
        log.error(f"严重错误: 加载 Oracle 依赖信息失败: {exc}")
        abort_run()

    log.info("Oracle 依赖信息加载完成，共 %d 条记录。", len(records))
    return records


def load_ob_dependencies(
    ob_cfg: ObConfig,
    target_schemas: Set[str],
    object_types: Optional[Set[str]] = None
) -> Set[Tuple[str, str, str, str]]:
    """
    通过 obclient 读取 OceanBase 侧的依赖信息。
    返回集合 { (OWNER.OBJ, TYPE, REF_OWNER.OBJ, REF_TYPE) }
    """
    if not target_schemas:
        return set()

    owners_list = sorted(target_schemas)
    enabled_types = {t.upper() for t in (object_types or set(ALL_TRACKED_OBJECT_TYPES))}
    enabled_types &= set(ALL_TRACKED_OBJECT_TYPES)
    if not enabled_types:
        return set()
    types_clause = ",".join(f"'{t}'" for t in sorted(enabled_types))

    sql_tpl = f"""
        SELECT OWNER, NAME, TYPE, REFERENCED_OWNER, REFERENCED_NAME, REFERENCED_TYPE
        FROM DBA_DEPENDENCIES
        WHERE OWNER IN ({{owners_in}})
          AND REFERENCED_OWNER IN ({{ref_owners_in}})
          AND TYPE IN ({types_clause})
          AND REFERENCED_TYPE IN ({types_clause})
    """
    ok, lines, err = obclient_query_by_owner_pairs(ob_cfg, sql_tpl, owners_list, owners_list)
    if not ok:
        log.error("无法从 OB 读取 DBA_DEPENDENCIES，程序退出。")
        abort_run()

    result: Set[Tuple[str, str, str, str]] = set()
    if lines:
        for line in lines:
            parts = line.split('\t')
            if len(parts) < 6:
                continue
            owner = normalize_public_owner(parts[0])
            name = (parts[1] or "").strip().upper()
            obj_type = parts[2].strip().upper()
            ref_owner = normalize_public_owner(parts[3])
            ref_name = (parts[4] or "").strip().upper()
            ref_type = parts[5].strip().upper()
            if not owner or not name or not ref_owner or not ref_name:
                continue
            result.add((
                f"{owner}.{name}",
                obj_type,
                f"{ref_owner}.{ref_name}",
                ref_type
            ))

    log.info("OceanBase 依赖信息加载完成，共 %d 条记录。", len(result))
    return result


def build_expected_dependency_pairs(
    dependencies: List[DependencyRecord],
    full_mapping: FullObjectMapping
) -> Tuple[Set[Tuple[str, str, str, str]], List[DependencyIssue]]:
    """
    将源端依赖映射到目标端 (schema/object 名已替换)。
    返回 (期望依赖集合, 被跳过的依赖列表)。
    """
    expected: Set[Tuple[str, str, str, str]] = set()
    skipped: List[DependencyIssue] = []

    def _is_builtin_dependency(owner: str, name: str) -> bool:
        owner_u = (owner or "").strip().upper()
        name_u = (name or "").strip().upper()
        return name_u == "DUAL" and owner_u in {"PUBLIC", "SYS"}

    for dep in dependencies:
        dep_key = f"{dep.owner}.{dep.name}".upper()
        ref_key = f"{dep.referenced_owner}.{dep.referenced_name}".upper()
        dep_target = get_mapped_target(full_mapping, dep_key, dep.object_type)
        ref_target = get_mapped_target(full_mapping, ref_key, dep.referenced_type)

        if dep_target is None:
            skipped.append(DependencyIssue(
                dependent=dep_key,
                dependent_type=dep.object_type.upper(),
                referenced=ref_key,
                referenced_type=dep.referenced_type.upper(),
                reason="源对象未纳入受管范围或缺少 remap 规则，无法建立依赖。"
            ))
            continue
        if ref_target is None:
            if _is_builtin_dependency(dep.referenced_owner, dep.referenced_name):
                skipped.append(DependencyIssue(
                    dependent=dep_key,
                    dependent_type=dep.object_type.upper(),
                    referenced=ref_key,
                    referenced_type=dep.referenced_type.upper(),
                    reason="内建对象依赖无需映射 (DUAL)。"
                ))
                continue
            skipped.append(DependencyIssue(
                dependent=dep_key,
                dependent_type=dep.object_type.upper(),
                referenced=ref_key,
                referenced_type=dep.referenced_type.upper(),
                reason="被依赖对象未纳入受管范围或缺少 remap 规则，无法建立依赖。"
            ))
            continue

        expected.add((
            dep_target.upper(),
            dep.object_type.upper(),
            ref_target.upper(),
            dep.referenced_type.upper()
        ))

    return expected, skipped


def to_raw_dependency_pairs(
    dependencies: List[DependencyRecord]
) -> Set[Tuple[str, str, str, str]]:
    """
    将 DependencyRecord 列表转换为 (dep_full, dep_type, ref_full, ref_type) 集合（源端视角）。
    """
    raw_pairs: Set[Tuple[str, str, str, str]] = set()
    for dep in dependencies:
        dep_key = f"{dep.owner}.{dep.name}".upper()
        ref_key = f"{dep.referenced_owner}.{dep.referenced_name}".upper()
        raw_pairs.add((dep_key, dep.object_type.upper(), ref_key, dep.referenced_type.upper()))
    return raw_pairs


def export_dependency_chains(
    expected_pairs: Set[Tuple[str, str, str, str]],
    output_path: Path,
    source_pairs: Optional[Set[Tuple[str, str, str, str]]] = None
) -> Optional[Path]:
    """
    输出依赖链，支持：
      - 源端（Oracle）依赖链
      - 目标端（Remap 后）依赖链
    依赖链会“下探”直到终点（无进一步依赖或 TABLE/MVIEW），并打印每条路径。
    """
    if not expected_pairs and not source_pairs:
        return None

    def _build_chains(pairs: Set[Tuple[str, str, str, str]], label: str) -> Tuple[List[str], List[str]]:
        if not pairs:
            return [], []
        graph: Dict[str, Set[str]] = defaultdict(set)  # dependent -> {referenced}
        type_map: Dict[str, str] = {}
        reverse_refs: Dict[str, Set[str]] = defaultdict(set)

        for dep_name, dep_type, ref_name, ref_type in pairs:
            dep = dep_name.upper()
            ref = ref_name.upper()
            graph[dep].add(ref)
            reverse_refs[ref].add(dep)
            type_map.setdefault(dep, dep_type.upper())
            type_map.setdefault(ref, ref_type.upper())

        # 选择根节点（未被其他对象引用的节点）；若无根，则全部节点皆为起点
        roots = [n for n in type_map.keys() if n not in reverse_refs]
        if not roots:
            roots = sorted(type_map.keys())

        chains: List[str] = []
        cycles: List[str] = []

        def dfs(node: str, path: List[Tuple[str, str]], seen: Set[str]) -> None:
            node_u = node.upper()
            obj_type = type_map.get(node_u, "UNKNOWN")
            if node_u in seen:
                cycle_path = " -> ".join([f"{n}({t})" for n, t in path] + [f"{node_u}(CYCLE)"])
                cycles.append(cycle_path)
                return
            path_next = path + [(node_u, obj_type)]
            refs = sorted(graph.get(node_u, set()))
            # 终点条件：无引用或到达 TABLE/MATERIALIZED VIEW
            if not refs or obj_type in ("TABLE", "MATERIALIZED VIEW"):
                chains.append(" -> ".join(f"{n}({t})" for n, t in path_next))
                return
            for ref in refs:
                dfs(ref, path_next, seen | {node_u})

        for root in sorted(roots):
            dfs(root, [], set())

        return chains, cycles

    target_chains, target_cycles = _build_chains(expected_pairs, "TARGET")
    source_chains, source_cycles = _build_chains(source_pairs or set(), "SOURCE") if source_pairs else ([], [])

    try:
        output_path.parent.mkdir(parents=True, exist_ok=True)
        with output_path.open('w', encoding='utf-8') as f:
            f.write("# 依赖链下探（终点为 TABLE/MVIEW 或无进一步依赖）\n")
            f.write(f"# 目标端依赖数: {len(expected_pairs)}, 源端依赖数: {len(source_pairs or [])}\n\n")

            if source_chains:
                f.write("[SOURCE - ORACLE] 依赖链:\n")
                for idx, line in enumerate(source_chains, 1):
                    f.write(f"{idx:05d}. {line}\n")
                if source_cycles:
                    f.write("\n[SOURCE] 检测到依赖环:\n")
                    for cyc in source_cycles:
                        f.write(f"- {cyc}\n")
                f.write("\n")

            f.write("[TARGET - REMAPPED] 依赖链:\n")
            for idx, line in enumerate(target_chains, 1):
                f.write(f"{idx:05d}. {line}\n")
            if target_cycles:
                f.write("\n[TARGET] 检测到依赖环:\n")
                for cyc in target_cycles:
                    f.write(f"- {cyc}\n")
            f.write("\n")
    except OSError as exc:
        log.warning("写入依赖链文件失败: %s", exc)
        return None

    return output_path


def build_target_to_source_mapping(
    full_object_mapping: FullObjectMapping
) -> Dict[Tuple[str, str], str]:
    mapping: Dict[Tuple[str, str], str] = {}
    for src_full, type_map in full_object_mapping.items():
        src_full_u = (src_full or "").upper()
        for obj_type, tgt_full in type_map.items():
            key = ((tgt_full or "").upper(), (obj_type or "").upper())
            if not key[0] or not key[1]:
                continue
            mapping.setdefault(key, src_full_u)
    return mapping


def infer_type_from_mapping(
    full_object_mapping: FullObjectMapping,
    source_full: str,
    preferred_types: Tuple[str, ...]
) -> Optional[str]:
    type_map = full_object_mapping.get((source_full or "").upper(), {})
    for obj_type in preferred_types:
        if obj_type in type_map:
            return obj_type
    if type_map:
        return sorted(type_map.keys())[0]
    return None


def resolve_synonym_chain_target(
    synonym_full: str,
    synonym_type: str,
    target_to_source: Dict[Tuple[str, str], str],
    synonym_meta: Optional[Dict[Tuple[str, str], SynonymMeta]],
    full_object_mapping: FullObjectMapping,
    remap_rules: RemapRules
) -> Tuple[Optional[str], Optional[str]]:
    if (synonym_type or "").upper() != "SYNONYM":
        return None, None
    if not synonym_meta:
        return None, None
    syn_full_u = (synonym_full or "").upper()
    src_full = target_to_source.get((syn_full_u, "SYNONYM"), syn_full_u)
    if '.' not in src_full:
        return None, None
    owner, name = src_full.split('.', 1)
    meta = synonym_meta.get((owner, name))
    if not meta or not meta.table_owner or not meta.table_name:
        return None, None
    if meta.db_link:
        return None, None
    target_source = f"{meta.table_owner}.{meta.table_name}".upper()
    mapped = find_mapped_target_any_type(
        full_object_mapping,
        target_source,
        preferred_types=("TABLE", "VIEW", "MATERIALIZED VIEW", "SYNONYM", "FUNCTION", "PROCEDURE", "PACKAGE", "TYPE")
    ) or remap_rules.get(target_source) or target_source
    target_type = infer_type_from_mapping(
        full_object_mapping,
        target_source,
        ("TABLE", "VIEW", "MATERIALIZED VIEW", "SYNONYM", "FUNCTION", "PROCEDURE", "PACKAGE", "TYPE")
    ) or "UNKNOWN"
    return (mapped or "").upper(), target_type


def build_view_fixup_chains(
    view_targets: List[str],
    dependency_pairs: Set[Tuple[str, str, str, str]],
    full_object_mapping: FullObjectMapping,
    remap_rules: RemapRules,
    synonym_meta: Optional[Dict[Tuple[str, str], SynonymMeta]] = None,
    ob_meta: Optional[ObMetadata] = None,
    ob_grant_catalog: Optional[ObGrantCatalog] = None,
    view_grant_targets: Optional[Set[str]] = None,
    max_depth: int = 30
) -> Tuple[List[str], List[str]]:
    if not view_targets:
        return [], []

    graph: Dict[DependencyNode, Set[DependencyNode]] = defaultdict(set)
    all_nodes: Set[DependencyNode] = set()

    for dep_name, dep_type, ref_name, ref_type in dependency_pairs:
        dep_node = ((dep_name or "").upper(), (dep_type or "").upper())
        ref_node = ((ref_name or "").upper(), (ref_type or "").upper())
        if not dep_node[0] or not dep_node[1] or not ref_node[0] or not ref_node[1]:
            continue
        graph[dep_node].add(ref_node)
        all_nodes.add(dep_node)
        all_nodes.add(ref_node)

    target_to_source = build_target_to_source_mapping(full_object_mapping)
    synonym_target_map: Dict[DependencyNode, DependencyNode] = {}
    for node in sorted(all_nodes):
        if node[1] != "SYNONYM":
            continue
        target_full, target_type = resolve_synonym_chain_target(
            node[0],
            node[1],
            target_to_source,
            synonym_meta,
            full_object_mapping,
            remap_rules
        )
        if not target_full or not target_type:
            continue
        target_node = (target_full, target_type)
        graph[node].add(target_node)
        synonym_target_map[node] = target_node

    def _exists(node: DependencyNode) -> str:
        if not ob_meta:
            return "UNKNOWN"
        obj_type = (node[1] or "").upper()
        obj_full = (node[0] or "").upper()
        obj_set = ob_meta.objects_by_type.get(obj_type)
        if obj_set is None:
            return "UNKNOWN"
        return "EXISTS" if obj_full in obj_set else "MISSING"

    view_grant_targets = {v.upper() for v in (view_grant_targets or set()) if v}

    def _grant_status(dep_node: DependencyNode, ref_node: DependencyNode, require_grantable: bool) -> str:
        if not ob_grant_catalog:
            return "GRANT_UNKNOWN"
        if dep_node in synonym_target_map and synonym_target_map.get(dep_node) == ref_node:
            return "GRANT_NA"
        dep_full = (dep_node[0] or "").upper()
        if '.' not in dep_full:
            return "GRANT_UNKNOWN"
        dep_schema = dep_full.split('.', 1)[0]

        ref_full, ref_type = ref_node
        if ref_node in synonym_target_map:
            ref_full, ref_type = synonym_target_map[ref_node]
        ref_full_u = (ref_full or "").upper()
        if '.' not in ref_full_u:
            return "GRANT_UNKNOWN"
        ref_schema = ref_full_u.split('.', 1)[0]
        if dep_schema == ref_schema:
            return "GRANT_OK"

        required_priv = GRANT_PRIVILEGE_BY_TYPE.get((ref_type or "").upper())
        if not required_priv:
            return "GRANT_UNKNOWN"
        obj_key = (dep_schema, required_priv, ref_full_u)
        if require_grantable:
            if obj_key in ob_grant_catalog.object_privs_grantable:
                return "GRANT_OK"
            if obj_key in ob_grant_catalog.object_privs:
                return "GRANT_MISSING_OPTION"
        else:
            if obj_key in ob_grant_catalog.object_privs or obj_key in ob_grant_catalog.object_privs_grantable:
                return "GRANT_OK"
        implied = SYS_PRIV_IMPLICATIONS.get(required_priv, set())
        for sys_priv in implied:
            sys_key = (dep_schema, sys_priv)
            if sys_key in ob_grant_catalog.sys_privs or sys_key in ob_grant_catalog.sys_privs_admin:
                return "GRANT_MISSING_OPTION" if require_grantable else "GRANT_OK"
        return "GRANT_MISSING_OPTION" if require_grantable else "GRANT_MISSING"

    def _format_chain(path: List[DependencyNode]) -> str:
        parts: List[str] = []
        require_grantable = bool(path and path[0][0] in view_grant_targets)
        for idx, node in enumerate(path):
            obj_type = (node[1] or "UNKNOWN").upper()
            exists = _exists(node)
            grant_status = "GRANT_NA" if idx == 0 else _grant_status(path[idx - 1], node, require_grantable)
            parts.append(f"{node[0]}[{obj_type}|{exists}|{grant_status}]")
        return " -> ".join(parts)

    chains: List[str] = []
    cycles: List[str] = []

    def dfs(node: DependencyNode, path: List[DependencyNode], seen: Set[DependencyNode]) -> None:
        if node in seen:
            cycles.append(_format_chain(path + [node]) + " (CYCLE)")
            return
        if len(path) >= max_depth:
            chains.append(_format_chain(path) + " -> ... (DEPTH_LIMIT)")
            return
        refs = sorted(graph.get(node, set()))
        if not refs:
            chains.append(_format_chain(path))
            return
        for ref in refs:
            dfs(ref, path + [ref], seen | {node})

    for view_full in sorted({v.upper() for v in view_targets if v}):
        view_node = (view_full, "VIEW")
        if view_node not in graph:
            chains.append(_format_chain([view_node]))
            continue
        dfs(view_node, [view_node], set())

    return chains, cycles


def export_view_fixup_chains(
    view_targets: List[str],
    dependency_pairs: Set[Tuple[str, str, str, str]],
    output_path: Path,
    full_object_mapping: FullObjectMapping,
    remap_rules: RemapRules,
    synonym_meta: Optional[Dict[Tuple[str, str], SynonymMeta]] = None,
    ob_meta: Optional[ObMetadata] = None,
    ob_grant_catalog: Optional[ObGrantCatalog] = None,
    view_grant_targets: Optional[Set[str]] = None
) -> Optional[Path]:
    chains, cycles = build_view_fixup_chains(
        view_targets,
        dependency_pairs,
        full_object_mapping,
        remap_rules,
        synonym_meta=synonym_meta,
        ob_meta=ob_meta,
        ob_grant_catalog=ob_grant_catalog,
        view_grant_targets=view_grant_targets
    )
    if not chains:
        return None

    try:
        output_path.parent.mkdir(parents=True, exist_ok=True)
        with output_path.open('w', encoding='utf-8') as f:
            f.write("# VIEW fixup dependency chains\n")
            f.write("# 格式: OWNER.OBJ[TYPE|EXISTS|GRANT_STATUS]\n")
            f.write(f"# views={len(set(view_targets))}, chains={len(chains)}, cycles={len(cycles)}\n\n")
            for idx, line in enumerate(chains, 1):
                f.write(f"{idx:05d}. {line}\n")
            if cycles:
                f.write("\n[CYCLES]\n")
                for cyc in cycles:
                    f.write(f"- {cyc}\n")
        return output_path
    except OSError as exc:
        log.warning("写入 VIEWs_chain 文件失败: %s", exc)
        return None


def check_dependencies_against_ob(
    expected_pairs: Set[Tuple[str, str, str, str]],
    actual_pairs: Set[Tuple[str, str, str, str]],
    skipped: List[DependencyIssue],
    ob_meta: ObMetadata,
    ob_grant_catalog: Optional[ObGrantCatalog] = None
) -> DependencyReport:
    """
    对比目标端依赖关系，返回缺失/多余/跳过的依赖项。
    """
    report: DependencyReport = {
        "missing": [],
        "unexpected": [],
        "skipped": skipped
    }

    def object_exists(full_name: str, obj_type: str) -> bool:
        return full_name in ob_meta.objects_by_type.get(obj_type.upper(), set())

    def resolve_grant_status(
        dep_name: str,
        ref_name: str,
        ref_type: str
    ) -> Tuple[str, str]:
        if not ob_grant_catalog:
            return "GRANT_UNKNOWN", "未加载权限目录"
        if "." not in dep_name or "." not in ref_name:
            return "GRANT_UNKNOWN", "对象名格式异常"
        dep_schema = dep_name.split('.', 1)[0].upper()
        ref_schema = ref_name.split('.', 1)[0].upper()
        if dep_schema == ref_schema:
            return "GRANT_OK", ""
        required_priv = GRANT_PRIVILEGE_BY_TYPE.get(ref_type.upper())
        if not required_priv:
            return "GRANT_UNKNOWN", "未定义权限映射"
        obj_key = (dep_schema, required_priv, ref_name.upper())
        if obj_key in ob_grant_catalog.object_privs or obj_key in ob_grant_catalog.object_privs_grantable:
            return "GRANT_OK", ""
        implied = SYS_PRIV_IMPLICATIONS.get(required_priv, set())
        for sys_priv in implied:
            sys_key = (dep_schema, sys_priv)
            if sys_key in ob_grant_catalog.sys_privs or sys_key in ob_grant_catalog.sys_privs_admin:
                return "GRANT_OK", ""
        return "GRANT_MISSING", ""

    def build_missing_reason(dep_name: str, dep_type: str, ref_name: str, ref_type: str) -> str:
        dep_obj = f"{dep_name} ({dep_type})"
        ref_obj = f"{ref_name} ({ref_type})"
        dep_schema = dep_name.split('.', 1)[0]
        ref_schema = ref_name.split('.', 1)[0]
        cross_schema_note = ""
        if dep_schema != ref_schema:
            cross_schema_note = " 跨 schema 依赖，请确认 remap 后的授权（SELECT/EXECUTE/REFERENCES）或同义词已就绪。"

        if dep_type in {"FUNCTION", "PROCEDURE"}:
            action = (
                f"依赖关系未建立：在 OceanBase 执行 ALTER {dep_type} {dep_name} COMPILE；"
                f"如仍失败，请检查 {dep_obj} 中对 {ref_obj} 的调用及授权/Remap。"
            )
        elif dep_type in {"PACKAGE", "PACKAGE BODY"}:
            action = (
                f"依赖关系未建立：执行 ALTER PACKAGE {dep_name} COMPILE 及 ALTER PACKAGE {dep_name} COMPILE BODY，"
                f"确认包定义能够访问 {ref_obj}。"
            )
        elif dep_type == "TRIGGER":
            action = (
                f"依赖关系未建立：执行 ALTER TRIGGER {dep_name} COMPILE，"
                f"确认触发器引用的对象 {ref_obj} 已存在且可访问。"
            )
        elif dep_type in {"VIEW", "MATERIALIZED VIEW"}:
            action = (
                f"依赖关系未建立：请 CREATE OR REPLACE {dep_type} {dep_name}，"
                f"确保所有底层对象（如 {ref_obj}）已存在，再执行 ALTER {dep_type} {dep_name} COMPILE。"
            )
        elif dep_type == "SYNONYM":
            action = (
                f"依赖关系未建立：请重新创建同义词（CREATE OR REPLACE SYNONYM {dep_name} FOR {ref_name}），"
                f"确认 remap 目标和授权正确。"
            )
        elif dep_type in {"TYPE", "TYPE BODY"}:
            compile_stmt = f"ALTER TYPE {dep_name} COMPILE{' BODY' if dep_type == 'TYPE BODY' else ''}"
            action = (
                f"依赖关系未建立：先创建/校验 TYPE 定义，再执行 {compile_stmt}，"
                f"确保 {ref_obj} 已存在且可访问。"
            )
        elif dep_type == "INDEX":
            action = (
                f"依赖关系未建立：请重建索引 {dep_obj}，"
                f"检查索引表达式或函数中对 {ref_obj} 的引用是否有效。"
            )
        elif dep_type == "SEQUENCE":
            action = (
                f"依赖关系未建立：请重新创建序列 {dep_obj}，"
                f"检查同义词或授权设置是否让 {ref_obj} 可见。"
            )
        else:
            action = (
                f"依赖关系未建立：请重新部署 {dep_obj}，"
                f"确认定义中对 {ref_obj} 的引用与 remap/授权保持一致。"
            )

        return action + cross_schema_note

    missing_pairs = expected_pairs - actual_pairs
    extra_pairs = actual_pairs - expected_pairs

    for dep_name, dep_type, ref_name, ref_type in sorted(missing_pairs):
        dep_obj = f"{dep_name} ({dep_type})"
        ref_obj = f"{ref_name} ({ref_type})"
        if not object_exists(dep_name, dep_type):
            reason = f"依赖对象 {dep_obj} 在目标端缺失，请补齐该对象后再重新编译依赖。"
        elif not object_exists(ref_name, ref_type):
            reason = f"被依赖对象 {ref_obj} 在目标端缺失，请先创建/迁移该对象，再重新部署 {dep_obj}。"
        else:
            reason = build_missing_reason(dep_name, dep_type, ref_name, ref_type)
            if ob_grant_catalog and "." in dep_name and "." in ref_name:
                dep_schema = dep_name.split('.', 1)[0]
                ref_schema = ref_name.split('.', 1)[0]
                if dep_schema != ref_schema:
                    grant_status, grant_note = resolve_grant_status(dep_name, ref_name, ref_type)
                    if grant_status != "GRANT_OK" or grant_note:
                        reason += f" 授权检查={grant_status}"
                        if grant_note:
                            reason += f"({grant_note})"
        report["missing"].append(DependencyIssue(
            dependent=dep_name,
            dependent_type=dep_type,
            referenced=ref_name,
            referenced_type=ref_type,
            reason=reason
        ))

    for dep_name, dep_type, ref_name, ref_type in sorted(extra_pairs):
        dep_obj = f"{dep_name} ({dep_type})"
        ref_obj = f"{ref_name} ({ref_type})"
        reason = (
            f"OceanBase 中存在额外依赖 {dep_obj} -> {ref_obj}，"
            f"请确认是否需要保留或清理。"
        )
        report["unexpected"].append(DependencyIssue(
            dependent=dep_name,
            dependent_type=dep_type,
            referenced=ref_name,
            referenced_type=ref_type,
            reason=reason
        ))

    return report


def compute_required_grants(
    expected_pairs: Set[Tuple[str, str, str, str]]
) -> Dict[str, Set[Tuple[str, str]]]:
    grants: Dict[str, Set[Tuple[str, str]]] = defaultdict(set)

    for dep_full, dep_type, ref_full, ref_type in expected_pairs:
        dep_schema, _ = dep_full.split('.', 1)
        ref_schema, _ = ref_full.split('.', 1)
        if dep_schema == ref_schema:
            continue
        privilege = GRANT_PRIVILEGE_BY_TYPE.get(ref_type.upper())
        if not privilege:
            continue
        grants[dep_schema].add((privilege, ref_full))
        # 对外键依赖的表补充 REFERENCES 权限，便于创建 FK
        if ref_type.upper() == 'TABLE' and dep_type.upper() == 'TABLE':
            grants[dep_schema].add(('REFERENCES', ref_full))

    return grants


def filter_existing_required_grants(
    required_grants: Dict[str, Set[Tuple[str, str]]],
    ob_cfg: ObConfig
) -> Dict[str, Set[Tuple[str, str]]]:
    """
    过滤已在 OceanBase 侧满足的 GRANT 建议。

    检查来源：
      1) DBA_TAB_PRIVS 直接对象权限（含对 ROLE 的授权）
      2) DBA_SYS_PRIVS 系统权限（如 SELECT ANY TABLE / EXECUTE ANY PROCEDURE）
      3) DBA_ROLE_PRIVS 角色授予（用于透传 1/2 的授权）

    若任一查询失败，将保守返回原始 required_grants。
    """
    if not required_grants:
        return required_grants

    def _sql_list(vals: Set[str]) -> str:
        safe_vals = [v.replace("'", "''") for v in vals if v]
        return ",".join(f"'{v}'" for v in sorted(safe_vals))

    grantees: Set[str] = {g.upper() for g in required_grants.keys() if g}
    required_entries: List[Tuple[str, str, str]] = []
    owners: Set[str] = set()
    names: Set[str] = set()

    for grantee, entries in required_grants.items():
        g_u = (grantee or "").upper()
        for priv, obj in entries:
            p_u = (priv or "").upper()
            o_u = (obj or "").upper()
            if not g_u or not p_u or not o_u or '.' not in o_u:
                continue
            owner, name = o_u.split('.', 1)
            owners.add(owner)
            names.add(name)
            required_entries.append((g_u, p_u, o_u))

    if not required_entries:
        return required_grants

    roles_by_grantee: Dict[str, Set[str]] = defaultdict(set)
    roles: Set[str] = set()
    try:
        role_sql = f"SELECT GRANTEE, GRANTED_ROLE FROM DBA_ROLE_PRIVS WHERE GRANTEE IN ({_sql_list(grantees)})"
        ok, out, _err = obclient_run_sql(ob_cfg, role_sql)
        if ok and out:
            for line in out.splitlines():
                parts = line.split('\t')
                if len(parts) < 2:
                    continue
                gr = parts[0].strip().upper()
                role = parts[1].strip().upper()
                if gr and role:
                    roles_by_grantee[gr].add(role)
                    roles.add(role)
    except Exception as exc:  # pragma: no cover
        log.warning("[GRANT_FILTER] 读取 DBA_ROLE_PRIVS 失败，将仅基于直接授权过滤: %s", exc)

    identities: Set[str] = grantees | roles

    tab_privs: Set[Tuple[str, str, str]] = set()
    try:
        tab_sql = textwrap.dedent(f"""
            SELECT GRANTEE, PRIVILEGE, OWNER, TABLE_NAME
            FROM DBA_TAB_PRIVS
            WHERE GRANTEE IN ({_sql_list(identities)})
              AND OWNER IN ({_sql_list(owners)})
              AND TABLE_NAME IN ({_sql_list(names)})
        """).strip()
        ok, out, _err = obclient_run_sql(ob_cfg, tab_sql)
        if ok and out:
            for line in out.splitlines():
                parts = line.split('\t')
                if len(parts) < 4:
                    continue
                gr = parts[0].strip().upper()
                pv = parts[1].strip().upper()
                ow = parts[2].strip().upper()
                nm = parts[3].strip().upper()
                if gr and pv and ow and nm:
                    tab_privs.add((gr, pv, f"{ow}.{nm}"))
    except Exception as exc:  # pragma: no cover
        log.warning("[GRANT_FILTER] 读取 DBA_TAB_PRIVS 失败，跳过过滤: %s", exc)
        return required_grants

    sys_privs: Dict[str, Set[str]] = defaultdict(set)
    try:
        sys_sql = f"SELECT GRANTEE, PRIVILEGE FROM DBA_SYS_PRIVS WHERE GRANTEE IN ({_sql_list(identities)})"
        ok, out, _err = obclient_run_sql(ob_cfg, sys_sql)
        if ok and out:
            for line in out.splitlines():
                parts = line.split('\t')
                if len(parts) < 2:
                    continue
                gr = parts[0].strip().upper()
                pv = parts[1].strip().upper()
                if gr and pv:
                    sys_privs[gr].add(pv)
    except Exception as exc:  # pragma: no cover
        log.warning("[GRANT_FILTER] 读取 DBA_SYS_PRIVS 失败，将仅基于对象授权过滤: %s", exc)

    def _sys_satisfies(identity: str, required_priv: str) -> bool:
        implied = SYS_PRIV_IMPLICATIONS.get(required_priv, set())
        if not implied:
            return False
        return any(pv in sys_privs.get(identity, set()) for pv in implied)

    def _has_priv(grantee: str, required_priv: str, obj_full: str) -> bool:
        if (grantee, required_priv, obj_full) in tab_privs:
            return True
        for role in roles_by_grantee.get(grantee, set()):
            if (role, required_priv, obj_full) in tab_privs:
                return True
        if _sys_satisfies(grantee, required_priv):
            return True
        for role in roles_by_grantee.get(grantee, set()):
            if _sys_satisfies(role, required_priv):
                return True
        return False

    filtered: Dict[str, Set[Tuple[str, str]]] = {}
    removed = 0
    for grantee, entries in required_grants.items():
        g_u = (grantee or "").upper()
        remaining: Set[Tuple[str, str]] = set()
        for priv, obj in entries:
            p_u = (priv or "").upper()
            o_u = (obj or "").upper()
            if g_u and p_u and o_u and '.' in o_u and _has_priv(g_u, p_u, o_u):
                removed += 1
                continue
            remaining.add((priv, obj))
        if remaining:
            filtered[g_u] = remaining

    if removed:
        log.info("[GRANT_FILTER] 已过滤 %d 条已存在的授权建议。", removed)
    return filtered


def filter_missing_grant_entries(
    object_grants_by_grantee: Dict[str, Set[ObjectGrantEntry]],
    sys_privs_by_grantee: Dict[str, Set[SystemGrantEntry]],
    role_privs_by_grantee: Dict[str, Set[RoleGrantEntry]],
    ob_catalog: Optional[ObGrantCatalog]
) -> Tuple[
    Dict[str, Set[ObjectGrantEntry]],
    Dict[str, Set[SystemGrantEntry]],
    Dict[str, Set[RoleGrantEntry]]
]:
    """
    基于 OB 权限目录过滤已存在的授权，返回缺失授权集合。
    如果 ob_catalog 为空，返回原始集合（缺失集合=全量）。
    """
    if ob_catalog is None:
        return object_grants_by_grantee, sys_privs_by_grantee, role_privs_by_grantee

    miss_obj: Dict[str, Set[ObjectGrantEntry]] = defaultdict(set)
    miss_sys: Dict[str, Set[SystemGrantEntry]] = defaultdict(set)
    miss_role: Dict[str, Set[RoleGrantEntry]] = defaultdict(set)

    obj_basic = ob_catalog.object_privs
    obj_grantable = ob_catalog.object_privs_grantable
    sys_basic = ob_catalog.sys_privs
    sys_admin = ob_catalog.sys_privs_admin
    role_basic = ob_catalog.role_privs
    role_admin = ob_catalog.role_privs_admin

    for grantee, entries in object_grants_by_grantee.items():
        g_u = (grantee or "").upper()
        if not g_u:
            continue
        for entry in entries:
            priv_u = (entry.privilege or "").upper()
            obj_u = (entry.object_full or "").upper()
            if not priv_u or not obj_u:
                continue
            key = (g_u, priv_u, obj_u)
            if entry.grantable:
                if key in obj_grantable:
                    continue
            else:
                if key in obj_basic or key in obj_grantable:
                    continue
            miss_obj[g_u].add(entry)

    for grantee, entries in sys_privs_by_grantee.items():
        g_u = (grantee or "").upper()
        if not g_u:
            continue
        for entry in entries:
            priv_u = (entry.privilege or "").upper()
            if not priv_u:
                continue
            key = (g_u, priv_u)
            if entry.admin_option:
                if key in sys_admin:
                    continue
            else:
                if key in sys_basic or key in sys_admin:
                    continue
            miss_sys[g_u].add(entry)

    for grantee, entries in role_privs_by_grantee.items():
        g_u = (grantee or "").upper()
        if not g_u:
            continue
        for entry in entries:
            role_u = (entry.role or "").upper()
            if not role_u:
                continue
            key = (g_u, role_u)
            if entry.admin_option:
                if key in role_admin:
                    continue
            else:
                if key in role_basic or key in role_admin:
                    continue
            miss_role[g_u].add(entry)

    return miss_obj, miss_sys, miss_role


def split_view_grants(
    view_targets: Set[str],
    expected_dependency_pairs: Optional[Set[Tuple[str, str, str, str]]],
    object_grants_missing_by_grantee: Dict[str, Set[ObjectGrantEntry]]
) -> Tuple[
    Dict[str, Set[ObjectGrantEntry]],
    Dict[str, Set[ObjectGrantEntry]],
    Dict[str, Set[ObjectGrantEntry]]
]:
    """
    拆分视图相关授权：
      - view_prereq_grants: 视图创建前的依赖授权（基于依赖对）
      - view_post_grants: 视图创建后授权（源端视图权限同步）
      - remaining: 其余授权
    """
    view_targets_u = {v.upper() for v in (view_targets or set()) if v}
    prereq_needed: Set[Tuple[str, str, str]] = set()

    if view_targets_u and expected_dependency_pairs:
        for dep_full, dep_type, ref_full, ref_type in expected_dependency_pairs:
            dep_full_u = (dep_full or "").upper()
            if dep_full_u not in view_targets_u:
                continue
            if (dep_type or "").upper() not in {"VIEW", "MATERIALIZED VIEW"}:
                continue
            if not dep_full or not ref_full or '.' not in dep_full or '.' not in ref_full:
                continue
            dep_schema = dep_full_u.split('.', 1)[0]
            ref_schema = (ref_full or "").upper().split('.', 1)[0]
            if dep_schema == ref_schema:
                continue
            privilege = GRANT_PRIVILEGE_BY_TYPE.get((ref_type or "").upper())
            if not privilege:
                continue
            prereq_needed.add((dep_schema, privilege.upper(), (ref_full or "").upper()))

    view_prereq: Dict[str, Set[ObjectGrantEntry]] = defaultdict(set)
    view_post: Dict[str, Set[ObjectGrantEntry]] = defaultdict(set)
    remaining: Dict[str, Set[ObjectGrantEntry]] = defaultdict(set)

    for grantee, entries in (object_grants_missing_by_grantee or {}).items():
        grantee_u = (grantee or "").upper()
        for entry in entries or set():
            priv_u = (entry.privilege or "").upper()
            obj_u = (entry.object_full or "").upper()
            key = (grantee_u, priv_u, obj_u)
            if key in prereq_needed:
                view_prereq[grantee_u].add(entry)
                continue
            if obj_u in view_targets_u:
                owner_u = obj_u.split('.', 1)[0] if '.' in obj_u else ""
                if grantee_u and owner_u and grantee_u != owner_u:
                    view_post[grantee_u].add(entry)
                    continue
            remaining[grantee_u].add(entry)

    return dict(view_prereq), dict(view_post), dict(remaining)


PRIVILEGE_TYPE_PRIORITY: Tuple[str, ...] = (
    'TABLE',
    'VIEW',
    'MATERIALIZED VIEW',
    'SEQUENCE',
    'TYPE',
    'PACKAGE',
    'FUNCTION',
    'PROCEDURE',
    'TRIGGER',
    'INDEX',
    'SYNONYM',
    'JOB',
    'SCHEDULE'
)


def normalize_privilege_object_type(obj_type: Optional[str]) -> str:
    if not obj_type:
        return ""
    obj_type_u = str(obj_type).strip().upper()
    if obj_type_u in ("PACKAGE BODY", "TYPE BODY"):
        return obj_type_u.replace(" BODY", "")
    return obj_type_u


def infer_privilege_object_type(
    src_full: str,
    source_objects: Optional[SourceObjectMap]
) -> str:
    if not source_objects:
        return ""
    types = source_objects.get(src_full.upper())
    if not types:
        return ""
    types_u = {t.upper() for t in types}
    for cand in PRIVILEGE_TYPE_PRIORITY:
        if cand in types_u:
            return cand
    return sorted(types_u)[0] if types_u else ""


def build_role_name_set(
    role_privileges: List[OracleRolePrivilege],
    source_schema_set: Set[str]
) -> Set[str]:
    roles: Set[str] = {item.role.upper() for item in role_privileges if item.role}
    for item in role_privileges:
        grantee = (item.grantee or "").upper()
        if grantee and grantee not in source_schema_set:
            roles.add(grantee)
    return roles


def remap_grantee_schema(
    grantee: str,
    schema_mapping: Optional[Dict[str, str]],
    role_names: Set[str]
) -> str:
    g_u = (grantee or "").upper()
    if not g_u:
        return g_u
    if g_u == "PUBLIC" or g_u in role_names:
        return g_u
    if schema_mapping and g_u in schema_mapping:
        return schema_mapping[g_u].upper()
    return g_u


def resolve_privilege_target(
    src_full: str,
    obj_type: str,
    full_object_mapping: FullObjectMapping,
    remap_rules: RemapRules,
    source_objects: Optional[SourceObjectMap],
    schema_mapping: Optional[Dict[str, str]],
    object_parent_map: Optional[ObjectParentMap],
    dependency_graph: Optional[DependencyGraph],
    transitive_table_cache: Optional[TransitiveTableCache],
    source_dependencies: Optional[SourceDependencySet],
    source_schema_set: Set[str],
    remap_conflicts: Optional[RemapConflictMap] = None,
    sequence_remap_policy: str = "source_only"
) -> Optional[str]:
    src_full_u = src_full.upper()
    obj_type_u = normalize_privilege_object_type(obj_type)
    if not obj_type_u:
        obj_type_u = infer_privilege_object_type(src_full_u, source_objects) or obj_type_u
    if remap_conflicts and (src_full_u, obj_type_u) in remap_conflicts:
        return None
    target = get_mapped_target(full_object_mapping, src_full_u, obj_type_u)
    if target:
        return target.upper()
    target = resolve_remap_target(
        src_full_u,
        obj_type_u or obj_type,
        remap_rules,
        source_objects=source_objects,
        schema_mapping=schema_mapping,
        object_parent_map=object_parent_map,
        dependency_graph=dependency_graph,
        transitive_table_cache=transitive_table_cache,
        source_dependencies=source_dependencies,
        remap_conflicts=None,
        sequence_remap_policy=sequence_remap_policy
    )
    if target:
        return target.upper()
    src_schema = src_full_u.split('.', 1)[0] if '.' in src_full_u else src_full_u
    if src_schema in source_schema_set:
        return None
    return src_full_u


def resolve_synonym_dependency(
    ref_full: str,
    synonym_meta: Optional[Dict[Tuple[str, str], SynonymMeta]],
    source_objects: Optional[SourceObjectMap]
) -> Tuple[str, str]:
    ref_full_u = ref_full.upper()
    if not synonym_meta or '.' not in ref_full_u:
        return ref_full_u, "SYNONYM"
    owner, name = ref_full_u.split('.', 1)
    visited: Set[Tuple[str, str]] = set()
    cur_owner = owner
    cur_name = name
    for _ in range(12):
        key = (cur_owner, cur_name)
        if key in visited:
            break
        visited.add(key)
        meta = synonym_meta.get(key)
        if not meta or not meta.table_name or not meta.table_owner:
            break
        if meta.db_link:
            break
        target_full = f"{meta.table_owner}.{meta.table_name}".upper()
        target_key = (meta.table_owner.upper(), meta.table_name.upper())
        if target_key in synonym_meta:
            cur_owner, cur_name = target_key
            continue
        obj_type = infer_privilege_object_type(target_full, source_objects) or "TABLE"
        return target_full, obj_type
    obj_type = infer_privilege_object_type(ref_full_u, source_objects) or "SYNONYM"
    return ref_full_u, obj_type


def build_dependency_pairs_for_grants(
    dependencies: List[DependencyRecord],
    full_mapping: FullObjectMapping,
    remap_rules: RemapRules,
    source_objects: Optional[SourceObjectMap],
    schema_mapping: Optional[Dict[str, str]],
    object_parent_map: Optional[ObjectParentMap],
    dependency_graph: Optional[DependencyGraph],
    transitive_table_cache: Optional[TransitiveTableCache],
    source_dependencies: Optional[SourceDependencySet],
    source_schema_set: Set[str],
    remap_conflicts: Optional[RemapConflictMap],
    synonym_meta: Optional[Dict[Tuple[str, str], SynonymMeta]],
    progress_interval: float = 10.0
) -> Set[Tuple[str, str, str, str]]:
    expected: Set[Tuple[str, str, str, str]] = set()
    if not dependencies:
        return expected

    graph: Dict[DependencyNode, Set[DependencyNode]] = defaultdict(set)
    for dep in dependencies:
        dep_full = f"{dep.owner}.{dep.name}".upper()
        dep_type = (dep.object_type or "").upper()
        ref_full = f"{dep.referenced_owner}.{dep.referenced_name}".upper()
        ref_type = (dep.referenced_type or "").upper()
        if not dep_full or not dep_type or not ref_full or not ref_type:
            continue
        graph[(dep_full, dep_type)].add((ref_full, ref_type))

    target_cache: Dict[Tuple[str, str], Optional[str]] = {}
    transitive_cache: Dict[DependencyNode, Set[DependencyNode]] = {}
    view_types = {"VIEW", "MATERIALIZED VIEW"}

    def resolve_target_cached(src_full: str, obj_type: str) -> Optional[str]:
        src_full_u = src_full.upper()
        obj_type_u = normalize_privilege_object_type(obj_type) or (obj_type or "").upper()
        key = (src_full_u, obj_type_u)
        if key in target_cache:
            return target_cache[key]
        target = resolve_privilege_target(
            src_full_u,
            obj_type_u,
            full_mapping,
            remap_rules,
            source_objects,
            schema_mapping,
            object_parent_map,
            dependency_graph,
            transitive_table_cache,
            source_dependencies,
            source_schema_set,
            remap_conflicts
        )
        target_cache[key] = target
        return target

    def add_pair(dep_full: str, dep_type: str, ref_full: str, ref_type: str) -> None:
        dep_full_u = dep_full.upper()
        dep_type_u = (dep_type or "").upper()
        if remap_conflicts and (dep_full_u, dep_type_u) in remap_conflicts:
            return
        dep_target = resolve_target_cached(dep_full_u, dep_type_u)
        if not dep_target:
            return
        ref_full_u = ref_full.upper()
        ref_type_u = (ref_type or "").upper()
        if ref_type_u == "SYNONYM":
            ref_full_u, ref_type_u = resolve_synonym_dependency(ref_full_u, synonym_meta, source_objects)
        ref_target = resolve_target_cached(ref_full_u, ref_type_u)
        if not ref_target:
            return
        if '.' not in dep_target or '.' not in ref_target:
            return
        expected.add((dep_target.upper(), dep_type_u, ref_target.upper(), ref_type_u))

    def get_transitive_refs(node: DependencyNode) -> Set[DependencyNode]:
        if node in transitive_cache:
            return transitive_cache[node]
        seen: Set[DependencyNode] = set()
        stack: List[DependencyNode] = list(graph.get(node, set()))
        while stack:
            cur = stack.pop()
            if cur in seen:
                continue
            seen.add(cur)
            for nxt in graph.get(cur, set()):
                if nxt not in seen:
                    stack.append(nxt)
        transitive_cache[node] = seen
        return seen

    total = len(dependencies)
    if total:
        log.info("[GRANT] 依赖推导授权处理中，共 %d 条依赖记录。", total)
    last_log = time.time()
    progress_interval = max(1.0, progress_interval)

    for idx, dep in enumerate(dependencies, 1):
        dep_full = f"{dep.owner}.{dep.name}".upper()
        dep_type = (dep.object_type or "").upper()
        ref_full = f"{dep.referenced_owner}.{dep.referenced_name}".upper()
        ref_type = (dep.referenced_type or "").upper()
        if not dep_full or not dep_type or not ref_full or not ref_type:
            continue
        add_pair(dep_full, dep_type, ref_full, ref_type)
        if dep_type in view_types:
            for ref_full_t, ref_type_t in get_transitive_refs((dep_full, dep_type)):
                add_pair(dep_full, dep_type, ref_full_t, ref_type_t)
        if total and (idx == total or (time.time() - last_log) >= progress_interval):
            pct = idx * 100.0 / total if total else 100.0
            log.info("[GRANT] 依赖推导进度 %d/%d (%.1f%%)。", idx, total, pct)
            last_log = time.time()

    return expected


def build_grant_plan(
    oracle_meta: OracleMetadata,
    full_mapping: FullObjectMapping,
    remap_rules: RemapRules,
    source_objects: Optional[SourceObjectMap],
    schema_mapping: Optional[Dict[str, str]],
    object_parent_map: Optional[ObjectParentMap],
    dependency_graph: Optional[DependencyGraph],
    transitive_table_cache: Optional[TransitiveTableCache],
    source_dependencies: Optional[SourceDependencySet],
    source_schema_set: Set[str],
    remap_conflicts: Optional[RemapConflictMap],
    synonym_meta: Optional[Dict[Tuple[str, str], SynonymMeta]],
    supported_sys_privs: Optional[Set[str]] = None,
    supported_object_privs: Optional[Set[str]] = None,
    oracle_sys_privs_map: Optional[Set[str]] = None,
    oracle_obj_privs_map: Optional[Set[str]] = None,
    oracle_roles: Optional[Dict[str, OracleRoleInfo]] = None,
    ob_roles: Optional[Set[str]] = None,
    ob_users: Optional[Set[str]] = None,
    include_oracle_maintained_roles: bool = False,
    dependencies: Optional[List[DependencyRecord]] = None,
    progress_interval: float = 10.0,
    sequence_remap_policy: str = "source_only"
) -> GrantPlan:
    object_grants: Dict[str, Set[ObjectGrantEntry]] = defaultdict(set)
    sys_privs: Dict[str, Set[SystemGrantEntry]] = defaultdict(set)
    role_privs: Dict[str, Set[RoleGrantEntry]] = defaultdict(set)
    role_ddls: List[str] = []
    filtered_grants: List[FilteredGrantEntry] = []

    schema_mapping = schema_mapping or {}
    role_names = build_role_name_set(oracle_meta.role_privileges, source_schema_set)
    skipped_object_privs = 0
    target_cache: Dict[Tuple[str, str], Optional[str]] = {}
    obj_type_cache: Dict[str, str] = {}
    grantee_cache: Dict[str, str] = {}
    supported_sys_privs = {p.upper() for p in (supported_sys_privs or set())}
    if not supported_object_privs:
        supported_object_privs = set(DEFAULT_SUPPORTED_OBJECT_PRIVS)
    supported_object_privs = {p.upper() for p in supported_object_privs}
    oracle_sys_privs_map = oracle_sys_privs_map or oracle_meta.system_privilege_map or set()
    oracle_obj_privs_map = oracle_obj_privs_map or oracle_meta.table_privilege_map or set()
    oracle_roles = oracle_roles or oracle_meta.role_metadata or {}
    ob_roles_input = ob_roles
    ob_users_input = ob_users
    ob_roles = {r.upper() for r in (ob_roles or set())}
    ob_users = {u.upper() for u in (ob_users or set())}
    ob_roles_loaded = ob_roles_input is not None
    ob_users_loaded = ob_users_input is not None
    role_candidates = {r.upper() for r in role_names}
    role_candidates.update({r.upper() for r in oracle_roles.keys()})
    missing_role_grantees: Set[str] = set()
    missing_user_grantees: Set[str] = set()
    skipped_missing_grants = 0

    def map_grantee(grantee: str) -> str:
        g_u = (grantee or "").upper()
        if not g_u:
            return g_u
        cached = grantee_cache.get(g_u)
        if cached is not None:
            return cached
        mapped = remap_grantee_schema(g_u, schema_mapping, role_names)
        grantee_cache[g_u] = mapped
        return mapped

    def resolve_target_cached(src_full: str, obj_type: str) -> Optional[str]:
        src_full_u = src_full.upper()
        obj_type_u = normalize_privilege_object_type(obj_type) or (obj_type or "").upper()
        key = (src_full_u, obj_type_u)
        if key in target_cache:
            return target_cache[key]
        target = resolve_privilege_target(
            src_full_u,
            obj_type_u,
            full_mapping,
            remap_rules,
            source_objects,
            schema_mapping,
            object_parent_map,
            dependency_graph,
            transitive_table_cache,
            source_dependencies,
            source_schema_set,
            remap_conflicts,
            sequence_remap_policy=sequence_remap_policy
        )
        target_cache[key] = target
        return target

    view_grant_targets: Set[str] = set()
    if oracle_meta.object_privileges:
        view_types = {"VIEW", "MATERIALIZED VIEW"}
        for item in oracle_meta.object_privileges:
            owner_u = (item.owner or "").upper()
            grantee_u = (item.grantee or "").upper()
            if not owner_u or not grantee_u or grantee_u == owner_u:
                continue
            obj_name_u = (item.object_name or "").upper()
            if not obj_name_u:
                continue
            src_full = f"{owner_u}.{obj_name_u}"
            obj_type = normalize_privilege_object_type(item.object_type)
            if not obj_type:
                obj_type = infer_privilege_object_type(src_full, source_objects) or ""
            if obj_type.upper() not in view_types:
                continue
            target = resolve_target_cached(src_full, obj_type)
            if target:
                view_grant_targets.add(target.upper())

    def record_filtered(category: str, grantee: str, privilege: str, obj_full: str, reason: str) -> None:
        filtered_grants.append(FilteredGrantEntry(
            category=category,
            grantee=grantee,
            privilege=privilege,
            object_full=obj_full,
            reason=reason
        ))

    def is_supported_sys_priv(privilege: str) -> Tuple[bool, Optional[str]]:
        priv_u = (privilege or "").upper()
        if oracle_sys_privs_map and priv_u not in oracle_sys_privs_map:
            return False, "NOT_IN_ORACLE_SYSTEM_PRIVILEGE_MAP"
        if supported_sys_privs and priv_u not in supported_sys_privs:
            return False, "UNSUPPORTED_SYS_PRIV_IN_OB"
        return True, None

    def is_supported_object_priv(privilege: str) -> Tuple[bool, Optional[str]]:
        priv_u = (privilege or "").upper()
        if oracle_obj_privs_map and priv_u not in oracle_obj_privs_map:
            return False, "NOT_IN_ORACLE_OBJECT_PRIVILEGE_MAP"
        if supported_object_privs and priv_u not in supported_object_privs:
            return False, "UNSUPPORTED_OBJECT_PRIV_IN_OB"
        return True, None

    def add_object_grant_entry(grantee: str, privilege: str, object_full: str, grantable: bool) -> None:
        grantee_u = (grantee or "").upper()
        object_u = (object_full or "").upper()
        priv_u = (privilege or "").upper()
        if not grantee_u or not object_u or not priv_u:
            return
        if grantable:
            object_grants[grantee_u].discard(ObjectGrantEntry(priv_u, object_u, False))
            object_grants[grantee_u].add(ObjectGrantEntry(priv_u, object_u, True))
            return
        if ObjectGrantEntry(priv_u, object_u, True) in object_grants[grantee_u]:
            return
        object_grants[grantee_u].add(ObjectGrantEntry(priv_u, object_u, False))

    def format_grantee_list(items: Set[str], limit: int = 30) -> str:
        if not items:
            return ""
        sorted_items = sorted(items)
        if len(sorted_items) <= limit:
            return ", ".join(sorted_items)
        return ", ".join(sorted_items[:limit]) + f"...(+{len(sorted_items) - limit})"

    def grantee_exists(grantee: str) -> bool:
        g_u = (grantee or "").upper()
        if not g_u:
            return False
        if g_u == "PUBLIC":
            return True
        if not ob_roles_loaded and not ob_users_loaded:
            return True
        if ob_users_loaded and g_u in ob_users:
            return True
        if ob_roles_loaded and g_u in ob_roles:
            return True
        is_role = g_u in role_candidates
        if is_role:
            if ob_roles_loaded:
                missing_role_grantees.add(g_u)
                return False
            return True
        if ob_users_loaded:
            missing_user_grantees.add(g_u)
            return False
        return True

    # 1) Source object grants (DBA_TAB_PRIVS)
    total_obj = len(oracle_meta.object_privileges)
    if total_obj:
        log.info("[GRANT] 正在处理对象权限 %d 条...", total_obj)
        if total_obj >= GRANT_WARN_THRESHOLD:
            log.warning(
                "[GRANT] 检测到对象授权规模较大（%d 条），授权规划可能耗时较久，请耐心等待。",
                total_obj
            )
    last_log = time.time()
    progress_interval = max(1.0, progress_interval)

    for idx, item in enumerate(oracle_meta.object_privileges, 1):
        grantee_u = map_grantee(item.grantee)
        if not grantee_exists(grantee_u):
            skipped_missing_grants += 1
            continue
        src_full = f"{item.owner}.{item.object_name}".upper()
        priv_u = (item.privilege or "").upper()
        ok, reason = is_supported_object_priv(priv_u)
        if not ok:
            record_filtered("OBJECT", grantee_u, priv_u, src_full, reason or "UNSUPPORTED_OBJECT_PRIV")
            if total_obj and (idx == total_obj or (time.time() - last_log) >= progress_interval):
                pct = idx * 100.0 / total_obj if total_obj else 100.0
                log.info("[GRANT] 对象权限进度 %d/%d (%.1f%%)。", idx, total_obj, pct)
                last_log = time.time()
            continue
        obj_type = normalize_privilege_object_type(item.object_type)
        if not obj_type:
            obj_type = obj_type_cache.get(src_full, "")
            if not obj_type:
                obj_type = infer_privilege_object_type(src_full, source_objects) or ""
                obj_type_cache[src_full] = obj_type
        target = resolve_target_cached(src_full, obj_type)
        if not target:
            skipped_object_privs += 1
            continue
        add_object_grant_entry(grantee_u, priv_u, target.upper(), bool(item.grantable))
        if total_obj and (idx == total_obj or (time.time() - last_log) >= progress_interval):
            pct = idx * 100.0 / total_obj if total_obj else 100.0
            log.info("[GRANT] 对象权限进度 %d/%d (%.1f%%)。", idx, total_obj, pct)
            last_log = time.time()

    if skipped_object_privs:
        log.warning("[GRANT] 已跳过 %d 条无法映射的对象授权。", skipped_object_privs)

    # 2) System and role grants
    if oracle_meta.sys_privileges:
        log.info("[GRANT] 正在处理系统权限 %d 条...", len(oracle_meta.sys_privileges))
    for item in oracle_meta.sys_privileges:
        grantee_u = map_grantee(item.grantee)
        if not grantee_u:
            continue
        if not grantee_exists(grantee_u):
            skipped_missing_grants += 1
            continue
        priv_u = (item.privilege or "").upper()
        ok, reason = is_supported_sys_priv(priv_u)
        if not ok:
            record_filtered("SYSTEM", grantee_u, priv_u, "", reason or "UNSUPPORTED_SYS_PRIV")
            continue
        sys_privs[grantee_u].add(SystemGrantEntry(priv_u, bool(item.admin_option)))

    if oracle_meta.role_privileges:
        log.info("[GRANT] 正在处理角色授权 %d 条...", len(oracle_meta.role_privileges))
    for item in oracle_meta.role_privileges:
        grantee_u = map_grantee(item.grantee)
        if not grantee_u or not item.role:
            continue
        if not grantee_exists(grantee_u):
            skipped_missing_grants += 1
            continue
        role_privs[grantee_u].add(RoleGrantEntry(item.role.upper(), bool(item.admin_option)))

    # 3) Dependency-derived grants
    dep_records = dependencies or []
    if dep_records:
        expected_pairs = build_dependency_pairs_for_grants(
            dep_records,
            full_mapping,
            remap_rules,
            source_objects,
            schema_mapping,
            object_parent_map,
            dependency_graph,
            transitive_table_cache,
            source_dependencies,
            source_schema_set,
            remap_conflicts,
            synonym_meta,
            progress_interval=progress_interval
        )
        for dep_full, dep_type, ref_full, ref_type in expected_pairs:
            if '.' not in dep_full or '.' not in ref_full:
                continue
            dep_schema = dep_full.split('.', 1)[0]
            ref_schema = ref_full.split('.', 1)[0]
            if dep_schema.upper() == "PUBLIC":
                # PUBLIC 同义词不要求 PUBLIC 授权，仅保留源端显式 PUBLIC 授权
                continue
            if dep_schema == ref_schema:
                continue
            if not grantee_exists(dep_schema):
                skipped_missing_grants += 1
                continue
            grantable_for_view = dep_full in view_grant_targets and dep_type.upper() in {"VIEW", "MATERIALIZED VIEW"}
            privilege = GRANT_PRIVILEGE_BY_TYPE.get(ref_type.upper())
            if privilege:
                ok, reason = is_supported_object_priv(privilege)
                if ok:
                    add_object_grant_entry(dep_schema, privilege, ref_full.upper(), grantable_for_view)
                else:
                    record_filtered("OBJECT", dep_schema, privilege, ref_full.upper(), reason or "UNSUPPORTED_OBJECT_PRIV")
            if ref_type.upper() == "TABLE" and dep_type.upper() == "TABLE":
                ok, reason = is_supported_object_priv("REFERENCES")
                if ok:
                    add_object_grant_entry(dep_schema, "REFERENCES", ref_full.upper(), False)
                else:
                    record_filtered("OBJECT", dep_schema, "REFERENCES", ref_full.upper(), reason or "UNSUPPORTED_OBJECT_PRIV")

    if missing_user_grantees:
        log.warning(
            "[GRANT] 目标端缺失用户 %d 个，已跳过相关授权: %s",
            len(missing_user_grantees),
            format_grantee_list(missing_user_grantees)
        )
    if missing_role_grantees:
        log.warning(
            "[GRANT] 目标端缺失角色 %d 个，已跳过相关授权: %s",
            len(missing_role_grantees),
            format_grantee_list(missing_role_grantees)
        )
    if skipped_missing_grants and (missing_user_grantees or missing_role_grantees):
        log.warning(
            "[GRANT] 共跳过 %d 条授权，请先在目标端创建对应用户/角色后重新生成。",
            skipped_missing_grants
        )

    # 4) Role DDL (user-defined roles referenced by grants)
    referenced_roles: Set[str] = set()
    known_roles: Set[str] = {r.upper() for r in oracle_roles.keys()}
    referenced_roles.update({item.role.upper() for item in oracle_meta.role_privileges if item.role})
    if not known_roles:
        known_roles = set(referenced_roles)
    if known_roles:
        for item in oracle_meta.role_privileges:
            grantee = (item.grantee or "").upper()
            if grantee in known_roles:
                referenced_roles.add(grantee)
        for item in oracle_meta.sys_privileges:
            grantee = (item.grantee or "").upper()
            if grantee in known_roles:
                referenced_roles.add(grantee)
        for item in oracle_meta.object_privileges:
            grantee = (item.grantee or "").upper()
            if grantee in known_roles:
                referenced_roles.add(grantee)

    skipped_existing: Set[str] = set()
    skipped_oracle_maintained: Set[str] = set()
    role_warnings: List[str] = []
    for role in sorted(referenced_roles):
        role_u = (role or "").upper()
        if not role_u or role_u == "PUBLIC":
            continue
        info = oracle_roles.get(role_u)
        if info and info.oracle_maintained and not include_oracle_maintained_roles:
            skipped_oracle_maintained.add(role_u)
            continue
        if role_u in ob_roles:
            skipped_existing.add(role_u)
            continue
        ddl_lines: List[str] = []
        if info:
            meta = [
                f"AUTHENTICATION_TYPE={info.authentication_type or '-'}",
                f"PASSWORD_REQUIRED={'YES' if info.password_required else 'NO'}"
            ]
            if info.oracle_maintained is not None:
                meta.append(f"ORACLE_MAINTAINED={'Y' if info.oracle_maintained else 'N'}")
            ddl_lines.append(f"-- ROLE: {role_u} ({', '.join(meta)})")
            if info.authentication_type in ROLE_AUTH_WARN_TYPES or info.password_required:
                role_warnings.append(role_u)
                ddl_lines.append("-- NOTE: 该角色需要密码/外部认证，已生成 NOT IDENTIFIED，需人工补充。")
        else:
            ddl_lines.append(f"-- ROLE: {role_u} (metadata unavailable)")
        ddl_lines.append(f"CREATE ROLE {role_u};")
        role_ddls.extend(ddl_lines)

    if role_ddls:
        log.info("[GRANT] 需生成角色 DDL %d 条。", sum(1 for line in role_ddls if line.startswith("CREATE ROLE ")))
    if skipped_existing:
        log.info("[GRANT] 已存在于 OB 的角色（跳过创建）: %s", ", ".join(sorted(skipped_existing)))
    if skipped_oracle_maintained:
        log.info("[GRANT] Oracle 维护角色已跳过创建: %s", ", ".join(sorted(skipped_oracle_maintained)))
    if role_warnings:
        log.warning("[GRANT] 以下角色需要密码/外部认证，请人工处理: %s", ", ".join(sorted(set(role_warnings))))

    return GrantPlan(
        object_grants=object_grants,
        sys_privs=sys_privs,
        role_privs=role_privs,
        role_ddls=role_ddls,
        filtered_grants=filtered_grants,
        view_grant_targets=view_grant_targets
    )


# ====================== TABLE / VIEW / 其他主对象校验 ======================

def check_primary_objects(
    master_list: MasterCheckList,
    extraneous_rules: List[str],
    ob_meta: ObMetadata,
    oracle_meta: OracleMetadata,
    enabled_primary_types: Optional[Set[str]] = None,
    print_only_types: Optional[Set[str]] = None,
    print_only_reasons: Optional[Dict[str, str]] = None,
    settings: Optional[Dict] = None
) -> ReportResults:
    """
    核心主对象校验：
      - TABLE: 存在性 + 列名集合 (忽略 OMS_OBJECT_NUMBER/OMS_RELATIVE_FNO/OMS_BLOCK_NUMBER/OMS_ROW_NUMBER)
      - LONG/LONG RAW 列要求目标端类型为 CLOB/BLOB
      - VIEW / PROCEDURE / FUNCTION / SYNONYM: 只校验存在性
      - MATERIALIZED VIEW: 可由版本门控/配置切换为仅打印或正常校验
      - PACKAGE / PACKAGE BODY: 单独走包有效性对比，不在这里处理
    """
    results: ReportResults = {
        "missing": [],
        "mismatched": [],
        "ok": [],
        "skipped": [],
        "column_order_mismatched": [],
        "column_order_skipped": [],
        "extraneous": extraneous_rules,
        "extra_targets": []
    }
    column_order_mismatched: List[ColumnOrderMismatch] = results["column_order_mismatched"]
    column_order_skipped: List[Tuple[str, str]] = results["column_order_skipped"]

    visibility_policy = normalize_column_visibility_policy(
        (settings or {}).get("column_visibility_policy", "auto")
    )
    visibility_enabled = False
    if visibility_policy == "enforce":
        visibility_enabled = True
        if not oracle_meta.invisible_column_supported:
            log.info("[CHECK] 源端未提供 INVISIBLE_COLUMN 元数据，列可见性校验可能不完整。")
        if not ob_meta.invisible_column_supported:
            log.info("[CHECK] 目标端未提供 INVISIBLE_COLUMN 元数据，列可见性校验可能不完整。")
    elif visibility_policy == "auto":
        if oracle_meta.invisible_column_supported and ob_meta.invisible_column_supported:
            visibility_enabled = True
        else:
            log.info("[CHECK] 列可见性校验已跳过 (column_visibility_policy=auto 且元数据不完整)。")

    identity_enabled = bool(getattr(oracle_meta, "identity_column_supported", False)) and bool(
        getattr(ob_meta, "identity_column_supported", False)
    )
    if not identity_enabled:
        log.info("[CHECK] IDENTITY 列校验已跳过 (IDENTITY_COLUMN 元数据不完整)。")

    default_on_null_enabled = bool(getattr(oracle_meta, "default_on_null_supported", False)) and bool(
        getattr(ob_meta, "default_on_null_supported", False)
    )
    if not default_on_null_enabled:
        log.info("[CHECK] DEFAULT ON NULL 校验已跳过 (DEFAULT_ON_NULL 元数据不完整)。")

    column_order_enabled = bool((settings or {}).get("enable_column_order_check", False))

    if not master_list:
        log.info("主校验清单为空，没有需要校验的对象。")
        return results

    log_subsection("主对象校验 (TABLE/VIEW/PROC/FUNC/SYNONYM + PACKAGE 有效性)")

    allowed_types = enabled_primary_types or set(PRIMARY_OBJECT_TYPES)
    print_only_types_u = {t.upper() for t in (print_only_types or set())}
    print_only_reasons_u = {
        str(k).upper(): str(v)
        for k, v in (print_only_reasons or PRINT_ONLY_PRIMARY_REASONS).items()
    }
    package_types_u = {t.upper() for t in PACKAGE_OBJECT_TYPES}
    primary_existence_only_types = {
        t.upper()
        for t in allowed_types
        if t.upper() != "TABLE" and t.upper() not in package_types_u and t.upper() not in print_only_types_u
    }

    total = len(master_list)
    expected_targets: Dict[str, Set[str]] = defaultdict(set)
    for i, (src_name, tgt_name, obj_type) in enumerate(master_list):

        if (i + 1) % 100 == 0:
            pct = (i + 1) * 100.0 / total if total else 100.0
            log.info("主对象校验进度 %d/%d (%.1f%%)...", i + 1, total, pct)

        obj_type_u = obj_type.upper()
        try:
            src_schema, src_obj = src_name.split('.')
            tgt_schema, tgt_obj = tgt_name.split('.')
        except ValueError:
            log.warning(f"  [跳过] 对象名格式不正确: src='{src_name}', tgt='{tgt_name}'")
            continue

        src_schema_u = src_schema.upper()
        src_obj_u = src_obj.upper()
        tgt_schema_u = tgt_schema.upper()
        tgt_obj_u = tgt_obj.upper()
        full_tgt = f"{tgt_schema_u}.{tgt_obj_u}"
        if obj_type_u not in allowed_types:
            continue

        if obj_type_u in PACKAGE_OBJECT_TYPES:
            # PACKAGE/PKG BODY 使用独立有效性对比逻辑
            continue

        expected_targets[obj_type_u].add(full_tgt)

        if obj_type_u in print_only_types_u:
            reason = print_only_reasons_u.get(obj_type_u, "仅打印不校验")
            results['skipped'].append((obj_type_u, full_tgt, src_name, reason))
            continue

        if obj_type_u == 'TABLE':
            # 1) OB 是否存在 TABLE
            ob_tables = ob_meta.objects_by_type.get('TABLE', set())
            if full_tgt not in ob_tables:
                results['missing'].append(('TABLE', full_tgt, src_name))
                continue

            # 2) 列级别详细对比 (VARCHAR/VARCHAR2 需 >= 源端长度 * 1.5 向上取整)
            src_cols_details = oracle_meta.table_columns.get((src_schema_u, src_obj_u))
            tgt_cols_details = ob_meta.tab_columns.get((tgt_schema_u, tgt_obj_u), {})

            if src_cols_details is None:
                results['mismatched'].append((
                    'TABLE',
                    f"{full_tgt} (源端列信息获取失败)",
                    set(),
                    set(),
                    [],
                    []
                ))
                continue

            hidden_src_cols = {col for col, meta in src_cols_details.items() if meta.get("hidden")}
            invisible_src_cols = {col for col, meta in src_cols_details.items() if meta.get("invisible") is True}
            src_col_names = {
                col for col, meta in src_cols_details.items()
                if not is_ignored_source_column(col, meta)
            }
            tgt_col_names = {
                col for col, meta in tgt_cols_details.items()
                if not is_ignored_oms_column(col, meta) and col not in hidden_src_cols
            }

            missing_in_tgt = src_col_names - tgt_col_names
            extra_in_tgt = tgt_col_names - src_col_names
            length_mismatches: List[ColumnLengthIssue] = []
            type_mismatches: List[ColumnTypeIssue] = []

            # 显式提示被忽略名单外的 OMS_* 列属于“多余列”
            extra_oms = {c for c in extra_in_tgt if c.upper().startswith("OMS_")}
            if extra_oms:
                log.debug("表 %s 发现额外 OMS_* 列: %s", full_tgt, sorted(extra_oms))

            # 检查公共列的长度
            common_cols = src_col_names & tgt_col_names
            for col_name in common_cols:
                src_info = src_cols_details[col_name]
                tgt_info = tgt_cols_details[col_name]

                src_dtype = (src_info.get("data_type") or "").upper()
                tgt_dtype = (tgt_info.get("data_type") or "").upper()

                src_virtual = bool(src_info.get("virtual"))
                tgt_virtual = bool(tgt_info.get("virtual"))
                if src_virtual:
                    src_expr_norm = normalize_sql_expression(src_info.get("virtual_expr"))
                    tgt_expr_norm = normalize_sql_expression(tgt_info.get("virtual_expr"))
                    if not tgt_virtual:
                        type_mismatches.append(
                            ColumnTypeIssue(
                                col_name,
                                format_oracle_column_type(src_info, prefer_ob_varchar=True),
                                format_oracle_column_type(tgt_info, prefer_ob_varchar=True),
                                f"VIRTUAL({src_expr_norm})" if src_expr_norm else "VIRTUAL",
                                "virtual_missing"
                            )
                        )
                        continue
                    if src_expr_norm and tgt_expr_norm and src_expr_norm != tgt_expr_norm:
                        type_mismatches.append(
                            ColumnTypeIssue(
                                col_name,
                                format_oracle_column_type(src_info, prefer_ob_varchar=True),
                                format_oracle_column_type(tgt_info, prefer_ob_varchar=True),
                                f"VIRTUAL({src_expr_norm})",
                                "virtual_expr_mismatch"
                            )
                        )
                        continue
                    if src_expr_norm and not tgt_expr_norm:
                        type_mismatches.append(
                            ColumnTypeIssue(
                                col_name,
                                format_oracle_column_type(src_info, prefer_ob_varchar=True),
                                format_oracle_column_type(tgt_info, prefer_ob_varchar=True),
                                f"VIRTUAL({src_expr_norm})",
                                "virtual_expr_mismatch"
                            )
                        )
                        continue
                    # 虚拟列表达式一致后，不再参与长度/类型等常规校验
                    continue

                def _is_true_flag(value: Optional[object]) -> bool:
                    if value is True:
                        return True
                    if value is False or value is None:
                        return False
                    return str(value).strip().upper() in ("YES", "Y", "TRUE", "1")

                if identity_enabled:
                    src_identity = _is_true_flag(src_info.get("identity"))
                    tgt_identity = _is_true_flag(tgt_info.get("identity"))
                    if src_identity and not tgt_identity:
                        type_mismatches.append(
                            ColumnTypeIssue(
                                col_name,
                                format_oracle_column_type(src_info, prefer_ob_varchar=True),
                                format_oracle_column_type(tgt_info, prefer_ob_varchar=True),
                                "IDENTITY",
                                "identity_missing"
                            )
                        )

                if default_on_null_enabled:
                    src_default_on_null = _is_true_flag(src_info.get("default_on_null"))
                    tgt_default_on_null = _is_true_flag(tgt_info.get("default_on_null"))
                    if src_default_on_null and not tgt_default_on_null:
                        type_mismatches.append(
                            ColumnTypeIssue(
                                col_name,
                                format_oracle_column_type(src_info, prefer_ob_varchar=True),
                                format_oracle_column_type(tgt_info, prefer_ob_varchar=True),
                                "DEFAULT ON NULL",
                                "default_on_null_missing"
                            )
                        )

                if visibility_enabled:
                    src_invisible = src_info.get("invisible")
                    tgt_invisible = tgt_info.get("invisible")
                    if src_invisible is True and tgt_invisible is not True:
                        type_mismatches.append(
                            ColumnTypeIssue(
                                col_name,
                                "INVISIBLE",
                                "VISIBLE" if tgt_invisible is False else "UNKNOWN",
                                "INVISIBLE",
                                "visibility_mismatch"
                            )
                        )
                    elif src_invisible is False and tgt_invisible is True:
                        type_mismatches.append(
                            ColumnTypeIssue(
                                col_name,
                                "VISIBLE",
                                "INVISIBLE",
                                "VISIBLE",
                                "visibility_mismatch"
                            )
                        )

                if is_long_type(src_dtype):
                    expected_type = map_long_type_to_ob(src_dtype)
                    if (tgt_dtype or "UNKNOWN") != expected_type:
                        type_mismatches.append(
                            ColumnTypeIssue(
                                col_name,
                                src_dtype or "UNKNOWN",
                                tgt_dtype or "UNKNOWN",
                                expected_type,
                                "long_type"
                            )
                        )
                    continue

                if src_dtype in ('VARCHAR2', 'VARCHAR'):
                    src_len = src_info.get("char_length") or src_info.get("data_length")
                    tgt_len = tgt_info.get("char_length") or tgt_info.get("data_length")

                    try:
                        src_len_int = int(src_len)
                        tgt_len_int = int(tgt_len)
                    except (TypeError, ValueError):
                        continue

                    # 区分BYTE和CHAR语义：CHAR_USED='C'表示CHAR语义
                    src_char_used = (src_info.get("char_used") or "").strip().upper()
                    tgt_char_used = (tgt_info.get("char_used") or "").strip().upper()
                    if not tgt_char_used:
                        tgt_char_used = "B"
                    
                    if src_char_used == 'C':
                        # CHAR语义：要求长度完全一致
                        if tgt_char_used != 'C' or tgt_len_int != src_len_int:
                            length_mismatches.append(
                                ColumnLengthIssue(col_name, src_len_int, tgt_len_int, src_len_int, 'char_mismatch')
                            )
                    else:
                        # BYTE语义：需要放大1.5倍
                        expected_min_len = int(math.ceil(src_len_int * VARCHAR_LEN_MIN_MULTIPLIER))
                        oversize_cap_len = int(math.ceil(src_len_int * VARCHAR_LEN_OVERSIZE_MULTIPLIER))
                        if tgt_len_int < expected_min_len:
                            length_mismatches.append(
                                ColumnLengthIssue(col_name, src_len_int, tgt_len_int, expected_min_len, 'short')
                            )
                        elif tgt_len_int > oversize_cap_len:
                            length_mismatches.append(
                                ColumnLengthIssue(col_name, src_len_int, tgt_len_int, oversize_cap_len, 'oversize')
                            )
                    continue

                if src_dtype in ("NUMBER", "DECIMAL", "NUMERIC"):
                    src_prec, src_scale = normalize_number_meta(
                        src_info.get("data_precision"),
                        src_info.get("data_scale")
                    )
                    tgt_prec, tgt_scale = normalize_number_meta(
                        tgt_info.get("data_precision"),
                        tgt_info.get("data_scale")
                    )
                    if not is_number_equivalent(src_prec, src_scale, tgt_prec, tgt_scale):
                        expected_type = build_number_fixup_type(src_info, tgt_info)
                        type_mismatches.append(
                            ColumnTypeIssue(
                                col_name,
                                format_oracle_column_type(src_info, prefer_ob_varchar=True),
                                format_oracle_column_type(tgt_info, prefer_ob_varchar=True),
                                expected_type,
                                "number_precision"
                            )
                        )

            if column_order_enabled:
                order_src_names = {
                    col for col in src_col_names
                    if is_column_order_candidate(col, src_cols_details.get(col), is_source=True)
                }
                order_tgt_names = {
                    col for col in tgt_col_names
                    if is_column_order_candidate(
                        col,
                        tgt_cols_details.get(col),
                        is_source=False,
                        hidden_src_cols=hidden_src_cols,
                        invisible_src_cols=invisible_src_cols
                    )
                }
                if order_src_names != order_tgt_names:
                    column_order_skipped.append((full_tgt, "column_set_mismatch"))
                else:
                    src_order, src_reason = build_column_order_sequence(src_cols_details, order_src_names)
                    tgt_order, tgt_reason = build_column_order_sequence(tgt_cols_details, order_tgt_names)
                    skip_reason = src_reason or tgt_reason
                    if skip_reason:
                        column_order_skipped.append((full_tgt, skip_reason))
                    elif src_order != tgt_order:
                        column_order_mismatched.append(ColumnOrderMismatch(
                            table=full_tgt,
                            src_order=src_order or tuple(),
                            tgt_order=tgt_order or tuple()
                        ))

            if not missing_in_tgt and not extra_in_tgt and not length_mismatches and not type_mismatches:
                results['ok'].append(('TABLE', full_tgt))
            else:
                results['mismatched'].append((
                    'TABLE',
                    full_tgt,
                    missing_in_tgt,
                    extra_in_tgt,
                    length_mismatches,
                    type_mismatches
                ))

        elif obj_type_u in primary_existence_only_types:
            ob_set = ob_meta.objects_by_type.get(obj_type_u, set())
            if full_tgt in ob_set:
                results['ok'].append((obj_type_u, full_tgt))
            else:
                results['missing'].append((obj_type_u, full_tgt, src_name))

        else:
            # 不在主对比范围的类型直接忽略
            continue

    # 记录目标端多出的对象（任何受管类型；print-only 类型也纳入 extra 统计）
    for obj_type in sorted(allowed_types - set(PACKAGE_OBJECT_TYPES)):
        actual = ob_meta.objects_by_type.get(obj_type, set())
        expected = expected_targets.get(obj_type, set())
        extras = sorted(actual - expected)
        for tgt in extras:
            results['extra_targets'].append((obj_type, tgt))

    return results


def supplement_missing_views_from_mapping(
    tv_results: ReportResults,
    full_object_mapping: FullObjectMapping,
    ob_meta: ObMetadata,
    enabled_primary_types: Optional[Set[str]] = None
) -> int:
    """
    当主对象校验未能产出 VIEW 缺失清单时，基于映射+目标端对象集补齐。
    用于保障 fixup/report 能进入 VIEW 生成流程。
    """
    enabled_types = {t.upper() for t in (enabled_primary_types or set(PRIMARY_OBJECT_TYPES))}
    if 'VIEW' not in enabled_types:
        return 0

    missing_list = tv_results.get("missing", [])
    existing_missing = {
        (src_name.upper(), tgt_name.upper())
        for obj_type, tgt_name, src_name in missing_list
        if (obj_type or "").upper() == "VIEW"
    }

    ob_views = {name.upper() for name in ob_meta.objects_by_type.get("VIEW", set())}
    expected_pairs: List[Tuple[str, str]] = []
    for src_full, type_map in full_object_mapping.items():
        tgt_full = type_map.get("VIEW")
        if not tgt_full or "." not in tgt_full or "." not in src_full:
            continue
        expected_pairs.append((src_full.upper(), tgt_full.upper()))

    added = 0
    for src_full, tgt_full in expected_pairs:
        if tgt_full in ob_views:
            continue
        if (src_full, tgt_full) in existing_missing:
            continue
        missing_list.append(("VIEW", tgt_full, src_full))
        existing_missing.add((src_full, tgt_full))
        added += 1

    if added:
        log.warning("[VIEW] 缺失视图清单补齐 %d 条（基于映射与目标端对象集）。", added)
    return added


# ====================== 扩展：索引 / 约束 / 序列 / 触发器 ======================

def normalize_object_status(status: Optional[str]) -> str:
    if status is None:
        return "UNKNOWN"
    status_u = str(status).strip().upper()
    return status_u if status_u else "UNKNOWN"


def normalize_error_text(text: Optional[str]) -> str:
    if text is None:
        return ""
    cleaned = str(text).replace("\r", " ").replace("\n", " ").replace("\t", " ")
    return " ".join(cleaned.split())


def compare_package_objects(
    master_list: MasterCheckList,
    oracle_meta: OracleMetadata,
    ob_meta: ObMetadata,
    enabled_primary_types: Optional[Set[str]] = None
) -> PackageCompareResults:
    """
    对 PACKAGE / PACKAGE BODY 做存在性 + VALID/INVALID 对比，并记录错误信息（若可用）。
    SOURCE_INVALID 不计入 mismatch 统计，但会列出详情。
    """
    enabled_types = {t.upper() for t in (enabled_primary_types or set(PRIMARY_OBJECT_TYPES))}
    if not (set(PACKAGE_OBJECT_TYPES) & enabled_types):
        return {"rows": [], "summary": {}}

    rows: List[PackageCompareRow] = []
    summary: Dict[str, int] = defaultdict(int)
    ob_pkg_set = ob_meta.objects_by_type.get("PACKAGE", set())
    ob_pkg_body_set = ob_meta.objects_by_type.get("PACKAGE BODY", set())

    for src_name, tgt_name, obj_type in master_list:
        obj_type_u = obj_type.upper()
        if obj_type_u not in PACKAGE_OBJECT_TYPES:
            continue
        if "." not in src_name or "." not in tgt_name:
            continue
        src_schema, src_obj = src_name.split(".", 1)
        tgt_schema, tgt_obj = tgt_name.split(".", 1)
        src_schema_u = src_schema.upper()
        src_obj_u = src_obj.upper()
        tgt_schema_u = tgt_schema.upper()
        tgt_obj_u = tgt_obj.upper()
        src_full = f"{src_schema_u}.{src_obj_u}"
        tgt_full = f"{tgt_schema_u}.{tgt_obj_u}"

        src_status = normalize_object_status(
            oracle_meta.object_statuses.get((src_schema_u, src_obj_u, obj_type_u))
        )
        if obj_type_u == "PACKAGE":
            tgt_exists = tgt_full in ob_pkg_set
        else:
            tgt_exists = tgt_full in ob_pkg_body_set
        tgt_status = "MISSING"
        if tgt_exists:
            tgt_status = normalize_object_status(
                ob_meta.object_statuses.get((tgt_schema_u, tgt_obj_u, obj_type_u))
            )

        error_count = 0
        first_error = ""

        if src_status == "INVALID":
            result = "SOURCE_INVALID"
            err = oracle_meta.package_errors.get((src_schema_u, src_obj_u, obj_type_u))
            if err:
                error_count = err.count
                first_error = err.first_error
        elif not tgt_exists:
            result = "MISSING_TARGET"
        else:
            if src_status == "VALID" and tgt_status == "VALID":
                result = "OK"
            elif tgt_status == "INVALID":
                result = "TARGET_INVALID"
                err = ob_meta.package_errors.get((tgt_schema_u, tgt_obj_u, obj_type_u))
                if err:
                    error_count = err.count
                    first_error = err.first_error
            else:
                result = "STATUS_MISMATCH"

        summary[result] += 1
        rows.append(PackageCompareRow(
            src_full=src_full,
            obj_type=obj_type_u,
            src_status=src_status,
            tgt_full=tgt_full,
            tgt_status=tgt_status,
            result=result,
            error_count=error_count,
            first_error=first_error
        ))

    return {
        "rows": rows,
        "summary": dict(summary),
        "diff_rows": [row for row in rows if row.result != "OK"]
    }


def is_ob_gtt_internal_index_name(index_name: Optional[str]) -> bool:
    name_u = (index_name or "").strip().upper()
    return bool(name_u) and name_u.startswith("IDX_FOR_HEAP_GTT_")


def should_normalize_ob_gtt_indexes(
    entries: Optional[Dict[str, Dict]],
    constraints: Optional[Dict[str, Dict]] = None
) -> bool:
    """
    判定是否启用 OB GTT 索引归一化（剥离 SYS_SESSION_ID 前缀列）。

    规则：
    1) 命中 IDX_FOR_HEAP_GTT_* 内部索引名；
    2) 或者 P/U 约束背靠索引首列为 SYS_SESSION_ID（部分版本无内部索引名也会出现）。
    """
    idx_entries = entries or {}
    if any(is_ob_gtt_internal_index_name(name) for name in idx_entries.keys()):
        return True
    cons_map = constraints or {}
    if not idx_entries or not cons_map:
        return False
    for cons in cons_map.values():
        ctype = (cons.get("type") or "").upper()
        if ctype not in {"P", "U"}:
            continue
        idx_name = (cons.get("index_name") or "").upper()
        if not idx_name:
            continue
        idx_info = idx_entries.get(idx_name)
        if not idx_info:
            continue
        cols = [str(c).upper() for c in (idx_info.get("columns") or []) if c]
        if cols and cols[0] == "SYS_SESSION_ID":
            return True
    return False


def normalize_ob_gtt_index_columns(
    cols: Tuple[str, ...],
    *,
    normalize_ob_gtt: bool
) -> Tuple[str, ...]:
    if not normalize_ob_gtt:
        return cols
    if cols and cols[0] == "SYS_SESSION_ID":
        return cols[1:]
    return cols


def build_index_map(
    entries: Dict[str, Dict],
    *,
    normalize_ob_gtt: bool = False
) -> Dict[Tuple[str, ...], Dict[str, Set[str]]]:
    result: Dict[Tuple[str, ...], Dict[str, Set[str]]] = {}
    for name, info in (entries or {}).items():
        name_u = (name or "").upper()
        if normalize_ob_gtt and is_ob_gtt_internal_index_name(name_u):
            # OB GTT internal helper index should not participate in semantic diff.
            continue
        expr_map = info.get("expressions") or {}
        cols = normalize_index_columns(info.get("columns") or [], expr_map)
        cols = normalize_ob_gtt_index_columns(cols, normalize_ob_gtt=normalize_ob_gtt)
        if not cols:
            continue
        uniq = (info.get("uniqueness") or "").upper()
        bucket = result.setdefault(cols, {"names": set(), "uniq": set()})
        if name_u:
            bucket["names"].add(name_u)
        bucket["uniq"].add(uniq)
    return result


def build_index_signature(
    index_map: Dict[Tuple[str, ...], Dict[str, Set[str]]]
) -> Dict[Tuple[str, ...], Set[str]]:
    return {
        cols: set(info.get("uniq") or set())
        for cols, info in (index_map or {}).items()
    }


def build_constraint_index_cols(
    tgt_constraints: Optional[Dict[str, Dict]],
    tgt_indexes: Optional[Dict[str, Dict]],
    *,
    normalize_ob_gtt: bool = False
) -> Set[Tuple[str, ...]]:
    cols_set: Set[Tuple[str, ...]] = set()
    for cons in (tgt_constraints or {}).values():
        ctype = (cons.get("type") or "").upper()
        if ctype not in ("P", "U"):
            continue
        idx_name = (cons.get("index_name") or "").upper()
        if idx_name:
            idx_info = (tgt_indexes or {}).get(idx_name)
            if idx_info:
                expr_map = idx_info.get("expressions") or {}
                cols = normalize_index_columns(idx_info.get("columns") or [], expr_map)
                cols = normalize_ob_gtt_index_columns(cols, normalize_ob_gtt=normalize_ob_gtt)
                if cols:
                    cols_set.add(cols)
                    continue
        cols = normalize_column_sequence(cons.get("columns"))
        if cols:
            cols_set.add(cols)
    return cols_set


def build_constraint_signature(
    cons_dict: Dict[str, Dict],
    norm_cols: Dict[str, Tuple[str, ...]],
    full_object_mapping: FullObjectMapping,
    *,
    is_source: bool,
    include_deferrable: bool = True
) -> ConstraintSignature:
    pk: Set[Tuple[str, ...]] = set()
    uk: Set[Tuple[str, ...]] = set()
    fk: Set[Tuple[Tuple[str, ...], Optional[str], str, str]] = set()
    ck: Set[str] = set()
    for name, cons in (cons_dict or {}).items():
        ctype = (cons.get("type") or "").upper()
        cols = norm_cols.get(name) or normalize_column_sequence(cons.get("columns"))
        if ctype == "P":
            pk.add(cols)
        elif ctype == "U":
            uk.add(cols)
        elif ctype == "R":
            ref_full: Optional[str] = None
            ref_owner = cons.get("ref_table_owner")
            ref_name = cons.get("ref_table_name")
            delete_rule = normalize_delete_rule(cons.get("delete_rule"))
            update_rule = normalize_update_rule(cons.get("update_rule"))
            if ref_owner and ref_name:
                ref_full_raw = f"{str(ref_owner).upper()}.{str(ref_name).upper()}"
                if is_source:
                    mapped = get_mapped_target(full_object_mapping, ref_full_raw, 'TABLE')
                    ref_full = (mapped or ref_full_raw).upper()
                else:
                    ref_full = ref_full_raw.upper()
            fk.add((cols, ref_full, delete_rule, update_rule))
        elif ctype == "C":
            if is_ob_notnull_constraint(name, cons.get("search_condition")):
                continue
            if is_system_notnull_check(name, cons.get("search_condition")):
                continue
            expr_norm = normalize_check_constraint_expression(cons.get("search_condition"), name)
            if include_deferrable:
                deferrable = normalize_deferrable_flag(cons.get("deferrable"))
                deferred = normalize_deferred_flag(cons.get("deferred"))
                expr_key = f"{expr_norm}||DEFERRABLE={deferrable}||DEFERRED={deferred}"
            else:
                expr_key = expr_norm
            if expr_key:
                ck.add(expr_key)
    return ConstraintSignature(pk=pk, uk=uk, fk=fk, ck=ck)


def build_index_cache_for_table(
    oracle_meta: OracleMetadata,
    ob_meta: ObMetadata,
    src_schema: str,
    src_table: str,
    tgt_schema: str,
    tgt_table: str
) -> IndexCompareCache:
    src_key = (src_schema.upper(), src_table.upper())
    tgt_key = (tgt_schema.upper(), tgt_table.upper())
    src_idx = oracle_meta.indexes.get(src_key)
    if src_idx is None:
        src_idx = {}
    tgt_idx = ob_meta.indexes.get(tgt_key, {})
    tgt_constraints = ob_meta.constraints.get(tgt_key, {})
    normalize_ob_gtt = should_normalize_ob_gtt_indexes(tgt_idx, tgt_constraints)
    src_map = build_index_map(src_idx)
    tgt_map = build_index_map(tgt_idx, normalize_ob_gtt=normalize_ob_gtt)
    src_sig = build_index_signature(src_map)
    tgt_sig = build_index_signature(tgt_map)
    constraint_index_cols = build_constraint_index_cols(
        tgt_constraints,
        tgt_idx,
        normalize_ob_gtt=normalize_ob_gtt
    )
    return IndexCompareCache(
        src_map=src_map,
        tgt_map=tgt_map,
        src_sig=src_sig,
        tgt_sig=tgt_sig,
        constraint_index_cols=constraint_index_cols
    )


def build_constraint_cache_for_table(
    oracle_meta: OracleMetadata,
    ob_meta: ObMetadata,
    src_schema: str,
    src_table: str,
    tgt_schema: str,
    tgt_table: str,
    full_object_mapping: FullObjectMapping
) -> ConstraintCompareCache:
    src_key = (src_schema.upper(), src_table.upper())
    tgt_key = (tgt_schema.upper(), tgt_table.upper())
    src_cons = oracle_meta.constraints.get(src_key)
    if src_cons is None:
        src_cons = {}
    tgt_cons = ob_meta.constraints.get(tgt_key, {})
    include_deferrable = bool(getattr(ob_meta, "constraint_deferrable_supported", False))
    src_norm_cols = {
        name: normalize_column_sequence(cons.get("columns"))
        for name, cons in src_cons.items()
    }
    tgt_norm_cols = {
        name: normalize_column_sequence(cons.get("columns"))
        for name, cons in tgt_cons.items()
    }
    src_sig = build_constraint_signature(
        src_cons, src_norm_cols, full_object_mapping, is_source=True, include_deferrable=include_deferrable
    )
    tgt_sig = build_constraint_signature(
        tgt_cons, tgt_norm_cols, full_object_mapping, is_source=False, include_deferrable=include_deferrable
    )
    source_all_cols: Set[Tuple[str, ...]] = set(src_norm_cols.values())
    partition_key_set = get_partition_key_set(oracle_meta, src_schema, src_table)
    return ConstraintCompareCache(
        src_cons=src_cons,
        tgt_cons=tgt_cons,
        src_norm_cols=src_norm_cols,
        tgt_norm_cols=tgt_norm_cols,
        src_sig=src_sig,
        tgt_sig=tgt_sig,
        source_all_cols=source_all_cols,
        partition_key_set=partition_key_set
    )


def build_trigger_cache_for_table(
    oracle_meta: OracleMetadata,
    ob_meta: ObMetadata,
    src_schema: str,
    src_table: str,
    tgt_schema: str,
    tgt_table: str,
    full_object_mapping: FullObjectMapping
) -> TriggerCompareCache:
    src_key = (src_schema.upper(), src_table.upper())
    tgt_key = (tgt_schema.upper(), tgt_table.upper())
    src_trg = oracle_meta.triggers.get(src_key) or {}
    tgt_trg = ob_meta.triggers.get(tgt_key, {}) or {}
    src_info_map: Dict[str, Dict[str, str]] = {}
    tgt_info_map: Dict[str, Dict] = {}
    src_sig: Set[Tuple[str, str, str, str]] = set()
    tgt_sig: Set[Tuple[str, str, str, str]] = set()

    for trigger_key, info in src_trg.items():
        trg_owner, name_u = normalize_trigger_identity(trigger_key, info, src_schema)
        if not name_u:
            continue
        src_full = f"{trg_owner}.{name_u}"
        mapped = get_mapped_target(full_object_mapping, src_full, 'TRIGGER')
        if mapped and '.' in mapped:
            tgt_owner, tgt_name = mapped.split('.', 1)
            tgt_owner_u = tgt_owner.upper()
            tgt_name_u = tgt_name.upper()
        else:
            tgt_owner_u = trg_owner or src_schema.upper()
            tgt_name_u = name_u
            ensure_mapping_entry(
                full_object_mapping,
                src_full,
                'TRIGGER',
                f"{tgt_owner_u}.{tgt_name_u}"
            )
        tgt_full = f"{tgt_owner_u}.{tgt_name_u}"
        event = (info.get("event") or "").strip()
        status = normalize_trigger_status(info.get("status"))
        valid = lookup_trigger_validity(oracle_meta, trg_owner, name_u)
        src_info_map[tgt_full] = {
            "src_owner": trg_owner,
            "src_name": name_u,
            "tgt_owner": tgt_owner_u,
            "tgt_name": tgt_name_u,
            "event": event,
            "status": status,
            "valid": valid
        }
        src_sig.add((tgt_full, event, status, valid))

    for trigger_key, info in tgt_trg.items():
        owner_u, name_u = normalize_trigger_identity(trigger_key, info, tgt_schema)
        if not name_u:
            continue
        full = f"{owner_u}.{name_u}"
        event = (info.get("event") or "").strip()
        status = normalize_trigger_status(info.get("status"))
        valid = lookup_trigger_validity(ob_meta, owner_u, name_u)
        info["valid"] = valid
        tgt_info_map[full] = info
        tgt_sig.add((full, event, status, valid))

    return TriggerCompareCache(
        src_info_map=src_info_map,
        tgt_info_map=tgt_info_map,
        src_sig=src_sig,
        tgt_sig=tgt_sig
    )


def ensure_trigger_mappings_for_extra_checks(
    master_list: MasterCheckList,
    oracle_meta: OracleMetadata,
    full_object_mapping: FullObjectMapping
) -> None:
    for src_name, _tgt_name, obj_type in master_list:
        if (obj_type or "").upper() != "TABLE":
            continue
        try:
            src_schema, src_table = src_name.split('.', 1)
        except ValueError:
            continue
        src_key = (src_schema.upper(), src_table.upper())
        src_trg = oracle_meta.triggers.get(src_key) or {}
        for trigger_key, info in src_trg.items():
            trg_owner, name_u = normalize_trigger_identity(trigger_key, info, src_schema)
            if not name_u:
                continue
            src_full = f"{trg_owner}.{name_u}"
            mapped = get_mapped_target(full_object_mapping, src_full, 'TRIGGER')
            if mapped and '.' in mapped:
                continue
            # 触发器默认保持源 schema，不跟随 table remap
            tgt_owner_u = trg_owner or src_schema.upper()
            tgt_name_u = name_u
            ensure_mapping_entry(
                full_object_mapping,
                src_full,
                'TRIGGER',
                f"{tgt_owner_u}.{tgt_name_u}"
            )


def compare_index_maps(
    src_map: Dict[Tuple[str, ...], Dict[str, Set[str]]],
    tgt_map: Dict[Tuple[str, ...], Dict[str, Set[str]]],
    constraint_index_cols: Set[Tuple[str, ...]],
    tgt_schema: str,
    tgt_table: str
) -> Tuple[bool, Optional[IndexMismatch]]:
    def rep_name(entry_map: Dict[Tuple[str, ...], Dict[str, Set[str]]], cols: Tuple[str, ...]) -> str:
        names = entry_map.get(cols, {}).get("names") or []
        return next(iter(names), f"{cols}")

    missing_cols = set(src_map.keys()) - set(tgt_map.keys())
    extra_cols = set(tgt_map.keys()) - set(src_map.keys())

    def normalize_sys_nc_columns(cols: Tuple[str, ...]) -> Tuple[str, ...]:
        normalized = []
        for col in cols:
            if re.match(r'^SYS_NC\d+\$', col) or re.match(r'^SYS_NC_[A-Z_]+\$', col):
                normalized.append('SYS_NC$')
            else:
                normalized.append(col)
        return tuple(normalized)

    def is_sys_nc_only_diff(src_cols: Tuple[str, ...], tgt_cols: Tuple[str, ...]) -> bool:
        return normalize_sys_nc_columns(src_cols) == normalize_sys_nc_columns(tgt_cols)

    def has_sys_nc(cols: Tuple[str, ...]) -> bool:
        return any(re.match(r'^SYS_NC', col) or col == 'SYS_NC$' for col in cols)

    def has_expression(cols: Tuple[str, ...]) -> bool:
        return any(is_index_expression_token(col) for col in cols)

    def is_expr_sys_nc_match(src_cols: Tuple[str, ...], tgt_cols: Tuple[str, ...]) -> bool:
        return (has_expression(src_cols) and has_sys_nc(tgt_cols)) or (has_expression(tgt_cols) and has_sys_nc(src_cols))

    for src_cols in list(missing_cols):
        for tgt_cols in list(extra_cols):
            if is_sys_nc_only_diff(src_cols, tgt_cols) or is_expr_sys_nc_match(src_cols, tgt_cols):
                missing_cols.discard(src_cols)
                extra_cols.discard(tgt_cols)
                break

    detail_mismatch: List[str] = []

    for cols in set(src_map.keys()) & set(tgt_map.keys()):
        src_uniq = src_map[cols]["uniq"]
        tgt_uniq = tgt_map[cols]["uniq"]
        if src_uniq != tgt_uniq:
            src_has_nonunique = "NONUNIQUE" in src_uniq
            tgt_has_unique = "UNIQUE" in tgt_uniq
            if src_has_nonunique and tgt_has_unique and cols in constraint_index_cols:
                log.debug("索引列 %s: 源端NONUNIQUE变目标端UNIQUE，有约束支撑，视为正常迁移", list(cols))
                continue
            detail_mismatch.append(
                f"索引列 {list(cols)} 唯一性不一致 (源 {sorted(src_uniq)}, 目标 {sorted(tgt_uniq)})。"
            )

    filtered_missing_cols: Set[Tuple[str, ...]] = set()
    for cols in missing_cols:
        if cols in constraint_index_cols:
            continue
        filtered_missing_cols.add(cols)

    missing = {rep_name(src_map, cols) for cols in filtered_missing_cols}
    extra = {rep_name(tgt_map, cols) for cols in extra_cols}

    for cols in filtered_missing_cols:
        detail_mismatch.append(
            f"索引列 {list(cols)} 在目标端未找到。"
        )

    for cols in extra_cols:
        detail_mismatch.append(
            f"目标端存在额外索引列集 {list(cols)}。"
        )

    all_good = (not missing) and (not extra) and not detail_mismatch
    if all_good:
        return True, None
    return False, IndexMismatch(
        table=f"{tgt_schema}.{tgt_table}",
        missing_indexes=missing,
        extra_indexes=extra,
        detail_mismatch=detail_mismatch
    )

def compare_indexes_for_table(
    oracle_meta: OracleMetadata,
    ob_meta: ObMetadata,
    src_schema: str,
    src_table: str,
    tgt_schema: str,
    tgt_table: str,
    cache: Optional[IndexCompareCache] = None
) -> Tuple[bool, Optional[IndexMismatch]]:
    if cache:
        if cache.src_sig == cache.tgt_sig:
            return True, None
        return compare_index_maps(
            cache.src_map,
            cache.tgt_map,
            cache.constraint_index_cols,
            tgt_schema,
            tgt_table
        )

    src_key = (src_schema.upper(), src_table.upper())
    tgt_key = (tgt_schema.upper(), tgt_table.upper())
    src_idx = oracle_meta.indexes.get(src_key)
    if src_idx is None:
        src_idx = {}
    tgt_idx = ob_meta.indexes.get(tgt_key, {})
    tgt_constraints = ob_meta.constraints.get(tgt_key, {})
    normalize_ob_gtt = should_normalize_ob_gtt_indexes(tgt_idx, tgt_constraints)
    constraint_index_cols = build_constraint_index_cols(
        tgt_constraints,
        tgt_idx,
        normalize_ob_gtt=normalize_ob_gtt
    )
    src_map = build_index_map(src_idx)
    tgt_map = build_index_map(tgt_idx, normalize_ob_gtt=normalize_ob_gtt)
    if build_index_signature(src_map) == build_index_signature(tgt_map):
        return True, None
    return compare_index_maps(src_map, tgt_map, constraint_index_cols, tgt_schema, tgt_table)


def get_partition_key_set(
    meta: OracleMetadata,
    schema: str,
    table: str
) -> Set[str]:
    cols = meta.partition_key_columns.get((schema.upper(), table.upper()), [])
    return {c.upper() for c in cols if c}


class IntervalSpec(NamedTuple):
    value: int
    unit: str
    kind: str  # YEAR_MONTH or DAY_SECOND


class NumericIntervalSpec(NamedTuple):
    value: Decimal


def parse_interval_expression(expr: str) -> Optional[IntervalSpec]:
    if not expr:
        return None
    text = str(expr).strip()
    match = re.search(
        r"NUMTOYMINTERVAL\s*\(\s*([0-9]+)\s*,\s*'([A-Za-z]+)'\s*\)",
        text,
        flags=re.IGNORECASE
    )
    if match:
        value = int(match.group(1))
        unit = match.group(2).upper()
        if unit not in ("MONTH", "YEAR"):
            return None
        return IntervalSpec(value=value, unit=unit, kind="YEAR_MONTH")

    match = re.search(
        r"NUMTODSINTERVAL\s*\(\s*([0-9]+)\s*,\s*'([A-Za-z]+)'\s*\)",
        text,
        flags=re.IGNORECASE
    )
    if match:
        value = int(match.group(1))
        unit = match.group(2).upper()
        if unit not in ("DAY", "HOUR", "MINUTE", "SECOND"):
            return None
        return IntervalSpec(value=value, unit=unit, kind="DAY_SECOND")

    return None


def parse_numeric_interval_expression(expr: str) -> Optional[NumericIntervalSpec]:
    if not expr:
        return None
    text = str(expr).strip()
    match = re.search(
        r"\bINTERVAL\s*\(\s*([+-]?\d+(?:\.\d+)?(?:[eE][+-]?\d+)?)\s*\)",
        text,
        flags=re.IGNORECASE
    )
    if match:
        try:
            value = Decimal(match.group(1))
        except (InvalidOperation, ValueError):
            return None
        return NumericIntervalSpec(value=value)

    raw = text.strip()
    if raw.startswith("(") and raw.endswith(")"):
        raw = raw[1:-1].strip()
    if not re.fullmatch(r"[+-]?\d+(?:\.\d+)?(?:[eE][+-]?\d+)?", raw):
        return None
    try:
        value = Decimal(raw)
    except (InvalidOperation, ValueError):
        return None
    return NumericIntervalSpec(value=value)


def is_numeric_partition_type(data_type: str) -> bool:
    dt = (data_type or "").strip().upper()
    if not dt:
        return False
    if dt.startswith(("NUMBER", "DECIMAL", "NUMERIC")):
        return True
    return dt in {
        "INTEGER",
        "INT",
        "SMALLINT",
        "FLOAT",
        "BINARY_FLOAT",
        "BINARY_DOUBLE",
        "REAL",
        "DOUBLE",
        "DOUBLE PRECISION",
    }


def format_decimal_literal(value: Decimal) -> str:
    text = format(value, "f")
    if "." in text:
        text = text.rstrip("0").rstrip(".")
    if text == "-0":
        text = "0"
    return text or "0"


def format_numeric_partition_name(value: Decimal) -> str:
    text = format_decimal_literal(value)
    if text.startswith("-"):
        text = "NEG_" + text[1:]
    text = text.replace("+", "")
    text = text.replace(".", "_")
    text = re.sub(r"[^A-Za-z0-9_]", "_", text)
    return f"P{text}"


def parse_partition_high_value_numeric(expr: str) -> Optional[Decimal]:
    if not expr:
        return None
    raw = str(expr).strip()
    if not raw:
        return None
    if "MAXVALUE" in raw.upper():
        return None
    match = re.search(r"TO_NUMBER\s*\(\s*'([^']+)'\s*\)", raw, flags=re.IGNORECASE)
    if match:
        raw = match.group(1).strip()
    else:
        match = re.search(r"TO_NUMBER\s*\(\s*([^)]+)\)", raw, flags=re.IGNORECASE)
        if match:
            raw = match.group(1).strip()
    if raw.startswith("(") and raw.endswith(")"):
        raw = raw[1:-1].strip()
    if not re.search(r"\d", raw):
        return None
    match = re.search(r"[+-]?\d+(?:\.\d+)?(?:[eE][+-]?\d+)?", raw)
    if not match:
        return None
    try:
        return Decimal(match.group(0))
    except (InvalidOperation, ValueError):
        return None


def days_in_month(year: int, month: int) -> int:
    if month == 12:
        next_month = datetime(year + 1, 1, 1)
    else:
        next_month = datetime(year, month + 1, 1)
    return (next_month - datetime(year, month, 1)).days


def add_months_to_datetime(value: datetime, months: int) -> datetime:
    total = (value.year * 12 + (value.month - 1)) + months
    year = total // 12
    month = total % 12 + 1
    day = min(value.day, days_in_month(year, month))
    return value.replace(year=year, month=month, day=day)


def add_interval(value: datetime, spec: IntervalSpec) -> Optional[datetime]:
    if spec.value <= 0:
        return None
    if spec.kind == "YEAR_MONTH":
        months = spec.value * (12 if spec.unit == "YEAR" else 1)
        return add_months_to_datetime(value, months)
    if spec.kind == "DAY_SECOND":
        if spec.unit == "DAY":
            return value + timedelta(days=spec.value)
        if spec.unit == "HOUR":
            return value + timedelta(hours=spec.value)
        if spec.unit == "MINUTE":
            return value + timedelta(minutes=spec.value)
        if spec.unit == "SECOND":
            return value + timedelta(seconds=spec.value)
    return None


def _normalize_datetime_literal(raw: str) -> str:
    text = (raw or "").strip()
    if not text:
        return ""
    text = text.replace("T", " ")
    match = re.search(r"(\d{4}[-/]\d{2}[-/]\d{2})(?:[ T](\d{2}:\d{2}:\d{2}))?", text)
    if match:
        date_part = match.group(1)
        time_part = match.group(2)
        if time_part:
            return f"{date_part} {time_part}"
        return date_part
    match = re.search(r"\b(\d{8})\b", text)
    if match:
        return match.group(1)
    return text


def _try_parse_datetime(raw: str) -> Optional[datetime]:
    normalized = _normalize_datetime_literal(raw)
    if not normalized:
        return None
    candidates = [
        "%Y-%m-%d %H:%M:%S",
        "%Y-%m-%d",
        "%Y/%m/%d %H:%M:%S",
        "%Y/%m/%d",
        "%Y%m%d",
    ]
    for fmt in candidates:
        try:
            return datetime.strptime(normalized, fmt)
        except ValueError:
            continue
    return None


def parse_partition_high_value(expr: str) -> Optional[datetime]:
    if not expr:
        return None
    raw = str(expr).strip()
    if not raw:
        return None
    if "MAXVALUE" in raw.upper():
        return None
    patterns = [
        r"TO_DATE\s*\(\s*'([^']+)'",
        r"TO_TIMESTAMP_TZ\s*\(\s*'([^']+)'",
        r"TO_TIMESTAMP\s*\(\s*'([^']+)'",
        r"DATE\s*'([^']+)'",
        r"TIMESTAMP\s*'([^']+)'",
    ]
    for pat in patterns:
        match = re.search(pat, raw, flags=re.IGNORECASE)
        if match:
            value = match.group(1).strip()
            parsed = _try_parse_datetime(value)
            if parsed:
                return parsed
    return _try_parse_datetime(raw)


def format_partition_boundary(value: datetime, data_type: str) -> str:
    data_type_u = (data_type or "").upper()
    if data_type_u.startswith("TIMESTAMP"):
        fmt = "%Y-%m-%d %H:%M:%S"
        return f"TO_TIMESTAMP('{value.strftime(fmt)}', 'YYYY-MM-DD HH24:MI:SS')"
    if value.time() == datetime.min.time():
        return f"TO_DATE('{value.strftime('%Y-%m-%d')}', 'YYYY-MM-DD')"
    return f"TO_DATE('{value.strftime('%Y-%m-%d %H:%M:%S')}', 'YYYY-MM-DD HH24:MI:SS')"


def generate_interval_partition_statements(
    info: IntervalPartitionInfo,
    cutoff_date: Optional[datetime],
    cutoff_numeric: Optional[Decimal],
    data_type: str,
    table_full: str
) -> Tuple[List[str], List[str], str]:
    warnings: List[str] = []
    if not info.interval_expr:
        return [], ["interval_expr 为空，已跳过"], ""
    if info.partitioning_type and info.partitioning_type.upper() != "RANGE":
        return [], [f"partitioning_type={info.partitioning_type} 不支持 interval 补齐"], ""
    if len(info.partition_key_columns) != 1:
        return [], ["分区键列数量不为 1，已跳过"], ""

    spec = parse_interval_expression(info.interval_expr)
    if spec:
        if not cutoff_date:
            return [], ["interval_partition_cutoff 未配置或非法"], "date"

        last_high = parse_partition_high_value(info.last_high_value)
        if not last_high:
            return [], [f"无法解析 HIGH_VALUE: {info.last_high_value}"], "date"

        statements: List[str] = []
        existing_names = {name.upper() for name in info.existing_partition_names}
        next_boundary = add_interval(last_high, spec)
        if not next_boundary:
            return [], ["interval 计算失败或步长为 0"], "date"

        def _format_partition_name(value: datetime, spec_item: IntervalSpec) -> str:
            if spec_item.kind == "DAY_SECOND" and spec_item.unit in ("HOUR", "MINUTE", "SECOND"):
                fmt = "%Y%m%d%H%M%S"
            elif value.time() != datetime.min.time():
                fmt = "%Y%m%d%H%M%S"
            else:
                fmt = "%Y%m%d"
            return f"P{value.strftime(fmt)}"

        max_iters = 10000
        while next_boundary and next_boundary <= cutoff_date and max_iters > 0:
            part_name = _format_partition_name(next_boundary, spec)
            if part_name.upper() in existing_names:
                suffix = 1
                candidate = f"{part_name}_{suffix}"
                while candidate.upper() in existing_names:
                    suffix += 1
                    candidate = f"{part_name}_{suffix}"
                part_name = candidate
            existing_names.add(part_name.upper())
            boundary_expr = format_partition_boundary(next_boundary, data_type)
            statements.append(
                f"ALTER TABLE {table_full} ADD PARTITION {part_name} VALUES LESS THAN ({boundary_expr});"
            )
            last_high = next_boundary
            next_boundary = add_interval(last_high, spec)
            max_iters -= 1

        if max_iters <= 0:
            warnings.append("生成分区数量过多，已提前终止")

        return statements, warnings, "date"

    numeric_spec = parse_numeric_interval_expression(info.interval_expr)
    if not numeric_spec:
        return [], [f"interval 解析失败: {info.interval_expr}"], ""

    if numeric_spec.value <= 0:
        return [], ["interval 计算失败或步长为 0"], "numeric"
    if not is_numeric_partition_type(data_type):
        return [], [f"分区键类型非数值: {data_type}"], "numeric"
    if not cutoff_numeric:
        return [], ["interval_partition_cutoff_numeric 未配置或非法"], "numeric"

    last_high_num = parse_partition_high_value_numeric(info.last_high_value)
    if last_high_num is None:
        return [], [f"无法解析 HIGH_VALUE: {info.last_high_value}"], "numeric"

    statements = []
    existing_names = {name.upper() for name in info.existing_partition_names}
    next_boundary_num = last_high_num + numeric_spec.value
    if next_boundary_num <= last_high_num:
        return [], ["interval 计算失败或步长为 0"], "numeric"

    max_iters = 10000
    while next_boundary_num <= cutoff_numeric and max_iters > 0:
        part_name = format_numeric_partition_name(next_boundary_num)
        if part_name.upper() in existing_names:
            suffix = 1
            candidate = f"{part_name}_{suffix}"
            while candidate.upper() in existing_names:
                suffix += 1
                candidate = f"{part_name}_{suffix}"
            part_name = candidate
        existing_names.add(part_name.upper())
        boundary_expr = format_decimal_literal(next_boundary_num)
        statements.append(
            f"ALTER TABLE {table_full} ADD PARTITION {part_name} VALUES LESS THAN ({boundary_expr});"
        )
        last_high_num = next_boundary_num
        next_boundary_num = last_high_num + numeric_spec.value
        max_iters -= 1

    if max_iters <= 0:
        warnings.append("生成分区数量过多，已提前终止")

    return statements, warnings, "numeric"


def compare_constraints_for_table(
    oracle_meta: OracleMetadata,
    ob_meta: ObMetadata,
    src_schema: str,
    src_table: str,
    tgt_schema: str,
    tgt_table: str,
    full_object_mapping: FullObjectMapping,
    cache: Optional[ConstraintCompareCache] = None
) -> Tuple[bool, Optional[ConstraintMismatch]]:
    include_deferrable = bool(getattr(ob_meta, "constraint_deferrable_supported", False))
    if cache:
        src_cons = cache.src_cons
        tgt_cons = cache.tgt_cons
        src_norm_cols = cache.src_norm_cols
        tgt_norm_cols = cache.tgt_norm_cols
        source_all_cols = cache.source_all_cols
        partition_key_set = cache.partition_key_set
        if cache.src_sig == cache.tgt_sig:
            return True, None
    else:
        src_key = (src_schema.upper(), src_table.upper())
        tgt_key = (tgt_schema.upper(), tgt_table.upper())
        src_cons = oracle_meta.constraints.get(src_key)
        tgt_cons = ob_meta.constraints.get(tgt_key, {})
        if src_cons is None:
            src_cons = {}
        src_norm_cols = {
            name: normalize_column_sequence(cons.get("columns"))
            for name, cons in src_cons.items()
        }
        tgt_norm_cols = {
            name: normalize_column_sequence(cons.get("columns"))
            for name, cons in tgt_cons.items()
        }
        source_all_cols = set(src_norm_cols.values())
        partition_key_set = get_partition_key_set(oracle_meta, src_schema, src_table)

    src_idx_unique_cols: Set[Tuple[str, ...]] = set()
    src_idx_unique_names: Set[str] = set()
    src_idx_entries = oracle_meta.indexes.get((src_schema.upper(), src_table.upper()), {}) or {}
    if src_idx_entries:
        src_idx_map = build_index_map(src_idx_entries)
        for cols, info in src_idx_map.items():
            uniq_set = info.get("uniq") or set()
            if "UNIQUE" in uniq_set:
                src_idx_unique_cols.add(cols)
                for name in info.get("names", set()):
                    if name:
                        src_idx_unique_names.add(name.upper())

    detail_mismatch: List[str] = []
    missing: Set[str] = set()
    extra: Set[str] = set()
    downgraded_missing: Set[str] = set()

    def _norm_cols(name: str, cons: Dict, *, is_source: bool) -> Tuple[str, ...]:
        if is_source:
            return src_norm_cols.get(name) or normalize_column_sequence(cons.get("columns"))
        return tgt_norm_cols.get(name) or normalize_column_sequence(cons.get("columns"))

    def bucket_pk_uk(cons_dict: Dict[str, Dict]) -> Dict[str, List[Tuple[Tuple[str, ...], str]]]:
        buckets: Dict[str, List[Tuple[Tuple[str, ...], str]]] = {'P': [], 'U': []}
        for name, cons in cons_dict.items():
            ctype = (cons.get("type") or "").upper()
            if ctype not in buckets:
                continue
            cols = _norm_cols(name, cons, is_source=(cons_dict is src_cons))
            buckets[ctype].append((cols, name))
        return buckets

    def bucket_fk(
        cons_dict: Dict[str, Dict],
        *,
        is_source: bool
    ) -> List[Tuple[Tuple[str, ...], str, Optional[str], str, str]]:
        entries: List[Tuple[Tuple[str, ...], str, Optional[str], str, str]] = []
        for name, cons in cons_dict.items():
            ctype = (cons.get("type") or "").upper()
            if ctype != 'R':
                continue
            cols = _norm_cols(name, cons, is_source=(cons_dict is src_cons))
            ref_full: Optional[str] = None
            ref_owner = cons.get("ref_table_owner")
            ref_name = cons.get("ref_table_name")
            delete_rule = normalize_delete_rule(cons.get("delete_rule"))
            update_rule = normalize_update_rule(cons.get("update_rule"))
            if ref_owner and ref_name:
                ref_full_raw = f"{str(ref_owner).upper()}.{str(ref_name).upper()}"
                if is_source:
                    # 源端FK引用：应用remap规则
                    mapped = get_mapped_target(full_object_mapping, ref_full_raw, 'TABLE')
                    ref_full = (mapped or ref_full_raw).upper()
                else:
                    # 目标端FK引用：使用原始名称（目标端已经是remapped之后的名称）
                    ref_full = ref_full_raw.upper()
            entries.append((cols, name, ref_full, delete_rule, update_rule))
        return entries

    def bucket_check(
        cons_dict: Dict[str, Dict],
        *,
        include_deferrable: bool
    ) -> List[Tuple[str, str, str, str, str]]:
        entries: List[Tuple[str, str, str, str, str]] = []
        for name, cons in cons_dict.items():
            ctype = (cons.get("type") or "").upper()
            if ctype != "C":
                continue
            raw_expr = cons.get("search_condition")
            if is_ob_notnull_constraint(name, raw_expr):
                continue
            if is_system_notnull_check(name, raw_expr):
                continue
            expr_norm = normalize_check_constraint_expression(raw_expr, name)
            if include_deferrable:
                deferrable = normalize_deferrable_flag(cons.get("deferrable"))
                deferred = normalize_deferred_flag(cons.get("deferred"))
                expr_key = f"{expr_norm}||DEFERRABLE={deferrable}||DEFERRED={deferred}"
            else:
                deferrable = ""
                deferred = ""
                expr_key = expr_norm
            entries.append((expr_key, name, str(raw_expr or ""), deferrable, deferred))
        return entries

    grouped_src_pkuk = bucket_pk_uk(src_cons)
    grouped_tgt_pkuk = bucket_pk_uk(tgt_cons)
    grouped_src_fk = bucket_fk(src_cons, is_source=True)
    grouped_tgt_fk = bucket_fk(tgt_cons, is_source=False)
    grouped_src_ck = bucket_check(src_cons, include_deferrable=include_deferrable)
    grouped_tgt_ck = bucket_check(tgt_cons, include_deferrable=include_deferrable)

    src_pk_list = grouped_src_pkuk.get('P', [])
    src_uk_list = grouped_src_pkuk.get('U', [])
    tgt_pk_list = grouped_tgt_pkuk.get('P', [])
    tgt_uk_list = grouped_tgt_pkuk.get('U', [])

    src_pk_inclusive: List[Tuple[Tuple[str, ...], str]] = []
    src_pk_downgrade: List[Tuple[Tuple[str, ...], str]] = []
    for cols, name in src_pk_list:
        if partition_key_set and not partition_key_set.issubset(set(cols)):
            src_pk_downgrade.append((cols, name))
        else:
            src_pk_inclusive.append((cols, name))

    tgt_pk_used = [False] * len(tgt_pk_list)
    tgt_uk_used = [False] * len(tgt_uk_list)

    def match_constraints(
        label: str,
        src_list: List[Tuple[Tuple[str, ...], str]],
        tgt_list: List[Tuple[Tuple[str, ...], str]],
        tgt_used: List[bool]
    ) -> None:
        for cols, name in src_list:
            found_idx = None
            for idx, (t_cols, _t_name) in enumerate(tgt_list):
                if tgt_used[idx]:
                    continue
                if cols == t_cols:
                    found_idx = idx
                    tgt_used[idx] = True
                    break
            if found_idx is None:
                missing.add(name)
                detail_mismatch.append(
                    f"{label}: 源约束 {name} (列 {list(cols)}) 在目标端未找到。"
                )

    def mark_extra_constraints(
        label: str,
        tgt_list: List[Tuple[Tuple[str, ...], str]],
        tgt_used: List[bool]
    ) -> None:
        for idx, used in enumerate(tgt_used):
            if used:
                continue
            extra_name = tgt_list[idx][1]
            extra_cols = tgt_list[idx][0]
            if extra_cols in source_all_cols:
                continue
            if label == "UNIQUE KEY":
                extra_name_u = (extra_name or "").upper()
                if extra_cols in src_idx_unique_cols or (extra_name_u and extra_name_u in src_idx_unique_names):
                    log.debug(
                        "忽略派生 UNIQUE 约束 %s (列 %s)，源端已存在 UNIQUE 索引。",
                        extra_name,
                        list(extra_cols)
                    )
                    continue
            if "_OMS_ROWID" in (extra_name or ""):
                continue
            extra.add(extra_name)
            detail_mismatch.append(
                f"{label}: 目标端存在额外约束 {extra_name} (列 {list(extra_cols)})。"
            )

    def match_foreign_keys(
        src_list: List[Tuple[Tuple[str, ...], str, Optional[str], str, str]],
        tgt_list: List[Tuple[Tuple[str, ...], str, Optional[str], str, str]]
    ) -> None:
        tgt_used = [False] * len(tgt_list)
        tgt_by_cols: Dict[Tuple[str, ...], Set[Optional[str]]] = defaultdict(set)
        for cols, _name, ref, _rule, _update_rule in tgt_list:
            tgt_by_cols[cols].add(ref)

        for cols, name, src_ref, src_rule, src_update_rule in src_list:
            found_idx = None
            for idx, (t_cols, _t_name, t_ref, t_rule, t_update_rule) in enumerate(tgt_list):
                if tgt_used[idx]:
                    continue
                if cols != t_cols:
                    continue
                if src_ref and t_ref and src_ref != t_ref:
                    continue
                found_idx = idx
                tgt_used[idx] = True
                break
            if found_idx is None:
                missing.add(name)
                if src_ref and cols in tgt_by_cols and (src_ref not in tgt_by_cols[cols]):
                    detail_mismatch.append(
                        f"FOREIGN KEY: 源约束 {name} (列 {list(cols)}) 引用 {src_ref}，但目标端同列集引用 {sorted(tgt_by_cols[cols])}。"
                    )
                else:
                    detail_mismatch.append(
                        f"FOREIGN KEY: 源约束 {name} (列 {list(cols)}) 在目标端未找到。"
                    )
            else:
                tgt_rule = tgt_list[found_idx][3]
                tgt_update_rule = tgt_list[found_idx][4]
                src_rule_norm = normalize_delete_rule(src_rule)
                tgt_rule_norm = normalize_delete_rule(tgt_rule)
                if src_rule_norm != tgt_rule_norm:
                    detail_mismatch.append(
                        f"FOREIGN KEY: 源约束 {name} DELETE_RULE={src_rule_norm or 'NO ACTION'}，"
                        f"目标端 DELETE_RULE={tgt_rule_norm or 'NO ACTION'}。"
                    )
                src_update_norm = normalize_update_rule(src_update_rule)
                tgt_update_norm = normalize_update_rule(tgt_update_rule)
                if src_update_norm != tgt_update_norm:
                    detail_mismatch.append(
                        f"FOREIGN KEY: 源约束 {name} UPDATE_RULE={src_update_norm or 'NO ACTION'}，"
                        f"目标端 UPDATE_RULE={tgt_update_norm or 'NO ACTION'}。"
                    )

        for idx, used in enumerate(tgt_used):
            if not used:
                extra_name, extra_cols, extra_ref, extra_rule, extra_update_rule = tgt_list[idx]
                if extra_cols in source_all_cols:
                    continue
                if "_OMS_ROWID" in (extra_name or ""):
                    continue
                extra.add(extra_name)
                ref_note = f" 引用 {extra_ref}" if extra_ref else ""
                rule_note = f" DELETE_RULE={normalize_delete_rule(extra_rule) or 'NO ACTION'}"
                update_note = f" UPDATE_RULE={normalize_update_rule(extra_update_rule) or 'NO ACTION'}"
                detail_mismatch.append(
                    f"FOREIGN KEY: 目标端存在额外约束 {extra_name} (列 {list(extra_cols)}){ref_note}{rule_note}{update_note}。"
                )

    def match_check_constraints(
        src_list: List[Tuple[str, str, str, str, str]],
        tgt_list: List[Tuple[str, str, str, str, str]]
    ) -> None:
        tgt_by_expr: Dict[str, List[str]] = defaultdict(list)
        tgt_by_name: Dict[str, Tuple[str, str, str, str]] = {}
        for expr_key, name, raw_expr, deferrable, deferred in tgt_list:
            tgt_by_expr[expr_key].append(name)
            tgt_by_name[name] = (expr_key, raw_expr, deferrable, deferred)

        used: Set[str] = set()
        for expr_key, name, raw_expr, deferrable, deferred in src_list:
            if name in tgt_by_name:
                tgt_expr_key, tgt_expr_raw, tgt_deferrable, tgt_deferred = tgt_by_name.get(
                    name, ("", "", "", "")
                )
                if tgt_expr_key != expr_key:
                    if include_deferrable:
                        detail_mismatch.append(
                            "CHECK: 源约束 %s 条件/延迟属性不一致 (源=%s [DEFERRABLE=%s,DEFERRED=%s], "
                            "目标=%s [DEFERRABLE=%s,DEFERRED=%s])。" % (
                                name,
                                raw_expr,
                                deferrable,
                                deferred,
                                tgt_expr_raw,
                                tgt_deferrable,
                                tgt_deferred
                            )
                        )
                    else:
                        detail_mismatch.append(
                            "CHECK: 源约束 %s 条件不一致 (源=%s, 目标=%s)。"
                            % (name, raw_expr, tgt_expr_raw)
                        )
                expr_matches = tgt_by_expr.get(expr_key)
                if expr_matches and name in expr_matches:
                    expr_matches.remove(name)
                used.add(name)
                continue
            expr_matches = tgt_by_expr.get(expr_key)
            if expr_matches:
                if name in expr_matches:
                    expr_matches.remove(name)
                    matched_name = name
                else:
                    matched_name = expr_matches.pop()
                used.add(matched_name)
                continue
            missing.add(name)
            detail_mismatch.append(
                f"CHECK: 源约束 {name} (条件 {raw_expr}) 在目标端未找到。"
            )

        for expr_key, name, raw_expr, _deferrable, _deferred in tgt_list:
            if name in used:
                continue
            extra.add(name)
            detail_mismatch.append(
                f"CHECK: 目标端存在额外约束 {name} (条件 {raw_expr})。"
            )

    match_constraints("PRIMARY KEY", src_pk_inclusive, tgt_pk_list, tgt_pk_used)

    # 分区键未包含在 PK 中时，按 UNIQUE 处理（允许目标端 PK/UK 任一匹配）
    for cols, name in src_pk_downgrade:
        matched = False
        for idx, (t_cols, _t_name) in enumerate(tgt_uk_list):
            if tgt_uk_used[idx]:
                continue
            if cols == t_cols:
                tgt_uk_used[idx] = True
                matched = True
                break
        if not matched:
            for idx, (t_cols, _t_name) in enumerate(tgt_pk_list):
                if tgt_pk_used[idx]:
                    continue
                if cols == t_cols:
                    tgt_pk_used[idx] = True
                    matched = True
                    break
        if not matched:
            missing.add(name)
            downgraded_missing.add(name)
            detail_mismatch.append(
                f"PRIMARY KEY(降级为UNIQUE): 源约束 {name} (列 {list(cols)}) 在目标端未找到。"
            )

    match_constraints("UNIQUE KEY", src_uk_list, tgt_uk_list, tgt_uk_used)
    mark_extra_constraints("PRIMARY KEY", tgt_pk_list, tgt_pk_used)
    mark_extra_constraints("UNIQUE KEY", tgt_uk_list, tgt_uk_used)
    match_foreign_keys(grouped_src_fk, grouped_tgt_fk)
    match_check_constraints(grouped_src_ck, grouped_tgt_ck)

    all_good = (not missing) and (not extra) and (not detail_mismatch)
    if all_good:
        return True, None
    else:
        return False, ConstraintMismatch(
            table=f"{tgt_schema}.{tgt_table}",
            missing_constraints=missing,
            extra_constraints=extra,
            detail_mismatch=detail_mismatch,
            downgraded_pk_constraints=downgraded_missing
        )


def compare_sequences_for_schema(
    oracle_meta: OracleMetadata,
    ob_meta: ObMetadata,
    src_schema: str,
    tgt_schema: str
) -> Tuple[bool, Optional[SequenceMismatch]]:
    src_seqs = oracle_meta.sequences.get(src_schema.upper())
    if src_seqs is None:
        log.warning(f"[序列检查] 未找到 {src_schema} 的 Oracle 序列元数据。")
        tgt_seqs_snapshot = ob_meta.sequences.get(tgt_schema.upper(), set())
        
        # 改进：区分元数据加载失败 vs schema确实没有序列
        # 检查该schema是否在已加载的对象元数据中出现（说明schema存在）
        schema_u = src_schema.upper()
        schema_has_objects = any(
            owner == schema_u
            for owner, _ in oracle_meta.table_columns.keys()
        )
        if not schema_has_objects:
            for owner, _ in oracle_meta.indexes.keys():
                if owner == schema_u:
                    schema_has_objects = True
                    break
        if not schema_has_objects:
            for owner, _ in oracle_meta.constraints.keys():
                if owner == schema_u:
                    schema_has_objects = True
                    break
        if not schema_has_objects:
            for owner, _ in oracle_meta.triggers.keys():
                if owner == schema_u:
                    schema_has_objects = True
                    break
        if not schema_has_objects:
            for owner, _ in oracle_meta.table_comments.keys():
                if owner == schema_u:
                    schema_has_objects = True
                    break
        if not schema_has_objects:
            for owner, _ in oracle_meta.column_comments.keys():
                if owner == schema_u:
                    schema_has_objects = True
                    break

        if not schema_has_objects:
            # Schema不存在于元数据中，可能是配置错误或权限问题，跳过比较
            note = f"源端schema {src_schema} 未在Oracle元数据中找到，跳过序列比较。"
            log.info(note)
            return True, None  # 跳过，不报告不一致
        
        # Schema存在但sequences为None，说明确实没有序列（或DBA_SEQUENCES查询为空）
        note = (
            f"Oracle schema {src_schema} 中无序列定义"
            f"（DBA_SEQUENCES未返回记录，可能schema确实无序列或权限不足）。"
        )
        if tgt_seqs_snapshot:
            note += f" 目标端存在序列：{', '.join(sorted(tgt_seqs_snapshot))}。"
            # 源端没有，目标端有，报告为额外序列
            return False, SequenceMismatch(
                src_schema=src_schema,
                tgt_schema=tgt_schema,
                missing_sequences=set(),
                extra_sequences=tgt_seqs_snapshot,
                note=note,
                missing_mappings=[],
                detail_mismatch=None
            )
        else:
            # 双方都没有序列，认为一致
            log.debug(f"[序列检查] 源端和目标端都无序列，认为一致。")
            return True, None

    tgt_seqs = ob_meta.sequences.get(tgt_schema.upper(), set())

    missing = src_seqs - tgt_seqs
    extra = tgt_seqs - src_seqs
    all_good = (not missing) and (not extra)
    if all_good:
        return True, None
    else:
        return False, SequenceMismatch(
            src_schema=src_schema,
            tgt_schema=tgt_schema,
            missing_sequences=missing,
            extra_sequences=extra,
            note=None,
            missing_mappings=[
                (f"{src_schema.upper()}.{seq}", f"{tgt_schema.upper()}.{seq}")
                for seq in sorted(missing)
            ],
            detail_mismatch=None
        )


def compare_triggers_for_table(
    oracle_meta: OracleMetadata,
    ob_meta: ObMetadata,
    src_schema: str,
    src_table: str,
    tgt_schema: str,
    tgt_table: str,
    full_object_mapping: FullObjectMapping,
    cache: Optional[TriggerCompareCache] = None
) -> Tuple[bool, Optional[TriggerMismatch]]:
    if cache is None:
        cache = build_trigger_cache_for_table(
            oracle_meta,
            ob_meta,
            src_schema,
            src_table,
            tgt_schema,
            tgt_table,
            full_object_mapping
        )
    if cache.src_sig == cache.tgt_sig:
        return True, None
    src_info_map = cache.src_info_map
    tgt_info_map = cache.tgt_info_map
    if not src_info_map:
        if not tgt_info_map:
            return True, None
        extra_triggers = set(tgt_info_map.keys())
        return False, TriggerMismatch(
            table=f"{tgt_schema}.{tgt_table}",
            missing_triggers=set(),
            extra_triggers=extra_triggers,
            detail_mismatch=[f"源端无触发器，目标端存在额外触发器: {', '.join(sorted(extra_triggers))}"],
            missing_mappings=[]
        )
    missing = set(src_info_map.keys()) - set(tgt_info_map.keys())
    extra = set(tgt_info_map.keys()) - set(src_info_map.keys())
    detail_mismatch: List[str] = []
    missing_mappings: List[Tuple[str, str]] = []
    for tgt_full in sorted(missing):
        src_info = src_info_map.get(tgt_full, {})
        missing_mappings.append(
            (
                f"{src_info.get('src_owner', src_schema.upper())}.{src_info.get('src_name', tgt_full.split('.', 1)[-1])}",
                f"{src_info.get('tgt_owner', tgt_schema.upper())}.{src_info.get('tgt_name', tgt_full.split('.', 1)[-1])}"
            )
        )
    common = set(src_info_map.keys()) & set(tgt_info_map.keys())
    for tgt_full in common:
        src_info = src_info_map.get(tgt_full, {})
        tgt_info = tgt_info_map.get(tgt_full, {})
        s_event = (src_info.get("event") or "").strip()
        s_status = normalize_trigger_status(src_info.get("status"))
        s_owner = (src_info.get("src_owner") or src_schema).upper()
        s_name = (src_info.get("src_name") or "").upper()
        s_valid = normalize_trigger_status(
            src_info.get("valid") or lookup_trigger_validity(oracle_meta, s_owner, s_name)
        )
        t_event = (tgt_info.get("event") or "").strip()
        t_status = normalize_trigger_status(tgt_info.get("status"))
        t_owner = (tgt_info.get("owner") or tgt_schema).upper()
        t_name = (tgt_full.split(".", 1)[1] if "." in tgt_full else tgt_full).upper()
        t_valid = normalize_trigger_status(
            tgt_info.get("valid") or lookup_trigger_validity(ob_meta, t_owner, t_name)
        )
        if s_event != t_event:
            detail_mismatch.append(
                f"{tgt_full}: 触发事件不一致 (src={s_event}, tgt={t_event})"
            )
        if s_status != t_status:
            detail_mismatch.append(
                f"{tgt_full}: 启用状态不一致 (src={s_status}, tgt={t_status})"
            )
        if s_valid != t_valid:
            detail_mismatch.append(
                f"{tgt_full}: 有效性不一致 (src={s_valid}, tgt={t_valid})"
            )
    all_good = (not missing) and (not extra) and not detail_mismatch
    if all_good:
        return True, None
    return False, TriggerMismatch(
        table=f"{tgt_schema}.{tgt_table}",
        missing_triggers=missing,
        extra_triggers=extra,
        detail_mismatch=detail_mismatch,
        missing_mappings=missing_mappings
    )


_EXTRA_CHECK_CONTEXT: Dict[str, object] = {}


def _init_extra_check_worker(
    oracle_meta: OracleMetadata,
    ob_meta: ObMetadata,
    full_object_mapping: FullObjectMapping,
    enabled_types: Set[str]
) -> None:
    global _EXTRA_CHECK_CONTEXT
    _EXTRA_CHECK_CONTEXT = {
        "oracle_meta": oracle_meta,
        "ob_meta": ob_meta,
        "full_object_mapping": full_object_mapping,
        "enabled_types": enabled_types
    }


def _extra_check_worker(table_entry: TableEntry) -> ExtraTableResult:
    ctx = _EXTRA_CHECK_CONTEXT
    return run_extra_check_for_table(
        table_entry,
        ctx["oracle_meta"],
        ctx["ob_meta"],
        ctx["full_object_mapping"],
        ctx["enabled_types"]
    )


def run_extra_check_for_table(
    table_entry: TableEntry,
    oracle_meta: OracleMetadata,
    ob_meta: ObMetadata,
    full_object_mapping: FullObjectMapping,
    enabled_types: Set[str]
) -> ExtraTableResult:
    src_schema, src_table, tgt_schema, tgt_table = table_entry
    tgt_name = f"{tgt_schema}.{tgt_table}"
    index_ok: Optional[bool] = None
    constraint_ok: Optional[bool] = None
    trigger_ok: Optional[bool] = None
    index_mismatch: Optional[IndexMismatch] = None
    constraint_mismatch: Optional[ConstraintMismatch] = None
    trigger_mismatch: Optional[TriggerMismatch] = None
    index_time = 0.0
    constraint_time = 0.0
    trigger_time = 0.0

    if 'INDEX' in enabled_types:
        idx_cache = build_index_cache_for_table(
            oracle_meta, ob_meta, src_schema, src_table, tgt_schema, tgt_table
        )
        start = time.monotonic()
        index_ok, index_mismatch = compare_indexes_for_table(
            oracle_meta,
            ob_meta,
            src_schema,
            src_table,
            tgt_schema,
            tgt_table,
            cache=idx_cache
        )
        index_time = time.monotonic() - start

    if 'CONSTRAINT' in enabled_types:
        cons_cache = build_constraint_cache_for_table(
            oracle_meta,
            ob_meta,
            src_schema,
            src_table,
            tgt_schema,
            tgt_table,
            full_object_mapping
        )
        start = time.monotonic()
        constraint_ok, constraint_mismatch = compare_constraints_for_table(
            oracle_meta,
            ob_meta,
            src_schema,
            src_table,
            tgt_schema,
            tgt_table,
            full_object_mapping,
            cache=cons_cache
        )
        constraint_time = time.monotonic() - start

    if 'TRIGGER' in enabled_types:
        trg_cache = build_trigger_cache_for_table(
            oracle_meta,
            ob_meta,
            src_schema,
            src_table,
            tgt_schema,
            tgt_table,
            full_object_mapping
        )
        start = time.monotonic()
        trigger_ok, trigger_mismatch = compare_triggers_for_table(
            oracle_meta,
            ob_meta,
            src_schema,
            src_table,
            tgt_schema,
            tgt_table,
            full_object_mapping,
            cache=trg_cache
        )
        trigger_time = time.monotonic() - start

    return ExtraTableResult(
        tgt_name=tgt_name,
        index_ok=index_ok,
        index_mismatch=index_mismatch,
        constraint_ok=constraint_ok,
        constraint_mismatch=constraint_mismatch,
        trigger_ok=trigger_ok,
        trigger_mismatch=trigger_mismatch,
        index_time=index_time,
        constraint_time=constraint_time,
        trigger_time=trigger_time
    )


def check_extra_objects(
    settings: Dict,
    master_list: MasterCheckList,
    ob_meta: ObMetadata,
    oracle_meta: OracleMetadata,
    full_object_mapping: FullObjectMapping,
    enabled_extra_types: Optional[Set[str]] = None
) -> ExtraCheckResults:
    """
    基于 master_list (TABLE 映射) 检查：
      - 索引
      - 约束 (PK/UK/FK/CHECK)
      - 触发器
    基于 schema 映射检查：
      - 序列
    """
    extra_results: ExtraCheckResults = {
        "index_ok": [],
        "index_mismatched": [],
        "index_unsupported": [],
        "constraint_ok": [],
        "constraint_mismatched": [],
        "constraint_unsupported": [],
        "sequence_ok": [],
        "sequence_mismatched": [],
        "trigger_ok": [],
        "trigger_mismatched": [],
        "trigger_status_drift": [],
        "constraint_status_drift": [],
    }

    enabled_types = {t.upper() for t in (enabled_extra_types or set(EXTRA_OBJECT_CHECK_TYPES))}
    table_check_types = enabled_types & {"INDEX", "CONSTRAINT", "TRIGGER"}

    if not enabled_types:
        log.info("已根据配置跳过扩展对象校验 (索引/约束/序列/触发器)。")
        return extra_results
    if not master_list and table_check_types:
        log.info("主校验清单为空，已跳过 INDEX/CONSTRAINT/TRIGGER 扩展校验，仅保留 SEQUENCE。")

    log_subsection("扩展对象校验 (索引/约束/序列/触发器)")

    # 1) 针对每个 TABLE 做索引/约束/触发器校验
    if 'TRIGGER' in table_check_types:
        ensure_trigger_mappings_for_extra_checks(master_list, oracle_meta, full_object_mapping)

    table_entries: List[TableEntry] = []
    if table_check_types:
        for src_name, tgt_name, obj_type in master_list:
            if (obj_type or "").upper() != 'TABLE':
                continue
            try:
                src_schema, src_table = src_name.split('.', 1)
                tgt_schema, tgt_table = tgt_name.split('.', 1)
            except ValueError:
                continue
            table_entries.append((
                src_schema.upper(),
                src_table.upper(),
                tgt_schema.upper(),
                tgt_table.upper()
            ))

    table_entries.sort(key=lambda item: (item[2], item[3]))
    total_tables = len(table_entries)
    idx_time_sum = 0.0
    cons_time_sum = 0.0
    trg_time_sum = 0.0

    if total_tables:
        worker_count = max(1, int(settings.get('extra_check_workers', 1) or 1))
        worker_count = min(worker_count, max(1, total_tables))
        chunk_size = max(1, int(settings.get('extra_check_chunk_size', 200) or 200))
        try:
            progress_interval = float(settings.get('extra_check_progress_interval', 10))
        except (TypeError, ValueError):
            progress_interval = 10.0
        progress_interval = max(1.0, progress_interval)

        if worker_count <= 1:
            log.info("[EXTRA] 并发未启用 (worker=1)，可通过 extra_check_workers=16 提升速度。")
        else:
            log.info(
                "[EXTRA] 启用并发校验 (worker=%d, chunk=%d)。",
                worker_count,
                chunk_size
            )
        log.info(
            "[EXTRA] 进度日志间隔 %.0f 秒，可通过 extra_check_progress_interval 配置。",
            progress_interval
        )

        start_ts = time.monotonic()
        last_log = start_ts
        done_tables = 0

        def _accumulate_result(result: ExtraTableResult) -> None:
            nonlocal idx_time_sum, cons_time_sum, trg_time_sum
            if result.index_ok is True:
                extra_results["index_ok"].append(result.tgt_name)
            elif result.index_ok is False and result.index_mismatch:
                extra_results["index_mismatched"].append(result.index_mismatch)
            if result.constraint_ok is True:
                extra_results["constraint_ok"].append(result.tgt_name)
            elif result.constraint_ok is False and result.constraint_mismatch:
                extra_results["constraint_mismatched"].append(result.constraint_mismatch)
            if result.trigger_ok is True:
                extra_results["trigger_ok"].append(result.tgt_name)
            elif result.trigger_ok is False and result.trigger_mismatch:
                extra_results["trigger_mismatched"].append(result.trigger_mismatch)
            idx_time_sum += result.index_time
            cons_time_sum += result.constraint_time
            trg_time_sum += result.trigger_time

        if worker_count > 1:
            use_process_pool = total_tables <= EXTRA_CHECK_PROCESS_MAX_TABLES
            if not use_process_pool:
                log.info(
                    "[EXTRA] 表数量=%d，禁用多进程以避免元数据复制过大，改用线程池。",
                    total_tables
                )
            if use_process_pool:
                with ProcessPoolExecutor(
                    max_workers=worker_count,
                    initializer=_init_extra_check_worker,
                    initargs=(oracle_meta, ob_meta, full_object_mapping, table_check_types)
                ) as executor:
                    future_to_entry = {
                        executor.submit(_extra_check_worker, entry): entry
                        for entry in table_entries
                    }
                    for future in as_completed(future_to_entry):
                        entry = future_to_entry[future]
                        try:
                            result = future.result()
                        except Exception as exc:
                            log.warning(
                                "[EXTRA] 并发任务失败，已回退串行重试 %s.%s: %s",
                                entry[2],
                                entry[3],
                                exc
                            )
                            result = run_extra_check_for_table(
                                entry,
                                oracle_meta,
                                ob_meta,
                                full_object_mapping,
                                table_check_types
                            )
                        done_tables += 1
                        _accumulate_result(result)
                        now = time.monotonic()
                        if done_tables == total_tables or (now - last_log) >= progress_interval:
                            pct = done_tables * 100.0 / total_tables if total_tables else 100.0
                            rate = done_tables / max(1e-6, now - start_ts)
                            log.info(
                                "扩展校验进度 %d/%d (%.1f%%, %.1f 表/秒)...",
                                done_tables, total_tables, pct, rate
                            )
                            last_log = now
            else:
                with ThreadPoolExecutor(max_workers=worker_count) as executor:
                    future_to_entry = {
                        executor.submit(
                            run_extra_check_for_table,
                            entry,
                            oracle_meta,
                            ob_meta,
                            full_object_mapping,
                            table_check_types
                        ): entry
                        for entry in table_entries
                    }
                    for future in as_completed(future_to_entry):
                        entry = future_to_entry[future]
                        try:
                            result = future.result()
                        except Exception as exc:
                            log.warning(
                                "[EXTRA] 线程任务失败，已回退串行重试 %s.%s: %s",
                                entry[2],
                                entry[3],
                                exc
                            )
                            result = run_extra_check_for_table(
                                entry,
                                oracle_meta,
                                ob_meta,
                                full_object_mapping,
                                table_check_types
                            )
                        done_tables += 1
                        _accumulate_result(result)
                        now = time.monotonic()
                        if done_tables == total_tables or (now - last_log) >= progress_interval:
                            pct = done_tables * 100.0 / total_tables if total_tables else 100.0
                            rate = done_tables / max(1e-6, now - start_ts)
                            log.info(
                                "扩展校验进度 %d/%d (%.1f%%, %.1f 表/秒)...",
                                done_tables, total_tables, pct, rate
                            )
                            last_log = now
        else:
            for entry in table_entries:
                result = run_extra_check_for_table(
                    entry, oracle_meta, ob_meta, full_object_mapping, table_check_types
                )
                done_tables += 1
                _accumulate_result(result)
                now = time.monotonic()
                if done_tables == total_tables or (now - last_log) >= progress_interval:
                    pct = done_tables * 100.0 / total_tables if total_tables else 100.0
                    rate = done_tables / max(1e-6, now - start_ts)
                    log.info(
                        "扩展校验进度 %d/%d (%.1f%%, %.1f 表/秒)...",
                        done_tables, total_tables, pct, rate
                    )
                    last_log = now

        wall_elapsed = time.monotonic() - start_ts
        log.info(
            "[EXTRA] 扩展对象校验完成: TABLE=%d, wall=%.1fs, INDEX=%.1fs, CONSTRAINT=%.1fs, TRIGGER=%.1fs (sum)",
            total_tables,
            wall_elapsed,
            idx_time_sum,
            cons_time_sum,
            trg_time_sum
        )

    # 2) 序列校验（考虑 remap 后的目标 schema）
    sequence_groups: Dict[Tuple[str, str], List[Tuple[str, str]]] = defaultdict(list)
    tgt_schema_all_expected: Dict[str, Set[str]] = defaultdict(set)
    if 'SEQUENCE' in enabled_types:
        for src_schema, seq_names in oracle_meta.sequences.items():
            src_schema_u = src_schema.upper()
            for seq_name in seq_names:
                seq_name_u = seq_name.upper()
                src_full = f"{src_schema_u}.{seq_name_u}"
                mapped = get_mapped_target(full_object_mapping, src_full, 'SEQUENCE')
                tgt_full = mapped or src_full
                if '.' not in tgt_full:
                    tgt_schema_u = src_schema_u
                    tgt_name_u = seq_name_u
                else:
                    tgt_schema_u, tgt_name_u = tgt_full.split('.', 1)
                    tgt_schema_u = tgt_schema_u.upper()
                    tgt_name_u = tgt_name_u.upper()
                sequence_groups[(src_schema_u, tgt_schema_u)].append((seq_name_u, tgt_name_u))
                tgt_schema_all_expected[tgt_schema_u].add(tgt_name_u)

        for (src_schema_u, tgt_schema_u), entries in sequence_groups.items():
            expected_tgt_names = {tgt_name for _, tgt_name in entries}
            actual_tgt_names = {name.upper() for name in ob_meta.sequences.get(tgt_schema_u, set())}
            all_expected_for_tgt = tgt_schema_all_expected[tgt_schema_u]

            missing_src = {
                src_name for src_name, tgt_name in entries
                if tgt_name not in actual_tgt_names
            }
            extra_tgt = actual_tgt_names - all_expected_for_tgt

            mapping_label = f"{src_schema_u}->{tgt_schema_u}"
            if not missing_src and not extra_tgt:
                extra_results["sequence_ok"].append(mapping_label)
            else:
                missing_map = [
                    (f"{src_schema_u}.{src_name}", f"{tgt_schema_u}.{tgt_name}")
                    for src_name, tgt_name in entries
                    if tgt_name not in actual_tgt_names
                ]
                extra_results["sequence_mismatched"].append(SequenceMismatch(
                    src_schema=src_schema_u,
                    tgt_schema=tgt_schema_u,
                    missing_sequences=missing_src,
                    extra_sequences=extra_tgt,
                    note=None,
                    missing_mappings=missing_map,
                    detail_mismatch=None
                ))

    extra_results = normalize_extra_results_names(extra_results)

    index_unsupported = classify_unsupported_indexes(
        extra_results,
        oracle_meta,
        build_table_target_map(master_list) if master_list else None
    )
    extra_results["index_unsupported"] = index_unsupported

    constraint_unsupported = classify_unsupported_check_constraints(
        extra_results,
        oracle_meta,
        build_table_target_map(master_list) if master_list else None
    )
    extra_results["constraint_unsupported"] = constraint_unsupported
    return extra_results


# ====================== 注释一致性检查 ======================

def check_comments(
    master_list: MasterCheckList,
    oracle_meta: OracleMetadata,
    ob_meta: ObMetadata,
    enable_comment_check: bool = True
) -> Dict[str, object]:
    results: Dict[str, object] = {
        "ok": [],
        "mismatched": [],
        "skipped_reason": None
    }

    if not enable_comment_check:
        results["skipped_reason"] = "根据配置关闭注释比对。"
        return results

    if not master_list:
        results["skipped_reason"] = "无表对象可供注释比对。"
        return results

    if not any(obj_type.upper() == 'TABLE' for _, _, obj_type in master_list):
        results["skipped_reason"] = "当前清单未包含 TABLE，对应注释比对已跳过。"
        return results

    if not oracle_meta.comments_complete:
        results["skipped_reason"] = "未成功加载 Oracle 注释元数据，已跳过注释比对。"
        return results

    if not ob_meta.comments_complete:
        results["skipped_reason"] = "未成功加载 OceanBase 注释元数据，已跳过注释比对。"
        return results

    for src_name, tgt_name, obj_type in master_list:
        if obj_type.upper() != 'TABLE':
            continue
        try:
            src_schema, src_table = src_name.split('.')
            tgt_schema, tgt_table = tgt_name.split('.')
        except ValueError:
            continue

        src_key = (src_schema.upper(), src_table.upper())
        tgt_key = (tgt_schema.upper(), tgt_table.upper())
        ob_tables = ob_meta.objects_by_type.get("TABLE", set())
        if ob_tables:
            full_tgt = f"{tgt_key[0]}.{tgt_key[1]}"
            if full_tgt not in ob_tables:
                continue

        src_table_cmt = normalize_comment_text(oracle_meta.table_comments.get(src_key))
        tgt_table_cmt = normalize_comment_text(ob_meta.table_comments.get(tgt_key))
        table_diff = src_table_cmt != tgt_table_cmt

        src_col_cmts = oracle_meta.column_comments.get(src_key, {})
        tgt_col_cmts = ob_meta.column_comments.get(tgt_key, {})

        src_col_meta = oracle_meta.table_columns.get(src_key, {}) or {}
        hidden_src_cols = {c for c, m in src_col_meta.items() if m.get("hidden")}

        # 过滤OMS列后计算缺失和额外的列注释
        missing_cols = {
            col for col in src_col_cmts.keys()
            if col not in tgt_col_cmts
            and col not in hidden_src_cols
            and not is_ignored_oms_column(col)
        }
        extra_cols = {
            col for col in tgt_col_cmts.keys()
            if col not in src_col_cmts and not is_ignored_oms_column(col)
        }

        # 额外验证：确保OMS列被完全过滤
        oms_filtered_extra = {col for col in extra_cols if not is_ignored_oms_column(col)}
        if len(oms_filtered_extra) != len(extra_cols):
            log.debug("表 %s.%s: 从额外列注释中过滤了 %d 个OMS列", 
                     tgt_key[0], tgt_key[1], len(extra_cols) - len(oms_filtered_extra))
            extra_cols = oms_filtered_extra

        column_diffs: List[Tuple[str, str, str]] = []
        for col in (src_col_cmts.keys() & tgt_col_cmts.keys()):
            if is_ignored_oms_column(col) or col in hidden_src_cols:
                continue
            src_cmt = normalize_comment_text(src_col_cmts.get(col))
            tgt_cmt = normalize_comment_text(tgt_col_cmts.get(col))
            if src_cmt != tgt_cmt:
                column_diffs.append((col, src_cmt, tgt_cmt))

        if table_diff or column_diffs or missing_cols or extra_cols:
            results["mismatched"].append(CommentMismatch(
                table=f"{tgt_key[0]}.{tgt_key[1]}",
                table_comment=(src_table_cmt, tgt_table_cmt) if table_diff else None,
                column_comment_diffs=column_diffs,
                missing_columns=missing_cols,
                extra_columns=extra_cols
            ))
        else:
            results["ok"].append(f"{tgt_key[0]}.{tgt_key[1]}")

    return results


# ====================== VIEW/SYNONYM 可用性校验 ======================

def classify_usability_status(
    src_checked: bool,
    src_usable: Optional[bool],
    tgt_usable: Optional[bool],
    timeout: bool = False
) -> str:
    if timeout:
        return USABILITY_STATUS_TIMEOUT
    if tgt_usable is True:
        if src_checked and src_usable is False:
            return USABILITY_STATUS_UNEXPECTED_USABLE
        return USABILITY_STATUS_OK
    if tgt_usable is False:
        if src_checked and src_usable is False:
            return USABILITY_STATUS_EXPECTED_UNUSABLE
        return USABILITY_STATUS_UNUSABLE
    return USABILITY_STATUS_SKIPPED


def analyze_usability_error(error_msg: str) -> Tuple[str, str]:
    """
    返回 (root_cause, recommendation)
    """
    if not error_msg:
        return "-", "-"
    msg = normalize_error_text(error_msg)
    msg_upper = msg.upper()
    if "ORA-00942" in msg_upper or "ORA-04043" in msg_upper:
        return "依赖对象不存在", "检查依赖对象是否已迁移，必要时执行缺失对象修补"
    if "ORA-00980" in msg_upper:
        return "同义词指向对象无效", "重建同义词或创建目标对象"
    if "ORA-01775" in msg_upper:
        return "同义词循环引用", "检查并修复同义词链条"
    if "ORA-00904" in msg_upper:
        return "列或标识符不存在", "检查依赖表结构或视图定义"
    if "ORA-01031" in msg_upper:
        return "权限不足", "授予查询权限或执行 grant 修补脚本"
    if "ORA-04063" in msg_upper:
        return "视图查询报错", "检查视图依赖对象或重新编译视图"
    if "TIMEOUT" in msg_upper or "DPY-4011" in msg_upper or "ORA-01013" in msg_upper:
        return "查询超时", "可增加超时或人工验证"
    if "ORA-00600" in msg_upper:
        return "OceanBase 内部错误", "检查 OB 日志或联系 DBA"
    if "ORA-00900" in msg_upper:
        return "SQL 语法错误", "检查 DDL 兼容性或清洗规则"
    return "未知错误", "查看错误信息并人工定位"


def _lookup_support_row(
    support_state_map: Optional[Dict[Tuple[str, str], ObjectSupportReportRow]],
    obj_type: str,
    src_full: str
) -> Optional[ObjectSupportReportRow]:
    if not support_state_map:
        return None
    obj_type_u = (obj_type or "").upper()
    src_key = (src_full or "").upper()
    return support_state_map.get((obj_type_u, src_key))


def _format_support_row_reason(row: ObjectSupportReportRow) -> Tuple[str, str]:
    parts: List[str] = []
    if row.reason_code and row.reason_code != "-":
        parts.append(row.reason_code)
    if row.reason and row.reason != "-":
        parts.append(row.reason)
    if row.dependency and row.dependency != "-":
        parts.append(f"依赖:{row.dependency}")
    if row.detail and row.detail != "-":
        parts.append(f"detail:{row.detail}")
    if row.root_cause and row.root_cause != "-":
        parts.append(f"root:{row.root_cause}")
    root_cause = " ; ".join(parts) if parts else "对象不支持/阻断"

    rec_parts: List[str] = []
    if row.action and row.action != "-":
        rec_parts.append(row.action)
    if row.reason and row.reason != "-":
        rec_parts.append(row.reason)
    if row.dependency and row.dependency != "-":
        rec_parts.append(f"依赖:{row.dependency}")
    recommendation = " ; ".join(rec_parts) if rec_parts else "-"
    return root_cause, recommendation


def _infer_ref_types_from_source(
    source_objects: Optional[SourceObjectMap],
    ref_full: str
) -> Set[str]:
    if not source_objects or not ref_full:
        return set()
    return {t.upper() for t in (source_objects.get(ref_full.upper()) or set())}


def _collect_usability_dependencies(
    obj_type_u: str,
    src_full: str,
    dependency_graph: Optional[DependencyGraph],
    synonym_meta_map: Optional[Dict[Tuple[str, str], SynonymMeta]],
    source_objects: Optional[SourceObjectMap]
) -> List[Tuple[str, str]]:
    obj_type_u = (obj_type_u or "").upper()
    src_full_u = (src_full or "").upper()
    deps: List[Tuple[str, str]] = []
    if obj_type_u == "VIEW" and dependency_graph:
        for ref_full, ref_type in sorted(dependency_graph.get((src_full_u, "VIEW"), set())):
            if ref_full:
                deps.append((ref_full.upper(), (ref_type or "").upper()))
        return deps
    if obj_type_u == "SYNONYM" and synonym_meta_map:
        parsed = parse_full_object_name(src_full_u)
        if parsed:
            meta = synonym_meta_map.get((parsed[0].upper(), parsed[1].upper()))
            if meta and meta.table_owner and meta.table_name:
                ref_full = f"{meta.table_owner.upper()}.{meta.table_name.upper()}"
                ref_types = _infer_ref_types_from_source(source_objects, ref_full)
                if ref_types:
                    for ref_type in sorted(ref_types):
                        deps.append((ref_full, ref_type))
                else:
                    deps.append((ref_full, ""))
        return deps
    return deps


def _resolve_dependency_target(
    ref_full: str,
    ref_type: str,
    full_object_mapping: Optional[FullObjectMapping]
) -> Optional[str]:
    if not ref_full or not full_object_mapping:
        return None
    if ref_type:
        return get_mapped_target(full_object_mapping, ref_full, ref_type)
    return find_mapped_target_any_type(
        full_object_mapping,
        ref_full,
        ("TABLE", "VIEW", "MATERIALIZED VIEW", "SYNONYM", "SEQUENCE", "PACKAGE", "FUNCTION", "PROCEDURE", "TYPE")
    )


def _pick_dependency_issue(
    deps: List[Tuple[str, str]],
    full_object_mapping: Optional[FullObjectMapping],
    support_state_map: Optional[Dict[Tuple[str, str], ObjectSupportReportRow]],
    missing_targets: Set[Tuple[str, str]]
) -> Optional[Tuple[str, str]]:
    if not deps:
        return None
    for ref_full, ref_type in deps:
        support_row = None
        if support_state_map:
            if ref_type:
                support_row = support_state_map.get((ref_type, ref_full))
            else:
                for cand_type in ("TABLE", "VIEW", "MATERIALIZED VIEW", "SYNONYM"):
                    support_row = support_state_map.get((cand_type, ref_full))
                    if support_row:
                        break
        if support_row and support_row.support_state != SUPPORT_STATE_SUPPORTED:
            root_cause, recommendation = _format_support_row_reason(support_row)
            return (f"依赖对象不支持/阻断: {ref_full} ; {root_cause}", recommendation)

        mapped = _resolve_dependency_target(ref_full, ref_type, full_object_mapping)
        mapped_u = mapped.upper() if mapped else ""
        if mapped_u:
            if ref_type:
                if (ref_type, mapped_u) in missing_targets:
                    return (f"依赖对象缺失: {mapped_u}", "先修补依赖对象后再校验")
            else:
                for cand_type in ("TABLE", "VIEW", "MATERIALIZED VIEW", "SYNONYM"):
                    if (cand_type, mapped_u) in missing_targets:
                        return (f"依赖对象缺失: {mapped_u}", "先修补依赖对象后再校验")
        else:
            return (f"依赖对象未纳入受管范围: {ref_full}", "补充 remap 或纳入范围后再校验")
    return None


def _build_usability_query(full_name: str, obj_type: Optional[str] = None) -> Optional[str]:
    parsed = parse_full_object_name(full_name)
    if not parsed:
        return None
    schema, obj = parsed
    obj_type_u = (obj_type or "").upper()
    schema_u = (schema or "").upper()
    # PUBLIC 同义词在 Oracle/OB 中都不支持 schema 形式引用，需使用非限定名
    if obj_type_u == "SYNONYM" and schema_u == "PUBLIC":
        return f"SELECT * FROM {quote_identifier(obj)} WHERE 1=2"
    return f"SELECT * FROM {quote_qualified_parts(schema, obj)} WHERE 1=2"


def _compute_usability_sample(
    total: int,
    max_objects: int,
    sample_ratio: float
) -> Tuple[int, int, bool]:
    if total <= 0:
        return 0, 0, False
    if max_objects <= 0 or sample_ratio <= 0 or sample_ratio >= 1:
        return total, 0, False
    if total <= max_objects:
        return total, 0, False
    sample_cnt = max(1, int(total * sample_ratio))
    if sample_cnt > max_objects:
        sample_cnt = max_objects
    return sample_cnt, max(0, total - sample_cnt), True


def check_object_usability(
    settings: Dict,
    master_list: MasterCheckList,
    tv_results: ReportResults,
    ob_cfg: ObConfig,
    ora_cfg: OraConfig,
    ob_meta: ObMetadata,
    enabled_primary_types: Set[str],
    support_summary: Optional[SupportClassificationResult] = None,
    dependency_graph: Optional[DependencyGraph] = None,
    source_objects: Optional[SourceObjectMap] = None,
    full_object_mapping: Optional[FullObjectMapping] = None,
    synonym_meta_map: Optional[Dict[Tuple[str, str], SynonymMeta]] = None
) -> Optional[UsabilitySummary]:
    if not settings:
        return None
    if not parse_bool_flag(settings.get("check_object_usability", "false"), False):
        return None
    if not master_list:
        return None

    target_types = {"VIEW", "SYNONYM"}
    if not (enabled_primary_types & target_types):
        log.info("[USABILITY] VIEW/SYNONYM 未启用主对象检查，可用性校验跳过。")
        return None

    check_source = parse_bool_flag(settings.get("check_source_usability", "true"), True)
    try:
        timeout_sec = int(settings.get("usability_check_timeout", "10"))
    except (TypeError, ValueError):
        timeout_sec = 10
    if timeout_sec <= 0:
        timeout_sec = 10
    try:
        workers = int(settings.get("usability_check_workers", "10"))
    except (TypeError, ValueError):
        workers = 10
    if workers <= 0:
        workers = 10
    try:
        max_objects = int(settings.get("max_usability_objects", "0"))
    except (TypeError, ValueError):
        max_objects = 0
    try:
        sample_ratio = float(settings.get("usability_sample_ratio", "0"))
    except (TypeError, ValueError):
        sample_ratio = 0.0

    missing_targets = {
        (obj_type.upper(), tgt_full.upper())
        for obj_type, _src_full, tgt_full in (tv_results.get("missing") or [])
    }
    support_state_map = support_summary.support_state_map if support_summary else {}
    unsupported_views = support_summary.unsupported_view_keys if support_summary else set()
    source_objects = source_objects or {}
    synonym_meta_map = synonym_meta_map or {}

    candidates: List[Tuple[str, str, str]] = []
    results: List[UsabilityCheckResult] = []
    for src_full, tgt_full, obj_type in master_list:
        obj_type_u = (obj_type or "").upper()
        if obj_type_u not in target_types:
            continue
        if obj_type_u not in enabled_primary_types:
            continue
        src_parsed = parse_full_object_name(src_full)
        tgt_parsed = parse_full_object_name(tgt_full)
        if not src_parsed or not tgt_parsed:
            results.append(UsabilityCheckResult(
                schema="",
                object_name=src_full,
                object_type=obj_type_u,
                src_exists=False,
                src_usable=None,
                tgt_exists=False,
                tgt_usable=None,
                status=USABILITY_STATUS_SKIPPED,
                src_error="-",
                tgt_error="-",
                root_cause="对象名无法解析",
                recommendation="检查映射或对象名格式",
                src_time_ms=0,
                tgt_time_ms=0
            ))
            continue
        src_schema, src_obj = src_parsed
        tgt_schema, tgt_obj = tgt_parsed
        tgt_key = f"{tgt_schema}.{tgt_obj}"
        if (obj_type_u, tgt_key) in missing_targets:
            support_row = _lookup_support_row(
                support_state_map,
                obj_type_u,
                f"{src_schema}.{src_obj}"
            )
            if support_row and support_row.support_state != SUPPORT_STATE_SUPPORTED:
                root_cause, recommendation = _format_support_row_reason(support_row)
                results.append(UsabilityCheckResult(
                    schema=tgt_schema,
                    object_name=tgt_obj,
                    object_type=obj_type_u,
                    src_exists=True,
                    src_usable=None,
                    tgt_exists=False,
                    tgt_usable=None,
                    status=USABILITY_STATUS_SKIPPED,
                    src_error="-",
                    tgt_error="-",
                    root_cause=root_cause,
                    recommendation=recommendation,
                    src_time_ms=0,
                    tgt_time_ms=0
                ))
                continue
            results.append(UsabilityCheckResult(
                schema=tgt_schema,
                object_name=tgt_obj,
                object_type=obj_type_u,
                src_exists=True,
                src_usable=None,
                tgt_exists=False,
                tgt_usable=None,
                status=USABILITY_STATUS_SKIPPED,
                src_error="-",
                tgt_error="-",
                root_cause="目标端对象缺失",
                recommendation="先修补缺失对象再校验可用性",
                src_time_ms=0,
                tgt_time_ms=0
            ))
            continue
        if obj_type_u == "VIEW" and (src_schema.upper(), src_obj.upper()) in unsupported_views:
            support_row = _lookup_support_row(
                support_state_map,
                obj_type_u,
                f"{src_schema}.{src_obj}"
            )
            if support_row and support_row.support_state != SUPPORT_STATE_SUPPORTED:
                root_cause, recommendation = _format_support_row_reason(support_row)
            else:
                root_cause, recommendation = "视图被标记为不支持/需改造", "先处理视图兼容性问题"
            results.append(UsabilityCheckResult(
                schema=tgt_schema,
                object_name=tgt_obj,
                object_type=obj_type_u,
                src_exists=True,
                src_usable=None,
                tgt_exists=True,
                tgt_usable=None,
                status=USABILITY_STATUS_SKIPPED,
                src_error="-",
                tgt_error="-",
                root_cause=root_cause,
                recommendation=recommendation,
                src_time_ms=0,
                tgt_time_ms=0
            ))
            continue
        if support_state_map:
            support_row = _lookup_support_row(
                support_state_map,
                obj_type_u,
                f"{src_schema}.{src_obj}"
            )
            if support_row and support_row.support_state != SUPPORT_STATE_SUPPORTED:
                root_cause, recommendation = _format_support_row_reason(support_row)
                results.append(UsabilityCheckResult(
                    schema=tgt_schema,
                    object_name=tgt_obj,
                    object_type=obj_type_u,
                    src_exists=True,
                    src_usable=None,
                    tgt_exists=True,
                    tgt_usable=None,
                    status=USABILITY_STATUS_SKIPPED,
                    src_error="-",
                    tgt_error="-",
                    root_cause=root_cause,
                    recommendation=recommendation,
                    src_time_ms=0,
                    tgt_time_ms=0
                ))
                continue
        candidates.append((src_full, tgt_full, obj_type_u))

    total_candidates = len(candidates)
    if total_candidates == 0:
        return UsabilitySummary(
            total_candidates=0,
            total_checked=0,
            total_usable=0,
            total_unusable=0,
            total_expected_unusable=0,
            total_unexpected_usable=0,
            total_timeout=0,
            total_skipped=len(results),
            total_sampled_out=0,
            duration_seconds=0.0,
            results=results
        )

    sample_count, sampled_out, sampled = _compute_usability_sample(
        total_candidates,
        max_objects,
        sample_ratio
    )
    if sampled:
        rng = random.Random(0)
        candidates = rng.sample(candidates, sample_count)
        log.info(
            "[USABILITY] 启用抽样：候选 %d，实际校验 %d，跳过 %d。",
            total_candidates,
            sample_count,
            sampled_out
        )

    ob_views = ob_meta.objects_by_type.get("VIEW", set()) if ob_meta else set()
    ob_synonyms = ob_meta.objects_by_type.get("SYNONYM", set()) if ob_meta else set()

    oracle_conns: List[oracledb.Connection] = []
    oracle_conn_lock = threading.Lock()
    oracle_local = threading.local()

    def _get_oracle_conn() -> oracledb.Connection:
        conn = getattr(oracle_local, "conn", None)
        if conn is None:
            conn = oracledb.connect(
                user=ora_cfg["user"],
                password=ora_cfg["password"],
                dsn=ora_cfg["dsn"]
            )
            with oracle_conn_lock:
                oracle_conns.append(conn)
            oracle_local.conn = conn
        return conn

    def _oracle_check(full_name: str, obj_type: str) -> Tuple[Optional[bool], str, int, bool]:
        sql = _build_usability_query(full_name, obj_type)
        if not sql:
            return None, "INVALID_NAME", 0, False
        start = time.perf_counter()
        try:
            conn = _get_oracle_conn()
            cur = conn.cursor()
            if timeout_sec:
                cur.call_timeout = int(timeout_sec * 1000)
            cur.execute(sql)
            cur.close()
            return True, "", int((time.perf_counter() - start) * 1000), False
        except Exception as exc:
            msg = normalize_error_text(str(exc))
            timeout_hit = any(token in msg.upper() for token in ("DPY-4011", "ORA-01013", "TIMEOUT"))
            return False, msg, int((time.perf_counter() - start) * 1000), timeout_hit

    def _ob_check(full_name: str, obj_type: str) -> Tuple[Optional[bool], str, int, bool]:
        sql = _build_usability_query(full_name, obj_type)
        if not sql:
            return None, "INVALID_NAME", 0, False
        start = time.perf_counter()
        ok, _out, err = obclient_run_sql(ob_cfg, sql, timeout=timeout_sec)
        duration_ms = int((time.perf_counter() - start) * 1000)
        if ok:
            return True, "", duration_ms, False
        err_msg = normalize_error_text(err)
        timeout_hit = "TIMEOUT" in err_msg.upper() or "TIMEOUTEXPIRED" in err_msg.upper()
        return False, err_msg, duration_ms, timeout_hit

    def _check_one(item: Tuple[str, str, str]) -> UsabilityCheckResult:
        src_full, tgt_full, obj_type_u = item
        src_schema, src_obj = parse_full_object_name(src_full) or ("", src_full)
        tgt_schema, tgt_obj = parse_full_object_name(tgt_full) or ("", tgt_full)
        tgt_exists = False
        if obj_type_u == "VIEW":
            tgt_exists = tgt_full.upper() in ob_views
        elif obj_type_u == "SYNONYM":
            tgt_exists = tgt_full.upper() in ob_synonyms
        if not tgt_exists:
            return UsabilityCheckResult(
                schema=tgt_schema,
                object_name=tgt_obj,
                object_type=obj_type_u,
                src_exists=True,
                src_usable=None,
                tgt_exists=False,
                tgt_usable=None,
                status=USABILITY_STATUS_SKIPPED,
                src_error="-",
                tgt_error="-",
                root_cause="目标端对象缺失",
                recommendation="先修补缺失对象再校验可用性",
                src_time_ms=0,
                tgt_time_ms=0
            )

        tgt_usable, tgt_err, tgt_ms, tgt_timeout = _ob_check(tgt_full, obj_type_u)
        src_usable = None
        src_err = "-"
        src_ms = 0
        src_timeout = False
        if check_source:
            src_usable, src_err, src_ms, src_timeout = _oracle_check(src_full, obj_type_u)

        timeout_hit = tgt_timeout or src_timeout
        status = classify_usability_status(check_source, src_usable, tgt_usable, timeout_hit)
        if status == USABILITY_STATUS_TIMEOUT:
            root_cause, recommendation = "查询超时", "可增加超时或人工验证"
        elif status == USABILITY_STATUS_SKIPPED:
            root_cause, recommendation = "跳过校验", "-"
        else:
            err_msg = tgt_err if tgt_usable is False else src_err if src_usable is False else ""
            root_cause, recommendation = analyze_usability_error(err_msg)
            deps = _collect_usability_dependencies(
                obj_type_u,
                src_full,
                dependency_graph,
                synonym_meta_map,
                source_objects
            )
            issue = _pick_dependency_issue(
                deps,
                full_object_mapping,
                support_state_map,
                missing_targets
            )
            msg_upper = normalize_error_text(err_msg).upper() if err_msg else ""
            if issue:
                root_cause, recommendation = issue
            elif "ORA-01031" in msg_upper and deps:
                dep_targets: List[str] = []
                for ref_full, ref_type in deps[:5]:
                    mapped = _resolve_dependency_target(ref_full, ref_type, full_object_mapping)
                    dep_targets.append(mapped or ref_full)
                if dep_targets:
                    uniq_targets = sorted({t.upper() for t in dep_targets if t})
                    privs: Set[str] = set()
                    for _ref_full, ref_type in deps[:5]:
                        privs.add(GRANT_PRIVILEGE_BY_TYPE.get(ref_type or "TABLE", "SELECT"))
                    priv_list = ",".join(sorted(privs)) if privs else "SELECT"
                    root_cause = f"权限不足: 依赖对象 {','.join(uniq_targets[:5])}"
                    recommendation = f"为 {tgt_schema.upper()} 授予 {priv_list} 权限 (对象: {','.join(uniq_targets[:5])})"
            elif obj_type_u == "SYNONYM" and deps:
                ref_full, ref_type = deps[0]
                mapped = _resolve_dependency_target(ref_full, ref_type, full_object_mapping) or ref_full
                root_cause = f"{root_cause}; 同义词指向 {mapped}"
                if "ORA-00980" in msg_upper:
                    recommendation = f"{recommendation}; 检查并重建同义词指向 {mapped}"
            elif deps:
                dep_list = ", ".join(ref_full for ref_full, _ in deps[:3])
                root_cause = f"{root_cause}; 依赖:{dep_list}"

        return UsabilityCheckResult(
            schema=tgt_schema,
            object_name=tgt_obj,
            object_type=obj_type_u,
            src_exists=True,
            src_usable=src_usable,
            tgt_exists=True,
            tgt_usable=tgt_usable,
            status=status,
            src_error=src_err or "-",
            tgt_error=tgt_err or "-",
            root_cause=root_cause,
            recommendation=recommendation,
            src_time_ms=src_ms,
            tgt_time_ms=tgt_ms
        )

    start_perf = time.perf_counter()
    with ThreadPoolExecutor(max_workers=workers) as executor:
        futures = [executor.submit(_check_one, item) for item in candidates]
        for future in as_completed(futures):
            results.append(future.result())

    for conn in oracle_conns:
        try:
            conn.close()
        except Exception:
            pass

    total_checked = len([r for r in results if r.status not in {USABILITY_STATUS_SKIPPED}])
    total_usable = len([r for r in results if r.status == USABILITY_STATUS_OK])
    total_unusable = len([r for r in results if r.status == USABILITY_STATUS_UNUSABLE])
    total_expected_unusable = len([r for r in results if r.status == USABILITY_STATUS_EXPECTED_UNUSABLE])
    total_unexpected_usable = len([r for r in results if r.status == USABILITY_STATUS_UNEXPECTED_USABLE])
    total_timeout = len([r for r in results if r.status == USABILITY_STATUS_TIMEOUT])
    total_skipped = len([r for r in results if r.status == USABILITY_STATUS_SKIPPED]) + sampled_out

    return UsabilitySummary(
        total_candidates=total_candidates,
        total_checked=total_checked,
        total_usable=total_usable,
        total_unusable=total_unusable,
        total_expected_unusable=total_expected_unusable,
        total_unexpected_usable=total_unexpected_usable,
        total_timeout=total_timeout,
        total_skipped=total_skipped,
        total_sampled_out=sampled_out,
        duration_seconds=time.perf_counter() - start_perf,
        results=results
    )


# ====================== DDL 抽取 & ALTER 级别修补 ======================

def parse_oracle_dsn(dsn: str) -> Tuple[str, str, Optional[str]]:
    try:
        host_port, service = dsn.split('/', 1)
        host, port = host_port.split(':', 1)
        return host.strip(), port.strip(), service.strip()
    except ValueError:
        log.error("严重错误: 无法解析 Oracle DSN (host:port/service_name): %s", dsn)
        abort_run()


def collect_oracle_env_info(ora_cfg: OraConfig) -> Dict[str, str]:
    """采集 Oracle 端基本信息（版本/容器/服务名/地址/用户）。"""
    info: Dict[str, str] = {}
    info["user"] = ora_cfg.get("user", "")
    info["dsn"] = ora_cfg.get("dsn", "")
    try:
        host, port, service = parse_oracle_dsn(ora_cfg.get("dsn", ""))
        info["host"] = host
        info["port"] = port
        info["service_name"] = service
    except Exception:
        pass

    try:
        with oracledb.connect(
            user=ora_cfg["user"],
            password=ora_cfg["password"],
            dsn=ora_cfg["dsn"]
        ) as conn:
            info["version"] = getattr(conn, "version", "") or info.get("version", "")
            with conn.cursor() as cursor:
                try:
                    cursor.execute("SELECT sys_context('userenv','con_name') FROM dual")
                    row = cursor.fetchone()
                    if row and row[0]:
                        info["container"] = str(row[0])
                except Exception:
                    pass
                try:
                    cursor.execute("SELECT cdb FROM v$database")
                    row = cursor.fetchone()
                    if row and row[0] is not None:
                        info["cdb_mode"] = "CDB" if str(row[0]).strip().upper() == "YES" else "Non-CDB"
                except Exception:
                    pass
                try:
                    cursor.execute("SELECT sys_context('userenv','service_name') FROM dual")
                    row = cursor.fetchone()
                    if row and row[0]:
                        info["service_name"] = str(row[0])
                except Exception:
                    pass
    except Exception as exc:
        log.warning("Oracle 基本信息获取失败（将使用配置值作为兜底）：%s", exc)

    return info


def parse_ob_status_output(output: str) -> Dict[str, str]:
    """解析 obclient status 输出中的关键信息。"""
    info: Dict[str, str] = {}
    for line in (output or "").splitlines():
        if ":" not in line:
            continue
        key, val = line.split(":", 1)
        key = key.strip().lower()
        val = val.strip()
        if not val:
            continue
        if key.startswith("server version"):
            info["version"] = val
        elif key == "current database":
            info["current_database"] = val
        elif key == "connection id":
            info["connection_id"] = val
        elif key == "ssl":
            info["ssl"] = val
    return info


def collect_ob_env_info(ob_cfg: ObConfig) -> Dict[str, str]:
    """采集 OceanBase 端基本信息（版本/连接/用户）。"""
    configured_user = ob_cfg.get("user_string", "")
    info: Dict[str, str] = {
        "host": ob_cfg.get("host", ""),
        "port": ob_cfg.get("port", ""),
        "configured_user": configured_user,
        "current_user": configured_user
    }

    status_cmd = [
        ob_cfg["executable"],
        "-h", ob_cfg["host"],
        "-P", ob_cfg["port"],
        "-u", ob_cfg["user_string"],
        "-p" + ob_cfg["password"],
        "-e", "status"
    ]
    try:
        result = subprocess.run(
            status_cmd,
            capture_output=True,
            text=True,
            encoding="utf-8",
            errors="ignore",
            timeout=OBC_TIMEOUT
        )
        if result.returncode == 0:
            info.update(parse_ob_status_output(result.stdout))
        else:
            log.warning("obclient status 调用失败(%s): %s", result.returncode, result.stderr.strip())
    except Exception as exc:
        log.warning("获取 obclient status 信息失败（将尝试 SQL 查询）：%s", exc)

    if "version" not in info or not info["version"].strip():
        ok, stdout, _ = obclient_run_sql(ob_cfg, "SELECT OB_VERSION() FROM DUAL")
        if ok and stdout.strip():
            info["version"] = stdout.strip().splitlines()[0]

    version_raw = info.get("version", "")
    if version_raw:
        matches = list(re.finditer(r'\([^()]*\)', version_raw))
        if len(matches) > 1:
            second = matches[1]
            version_raw = version_raw[:second.start()] + version_raw[second.end():]
        info["version"] = " ".join(version_raw.split())

    return info


def resolve_dbcat_cli(settings: Dict) -> Path:
    bin_path = settings.get('dbcat_bin', '').strip()
    if not bin_path:
        log.error("严重错误: 未配置 dbcat_bin，请在 config.ini 的 [SETTINGS] 中指定 dbcat 目录。")
        abort_run()
    cli_path = Path(bin_path)
    if cli_path.is_dir():
        cli_path = cli_path / 'bin' / 'dbcat'
    if not cli_path.exists():
        log.error("严重错误: 找不到 dbcat 可执行文件: %s", cli_path)
        abort_run()
    return cli_path


def locate_dbcat_schema_dir(base_dir: Path, schema: str) -> Optional[Path]:
    schema_upper = schema.upper()
    if base_dir.name.upper().startswith(f"{schema_upper}_"):
        return base_dir
    direct = base_dir / schema
    if direct.exists():
        return direct
    for child in base_dir.iterdir():
        if not child.is_dir():
            continue
        if child.name.upper().startswith(f"{schema_upper}_"):
            return child
        candidate = child / schema
        if candidate.exists():
            return candidate
    return None


def find_dbcat_object_file(schema_dir: Path, object_type: str, object_name: str) -> Optional[Path]:
    name_upper = object_name.upper()
    hints = DBCAT_OUTPUT_DIR_HINTS.get(object_type.upper(), ())
    for hint in hints:
        candidate = schema_dir / hint / f"{name_upper}-schema.sql"
        if candidate.exists():
            return candidate
    matches = list(schema_dir.rglob(f"{name_upper}-schema.sql"))
    if matches:
        hint_upper = tuple(h.upper() for h in hints if h)
        for candidate in matches:
            parent_names = {parent.name.upper() for parent in candidate.parents}
            if hint_upper:
                if any(h in parent_names for h in hint_upper):
                    return candidate
            else:
                return candidate
    return None


def build_dbcat_file_index(schema_dir: Path) -> Dict[str, Dict[str, Path]]:
    """
    为 dbcat 输出构建一次性索引，避免对每个对象都遍历深目录。
    返回: {OBJECT_TYPE: {OBJECT_NAME: Path}}
    """
    index: Dict[str, Dict[str, Path]] = defaultdict(dict)
    try:
        for path in schema_dir.rglob("*-schema.sql"):
            name_upper = path.name.replace("-schema.sql", "").upper()
            obj_type = None
            for parent in path.parents:
                hint_type = DBCAT_DIR_TO_TYPE.get(parent.name.upper())
                if hint_type:
                    obj_type = hint_type
                    break
            if not obj_type:
                continue
            if name_upper not in index[obj_type]:
                index[obj_type][name_upper] = path
    except OSError as exc:
        log.warning("[dbcat] 遍历目录 %s 失败: %s", schema_dir, exc)
    return index


def build_dbcat_global_index(
    base_output: Path,
    schema_names: Set[str]
) -> Dict[str, Dict[str, Dict[str, Path]]]:
    """
    针对 dbcat_output 下所有历史 run 目录构建全局索引，降低深层遍历次数。
    返回: {SCHEMA: {OBJECT_TYPE: {OBJECT_NAME: Path}}}
    """
    schema_upper = {s.upper() for s in schema_names}
    global_index: Dict[str, Dict[str, Dict[str, Path]]] = defaultdict(lambda: defaultdict(dict))
    if not base_output.exists():
        return {}

    try:
        for path in base_output.rglob("*-schema.sql"):
            name_upper = path.name.replace("-schema.sql", "").upper()
            obj_type = None
            for parent in path.parents:
                hint_type = DBCAT_DIR_TO_TYPE.get(parent.name.upper())
                if hint_type:
                    obj_type = hint_type
                    break
            if not obj_type:
                continue
            schema_name = None
            for parent in path.parents:
                pname = parent.name.upper()
                if pname in schema_upper:
                    schema_name = pname
                    break
                # 兼容 schema_xxxx_xxxx 这种前缀匹配
                for candidate in schema_upper:
                    if pname.startswith(f"{candidate}_"):
                        schema_name = candidate
                        break
                if schema_name:
                    break
            if not schema_name:
                continue
            if name_upper not in global_index[schema_name][obj_type]:
                global_index[schema_name][obj_type][name_upper] = path
    except OSError as exc:
        log.warning("[dbcat] 构建全局索引失败: %s", exc)
    total_files = sum(len(obj_map) for schema_map in global_index.values() for obj_map in schema_map.values())
    if total_files:
        log.info("[dbcat] 全局索引构建完成，覆盖 %d 个对象文件。", total_files)
    return global_index


# ====================== 扁平化缓存目录 ======================

FLAT_CACHE_DIR_NAME = "cache"
FLAT_CACHE_INDEX_FILE = "index.json"

# 索引结构: {SCHEMA: {OBJECT_TYPE: [OBJECT_NAME, ...]}}
FlatCacheIndex = Dict[str, Dict[str, List[str]]]


def get_flat_cache_path(base_output: Path, schema: str, obj_type: str, obj_name: str) -> Path:
    """
    返回扁平化缓存的文件路径: dbcat_output/cache/SCHEMA/OBJECT_TYPE/name.sql
    """
    return base_output / FLAT_CACHE_DIR_NAME / schema.upper() / obj_type.upper() / f"{obj_name.upper()}.sql"


def get_flat_cache_index_path(base_output: Path) -> Path:
    """返回索引文件路径: dbcat_output/cache/index.json"""
    return base_output / FLAT_CACHE_DIR_NAME / FLAT_CACHE_INDEX_FILE


def load_flat_cache_index(base_output: Path) -> FlatCacheIndex:
    """
    加载扁平缓存的 JSON 索引文件。
    索引结构: {SCHEMA: {OBJECT_TYPE: [OBJECT_NAME, ...]}}
    """
    index_path = get_flat_cache_index_path(base_output)
    if not index_path.exists():
        return {}
    try:
        with open(index_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
            return data.get("objects", {})
    except (OSError, json.JSONDecodeError) as exc:
        log.warning("[dbcat] 加载缓存索引失败 %s: %s", index_path, exc)
        return {}


def save_flat_cache_index(base_output: Path, index: FlatCacheIndex) -> bool:
    """
    保存扁平缓存的 JSON 索引文件。
    """
    index_path = get_flat_cache_index_path(base_output)
    try:
        index_path.parent.mkdir(parents=True, exist_ok=True)
        data = {
            "version": 1,
            "updated_at": datetime.now().isoformat(),
            "objects": index
        }
        with open(index_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        return True
    except OSError as exc:
        log.warning("[dbcat] 保存缓存索引失败 %s: %s", index_path, exc)
        return False


def update_flat_cache_index(
    base_output: Path,
    schema: str,
    obj_type: str,
    obj_name: str,
    index: Optional[FlatCacheIndex] = None
) -> FlatCacheIndex:
    """
    更新索引，添加单个对象。如果传入 index 则直接修改，否则先加载再更新。
    """
    if index is None:
        index = load_flat_cache_index(base_output)
    
    schema_u = schema.upper()
    obj_type_u = obj_type.upper()
    obj_name_u = obj_name.upper()
    
    if schema_u not in index:
        index[schema_u] = {}
    if obj_type_u not in index[schema_u]:
        index[schema_u][obj_type_u] = []
    if obj_name_u not in index[schema_u][obj_type_u]:
        index[schema_u][obj_type_u].append(obj_name_u)
    
    return index


def load_from_flat_cache(
    base_output: Path,
    schema_requests: Dict[str, Dict[str, Set[str]]],
    accumulator: Dict[str, Dict[str, Dict[str, str]]],
    source_meta: Optional[Dict[Tuple[str, str, str], Tuple[str, float]]] = None,
    parallel_workers: int = 1
) -> int:
    """
    从扁平化缓存目录加载 DDL。
    parallel_workers: 并行读取线程数，默认1（顺序读取），建议4-8用于慢速磁盘
    """
    flat_cache = base_output / FLAT_CACHE_DIR_NAME
    if not flat_cache.exists():
        return 0
    
    index_start = time.time()
    cache_index = load_flat_cache_index(base_output)
    index_elapsed = time.time() - index_start
    if index_elapsed > 1.0:
        log.warning("[性能] 加载缓存索引耗时 %.2fs，磁盘IO可能较慢", index_elapsed)
    
    # 收集所有需要加载的文件
    load_tasks: List[Tuple[str, str, str, Path]] = []
    for schema in list(schema_requests.keys()):
        schema_u = schema.upper()
        type_map = schema_requests.get(schema) or {}
        schema_index = cache_index.get(schema_u, {})
        
        for obj_type in list(type_map.keys()):
            obj_type_u = obj_type.upper()
            names = type_map[obj_type]
            cached_names = set(schema_index.get(obj_type_u, []))
            
            for name in names:
                name_u = name.upper()
                if cached_names and name_u not in cached_names:
                    continue
                file_path = get_flat_cache_path(base_output, schema, obj_type, name)
                if file_path.exists():
                    load_tasks.append((schema, obj_type, name, file_path))
    
    if not load_tasks:
        return 0
    
    loaded_count = 0
    total_read_time = 0.0
    slow_files = []
    results_lock = threading.Lock()
    
    def _load_file(task: Tuple[str, str, str, Path]) -> Optional[Tuple]:
        schema, obj_type, name, file_path = task
        try:
            start = time.time()
            ddl_text = file_path.read_text('utf-8')
            elapsed = time.time() - start
            return (schema, obj_type, name, ddl_text, elapsed)
        except OSError as e:
            log.debug("[缓存] 读取失败 %s: %s", file_path, e)
            return None
    
    # 并行或顺序加载
    if parallel_workers > 1 and len(load_tasks) > 20:
        log.info("[缓存] 并行加载 %d 个文件 (workers=%d)...", len(load_tasks), parallel_workers)
        from concurrent.futures import ThreadPoolExecutor
        with ThreadPoolExecutor(max_workers=parallel_workers) as executor:
            for result in executor.map(_load_file, load_tasks):
                if result:
                    schema, obj_type, name, ddl_text, elapsed = result
                    with results_lock:
                        accumulator.setdefault(schema.upper(), {}).setdefault(obj_type.upper(), {})[name.upper()] = ddl_text
                        if source_meta:
                            source_meta[(schema.upper(), obj_type.upper(), name.upper())] = ("flat_cache", elapsed)
                        total_read_time += elapsed
                        if elapsed > 0.5:
                            slow_files.append((f"{schema}.{name}", elapsed))
                        loaded_count += 1
    else:
        for task in load_tasks:
            result = _load_file(task)
            if result:
                schema, obj_type, name, ddl_text, elapsed = result
                accumulator.setdefault(schema.upper(), {}).setdefault(obj_type.upper(), {})[name.upper()] = ddl_text
                if source_meta:
                    source_meta[(schema.upper(), obj_type.upper(), name.upper())] = ("flat_cache", elapsed)
                total_read_time += elapsed
                if elapsed > 0.5:
                    slow_files.append((f"{schema}.{name}", elapsed))
                loaded_count += 1
    
    # 更新schema_requests
    for schema, obj_type, name, _ in load_tasks:
        type_map = schema_requests.get(schema, {})
        if obj_type in type_map:
            type_map[obj_type].discard(name)
            if not type_map[obj_type]:
                del type_map[obj_type]
        if not type_map and schema in schema_requests:
            del schema_requests[schema]
    
    # 性能诊断
    if loaded_count > 0:
        avg_time = total_read_time / loaded_count
        if avg_time > 0.1:
            log.warning(
                "[性能警告] 缓存加载平均 %.3fs/文件 (%.2fs/%d文件) - 磁盘IO慢",
                avg_time, total_read_time, loaded_count
            )
            log.warning("[建议] 1) 使用本地SSD  2) 设置cache_parallel_workers=4-8  3) 或禁用缓存")
        if slow_files:
            log.warning("[性能] %d个文件>0.5s，前5个：", len(slow_files))
            for obj_name, elapsed in slow_files[:5]:
                log.warning("  %s: %.2fs", obj_name, elapsed)
    
    return loaded_count


def save_to_flat_cache(
    base_output: Path,
    schema: str,
    obj_type: str,
    obj_name: str,
    ddl_content: str
) -> bool:
    """
    将 DDL 保存到扁平化缓存目录。
    """
    file_path = get_flat_cache_path(base_output, schema, obj_type, obj_name)
    try:
        file_path.parent.mkdir(parents=True, exist_ok=True)
        
        # 验证：检查DDL是否包含异常字符
        if '\x00' in ddl_content:
            log.warning("[dbcat] DDL包含NULL字符，已清理: %s.%s", schema, obj_name)
            ddl_content = ddl_content.replace('\x00', '')
        
        file_path.write_text(ddl_content, encoding='utf-8')
        
        # 验证：读回并比较
        readback = file_path.read_text(encoding='utf-8')
        if readback != ddl_content:
            log.error("[dbcat] 缓存验证失败: %s.%s (写入%d字节，读回%d字节)",
                     schema, obj_name, len(ddl_content), len(readback))
            return False
        
        return True
    except OSError as exc:
        log.warning("[dbcat] 保存到扁平缓存失败 %s: %s", file_path, exc)
        return False


def normalize_dbcat_run_to_flat_cache(
    base_output: Path,
    run_dir: Path,
    schema: str,
    results: Dict[str, Dict[str, str]],
    cleanup_run_dir: bool = True
) -> int:
    """
    将 dbcat 运行结果整理到扁平化缓存目录，并更新 JSON 索引。
    
    Args:
        base_output: dbcat_output 基目录
        run_dir: 本次 dbcat 运行的输出目录（带时间戳）
        schema: schema 名称
        results: 本次导出的 DDL 内容 {OBJECT_TYPE: {OBJECT_NAME: ddl_content}}
        cleanup_run_dir: 是否在整理完成后删除原始运行目录
    
    Returns:
        保存的文件数量
    """
    # 先加载现有索引
    cache_index = load_flat_cache_index(base_output)
    
    saved_count = 0
    for obj_type, name_map in results.items():
        for obj_name, ddl_content in name_map.items():
            if save_to_flat_cache(base_output, schema, obj_type, obj_name, ddl_content):
                # 更新内存中的索引
                update_flat_cache_index(base_output, schema, obj_type, obj_name, cache_index)
                saved_count += 1
    
    # 批量保存索引（只写一次磁盘）
    if saved_count:
        save_flat_cache_index(base_output, cache_index)
        log.info("[dbcat] 已将 %d 个对象 DDL 整理到扁平缓存 %s/cache/%s/",
                 saved_count, base_output, schema.upper())
    
    # 清理原始运行目录
    if cleanup_run_dir and run_dir.exists():
        try:
            shutil.rmtree(run_dir)
            log.info("[dbcat] 已清理临时目录 %s", run_dir)
        except OSError as exc:
            log.warning("[dbcat] 清理临时目录失败 %s: %s", run_dir, exc)
    
    return saved_count


def load_cached_dbcat_results(
    base_output: Path,
    schema_requests: Dict[str, Dict[str, Set[str]]],
    accumulator: Dict[str, Dict[str, Dict[str, str]]],
    source_meta: Optional[Dict[Tuple[str, str, str], Tuple[str, float]]] = None,
    global_index: Optional[Dict[str, Dict[str, Dict[str, Path]]]] = None
) -> None:
    if not base_output.exists():
        return

    if global_index is None:
        global_index = build_dbcat_global_index(base_output, set(schema_requests.keys()))

    def _read_and_record(schema: str, obj_type: str, name: str, file_path: Path) -> bool:
        try:
            start_time = time.time()
            ddl_text = file_path.read_text('utf-8')
            elapsed = time.time() - start_time
        except OSError:
            return False
        accumulator.setdefault(schema, {}).setdefault(obj_type, {})[name] = ddl_text
        if source_meta is not None:
            source_meta[(schema, obj_type, name)] = ("cache", elapsed)
        return True

    # 先使用全局索引直接命中，避免逐 run 深层遍历
    preload_hit = 0
    for schema in list(schema_requests.keys()):
        type_map = schema_requests.get(schema) or {}
        for obj_type in list(type_map.keys()):
            names = type_map[obj_type]
            satisfied: Set[str] = set()
            for name in list(names):
                file_path = (
                    global_index
                    .get(schema.upper(), {})
                    .get(obj_type.upper(), {})
                    .get(name.upper())
                )
                if file_path and file_path.exists():
                    if _read_and_record(schema.upper(), obj_type.upper(), name.upper(), file_path):
                        satisfied.add(name)
                        preload_hit += 1
            names -= satisfied
            if not names:
                type_map.pop(obj_type, None)
        if not type_map:
            schema_requests.pop(schema, None)

    if preload_hit:
        log.info("[dbcat] 全局索引命中 %d 个对象，已直接载入缓存。", preload_hit)

    if not schema_requests:
        return

    candidates: List[Path] = [p for p in base_output.iterdir() if p.is_dir()]

    def _safe_mtime(p: Path) -> float:
        try:
            return p.stat().st_mtime
        except OSError:
            return 0.0

    run_dirs = sorted(candidates, key=_safe_mtime, reverse=True)

    for run_dir in run_dirs:
        if not schema_requests:
            break
        for schema in list(schema_requests.keys()):
            schema_dir = locate_dbcat_schema_dir(run_dir, schema)
            if not schema_dir or not schema_dir.exists():
                continue
            file_index = global_index.get(schema.upper()) or build_dbcat_file_index(schema_dir)
            type_map = schema_requests[schema]
            for obj_type in list(type_map.keys()):
                names = type_map[obj_type]
                satisfied: Set[str] = set()
                for name in list(names):
                    file_path = file_index.get(obj_type.upper(), {}).get(name.upper())
                    if file_path is None:
                        file_path = find_dbcat_object_file(schema_dir, obj_type, name)
                    if not file_path or not file_path.exists():
                        continue
                    try:
                        start_time = time.time()
                        ddl_text = file_path.read_text('utf-8')
                        elapsed = time.time() - start_time
                    except OSError:
                        continue
                    schema_u = schema.upper()
                    obj_type_u = obj_type.upper()
                    name_u = name.upper()
                    accumulator.setdefault(schema_u, {}).setdefault(obj_type_u, {})[name_u] = ddl_text
                    if source_meta is not None:
                        key = (schema_u, obj_type_u, name_u)
                        source_meta[key] = ("cache", elapsed)
                    satisfied.add(name)
                names -= satisfied
                if not names:
                    del type_map[obj_type]
            if not type_map:
                del schema_requests[schema]


def fetch_dbcat_schema_objects(
    ora_cfg: OraConfig,
    settings: Dict,
    schema_requests: Dict[str, Dict[str, Set[str]]]
) -> Tuple[Dict[str, Dict[str, Dict[str, str]]], Dict[Tuple[str, str, str], Tuple[str, float]]]:
    results: Dict[str, Dict[str, Dict[str, str]]] = {}
    source_meta: Dict[Tuple[str, str, str], Tuple[str, float]] = {}
    if not schema_requests:
        return results, source_meta

    base_output = Path(settings.get('dbcat_output_dir', 'dbcat_output'))
    ensure_dir(base_output)
    
    # 获取并行加载配置
    cache_parallel_workers = int(settings.get('cache_parallel_workers', 1))
    
    # 1) 优先从扁平缓存加载
    before_req = sum(len(names) for type_map in schema_requests.values() for names in type_map.values())
    flat_loaded = load_from_flat_cache(base_output, schema_requests, results, source_meta, cache_parallel_workers)
    if flat_loaded:
        log.info("[dbcat] 从扁平缓存加载 %d 个对象 DDL。", flat_loaded)
    
    # 2) 扁平缓存未命中的，尝试从旧的层级缓存加载（兼容）
    after_flat = sum(len(names) for type_map in schema_requests.values() for names in type_map.values())
    if after_flat > 0:
        global_index = build_dbcat_global_index(base_output, set(schema_requests.keys()))
        load_cached_dbcat_results(base_output, schema_requests, results, source_meta, global_index)
        after_legacy = sum(len(names) for type_map in schema_requests.values() for names in type_map.values())
        legacy_loaded = after_flat - after_legacy
        if legacy_loaded:
            log.info("[dbcat] 从层级缓存加载 %d 个对象 DDL。", legacy_loaded)
    
    after_req = sum(len(names) for type_map in schema_requests.values() for names in type_map.values())
    total_loaded = before_req - after_req
    if total_loaded:
        log.info("[dbcat] 缓存总计加载 %d 个对象 DDL，剩余待导出 %d。", total_loaded, after_req)
    else:
        log.info("[dbcat] 缓存未命中，将调用 dbcat 导出 %d 个对象。", after_req)

    if not schema_requests:
        return results, source_meta

    host, port, service = parse_oracle_dsn(ora_cfg['dsn'])
    dbcat_cli = resolve_dbcat_cli(settings)
    java_home = settings.get('java_home') or os.environ.get('JAVA_HOME')
    if not java_home:
        log.error("严重错误: 需要 JAVA_HOME 才能运行 dbcat，请在环境或 config.ini 中配置。")
        abort_run()

    max_chunk = int(settings.get('dbcat_chunk_size', 150)) or 150
    cleanup_run = settings.get('dbcat_cleanup_run_dirs', 'true').lower() in ('true', '1', 'yes')
    cli_timeout = int(settings.get('cli_timeout', 600))
    dbcat_from = settings.get('dbcat_from', '')
    dbcat_to = settings.get('dbcat_to', '')
    dbcat_no_cal_dep = parse_bool_flag(settings.get('dbcat_no_cal_dep', 'false'), False)
    dbcat_query_meta_thread = int(settings.get('dbcat_query_meta_thread') or 0)
    dbcat_progress_interval = int(settings.get('dbcat_progress_interval') or 0)
    if dbcat_query_meta_thread < 0:
        dbcat_query_meta_thread = 0
    if dbcat_progress_interval < 1:
        dbcat_progress_interval = 0
    
    # 并行导出的 worker 数量，默认 4，防止打爆主机
    parallel_workers = int(settings.get('dbcat_parallel_workers', 4))
    parallel_workers = max(1, min(parallel_workers, 8))  # 限制在 1-8 之间
    
    # 准备每个 schema 的导出任务
    schema_tasks: List[Tuple[str, List[Tuple[str, str, List[str]]]]] = []
    for schema in list(schema_requests.keys()):
        type_map = schema_requests.get(schema)
        if not type_map:
            continue
        prepared: List[Tuple[str, str, List[str]]] = []
        for obj_type, names in type_map.items():
            option = DBCAT_OPTION_MAP.get(obj_type.upper())
            if not option:
                continue
            if obj_type.upper() == "MATERIALIZED VIEW":
                log.info("[dbcat] 跳过 MATERIALIZED VIEW 自动导出 (dbcat 不支持 --mview)，需要时请手工处理。")
                continue
            name_list = sorted(set(n.upper() for n in names if n))
            if not name_list:
                continue
            prepared.append((option, obj_type.upper(), name_list))
        if prepared:
            schema_tasks.append((schema, prepared))
    
    if not schema_tasks:
        return results, source_meta
    
    # 用于保护共享数据的锁
    results_lock = threading.Lock()
    error_occurred = threading.Event()
    
    def _export_single_schema(schema: str, prepared: List[Tuple[str, str, List[str]]]) -> Optional[str]:
        """导出单个 schema 的所有对象，返回错误信息或 None"""
        if error_occurred.is_set():
            return None
            
        run_dir = base_output / f"{schema}_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{uuid.uuid4().hex[:6]}"
        ensure_dir(run_dir)
        chunk_time_map: Dict[Tuple[str, str, str], float] = {}
        
        def _run_dbcat_chunk(
            option: str,
            chunk_names: List[str],
            obj_type: str,
            chunk_idx: int,
            total_chunks: int
        ) -> Optional[str]:
            cmd = [
                str(dbcat_cli),
                'convert',
                '-H', host,
                '-P', port,
                '-u', ora_cfg['user'],
                '-p', ora_cfg['password'],
                '-D', schema,
                '--from', dbcat_from,
                '--to', dbcat_to,
                '--file-per-object',
                '-f', str(run_dir)
            ]
            if service:
                cmd.extend(['--service-name', service])
            if dbcat_no_cal_dep:
                cmd.append('--no-cal-dep')
            if dbcat_query_meta_thread:
                cmd.extend(['--query-meta-thread', str(dbcat_query_meta_thread)])
            cmd.extend([option, ','.join(chunk_names)])

            env = os.environ.copy()
            env['JAVA_HOME'] = java_home
            env.setdefault('JRE_HOME', java_home)

            log.info(
                "[dbcat] 导出 schema=%s option=%s chunk=%d/%d 对象数=%d...",
                schema, option, chunk_idx, total_chunks, len(chunk_names)
            )
            start_time = time.time()
            try:
                with tempfile.TemporaryFile() as stdout_buf, tempfile.TemporaryFile() as stderr_buf:
                    proc = subprocess.Popen(
                        cmd,
                        stdout=stdout_buf,
                        stderr=stderr_buf,
                        env=env
                    )
                    last_log = start_time
                    while True:
                        if proc.poll() is not None:
                            break
                        now = time.time()
                        elapsed = now - start_time
                        if cli_timeout and elapsed > cli_timeout:
                            proc.kill()
                            proc.wait(timeout=5)
                            return f"[dbcat] 转换 schema={schema} 超时 ({cli_timeout}s)"
                        if dbcat_progress_interval and (now - last_log) >= dbcat_progress_interval:
                            log.info(
                                "[dbcat] 导出 schema=%s option=%s chunk=%d/%d 仍在运行 (已耗时 %.1fs)...",
                                schema, option, chunk_idx, total_chunks, elapsed
                            )
                            last_log = now
                        time.sleep(0.5)
                    elapsed = time.time() - start_time
                    stdout_buf.seek(0)
                    stderr_buf.seek(0)
                    stdout_text = stdout_buf.read().decode('utf-8', errors='ignore')
                    stderr_text = stderr_buf.read().decode('utf-8', errors='ignore')
                    if proc.returncode != 0:
                        return f"[dbcat] 转换 schema={schema} 失败: {stderr_text or stdout_text}"
                log.info(
                    "[dbcat] 导出 schema=%s option=%s chunk=%d/%d 完成，用时 %.2fs。",
                    schema, option, chunk_idx, total_chunks, elapsed
                )
                # 将批次总耗时平均分配给每个对象
                avg_elapsed = elapsed / len(chunk_names) if chunk_names else elapsed
                for obj_name in chunk_names:
                    key = (schema.upper(), obj_type.upper(), obj_name.upper())
                    chunk_time_map[key] = avg_elapsed
                return None
            except Exception as e:
                return f"[dbcat] 转换 schema={schema} 异常: {e}"
        
        # 执行所有 chunk
        for option, obj_type, name_list in prepared:
            chunks = [name_list[i:i + max_chunk] for i in range(0, len(name_list), max_chunk)]
            total_chunks = len(chunks)
            for idx, chunk in enumerate(chunks, start=1):
                if error_occurred.is_set():
                    return None
                err = _run_dbcat_chunk(option, chunk, obj_type, idx, total_chunks)
                if err:
                    error_occurred.set()
                    return err
        
        # 读取导出结果
        schema_dir = locate_dbcat_schema_dir(run_dir, schema)
        if not schema_dir:
            error_occurred.set()
            return f"[dbcat] 未在输出目录 {run_dir} 下找到 schema={schema} 的 DDL。"
        
        file_index = build_dbcat_file_index(schema_dir)
        schema_result: Dict[str, Dict[str, str]] = {}
        local_source_meta: Dict[Tuple[str, str, str], Tuple[str, float]] = {}
        
        for option, obj_type, name_list in prepared:
            type_result = schema_result.setdefault(obj_type, {})
            for obj_name in name_list:
                file_path = file_index.get(obj_type.upper(), {}).get(obj_name.upper())
                if file_path is None:
                    file_path = find_dbcat_object_file(schema_dir, obj_type, obj_name)
                if not file_path or not file_path.exists():
                    log.warning("[dbcat] 未找到对象 %s.%s (%s) 的 DDL 文件。", schema, obj_name, obj_type)
                    continue
                try:
                    read_start = time.time()
                    ddl_text = file_path.read_text('utf-8')
                    read_elapsed = time.time() - read_start
                except OSError as exc:
                    log.warning("[dbcat] 读取 %s 失败: %s", file_path, exc)
                    continue
                obj_name_u = obj_name.upper()
                type_result[obj_name_u] = ddl_text
                key = (schema.upper(), obj_type.upper(), obj_name_u)
                local_source_meta[key] = ("dbcat_run", chunk_time_map.get(key, read_elapsed))
        
        # 整理到扁平缓存
        normalize_dbcat_run_to_flat_cache(
            base_output, run_dir, schema, schema_result, cleanup_run_dir=cleanup_run
        )
        
        # 合并结果到共享数据
        with results_lock:
            results.setdefault(schema.upper(), {}).update(schema_result)
            source_meta.update(local_source_meta)
        
        return None
    
    # 并行执行导出
    if len(schema_tasks) == 1 or parallel_workers == 1:
        # 单个 schema 或单线程，直接顺序执行
        for schema, prepared in schema_tasks:
            err = _export_single_schema(schema, prepared)
            if err:
                log.error(err)
                abort_run()
    else:
        # 多 schema 并行执行
        log.info("[dbcat] 启用并行导出 (workers=%d)，共 %d 个 schema 待导出。", 
                 parallel_workers, len(schema_tasks))
        with ThreadPoolExecutor(max_workers=parallel_workers) as executor:
            futures = {
                executor.submit(_export_single_schema, schema, prepared): schema
                for schema, prepared in schema_tasks
            }
            for future in as_completed(futures):
                schema = futures[future]
                try:
                    err = future.result()
                    if err:
                        log.error(err)
                        # 取消其他任务
                        for f in futures:
                            f.cancel()
                        abort_run()
                except Exception as exc:
                    log.error("[dbcat] schema=%s 导出异常: %s", schema, exc)
                    abort_run()

    return results, source_meta


def setup_metadata_session(ora_conn):
    plsql = """
    BEGIN
      DBMS_METADATA.SET_TRANSFORM_PARAM(DBMS_METADATA.SESSION_TRANSFORM,'SEGMENT_ATTRIBUTES',FALSE);
      DBMS_METADATA.SET_TRANSFORM_PARAM(DBMS_METADATA.SESSION_TRANSFORM,'STORAGE',FALSE);
      DBMS_METADATA.SET_TRANSFORM_PARAM(DBMS_METADATA.SESSION_TRANSFORM,'TABLESPACE',FALSE);
      DBMS_METADATA.SET_TRANSFORM_PARAM(DBMS_METADATA.SESSION_TRANSFORM,'CONSTRAINTS',TRUE);
    END;
    """
    try:
        with ora_conn.cursor() as cursor:
            cursor.execute(plsql)
    except oracledb.Error as e:
        log.warning(f"[DDL] 设置 DBMS_METADATA transform 失败: {e}")


DDL_OBJ_TYPE_MAPPING = {
    'PACKAGE BODY': 'PACKAGE_BODY',
    'MATERIALIZED VIEW': 'MATERIALIZED_VIEW',
    'TYPE BODY': 'TYPE_BODY'
}

CREATE_OBJECT_PATTERNS = {
    'TABLE': r'TABLE',
    'VIEW': r'(?:FORCE\s+)?VIEW',
    'MATERIALIZED VIEW': r'(?:FORCE\s+)?MATERIALIZED\s+VIEW',
    'PROCEDURE': r'PROCEDURE',
    'FUNCTION': r'FUNCTION',
    'PACKAGE': r'PACKAGE',
    'PACKAGE BODY': r'PACKAGE\s+BODY',
    'SYNONYM': r'(?:PUBLIC\s+)?SYNONYM',
    'SEQUENCE': r'SEQUENCE',
    'TRIGGER': r'TRIGGER',
    'TYPE': r'TYPE',
    'TYPE BODY': r'TYPE\s+BODY',
    'JOB': r'JOB',
    'SCHEDULE': r'SCHEDULE',
    'INDEX': r'(?:UNIQUE\s+|BITMAP\s+)?INDEX'
}


def get_oceanbase_version(ob_cfg: ObConfig) -> Optional[str]:
    """获取OceanBase版本号"""
    sql = "SELECT OB_VERSION() FROM DUAL"
    ok, out, err = obclient_run_sql(ob_cfg, sql)
    if not ok or not out:
        log.warning("无法获取OceanBase版本信息: %s", err)
        return None
    
    # 解析版本号，OB_VERSION()直接返回版本号如 "4.2.5.7"
    for line in out.splitlines():
        line = line.strip()
        if line and line != 'OB_VERSION()':  # 跳过列标题
            # 检查是否是版本号格式 (数字.数字.数字.数字)
            if '.' in line and line.replace('.', '').replace('-', '').isdigit():
                return line.split('-')[0]  # 去掉可能的后缀
    return None


def compare_version(version1: str, version2: str) -> int:
    """比较版本号，返回 -1(v1<v2), 0(v1==v2), 1(v1>v2)"""
    try:
        v1_parts = [int(x) for x in re.findall(r'\d+', version1 or "")]
        v2_parts = [int(x) for x in re.findall(r'\d+', version2 or "")]
        if not v1_parts and not v2_parts:
            return 0
        if not v1_parts:
            return -1
        if not v2_parts:
            return 1
        
        # 补齐长度
        max_len = max(len(v1_parts), len(v2_parts))
        v1_parts.extend([0] * (max_len - len(v1_parts)))
        v2_parts.extend([0] * (max_len - len(v2_parts)))
        
        for i in range(max_len):
            if v1_parts[i] < v2_parts[i]:
                return -1
            elif v1_parts[i] > v2_parts[i]:
                return 1
        return 0
    except (ValueError, AttributeError):
        return 0


def apply_ob_feature_gates(settings: Dict, ob_version: Optional[str]) -> Dict[str, object]:
    """
    根据 OB 版本与配置计算运行态门控生效值。
    - interval: auto 在 OB>=4.4.2 默认关闭，低版本默认开启
    - mview: auto 在 OB>=4.4.2 默认开启，低版本默认仅打印
    """
    version_value = extract_ob_version_number(ob_version)
    version_known = bool(version_value)
    is_ob_442_plus = bool(version_known and compare_version(version_value, OB_FEATURE_GATE_VERSION) >= 0)

    interval_mode = normalize_interval_partition_fixup_mode(
        settings.get("generate_interval_partition_fixup_mode")
        or settings.get("generate_interval_partition_fixup")
    )
    mview_mode = normalize_mview_check_fixup_mode(settings.get("mview_check_fixup_mode", "auto"))

    if interval_mode == "auto":
        interval_enabled = (not is_ob_442_plus) if version_known else True
        interval_reason = (
            f"auto(OB {'>=' if is_ob_442_plus else '<'} {OB_FEATURE_GATE_VERSION})"
            if version_known else "auto(version_unknown_fallback)"
        )
    else:
        interval_enabled = (interval_mode == "true")
        interval_reason = f"manual({interval_mode})"

    if mview_mode == "auto":
        mview_enabled = is_ob_442_plus if version_known else False
        mview_reason = (
            f"auto(OB {'>=' if is_ob_442_plus else '<'} {OB_FEATURE_GATE_VERSION})"
            if version_known else "auto(version_unknown_fallback)"
        )
    else:
        mview_enabled = (mview_mode == "on")
        mview_reason = f"manual({mview_mode})"

    print_only_types = set(PRINT_ONLY_PRIMARY_TYPES)
    print_only_reasons = dict(PRINT_ONLY_PRIMARY_REASONS)
    if mview_enabled:
        print_only_types.discard("MATERIALIZED VIEW")
        print_only_reasons.pop("MATERIALIZED VIEW", None)
    else:
        print_only_reasons["MATERIALIZED VIEW"] = (
            f"MATERIALIZED VIEW 当前按 {mview_reason} 仅打印不校验"
        )

    settings["generate_interval_partition_fixup_mode"] = interval_mode
    settings["mview_check_fixup_mode"] = mview_mode
    settings["ob_version"] = version_value or ""
    settings["ob_version_known"] = version_known
    settings["ob_is_442_plus"] = is_ob_442_plus
    settings["effective_interval_fixup_enabled"] = interval_enabled
    settings["effective_mview_enabled"] = mview_enabled
    settings["effective_print_only_primary_types"] = print_only_types
    settings["effective_print_only_primary_reasons"] = print_only_reasons
    # 保持旧键为布尔值，减少下游改动
    settings["generate_interval_partition_fixup"] = interval_enabled
    settings["ob_feature_gate_decisions"] = {
        "interval": interval_reason,
        "mview": mview_reason,
    }
    return {
        "version": version_value,
        "version_known": version_known,
        "is_ob_442_plus": is_ob_442_plus,
        "interval_mode": interval_mode,
        "interval_enabled": interval_enabled,
        "interval_reason": interval_reason,
        "mview_mode": mview_mode,
        "mview_enabled": mview_enabled,
        "mview_reason": mview_reason,
    }


def clean_view_ddl_for_oceanbase(ddl: str, ob_version: Optional[str] = None) -> str:
    """
    清理Oracle VIEW DDL，使其兼容OceanBase
    
    Args:
        ddl: Oracle VIEW的DDL语句
        ob_version: OceanBase版本号
    
    Returns:
        清理后的DDL
    """
    if not ddl:
        return ddl
    
    cleaned_ddl = ddl

    # 移除 CREATE ... FORCE [EDITIONABLE] VIEW 中的 FORCE 关键字（兼容多关键字场景）
    def _strip_force_in_create_view(match: re.Match) -> str:
        prefix = match.group(1)
        middle = match.group(2) or ""
        view_kw = match.group(3)
        # 移除 NO FORCE / FORCE
        middle = re.sub(r'\bNO\s+FORCE\b', ' ', middle, flags=re.IGNORECASE)
        middle = re.sub(r'\bFORCE\b', ' ', middle, flags=re.IGNORECASE)
        middle = re.sub(r'[ \t]+', ' ', middle).strip()
        if middle:
            return f"{prefix}{middle} {view_kw}"
        return f"{prefix}{view_kw}"

    cleaned_ddl = re.sub(
        r'(\bCREATE\s+(?:OR\s+REPLACE\s+)?)(.*?)\b((?:MATERIALIZED\s+)?VIEW)\b',
        _strip_force_in_create_view,
        cleaned_ddl,
        flags=re.IGNORECASE | re.DOTALL
    )

    # 先移除 WITH CHECK OPTION 的 CONSTRAINT 名称（保留 CHECK OPTION 本身）
    cleaned_ddl = re.sub(
        r'(\bWITH\s+CHECK\s+OPTION)\s+CONSTRAINT\s+("(?:""|[^"])*"|[A-Za-z0-9_#$]+)',
        r'\1',
        cleaned_ddl,
        flags=re.IGNORECASE
    )
    # 兜底移除尾部残留的 CONSTRAINT 名称
    cleaned_ddl = re.sub(
        r'\s+CONSTRAINT\s+("(?:""|[^"])*"|[A-Za-z0-9_#$]+)\s*(;)?\s*$',
        r'\2',
        cleaned_ddl,
        flags=re.IGNORECASE
    )

    # 需要移除的关键字模式
    patterns_to_remove = [
        # EDITIONABLE 在所有版本都需要移除
        r'\s+EDITIONABLE\s+',
        r'\s+NONEDITIONABLE\s+',
        # BEQUEATH 子句
        r'\s+BEQUEATH\s+(?:CURRENT_USER|DEFINER)',
        # SHARING 子句 (Oracle 12c+)
        r'\s+SHARING\s*=\s*(?:METADATA|DATA|EXTENDED\s+DATA|NONE)',
        # DEFAULT COLLATION 子句
        r'\s+DEFAULT\s+COLLATION\s+\w+',
        # CONTAINER 子句
        r'\s+CONTAINER_MAP\s*',
        r'\s+CONTAINERS_DEFAULT\s*',
    ]
    
    # 版本相关的清理
    remove_check_option = True
    if ob_version:
        # 如果版本 >= 4.2.5.7，可保留 WITH CHECK OPTION
        remove_check_option = compare_version(ob_version, "4.2.5.7") < 0
    if remove_check_option:
        patterns_to_remove.append(r'\s+WITH\s+CHECK\s+OPTION')

    for pattern in patterns_to_remove:
        cleaned_ddl = re.sub(pattern, ' ', cleaned_ddl, flags=re.IGNORECASE)
    
    # 清理多余的空格（保留换行，避免行注释吞行）
    cleaned_ddl = re.sub(r'[ \t\r\f\v]+', ' ', cleaned_ddl)
    cleaned_ddl = re.sub(r' *\n *', '\n', cleaned_ddl)
    cleaned_ddl = cleaned_ddl.strip()
    
    return cleaned_ddl


class SqlMasker:
    """
    辅助类：用于对 SQL 中的字符串字面量和注释进行掩码处理。
    防止正则替换时误伤字符串或注释内的内容。
    """
    def __init__(self, sql: str):
        self.original_sql = sql
        self.masked_sql = sql
        self.literals: Dict[str, str] = {}
        self.comments: Dict[str, str] = {}
        self._mask()

    def _mask(self):
        # 1. Mask String Literals: 'text'
        # 注意: Oracle 字符串内部的单引号转义为 ''
        def mask_str(match):
            key = f"###STR_{len(self.literals)}###"
            self.literals[key] = match.group(0)
            return key
        
        self.masked_sql = re.sub(r"'(?:''|[^'])*'", mask_str, self.masked_sql)

        # 2. Mask Block Comments: /* ... */
        def mask_block_cmt(match):
            key = f"###CMT_BLK_{len(self.comments)}###"
            self.comments[key] = match.group(0)
            return key
        
        self.masked_sql = re.sub(r'/\*.*?\*/', mask_block_cmt, self.masked_sql, flags=re.DOTALL)

        # 3. Mask Line Comments: -- ...
        def mask_line_cmt(match):
            key = f"###CMT_LN_{len(self.comments)}###"
            self.comments[key] = match.group(0)
            return key
        
        self.masked_sql = re.sub(r'--.*?$', mask_line_cmt, self.masked_sql, flags=re.MULTILINE)

    def unmask(self, sql: str) -> str:
        # 恢复掩码内容
        for k, v in self.comments.items():
            sql = sql.replace(k, v)
        for k, v in self.literals.items():
            sql = sql.replace(k, v)
        return sql


class SqlPunctuationMasker:
    """
    针对全角标点清洗的掩码器：
    - 屏蔽字符串字面量、注释、双引号标识符
    - 避免清洗时误改语义
    """
    def __init__(self, sql: str):
        self.original_sql = sql
        self.masked_sql = sql
        self.literals: Dict[str, str] = {}
        self.comments: Dict[str, str] = {}
        self.quoted_identifiers: Dict[str, str] = {}
        self._mask()

    def _mask_pattern(self, pattern: str, store: Dict[str, str], prefix: str, flags: int = 0) -> None:
        def _repl(match):
            key = f"###{prefix}_{len(store)}###"
            store[key] = match.group(0)
            return key
        self.masked_sql = re.sub(pattern, _repl, self.masked_sql, flags=flags)

    def _mask(self) -> None:
        # 1) 字符串字面量
        self._mask_pattern(r"'(?:''|[^'])*'", self.literals, "PUNC_STR")
        # 2) 块注释
        self._mask_pattern(r"/\*.*?\*/", self.comments, "PUNC_CMT_BLK", flags=re.DOTALL)
        # 3) 行注释
        self._mask_pattern(r"--.*?$", self.comments, "PUNC_CMT_LN", flags=re.MULTILINE)
        # 4) 双引号标识符
        self._mask_pattern(r'"(?:\"\"|[^"])*"', self.quoted_identifiers, "PUNC_QID")

    def unmask(self, sql: str) -> str:
        for k, v in self.quoted_identifiers.items():
            sql = sql.replace(k, v)
        for k, v in self.comments.items():
            sql = sql.replace(k, v)
        for k, v in self.literals.items():
            sql = sql.replace(k, v)
        return sql


ASCII_PUNCT_FOR_FULLWIDTH = "!#$%&()*+,-./:;<=>?@[\\]^_`{|}~"
FULLWIDTH_PUNCT_REPLACEMENTS = {
    chr(ord(ch) + 0xFEE0): ch for ch in ASCII_PUNCT_FOR_FULLWIDTH
}
FULLWIDTH_PUNCT_REPLACEMENTS.update({
    "\u3000": " ",  # IDEOGRAPHIC SPACE
    "\u3001": ",",  # IDEOGRAPHIC COMMA
    "\u3002": ".",  # IDEOGRAPHIC PERIOD
    "\u3010": "[",  # LEFT BLACK LENTICULAR BRACKET
    "\u3011": "]",  # RIGHT BLACK LENTICULAR BRACKET
})

PLSQL_PUNCT_SANITIZE_TYPES = {
    "PROCEDURE",
    "FUNCTION",
    "PACKAGE",
    "PACKAGE BODY",
    "TYPE",
    "TYPE BODY",
    "TRIGGER",
}


def sanitize_plsql_punctuation(
    ddl: str,
    obj_type: str,
    sample_limit: int = 5
) -> Tuple[str, int, List[Tuple[str, str]]]:
    """
    将PL/SQL DDL中的全角标点替换为半角，避免目标端解析失败。
    保护字符串字面量、注释、双引号标识符不被替换。

    Returns:
        (sanitized_ddl, replaced_count, samples)
    """
    if not ddl:
        return ddl, 0, []
    if (obj_type or "").upper() not in PLSQL_PUNCT_SANITIZE_TYPES:
        return ddl, 0, []

    masker = SqlPunctuationMasker(ddl)
    masked = masker.masked_sql
    replaced = 0
    samples: List[Tuple[str, str]] = []
    out_chars: List[str] = []

    for ch in masked:
        repl = FULLWIDTH_PUNCT_REPLACEMENTS.get(ch)
        if repl is not None:
            replaced += 1
            if len(samples) < sample_limit and (ch, repl) not in samples:
                samples.append((ch, repl))
            out_chars.append(repl)
        else:
            out_chars.append(ch)

    sanitized = masker.unmask("".join(out_chars))
    return sanitized, replaced, samples


def _find_inline_comment_split(segment: str) -> Optional[int]:
    if not segment.startswith("--"):
        return None
    # Try to detect the next column token after a collapsed inline comment.
    patterns = [
        r"\s+\(\s*(?:[A-Za-z_][A-Za-z0-9_#$]*|\"[^\"]+\")\.",
        r"\s+(?:[A-Za-z_][A-Za-z0-9_#$]*|\"[^\"]+\")\.",
        r"\s+\(\s*(?:SELECT|WITH|CASE)\b",
        (
            r"\s+\b(?:SELECT|WITH|CASE|DECODE|NVL|COALESCE|TO_CHAR|TO_DATE|TRUNC|"
            r"ROUND|SUBSTR|INSTR|REGEXP_SUBSTR|REGEXP_REPLACE|REGEXP_LIKE|"
            r"COUNT|SUM|MIN|MAX|AVG)\b"
        ),
        r"\s+FROM\b\s+[A-Za-z_\"(]",
    ]
    best: Optional[int] = None
    for pattern in patterns:
        m = re.search(pattern, segment, flags=re.IGNORECASE)
        if not m:
            continue
        idx = m.start()
        if best is None or idx < best:
            best = idx
    return best


def fix_inline_comment_collapse(ddl: str) -> str:
    """
    修复 SELECT 列表中因换行被压缩导致的行内注释吞行问题。
    仅在检测到注释后仍有列/FROM 关键字时插入换行。
    """
    if not ddl or "--" not in ddl:
        return ddl
    out: List[str] = []
    i = 0
    in_single = False
    in_double = False
    in_block = False
    while i < len(ddl):
        ch = ddl[i]
        nxt = ddl[i + 1] if i + 1 < len(ddl) else ""

        if in_block:
            if ch == "*" and nxt == "/":
                out.append("*/")
                i += 2
                in_block = False
                continue
            out.append(ch)
            i += 1
            continue

        if in_single:
            out.append(ch)
            if ch == "'" and nxt == "'":
                out.append(nxt)
                i += 2
                continue
            if ch == "'":
                in_single = False
            i += 1
            continue

        if in_double:
            out.append(ch)
            if ch == '"' and nxt == '"':
                out.append(nxt)
                i += 2
                continue
            if ch == '"':
                in_double = False
            i += 1
            continue

        if ch == "'" and not in_double:
            in_single = True
            out.append(ch)
            i += 1
            continue
        if ch == '"' and not in_single:
            in_double = True
            out.append(ch)
            i += 1
            continue
        if ch == "/" and nxt == "*":
            in_block = True
            out.append("/*")
            i += 2
            continue

        if ch == "-" and nxt == "-":
            line_end = ddl.find("\n", i)
            if line_end == -1:
                line_end = len(ddl)
            segment = ddl[i:line_end]
            split_at = _find_inline_comment_split(segment)
            if split_at is not None:
                out.append(segment[:split_at])
                out.append("\n")
                i += split_at
                continue
            out.append(segment)
            i = line_end
            continue

        out.append(ch)
        i += 1

    return "".join(out)


def repair_split_identifiers(ddl: str, column_names: Set[str]) -> str:
    """
    修复被错误拆分的列标识符，例如 TOT_P ERM -> TOT_PERM。
    仅当拼接后命中视图列元数据时才会修复。
    """
    if not ddl or not column_names:
        return ddl
    names_upper = {c.upper() for c in column_names if c}
    masker = SqlMasker(ddl)
    masked = masker.masked_sql
    chars = list(masked)
    length = len(masked)
    i = 0

    def _is_ident_start(ch: str) -> bool:
        return ch.isalpha() or ch == "_"

    def _is_ident_char(ch: str) -> bool:
        return ch.isalnum() or ch in "_#$"

    while i < length:
        ch = masked[i]
        if not _is_ident_start(ch):
            i += 1
            continue
        if i > 0 and masked[i - 1] == '"':
            i += 1
            continue
        j = i + 1
        while j < length and _is_ident_char(masked[j]):
            j += 1
        k = j
        while k < length and masked[k].isspace():
            k += 1
        if k == j or k >= length or not _is_ident_start(masked[k]):
            i = j
            continue
        if masked[k - 1] == '"' or (k + 1 < length and masked[k + 1] == '"'):
            i = j
            continue
        l = k + 1
        while l < length and _is_ident_char(masked[l]):
            l += 1
        combined = f"{masked[i:j]}{masked[k:l]}".upper()
        if combined in names_upper:
            for idx in range(j, k):
                chars[idx] = ""
            i = l
            continue
        i = j

    return masker.unmask("".join(chars))


def sanitize_view_ddl(ddl: str, column_names: Optional[Set[str]] = None) -> str:
    """
    对 VIEW DDL 进行质量修复：修复行内注释吞行、拆分列名。
    """
    if not ddl:
        return ddl
    cleaned = fix_inline_comment_collapse(ddl)
    if column_names:
        cleaned = repair_split_identifiers(cleaned, column_names)
    return cleaned


def mask_sql_for_scan(sql: str) -> str:
    """
    将 SQL 中的字符串/注释替换为空格，以便进行关键字/括号扫描。
    保持原长度，确保索引位置可复用。
    """
    if not sql:
        return sql
    chars = list(sql)
    i = 0
    in_single = False
    in_double = False
    in_line_comment = False
    in_block_comment = False
    length = len(chars)
    while i < length:
        ch = chars[i]
        nxt = chars[i + 1] if i + 1 < length else ""
        if in_line_comment:
            if ch == "\n":
                in_line_comment = False
            else:
                chars[i] = " "
            i += 1
            continue
        if in_block_comment:
            chars[i] = " "
            if ch == "*" and nxt == "/":
                chars[i + 1] = " "
                i += 2
                in_block_comment = False
                continue
            i += 1
            continue
        if in_single:
            chars[i] = " "
            if ch == "'" and nxt == "'":
                chars[i + 1] = " "
                i += 2
                continue
            if ch == "'":
                in_single = False
            i += 1
            continue
        if in_double:
            chars[i] = " "
            if ch == '"' and nxt == '"':
                chars[i + 1] = " "
                i += 2
                continue
            if ch == '"':
                in_double = False
            i += 1
            continue
        if ch == "-" and nxt == "-":
            chars[i] = " "
            chars[i + 1] = " "
            i += 2
            in_line_comment = True
            continue
        if ch == "/" and nxt == "*":
            chars[i] = " "
            chars[i + 1] = " "
            i += 2
            in_block_comment = True
            continue
        if ch == "'":
            chars[i] = " "
            in_single = True
            i += 1
            continue
        if ch == '"':
            chars[i] = " "
            in_double = True
            i += 1
            continue
        i += 1
    return "".join(chars)


def locate_view_column_list_span(ddl: str) -> Optional[Tuple[int, int]]:
    """
    定位 CREATE VIEW (...) AS 语句中列清单的起止位置（不含括号）。
    若不存在列清单则返回 None。
    """
    if not ddl:
        return None
    masked = mask_sql_for_scan(ddl)
    create_match = re.search(r"\bCREATE\b", masked, flags=re.IGNORECASE)
    if not create_match:
        return None
    view_match = re.search(r"\bVIEW\b", masked[create_match.end():], flags=re.IGNORECASE)
    if not view_match:
        return None
    view_pos = create_match.end() + view_match.start()
    as_match = re.search(r"\bAS\b", masked[view_pos:], flags=re.IGNORECASE)
    if not as_match:
        return None
    as_pos = view_pos + as_match.start()
    open_pos = masked.find("(", view_pos, as_pos)
    if open_pos == -1:
        return None
    depth = 0
    close_pos = None
    for idx in range(open_pos, as_pos):
        ch = masked[idx]
        if ch == "(":
            depth += 1
        elif ch == ")":
            depth -= 1
            if depth == 0:
                close_pos = idx
                break
    if close_pos is None:
        return None
    return open_pos + 1, close_pos


def split_sql_list_items(segment: str) -> List[str]:
    if not segment:
        return []
    items: List[str] = []
    buf: List[str] = []
    depth = 0
    in_single = False
    in_double = False
    i = 0
    length = len(segment)
    while i < length:
        ch = segment[i]
        nxt = segment[i + 1] if i + 1 < length else ""
        if in_single:
            buf.append(ch)
            if ch == "'" and nxt == "'":
                buf.append(nxt)
                i += 2
                continue
            if ch == "'":
                in_single = False
            i += 1
            continue
        if in_double:
            buf.append(ch)
            if ch == '"' and nxt == '"':
                buf.append(nxt)
                i += 2
                continue
            if ch == '"':
                in_double = False
            i += 1
            continue
        if ch == "'":
            in_single = True
            buf.append(ch)
            i += 1
            continue
        if ch == '"':
            in_double = True
            buf.append(ch)
            i += 1
            continue
        if ch == "(":
            depth += 1
        elif ch == ")":
            if depth > 0:
                depth -= 1
        if ch == "," and depth == 0:
            item = "".join(buf).strip()
            if item:
                items.append(item)
            buf = []
            i += 1
            continue
        buf.append(ch)
        i += 1
    tail = "".join(buf).strip()
    if tail:
        items.append(tail)
    return items


def is_view_constraint_item(item: str) -> bool:
    if not item:
        return False
    head = item.lstrip()
    if not head:
        return False
    return bool(re.match(r'^(CONSTRAINT|PRIMARY|UNIQUE|CHECK)\b', head, flags=re.IGNORECASE))


def _summarize_view_constraint_item(item: str) -> str:
    summary = re.sub(r'\s+', ' ', item.strip())
    if len(summary) > 200:
        return summary[:197] + "..."
    return summary


def _detect_view_constraint_state(item: str) -> Tuple[bool, str]:
    upper = re.sub(r'\s+', ' ', item.strip()).upper()
    if "ENABLE" in upper and "DISABLE" not in upper:
        return False, "ENABLE"
    if "DISABLE" in upper:
        return True, "DISABLE"
    if "NOVALIDATE" in upper:
        return True, "NOVALIDATE"
    return False, "AMBIGUOUS"


class ViewConstraintCleanupResult(NamedTuple):
    action: str
    reason: str
    detail: str
    constraints: List[str]
    cleaned_ddl: str


VIEW_CONSTRAINT_ACTION_NONE = "NONE"
VIEW_CONSTRAINT_ACTION_CLEANED = "CLEANED"
VIEW_CONSTRAINT_ACTION_UNCLEANABLE = "UNCLEANABLE"


def apply_view_constraint_cleanup(
    ddl: str,
    mode: str
) -> ViewConstraintCleanupResult:
    if not ddl:
        return ViewConstraintCleanupResult(
            action=VIEW_CONSTRAINT_ACTION_NONE,
            reason="",
            detail="",
            constraints=[],
            cleaned_ddl=ddl
        )
    mode = normalize_view_constraint_cleanup(mode)
    span = locate_view_column_list_span(ddl)
    if not span:
        return ViewConstraintCleanupResult(
            action=VIEW_CONSTRAINT_ACTION_NONE,
            reason="",
            detail="",
            constraints=[],
            cleaned_ddl=ddl
        )
    start, end = span
    column_list = ddl[start:end]
    items = split_sql_list_items(column_list)
    if not items:
        return ViewConstraintCleanupResult(
            action=VIEW_CONSTRAINT_ACTION_NONE,
            reason="",
            detail="",
            constraints=[],
            cleaned_ddl=ddl
        )
    constraint_items = [item for item in items if is_view_constraint_item(item)]
    if not constraint_items:
        return ViewConstraintCleanupResult(
            action=VIEW_CONSTRAINT_ACTION_NONE,
            reason="",
            detail="",
            constraints=[],
            cleaned_ddl=ddl
        )
    constraints_summary = [_summarize_view_constraint_item(item) for item in constraint_items]
    cleanable = True
    state_labels: List[str] = []
    for item in constraint_items:
        ok, state = _detect_view_constraint_state(item)
        state_labels.append(state)
        if not ok:
            cleanable = False
    detail = "; ".join(
        f"{state}:{summary}" for state, summary in zip(state_labels, constraints_summary)
    )
    if mode == "off":
        return ViewConstraintCleanupResult(
            action=VIEW_CONSTRAINT_ACTION_UNCLEANABLE,
            reason="view_constraint_cleanup=off",
            detail=detail,
            constraints=constraints_summary,
            cleaned_ddl=ddl
        )
    if mode == "auto" and not cleanable:
        return ViewConstraintCleanupResult(
            action=VIEW_CONSTRAINT_ACTION_UNCLEANABLE,
            reason="存在 ENABLE 或无法判断状态的约束",
            detail=detail,
            constraints=constraints_summary,
            cleaned_ddl=ddl
        )
    kept_items = [item for item in items if not is_view_constraint_item(item)]
    if not kept_items:
        return ViewConstraintCleanupResult(
            action=VIEW_CONSTRAINT_ACTION_UNCLEANABLE,
            reason="清洗后列清单为空",
            detail=detail,
            constraints=constraints_summary,
            cleaned_ddl=ddl
        )
    indent = ""
    for line in column_list.splitlines():
        if line.strip():
            indent = line[:len(line) - len(line.lstrip())]
            break
    joiner = ",\n" + indent if indent else ", "
    rebuilt = (indent + joiner.join(item.strip() for item in kept_items)).rstrip()
    cleaned_ddl = ddl[:start] + rebuilt + ddl[end:]
    return ViewConstraintCleanupResult(
        action=VIEW_CONSTRAINT_ACTION_CLEANED,
        reason="force" if mode == "force" else "auto",
        detail=detail,
        constraints=constraints_summary,
        cleaned_ddl=cleaned_ddl
    )


def analyze_view_compatibility(
    ddl: Optional[str],
    rules: Dict[str, object],
    dblink_policy: str
) -> ViewCompatResult:
    """
    分析视图 DDL 是否存在 OceanBase 不支持的语法/对象，并执行必要的兼容性重写。
    """
    if not ddl:
        return ViewCompatResult(
            support_state=SUPPORT_STATE_BLOCKED,
            reason_code="VIEW_DDL_MISSING",
            reason="视图 DDL 获取失败",
            detail="DDL 为空或不可读",
            cleaned_ddl="",
            rewrite_notes=[]
        )

    masker = SqlMasker(ddl)
    masked = masker.masked_sql
    masked_upper = masked.upper()
    rewrite_notes: List[str] = []

    if "DBA_USERS" in masked_upper and re.search(r"\bUSER_ID\b", masked_upper):
        masked = re.sub(r"\bUSER_ID\b", "USERID", masked, flags=re.IGNORECASE)
        rewrite_notes.append("DBA_USERS.USER_ID -> USERID")

    cleaned = masker.unmask(masked)

    unsupported_views = {v.upper() for v in (rules.get("unsupported_views") or set())}
    privilege_views = {v.upper() for v in (rules.get("privilege_views") or set())}
    compiled_patterns = rules.get("compiled_patterns") or []

    sys_obj_hit = False
    for pattern in compiled_patterns:
        if pattern.search(masked_upper):
            sys_obj_hit = True
            break

    x_hits: Set[str] = set()
    for match in VIEW_X_DOLLAR_PATTERN.finditer(masked_upper):
        token = (match.group(0) or "").replace('"', '').upper()
        if token.startswith("X$"):
            x_hits.add(token)

    dblink_hit = (dblink_policy == "block") and ("@" in masked_upper)

    unsupported_hits: List[str] = []
    for view_name in sorted(unsupported_views):
        if re.search(rf"\b{re.escape(view_name)}\b", masked_upper):
            unsupported_hits.append(view_name)

    privilege_hits: List[str] = []
    for view_name in sorted(privilege_views):
        if re.search(rf"\b{re.escape(view_name)}\b", masked_upper):
            privilege_hits.append(view_name)

    if sys_obj_hit:
        return ViewCompatResult(
            support_state=SUPPORT_STATE_UNSUPPORTED,
            reason_code="VIEW_SYS_OBJ",
            reason="引用 SYS.OBJ$ 基表，OB 不支持",
            detail="SYS.OBJ$",
            cleaned_ddl=cleaned,
            rewrite_notes=rewrite_notes
        )
    if dblink_hit:
        return ViewCompatResult(
            support_state=SUPPORT_STATE_UNSUPPORTED,
            reason_code="VIEW_DBLINK",
            reason="视图包含 DBLINK 引用（策略禁止）",
            detail="DBLINK",
            cleaned_ddl=cleaned,
            rewrite_notes=rewrite_notes
        )
    if unsupported_hits:
        return ViewCompatResult(
            support_state=SUPPORT_STATE_UNSUPPORTED,
            reason_code="VIEW_UNSUPPORTED_SYS_VIEW",
            reason="引用 OB 不支持的系统视图",
            detail=",".join(unsupported_hits),
            cleaned_ddl=cleaned,
            rewrite_notes=rewrite_notes
        )
    if x_hits:
        return ViewCompatResult(
            support_state=SUPPORT_STATE_UNSUPPORTED,
            reason_code="VIEW_X$",
            reason="引用 X$ 系统表，OB 不支持",
            detail=",".join(sorted(x_hits)),
            cleaned_ddl=cleaned,
            rewrite_notes=rewrite_notes
        )
    if privilege_hits:
        return ViewCompatResult(
            support_state=SUPPORT_STATE_BLOCKED,
            reason_code="VIEW_PRIVILEGE_REQUIRED",
            reason="引用 DBA 视图可能缺少权限",
            detail=",".join(privilege_hits),
            cleaned_ddl=cleaned,
            rewrite_notes=rewrite_notes
        )

    return ViewCompatResult(
        support_state=SUPPORT_STATE_SUPPORTED,
        reason_code="",
        reason="",
        detail="",
        cleaned_ddl=cleaned,
        rewrite_notes=rewrite_notes
    )


def extract_view_dependencies(
    ddl: str,
    default_schema: Optional[str] = None,
    max_depth: int = 1
) -> Set[str]:
    """
    从 VIEW DDL 中提取依赖的对象名（表/视图/同义词等）。
    改进版：
    - 使用 SqlMasker 保护字符串和注释
    - 支持提取 FROM/JOIN 后逗号分隔的多个表
    """
    dependencies: Set[str] = set()
    if not ddl:
        return dependencies

    masker = SqlMasker(ddl)
    clean_sql = masker.masked_sql

    # 归一化空白
    clean_sql = re.sub(r'\s+', ' ', clean_sql)

    # 关键词正则
    start_keywords = r'(?:FROM|JOIN|UPDATE|INTO|MERGE\s+INTO)'
    stopper_keywords = [
        'WHERE', 'GROUP', 'HAVING', 'ORDER', 'UNION', 'INTERSECT', 'MINUS', 'EXCEPT',
        'START', 'CONNECT', 'MODEL', 'WINDOW', 'FETCH', 'OFFSET', 'FOR',
        'ON', 'USING', 'LEFT', 'RIGHT', 'FULL', 'INNER', 'CROSS', 'NATURAL',
        'SELECT', 'SET', 'VALUES', 'RETURNING', 'AS'
    ]
    stopper_pattern = r'\b(?:' + '|'.join(stopper_keywords) + r')\b'

    combined_pattern = re.compile(rf'\b{start_keywords}\b', re.IGNORECASE)
    stopper_regex = re.compile(stopper_pattern + r'|' + rf'\b{start_keywords}\b', re.IGNORECASE)

    current_pos = 0
    while True:
        match = combined_pattern.search(clean_sql, current_pos)
        if not match:
            break
        
        content_start = match.end()
        stop_match = stopper_regex.search(clean_sql, content_start)
        
        if stop_match:
            content_end = stop_match.start()
            next_scan_pos = stop_match.start()
        else:
            content_end = len(clean_sql)
            next_scan_pos = len(clean_sql)

        segment = clean_sql[content_start:content_end]
        
        # 处理逗号分隔的表列表
        parts = segment.split(',')
        for part in parts:
            part = part.strip()
            # 简单过滤：忽略括号开头的子查询
            if not part:
                continue
            if part.startswith('('):
                if max_depth <= 0:
                    continue
                inner = part
                if ')' in inner:
                    inner = inner[1:inner.rfind(')')]
                else:
                    inner = inner[1:]
                if re.search(r'\bSELECT\b', inner, re.IGNORECASE):
                    dependencies.update(
                        extract_view_dependencies(inner, default_schema=default_schema, max_depth=max_depth - 1)
                    )
                continue
                
            # 取第一个 token (忽略别名)
            first_token = part.split()[0]
            candidate = first_token.replace('"', '').upper()
            candidate = re.sub(r'[^A-Z0-9_\$#\.]+$', '', candidate)
            
            if '@' in candidate:
                candidate = candidate.split('@')[0]
            
            if not candidate or candidate == 'DUAL':
                continue
            
            # 简单的合法性检查
            if not re.match(r'^[A-Z0-9_\$#\.]+$', candidate):
                continue
                
            if '.' in candidate:
                dependencies.add(candidate)
            elif default_schema:
                dependencies.add(f"{default_schema.upper()}.{candidate}")

        current_pos = next_scan_pos
        # 防止死循环
        if current_pos <= match.start():
            current_pos = match.end()

    return dependencies


def replace_unqualified_table_refs(
    ddl: str,
    replacements: Dict[str, str]
) -> str:
    """
    仅在 FROM/JOIN/UPDATE/INTO/MERGE INTO 段落中替换未带 schema 的对象名。
    避免误替换别名或 SELECT 列表中的同名标识符。
    """
    if not ddl or not replacements:
        return ddl

    masked = mask_sql_for_scan(ddl)
    start_keywords = r'(?:FROM|JOIN|UPDATE|INTO|MERGE\s+INTO)'
    stopper_keywords = [
        'WHERE', 'GROUP', 'HAVING', 'ORDER', 'UNION', 'INTERSECT', 'MINUS', 'EXCEPT',
        'START', 'CONNECT', 'MODEL', 'WINDOW', 'FETCH', 'OFFSET', 'FOR',
        'ON', 'USING', 'LEFT', 'RIGHT', 'FULL', 'INNER', 'CROSS', 'NATURAL',
        'SELECT', 'SET', 'VALUES', 'RETURNING', 'AS'
    ]
    stopper_pattern = r'\b(?:' + '|'.join(stopper_keywords) + r')\b'
    combined_pattern = re.compile(rf'\b{start_keywords}\b', re.IGNORECASE)
    stopper_regex = re.compile(stopper_pattern + r'|' + rf'\b{start_keywords}\b', re.IGNORECASE)

    edits: List[Tuple[int, int, str]] = []

    def _process_part(part_start: int, part_end: int, segment_offset: int) -> None:
        j = part_start
        while j < part_end and masked[segment_offset + j].isspace():
            j += 1
        if j >= part_end:
            return
        if masked[segment_offset + j] == '(':
            return
        k = j
        while k < part_end:
            ch = masked[segment_offset + k]
            if ch.isspace() or ch == ',' or ch == ')':
                break
            k += 1
        if k <= j:
            return
        token_raw = ddl[segment_offset + j:segment_offset + k]
        norm = token_raw.strip().strip('"')
        if not norm:
            return
        norm_u = norm.upper()
        if '.' in norm_u or '@' in norm_u:
            return
        tgt = replacements.get(norm_u)
        if not tgt:
            return
        edits.append((segment_offset + j, segment_offset + k, tgt))

    current_pos = 0
    while True:
        match = combined_pattern.search(masked, current_pos)
        if not match:
            break
        content_start = match.end()
        stop_match = stopper_regex.search(masked, content_start)
        if stop_match:
            content_end = stop_match.start()
            next_scan_pos = stop_match.start()
        else:
            content_end = len(masked)
            next_scan_pos = len(masked)
        segment = masked[content_start:content_end]
        depth = 0
        part_start = 0
        for idx, ch in enumerate(segment):
            if ch == '(':
                depth += 1
            elif ch == ')':
                if depth > 0:
                    depth -= 1
            elif ch == ',' and depth == 0:
                _process_part(part_start, idx, content_start)
                part_start = idx + 1
        _process_part(part_start, len(segment), content_start)

        current_pos = next_scan_pos
        if current_pos <= match.start():
            current_pos = match.end()

    if not edits:
        return ddl

    for start, end, tgt in sorted(edits, key=lambda x: x[0], reverse=True):
        ddl = ddl[:start] + tgt + ddl[end:]
    return ddl


def remap_view_dependencies(
    ddl: str, 
    view_schema: str,
    view_name: str,
    remap_rules: RemapRules,
    full_object_mapping: FullObjectMapping,
    synonym_meta: Optional[Dict[Tuple[str, str], SynonymMeta]] = None,
    view_dependency_map: Optional[Dict[Tuple[str, str], Set[str]]] = None
) -> str:
    """
    根据remap规则重写VIEW DDL中的依赖对象引用
    改进：
    - 使用 SqlMasker 确保只替换 SQL 代码，不替换注释/字符串
    - 支持 PUBLIC/私有同义词解析到基表后再 remap
    - 当 SQL 提取不足时可使用依赖元数据 fallback
    """
    if not ddl:
        return ddl
    
    # 提取依赖对象（无 schema 的引用按 view_schema 兜底）
    dependencies = extract_view_dependencies(ddl, default_schema=view_schema)
    if view_dependency_map:
        dep_key = ((view_schema or "").upper(), (view_name or "").upper())
        fallback_deps = view_dependency_map.get(dep_key)
        if fallback_deps:
            dependencies |= {d.upper() for d in fallback_deps}

    # 构建替换映射（全名与无前缀引用分别处理）
    replacements_qualified: Dict[str, str] = {}
    replacements_unqualified: Dict[str, str] = {}
    view_schema_u = (view_schema or "").upper()
    synonym_meta = synonym_meta or {}
    preferred_types = ("TABLE", "VIEW", "MATERIALIZED VIEW", "SYNONYM", "FUNCTION")

    def _resolve_synonym(owner: str, dep_obj: str) -> Optional[str]:
        meta = synonym_meta.get((owner.upper(), dep_obj.upper()))
        if not meta or meta.db_link:
            return None
        base_full = f"{meta.table_owner}.{meta.table_name}"
        mapped = find_mapped_target_any_type(
            full_object_mapping,
            base_full,
            preferred_types=preferred_types
        )
        explicit = remap_rules.get(base_full)
        # 当 full_object_mapping 因冲突回退为 1:1 时，显式 remap 规则优先。
        if mapped and explicit and mapped.upper() == base_full and explicit.upper() != base_full:
            mapped = explicit
        if not mapped:
            mapped = explicit
        return (mapped or base_full).upper()

    for dep in dependencies:
        dep_u = dep.upper()
        if not dep_u:
            continue
        dep_schema = view_schema_u
        dep_obj = dep_u
        if '.' in dep_u:
            dep_schema, dep_obj = dep_u.split('.', 1)

        mapped_target: Optional[str] = None
        if dep_schema == "PUBLIC":
            mapped_target = _resolve_synonym("PUBLIC", dep_obj)
        else:
            mapped_target = find_mapped_target_any_type(
                full_object_mapping,
                dep_u,
                preferred_types=preferred_types
            )
            explicit = remap_rules.get(dep_u)
            # 当 full_object_mapping 因冲突回退为 1:1 时，显式 remap 规则优先。
            if mapped_target and explicit and mapped_target.upper() == dep_u and explicit.upper() != dep_u:
                mapped_target = explicit
            if not mapped_target:
                mapped_target = explicit
            # 若直接映射缺失，或落在 identity 映射，尝试同 schema 私有同义词
            if (not mapped_target) or (mapped_target.upper() == dep_u):
                mapped_syn = _resolve_synonym(dep_schema, dep_obj)
                if mapped_syn:
                    mapped_target = mapped_syn
            # 对当前 VIEW schema 的裸名引用，继续兜底 PUBLIC 同义词
            if ((not mapped_target) or (mapped_target.upper() == dep_u)) and dep_schema == view_schema_u:
                mapped_public = _resolve_synonym("PUBLIC", dep_obj)
                if mapped_public:
                    mapped_target = mapped_public
        if not mapped_target:
            continue
        tgt_u = mapped_target.upper()
        replacements_qualified[dep_u] = tgt_u
        if dep_schema == view_schema_u:
            # 无前缀引用也替换为全名(或目标名)，避免跨 schema 迁移后失效
            replacements_unqualified.setdefault(dep_obj.upper(), tgt_u)

    if not replacements_qualified and not replacements_unqualified:
        return ddl

    # 使用 Masker 保护
    masker = SqlMasker(ddl)
    working_sql = masker.masked_sql

    for src_ref in sorted(replacements_qualified.keys(), key=len, reverse=True):
        tgt_ref = replacements_qualified[src_ref]
        pattern = re.compile(
            rf'(?<![A-Z0-9_\$#"]){re.escape(src_ref)}(?![A-Z0-9_\$#"])',
            re.IGNORECASE
        )
        working_sql = pattern.sub(tgt_ref, working_sql)

    rewritten = masker.unmask(working_sql)
    if replacements_unqualified:
        rewritten = replace_unqualified_table_refs(rewritten, replacements_unqualified)
    return rewritten


def remap_synonym_target(
    ddl: str,
    remap_rules: RemapRules,
    full_object_mapping: FullObjectMapping
) -> str:
    """
    将 SYNONYM 的 FOR 子句指向 remap 后的目标对象（支持跨 schema，如 PUBLIC 同义词）。
    """
    if not ddl:
        return ddl

    pattern = re.compile(
        r'\bFOR\s+("?[A-Z0-9_\$#]+"?(?:\s*\.\s*"?[A-Z0-9_\$#]+"?)?)',
        re.IGNORECASE
    )

    def _format_target(target: str, quoted_like_src: bool) -> str:
        parts = target.upper().split('.', 1)
        if len(parts) != 2:
            return target.upper()
        if quoted_like_src:
            return '.'.join(f'"{p}"' for p in parts)
        return target.upper()

    def _repl(match: re.Match) -> str:
        raw_target = match.group(1)
        normalized = raw_target.replace('"', '').replace(' ', '').upper()
        if '.' not in normalized:
            return match.group(0)

        mapped = find_mapped_target_any_type(
            full_object_mapping,
            normalized,
            preferred_types=("TABLE", "VIEW", "MATERIALIZED VIEW", "SYNONYM")
        ) or remap_rules.get(normalized)

        if not mapped or '.' not in mapped:
            return match.group(0)

        new_target = _format_target(mapped, '"' in raw_target)
        return f"FOR {new_target}"

    return pattern.sub(_repl, ddl, count=1)


def normalize_public_synonym_name(ddl: str, synonym_name: str) -> str:
    """
    确保 PUBLIC SYNONYM 的名称不带 schema 前缀。
    将 "CREATE OR REPLACE PUBLIC SYNONYM SCHEMA.NAME" 归一为 "CREATE OR REPLACE PUBLIC SYNONYM NAME"。
    """
    if not ddl or not synonym_name:
        return ddl
    name_u = synonym_name.upper().split('.', 1)[-1]
    pattern = re.compile(
        r'(CREATE\s+(?:OR\s+REPLACE\s+)?PUBLIC\s+SYNONYM\s+)(?:"?[A-Z0-9_\$#]+"?\s*\.)?"?([A-Z0-9_\$#]+)"?',
        re.IGNORECASE
    )

    def _repl(match: re.Match) -> str:
        return f"{match.group(1)}{name_u}"

    return pattern.sub(_repl, ddl, count=1)


def oracle_get_views_ddl_batch(
    ora_cfg: OraConfig,
    view_objects: List[Tuple[str, str]]  # [(schema, view_name), ...]
) -> Dict[Tuple[str, str], str]:
    """
    批量获取VIEW的DDL，使用DBMS_METADATA
    
    Args:
        ora_cfg: Oracle连接配置
        view_objects: 视图对象列表 [(schema, view_name), ...]
    
    Returns:
        {(schema, view_name): ddl_text}
    """
    if not view_objects:
        return {}
    
    log.info("[VIEW] 正在批量获取 %d 个VIEW的DDL (使用DBMS_METADATA)...", len(view_objects))
    
    results = {}
    
    try:
        with oracledb.connect(
            user=ora_cfg['user'],
            password=ora_cfg['password'],
            dsn=ora_cfg['dsn']
        ) as connection:
            setup_metadata_session(connection)
            
            for schema, view_name in view_objects:
                try:
                    ddl = oracle_get_ddl(connection, 'VIEW', schema, view_name)
                    if ddl:
                        results[(schema, view_name)] = ddl
                        log.debug("[VIEW] 成功获取 %s.%s 的DDL", schema, view_name)
                    else:
                        log.warning("[VIEW] 未能获取 %s.%s 的DDL", schema, view_name)
                except Exception as exc:
                    log.warning("[VIEW] 获取 %s.%s DDL失败: %s", schema, view_name, exc)
                    
    except Exception as exc:
        log.error("[VIEW] 批量获取VIEW DDL时连接失败: %s", exc)
    
    log.info("[VIEW] 成功获取 %d/%d 个VIEW的DDL", len(results), len(view_objects))
    return results


def oracle_get_ddl(ora_conn, obj_type: str, owner: str, name: str) -> Optional[str]:
    sql = "SELECT DBMS_METADATA.GET_DDL(:1, :2, :3) FROM DUAL"
    obj_type_norm = DDL_OBJ_TYPE_MAPPING.get(obj_type.upper(), obj_type.upper())
    try:
        with ora_conn.cursor() as cursor:
            cursor.execute(sql, [obj_type_norm, name.upper(), owner.upper()])
            row = cursor.fetchone()
            if not row or row[0] is None:
                return None
            return str(row[0])
    except (oracledb.Error, Exception) as e:
        log.warning(f"[DDL] 获取 {obj_type} {owner}.{name} DDL 失败: {e}")
        return None


def oracle_get_view_text(
    ora_conn,
    owner: str,
    name: str
) -> Optional[Tuple[str, str, str]]:
    sql = """
        SELECT TEXT, READ_ONLY, CHECK_OPTION
        FROM DBA_VIEWS
        WHERE OWNER = :1 AND VIEW_NAME = :2
    """
    try:
        with ora_conn.cursor() as cursor:
            cursor.execute(sql, [owner.upper(), name.upper()])
            row = cursor.fetchone()
            if not row or row[0] is None:
                return None
            text = str(row[0])
            read_only = (row[1] or "").strip().upper()
            check_option = (row[2] or "").strip().upper()
            return text, read_only, check_option
    except (oracledb.Error, Exception) as exc:
        log.warning("[DDL] 读取 DBA_VIEWS.%s.%s 失败: %s", owner, name, exc)
        return None


QUALIFIED_NAME_PATTERN = re.compile(
    r'^\s*(?:"([^"]+)"|([A-Z0-9_\$#]+))\s*\.\s*(?:"([^"]+)"|([A-Z0-9_\$#]+))\s*$',
    re.IGNORECASE
)


def quote_identifier(name: str) -> str:
    if name is None:
        return ""
    text = str(name).strip()
    if not text:
        return text
    if text.startswith('"') and text.endswith('"'):
        return text
    return f'"{text}"'


def quote_qualified_parts(*parts: str) -> str:
    return ".".join(
        quote_identifier(part)
        for part in parts
        if part is not None and str(part).strip() != ""
    )


def normalize_qualified_name(name: str) -> Optional[str]:
    if not name:
        return None
    text = str(name).strip()
    match = QUALIFIED_NAME_PATTERN.match(text)
    if not match:
        return None
    schema = match.group(1) or match.group(2)
    obj = match.group(3) or match.group(4)
    return quote_qualified_parts(schema, obj)


def ensure_quoted_qualified(name: str) -> str:
    return normalize_qualified_name(name) or name


def build_view_ddl_from_text(
    owner: str,
    name: str,
    text: str,
    read_only: str,
    check_option: str
) -> Optional[str]:
    if not text:
        return None
    view_full = quote_qualified_parts(owner.upper(), name.upper())
    ddl = f"CREATE OR REPLACE VIEW {view_full} AS {text.strip()}"
    upper_text = text.upper()
    if check_option and check_option != "NONE" and "WITH CHECK OPTION" not in upper_text:
        ddl = f"{ddl} WITH CHECK OPTION"
    if read_only == "Y" and "WITH READ ONLY" not in upper_text:
        ddl = f"{ddl} WITH READ ONLY"
    if not ddl.rstrip().endswith(";"):
        ddl = ddl.rstrip() + ";"
    return ddl


# 批量获取 DDL 的对象类型（仅支持这些类型）
BATCH_DDL_ALLOWED_TYPES = {
    'TABLE', 'VIEW', 'MATERIALIZED VIEW',
    'PROCEDURE', 'FUNCTION', 'PACKAGE', 'PACKAGE BODY',
    'SYNONYM', 'SEQUENCE', 'TRIGGER',
    'TYPE', 'TYPE BODY',
    'CONSTRAINT'
}


def oracle_get_ddl_batch(
    ora_cfg: OraConfig,
    objects: List[Tuple[str, str, str]]  # [(schema, obj_type, obj_name), ...]
) -> Dict[Tuple[str, str, str], str]:
    """
    批量获取多个对象的 DDL，在一次连接中完成。
    
    Args:
        ora_cfg: Oracle 连接配置
        objects: 对象列表 [(schema, obj_type, obj_name), ...]
    
    Returns:
        {(schema, obj_type, obj_name): ddl_text, ...}
    """
    if not objects:
        return {}
    
    # 过滤不支持的类型
    valid_objects = [
        (s.upper(), t.upper(), n.upper()) 
        for s, t, n in objects 
        if t.upper() in BATCH_DDL_ALLOWED_TYPES
    ]
    if not valid_objects:
        return {}
    
    results: Dict[Tuple[str, str, str], str] = {}
    failed_count = 0
    
    try:
        with oracledb.connect(
            user=ora_cfg['user'],
            password=ora_cfg['password'],
            dsn=ora_cfg['dsn']
        ) as conn:
            # 设置 DBMS_METADATA 参数
            setup_metadata_session(conn)
            
            log.info("[DDL] 批量获取 %d 个对象的 DBMS_METADATA DDL...", len(valid_objects))
            start_time = time.time()
            
            with conn.cursor() as cursor:
                sql = "SELECT DBMS_METADATA.GET_DDL(:1, :2, :3) FROM DUAL"
                for schema, obj_type, obj_name in valid_objects:
                    obj_type_norm = DDL_OBJ_TYPE_MAPPING.get(obj_type, obj_type)
                    try:
                        cursor.execute(sql, [obj_type_norm, obj_name, schema])
                        row = cursor.fetchone()
                        if row and row[0] is not None:
                            results[(schema, obj_type, obj_name)] = str(row[0])
                    except oracledb.Error as e:
                        err_msg = str(e).upper()
                        # 忽略对象不存在的错误
                        if not any(code in err_msg for code in ("ORA-31603", "ORA-04043", "ORA-00942")):
                            failed_count += 1
                            if failed_count <= 3:
                                log.warning("[DDL] 获取 %s.%s (%s) 失败: %s", schema, obj_name, obj_type, e)
            
            elapsed = time.time() - start_time
            log.info("[DDL] 批量获取完成，成功 %d/%d，用时 %.2fs。", 
                     len(results), len(valid_objects), elapsed)
    except oracledb.Error as e:
        log.error("[DDL] 批量获取 DDL 连接失败: %s", e)
    
    return results


def adjust_ddl_for_object(
    ddl: str,
    src_schema: str,
    src_name: str,
    tgt_schema: str,
    tgt_name: str,
    extra_identifiers: Optional[List[Tuple[Tuple[str, str], Tuple[str, str]]]] = None,
    obj_type: Optional[str] = None
) -> str:
    """
    依据 remap 结果调整 DBMS_METADATA 生成的 DDL：
      - 先替换主对象 (schema+name)
      - 再按需替换依赖对象 (如索引/触发器引用的表)
      - 若发生 remap，确保 CREATE 语句显式带上目标 schema
    extra_identifiers: [ ((src_schema, src_name), (tgt_schema, tgt_name)), ... ]
    obj_type: 供定位 CREATE 语句使用的对象类型
    """
    src_schema_u = (src_schema or "").upper()
    src_name_u = (src_name or "").upper()
    tgt_schema_u = (tgt_schema or "").upper()
    tgt_name_u = (tgt_name or "").upper()
    mapping_changed = (src_schema_u != tgt_schema_u) or (src_name_u != tgt_name_u)

    def replace_identifier(text: str, src_s: str, src_n: str, tgt_s: str, tgt_n: str) -> str:
        if not src_s or not src_n or not tgt_s or not tgt_n:
            return text
        src_s_u = src_s.upper()
        src_n_u = src_n.upper()
        tgt_s_u = tgt_s.upper()
        tgt_n_u = tgt_n.upper()

        pattern_quoted = re.compile(
            rf'"{re.escape(src_s_u)}"\."{re.escape(src_n_u)}"',
            re.IGNORECASE
        )
        pattern_unquoted = re.compile(
            rf'\b{re.escape(src_s_u)}\.{re.escape(src_n_u)}\b',
            re.IGNORECASE
        )

        text = pattern_quoted.sub(f'"{tgt_s_u}"."{tgt_n_u}"', text)
        text = pattern_unquoted.sub(f'{tgt_s_u}.{tgt_n_u}', text)
        return text

    def replace_unqualified_identifier(text: str, src_n: str, tgt_s: str, tgt_n: str) -> str:
        """
        当源对象在自身 schema 内被 remap 到其他 schema 时，源 DDL 中的无前缀引用
        会错误地落到当前 schema。这里尽量只在“疑似对象引用”的上下文中替换，避免误伤列名/变量。
        """
        if not src_n or not tgt_s or not tgt_n:
            return text
        src_n_u = src_n.upper()
        tgt_s_u = tgt_s.upper()
        tgt_n_u = tgt_n.upper()
        tgt_full = f"{tgt_s_u}.{tgt_n_u}"

        name_pattern = re.compile(rf'\b{re.escape(src_n_u)}\b', re.IGNORECASE)
        token_pattern = re.compile(r'[A-Z_][A-Z0-9_\$#]*', re.IGNORECASE)
        stop_tokens = {
            'SELECT', 'WHERE', 'GROUP', 'HAVING', 'ORDER', 'CONNECT', 'START',
            'WITH', 'UNION', 'INTERSECT', 'MINUS', 'EXCEPT',
            'WHEN', 'THEN', 'ELSE', 'BEGIN', 'DECLARE', 'IS', 'AS', 'LOOP', 'END',
            'FETCH', 'CLOSE', 'OPEN', 'VALUES', 'SET', 'RETURN', 'CASE', 'OVER',
            'PARTITION', 'CHECK', 'CONSTRAINT', 'PRIMARY', 'FOREIGN', 'UNIQUE',
            'DEFAULT'
        }

        def _has_insert_or_merge(tokens: List[str]) -> bool:
            """向前寻找 INSERT/MERGE（在遇到 stop token 前）。"""
            for tok in reversed(tokens):
                if tok in stop_tokens:
                    return False
                if tok in ('INSERT', 'MERGE'):
                    return True
            return False

        def _nearest_context(tokens: List[str]) -> Optional[str]:
            """
            从后往前查找最近的上下文关键词（FROM/JOIN/UPDATE/DELETE/TRUNCATE/TABLE/INTO/USING/ON）。
            遇到 stop token 则终止。
            """
            for tok in reversed(tokens):
                if tok in stop_tokens:
                    return None
                if tok in ('FROM', 'JOIN', 'UPDATE', 'DELETE', 'TRUNCATE', 'TABLE'):
                    return tok
                if tok == 'INTO':
                    return tok
                if tok == 'USING':
                    return 'USING'
                if tok == 'ON':
                    return 'ON'
                if tok == 'REFERENCES':
                    return 'REFERENCES'
            return None

        def _looks_like_namespace(after: str) -> bool:
            """
            判断是否是 pkg.func / seq.NEXTVAL 这类“名称后跟点”的调用。
            允许 NEXTVAL/CURRVAL，或后续紧跟 ( / . 认为是包/类型访问。
            """
            m = re.match(r'\s*\.\s*([A-Z_][A-Z0-9_\$#]*)', after, re.IGNORECASE)
            if not m:
                return False
            next_token = m.group(1).upper()
            if next_token in ('NEXTVAL', 'CURRVAL'):
                return True
            rest = after[m.end():].lstrip()
            return rest.startswith('(') or rest.startswith('.')

        def _ddl_on_context(prefix: str) -> bool:
            """
            判断是否位于 DDL 中的 ON 子句（如 CREATE INDEX ... ON / TRIGGER ... ON）。
            仅在 ON 紧跟对象名场景下允许替换，避免 JOIN ... ON 条件被误替换。
            """
            prefix_upper = prefix.upper().rstrip()
            if not prefix_upper.endswith(' ON'):
                return False
            return bool(
                re.search(r'\bINDEX\b', prefix_upper)
                or re.search(r'\bTRIGGER\b', prefix_upper)
                or re.search(r'\bMATERIALIZED\s+VIEW\s+LOG\b', prefix_upper)
            )

        def _repl(match: re.Match) -> str:
            start, end = match.span()
            # 向前查首个非空白字符，若为 '.' 则已限定 schema
            idx = start - 1
            while idx >= 0 and text[idx].isspace():
                idx -= 1
            if idx >= 0 and text[idx] == '"':
                idx -= 1
                while idx >= 0 and text[idx].isspace():
                    idx -= 1
            if idx >= 0 and text[idx] == '.':
                return match.group(0)

            after = text[end:]
            if _looks_like_namespace(after):
                return tgt_full

            tokens_before = [t.upper() for t in token_pattern.findall(text[:start])]
            context = _nearest_context(tokens_before)
            if not context:
                prefix = text[max(0, start - 160):start]
                if _ddl_on_context(prefix):
                    return tgt_full
                return match.group(0)
            if context == 'INTO' and not _has_insert_or_merge(tokens_before):
                return match.group(0)
            if context == 'USING' and not _has_insert_or_merge(tokens_before):
                return match.group(0)
            if context == 'ON':
                prefix = text[max(0, start - 160):start]
                if not _ddl_on_context(prefix):
                    return match.group(0)
            if context == 'REFERENCES':
                prefix = text[max(0, start - 80):start].upper().rstrip()
                if not prefix.endswith('REFERENCES'):
                    return match.group(0)
                return tgt_full
            return tgt_full

        return name_pattern.sub(_repl, text)

    result = replace_identifier(ddl, src_schema, src_name, tgt_schema, tgt_name)
    
    # 处理主对象的裸名引用（如 END package_name）
    if mapping_changed and src_name_u != tgt_name_u:
        pattern = re.compile(rf'\b{re.escape(src_name_u)}\b', re.IGNORECASE)
        def _repl_main(match: re.Match) -> str:
            start = match.start()
            idx = start - 1
            while idx >= 0 and result[idx].isspace():
                idx -= 1
            if idx >= 0 and result[idx] == '"':
                idx -= 1
                while idx >= 0 and result[idx].isspace():
                    idx -= 1
            if idx >= 0 and result[idx] == '.':
                return match.group(0)
            return tgt_name_u
        result = pattern.sub(_repl_main, result)

    if extra_identifiers:
        # 构建快速查找字典：{(src_schema, src_obj): (tgt_schema, tgt_obj)}
        replacement_dict: Dict[Tuple[str, str], Tuple[str, str]] = {}
        for (src_pair, tgt_pair) in extra_identifiers:
            key = (src_pair[0].upper(), src_pair[1].upper())
            candidate = (tgt_pair[0].upper(), tgt_pair[1].upper())
            existing = replacement_dict.get(key)
            # 同一源对象出现多条映射时，优先保留“非自映射”目标，避免被 A=A 覆盖。
            if existing is None:
                replacement_dict[key] = candidate
                continue
            src_identity = key
            existing_is_identity = existing == src_identity
            candidate_is_identity = candidate == src_identity
            if existing_is_identity and not candidate_is_identity:
                replacement_dict[key] = candidate
        
        # 只对字典中存在的对象执行替换（避免循环所有规则）
        # 使用原有的 replace_identifier 逻辑，但只处理实际需要替换的
        for (src_s, src_o), (tgt_s, tgt_o) in replacement_dict.items():
            # 检查DDL中是否包含这个对象引用（快速预检）
            src_s_u = src_s.upper()
            src_o_u = src_o.upper()
            if src_s_u not in result.upper() or src_o_u not in result.upper():
                continue
            
            # 执行精确替换
            result = replace_identifier(result, src_s, src_o, tgt_s, tgt_o)
        
        # 处理裸名引用（无schema前缀）
        # 规则：
        # - 仅处理“源 schema 内的引用”（裸名默认解析到 CURRENT_SCHEMA）
        # - 当依赖对象的目标 schema != 主对象目标 schema，或目标名称发生变化时，需要补全/替换
        src_schema_u = src_schema.upper()
        main_tgt_schema_u = tgt_schema_u
        for (src_s, src_o), (tgt_s, tgt_o) in replacement_dict.items():
            if src_s.upper() != src_schema_u:
                continue
            src_o_u = src_o.upper()
            if src_o_u == src_name.upper():
                continue
            tgt_s_u = tgt_s.upper()
            tgt_o_u = tgt_o.upper()
            # 若依赖最终落在主对象目标 schema 且名称未变，裸名仍可正确解析
            if tgt_s_u == main_tgt_schema_u and tgt_o_u == src_o_u:
                continue
            if src_o_u not in result.upper():
                continue
            result = replace_unqualified_identifier(result, src_o, tgt_s_u, tgt_o_u)

    def qualify_main_object_creation(text: str) -> str:
        """在 remap 后为主对象的 CREATE 语句补全 schema 前缀。"""
        if not obj_type or not mapping_changed:
            return text
        type_pattern = CREATE_OBJECT_PATTERNS.get(obj_type.upper())
        if not type_pattern:
            return text
        create_prefix = (
            r'^\s*CREATE\s+(?:OR\s+REPLACE\s+)?'
            r'(?:FORCE\s+)?'
            r'(?:EDITIONABLE\s+|NONEDITIONABLE\s+)?'
            + type_pattern +
            r'\s+'
        )
        candidates = [tgt_name_u]
        if src_name_u and src_name_u != tgt_name_u:
            candidates.append(src_name_u)

        for cand in candidates:
            pattern = re.compile(
                create_prefix + rf'(?P<name>"?{re.escape(cand)}"?)(?!\s*\.)',
                re.IGNORECASE | re.MULTILINE
            )

            def _repl(match: re.Match) -> str:
                name_txt = match.group('name')
                if '.' in name_txt:
                    return match.group(0)
                return match.group(0).replace(name_txt, f"{tgt_schema_u}.{tgt_name_u}", 1)

            new_text = pattern.sub(_repl, text, count=1)
            if new_text != text:
                return new_text
        return text

    if mapping_changed:
        # PUBLIC SYNONYM 不应在对象名上补 schema 前缀，只需要改名
        if obj_type and obj_type.upper() == 'SYNONYM' and tgt_schema_u == 'PUBLIC':
            return result
        result = qualify_main_object_creation(result)

    return result


DELIMITER_LINE_PATTERN = re.compile(r'^\s*DELIMITER\b.*$', re.IGNORECASE)
BLOCK_END_PATTERN = re.compile(r'^\s*\$\$\s*;?\s*$', re.IGNORECASE)


def cleanup_dbcat_wrappers(ddl: str) -> str:
    """
    dbcat 在导出 PL/SQL 时可能使用 DELIMITER/$$ 包裹。
    这些标记在 OceanBase (Oracle 模式) 中无效，需要移除。
    """
    lines = []
    for line in ddl.splitlines():
        if DELIMITER_LINE_PATTERN.match(line):
            continue
        if BLOCK_END_PATTERN.match(line):
            lines.append('/')
            continue
        lines.append(line)
    return "\n".join(lines)


def prepend_set_schema(ddl: str, schema: str) -> str:
    """
    在 ddl 前加上 ALTER SESSION SET CURRENT_SCHEMA，避免对象落到错误的 schema。
    若已存在 set current schema 指令则不重复添加。
    """
    schema_u = schema.upper()
    lines = ddl.splitlines()
    head = "\n".join(lines[:3]).lower()
    if 'set current_schema' in head:
        return ddl
    prefix = f"ALTER SESSION SET CURRENT_SCHEMA = {schema_u};"
    return "\n".join([prefix, ddl])


USING_INDEX_PATTERN_WITH_OPTIONS = re.compile(
    r'USING\s+INDEX\s*\((?:[^)(]+|\((?:[^)(]+|\([^)(]*\))*\))*\)\s*(ENABLE|DISABLE)',
    re.IGNORECASE
)
USING_INDEX_PATTERN_SIMPLE = re.compile(
    r'USING\s+INDEX\s+(ENABLE|DISABLE)',
    re.IGNORECASE
)
USING_INDEX_PATTERN_NAME = re.compile(
    r'USING\s+INDEX\s+("[^"]+"|[A-Za-z0-9_$#]+)(\s+(ENABLE|DISABLE))?',
    re.IGNORECASE
)
MV_REFRESH_ON_DEMAND_PATTERN = re.compile(r'\s+ON\s+DEMAND', re.IGNORECASE)


def clean_plsql_ending(ddl: str) -> str:
    """
    清理PL/SQL对象结尾的语法问题
    
    问题：Oracle允许 END XXXX; 后跟单独的 ; 和 /，但OceanBase要求只能是 /
    修复：移除 END 语句后多余的分号，保留最后的 /
    """
    if not ddl:
        return ddl
    
    lines = ddl.split('\n')
    cleaned_lines = []
    i = 0
    
    while i < len(lines):
        line = lines[i].strip()
        # 检查是否是 END 语句（支持 END "OBJ"; / END OBJ; / END;）
        if re.match(r'^\s*END(?:\s+(?:"[^"]+"|[A-Z0-9_$#]+))?\s*;\s*$', line, re.IGNORECASE):
            cleaned_lines.append(lines[i])  # 保留原始格式的 END 语句
            i += 1
            
            # 跳过后续的单独分号行
            while i < len(lines):
                next_line = lines[i].strip()
                if next_line == ';':
                    i += 1  # 跳过单独的分号
                elif next_line == '/':
                    cleaned_lines.append(lines[i])  # 保留斜杠
                    i += 1
                    break
                elif next_line == '':
                    cleaned_lines.append(lines[i])  # 保留空行
                    i += 1
                else:
                    # 遇到其他内容，停止处理
                    break
        else:
            cleaned_lines.append(lines[i])
            i += 1
    
    return '\n'.join(cleaned_lines)


END_SCHEMA_PREFIX_PATTERN = re.compile(
    r'(\bEND\s+)(?:(?:"[^"]+"|[A-Z0-9_$#]+)\s*\.\s*)(?P<obj>"[^"]+"|[A-Z0-9_$#]+)(\s*;\s*(?:--.*)?)',
    re.IGNORECASE
)


def clean_end_schema_prefix(ddl: str) -> str:
    """
    清理 PL/SQL 结尾 END 子句中的 schema 前缀。
    Oracle 允许 END schema.obj; 但 OceanBase 仅支持 END obj; 或 END;。
    支持单行 DDL（END 前后可能没有换行）。
    """
    if not ddl:
        return ddl
    return END_SCHEMA_PREFIX_PATTERN.sub(r"\1\g<obj>\3", ddl)


FOR_LOOP_RANGE_SINGLE_DOT_PATTERN = re.compile(
    r'(\bIN\s+-?\d+)\s*\.(\s*)(?P<right>"[^"]+"|[A-Z_][A-Z0-9_$#]*)',
    re.IGNORECASE
)


def clean_for_loop_single_dot_range(ddl: str) -> str:
    """
    修复 FOR ... IN 1.var 这种单点范围写法为 1..var。
    仅在 IN 后接整数(含 0/1/其他)且点号后为标识符时生效，避免误伤小数。
    """
    if not ddl:
        return ddl
    def _repl(match: re.Match) -> str:
        right_token = (match.group("right") or "").strip().strip('"').upper()
        # 避免将科学计数法误判为范围（例如 IN 1.E10）
        if re.fullmatch(r"E[+-]?\d+", right_token):
            return match.group(0)
        return f"{match.group(1)}..{match.group(2)}{match.group('right')}"

    return FOR_LOOP_RANGE_SINGLE_DOT_PATTERN.sub(_repl, ddl)


FOR_LOOP_COLLECTION_ATTR_PATTERN = re.compile(
    r'(\.(?:FIRST|LAST|COUNT))\s*\.(?!\.)(\s*(?:[A-Z_][A-Z0-9_$#]*|\d+))',
    re.IGNORECASE
)


def clean_for_loop_collection_attr_range(ddl: str) -> str:
    """
    修复 FOR ... IN collection.FIRST.collection.LAST 这种写法为 collection.FIRST..collection.LAST。
    """
    if not ddl:
        return ddl
    return FOR_LOOP_COLLECTION_ATTR_PATTERN.sub(r"\1..\2", ddl)


def clean_extra_semicolons(ddl: str) -> str:
    """
    清理多余的分号
    
    问题：某些语句可能有连续的分号 ;;
    修复：将连续分号替换为单个分号
    """
    if not ddl:
        return ddl

    out: List[str] = []
    in_single = False
    in_double = False
    in_line_comment = False
    in_block_comment = False
    prev_semicolon = False

    i = 0
    n = len(ddl)
    while i < n:
        ch = ddl[i]
        nxt = ddl[i + 1] if i + 1 < n else ''

        if in_line_comment:
            out.append(ch)
            if ch == '\n':
                in_line_comment = False
                prev_semicolon = False
            i += 1
            continue

        if in_block_comment:
            out.append(ch)
            if ch == '*' and nxt == '/':
                out.append(nxt)
                i += 2
                in_block_comment = False
                prev_semicolon = False
                continue
            i += 1
            continue

        if not in_single and not in_double:
            if ch == '-' and nxt == '-':
                out.append(ch)
                out.append(nxt)
                i += 2
                in_line_comment = True
                prev_semicolon = False
                continue
            if ch == '/' and nxt == '*':
                out.append(ch)
                out.append(nxt)
                i += 2
                in_block_comment = True
                prev_semicolon = False
                continue

        if not in_double and ch == "'":
            out.append(ch)
            if in_single and nxt == "'":
                out.append(nxt)
                i += 2
                continue
            in_single = not in_single
            prev_semicolon = False
            i += 1
            continue

        if not in_single and ch == '"':
            out.append(ch)
            if in_double and nxt == '"':
                out.append(nxt)
                i += 2
                continue
            in_double = not in_double
            prev_semicolon = False
            i += 1
            continue

        if not in_single and not in_double and ch == ';':
            if prev_semicolon:
                i += 1
                continue
            out.append(ch)
            prev_semicolon = True
            i += 1
            continue

        out.append(ch)
        if not ch.isspace():
            prev_semicolon = False
        i += 1

    return "".join(out)


EXTRA_DOTS_PATTERN = re.compile(
    r'(?P<left>"[^"]+"|[A-Z_][A-Z0-9_$#]*)\s*\.(?:\s*\.){2,}\s*(?P<right>"[^"]+"|[A-Z_][A-Z0-9_$#]*)',
    re.IGNORECASE
)


def clean_extra_dots(ddl: str) -> str:
    """
    清理多余的点号
    
    问题：对象名可能有多余的点，如 SCHEMA..TABLE
    修复：将连续点号替换为单个点号
    """
    if not ddl:
        return ddl

    # 仅收敛 3 个及以上连续点号，避免破坏 PL/SQL 合法范围运算符 ".."
    cleaned = EXTRA_DOTS_PATTERN.sub(r"\g<left>.\g<right>", ddl)
    return cleaned


def clean_trailing_whitespace(ddl: str) -> str:
    """
    清理行尾空白字符
    """
    if not ddl:
        return ddl
    
    lines = ddl.split('\n')
    cleaned_lines = [line.rstrip() for line in lines]
    return '\n'.join(cleaned_lines)


def clean_empty_lines(ddl: str) -> str:
    """
    清理多余的空行（保留适当的空行）
    """
    if not ddl:
        return ddl
    
    # 将连续的多个空行替换为最多2个空行
    cleaned = re.sub(r'\n\s*\n\s*\n+', '\n\n\n', ddl)
    return cleaned


def extract_trigger_table_references(ddl: str) -> Set[str]:
    """
    从触发器DDL中提取引用的表名
    
    Args:
        ddl: 触发器的DDL语句
    
    Returns:
        引用表名的集合，格式为 SCHEMA.TABLE_NAME
    """
    if not ddl:
        return set()

    table_refs: Set[str] = set()
    masker = SqlMasker(ddl)
    working_sql = masker.masked_sql

    qname_pattern = r'(?P<schema>"[^"]+"|[A-Z0-9_$#]+)\s*\.\s*(?P<name>"[^"]+"|[A-Z0-9_$#]+)'
    on_pattern = re.compile(rf'\bON\s+{qname_pattern}', re.IGNORECASE)
    body_patterns = [
        re.compile(rf'\bINSERT\s+INTO\s+{qname_pattern}', re.IGNORECASE),
        re.compile(rf'\bUPDATE\s+{qname_pattern}', re.IGNORECASE),
        re.compile(rf'\bDELETE\s+FROM\s+{qname_pattern}', re.IGNORECASE),
        re.compile(rf'\bFROM\s+{qname_pattern}', re.IGNORECASE),
        re.compile(rf'\bJOIN\s+{qname_pattern}', re.IGNORECASE),
    ]

    def _normalize(match: re.Match) -> str:
        schema = (match.group("schema") or "").strip().strip('"').upper()
        name = (match.group("name") or "").strip().strip('"').upper()
        return f"{schema}.{name}" if schema and name else ""

    for m in on_pattern.finditer(working_sql):
        normalized = _normalize(m)
        if normalized:
            table_refs.add(normalized)

    for pattern in body_patterns:
        for m in pattern.finditer(working_sql):
            normalized = _normalize(m)
            if normalized:
                table_refs.add(normalized)

    return table_refs


def remap_trigger_table_references(
    ddl: str,
    full_object_mapping: FullObjectMapping
) -> str:
    """
    根据remap规则重写触发器DDL中的表引用
    
    Args:
        ddl: 原始触发器DDL
        full_object_mapping: 完整的对象映射
    
    Returns:
        重写后的DDL
    """
    if not ddl:
        return ddl

    table_refs = extract_trigger_table_references(ddl)
    replacements: Dict[str, str] = {}
    for table_ref in table_refs:
        tgt_name = find_mapped_target_any_type(
            full_object_mapping,
            table_ref,
            preferred_types=("TABLE",)
        )
        if tgt_name:
            replacements[table_ref.upper()] = ensure_quoted_qualified(tgt_name)

    if not replacements:
        return ddl

    masker = SqlMasker(ddl)
    working_sql = masker.masked_sql
    result_sql = working_sql

    for src_ref, tgt_ref in sorted(replacements.items(), key=lambda item: len(item[0]), reverse=True):
        try:
            src_schema, src_name = src_ref.split(".", 1)
        except ValueError:
            continue
        pattern = re.compile(
            rf'(?<![A-Z0-9_$#"])'
            rf'(?:"?{re.escape(src_schema)}"?\s*\.\s*"?{re.escape(src_name)}"?)'
            rf'(?![A-Z0-9_$#"])',
            re.IGNORECASE
        )
        result_sql = pattern.sub(tgt_ref, result_sql)
        log.debug("[TRIGGER] 重映射表引用: %s -> %s", src_ref, tgt_ref)

    return masker.unmask(result_sql)


TRIGGER_REF_PREFERRED_TYPES: Tuple[str, ...] = (
    "TABLE",
    "VIEW",
    "MATERIALIZED VIEW",
    "SEQUENCE",
    "SYNONYM",
    "PACKAGE",
    "PACKAGE BODY",
    "FUNCTION",
    "PROCEDURE",
    "TYPE",
    "TYPE BODY",
    "TRIGGER"
)

TRIGGER_QUALIFIED_REF_PATTERN = re.compile(
    r'(?P<schema>"[^"]+"|[A-Z0-9_\$#]+)\s*\.\s*(?P<object>"[^"]+"|[A-Z0-9_\$#]+)',
    re.IGNORECASE
)
TRIGGER_SEQ_UNQUALIFIED_PATTERN = re.compile(
    r'(?<!\.)\b(?P<name>"?[A-Z0-9_\$#]+"?)\s*\.\s*(?P<suffix>NEXTVAL|CURRVAL)\b',
    re.IGNORECASE
)
TRIGGER_DML_PATTERNS: Tuple[re.Pattern, ...] = (
    re.compile(r'(\bINSERT\s+INTO\s+)(?P<name>"?[A-Z0-9_\$#]+"?)', re.IGNORECASE),
    re.compile(r'(\bUPDATE\s+)(?!OF\b)(?P<name>"?[A-Z0-9_\$#]+"?)', re.IGNORECASE),
    re.compile(r'(\bDELETE\s+FROM\s+)(?P<name>"?[A-Z0-9_\$#]+"?)', re.IGNORECASE),
    re.compile(r'(\bMERGE\s+INTO\s+)(?P<name>"?[A-Z0-9_\$#]+"?)', re.IGNORECASE),
    re.compile(r'(\bFROM\s+)(?P<name>"?[A-Z0-9_\$#]+"?)', re.IGNORECASE),
    re.compile(r'(\bJOIN\s+)(?P<name>"?[A-Z0-9_\$#]+"?)', re.IGNORECASE),
)


def remap_trigger_object_references(
    ddl: str,
    full_object_mapping: FullObjectMapping,
    source_schema: Optional[str],
    tgt_schema: Optional[str],
    tgt_trigger: Optional[str],
    *,
    on_target: Optional[Tuple[str, str]] = None,
    qualify_schema: bool = True
) -> str:
    """
    触发器 DDL 的 schema 补全与 remap 重写：
    - CREATE TRIGGER 主对象名强制带目标 schema
    - ON 子句与触发器体内 DML/序列引用补全 schema
    - 已带 schema 的引用按 remap 规则重写
    """
    if not ddl:
        return ddl
    if not qualify_schema:
        return remap_trigger_table_references(ddl, full_object_mapping)

    src_schema_u = (source_schema or "").upper()
    tgt_schema_u = (tgt_schema or "").upper()
    tgt_trigger_u = (tgt_trigger or "").upper()
    on_schema_u = (on_target[0] or "").upper() if on_target else ""
    on_table_u = (on_target[1] or "").upper() if on_target else ""

    def _strip_quotes(text: str) -> str:
        return (text or "").strip().strip('"').upper()

    def _map_full_name(schema: str, obj: str) -> Optional[str]:
        if not schema or not obj:
            return None
        src_full = f"{schema}.{obj}".upper()
        return find_mapped_target_any_type(
            full_object_mapping,
            src_full,
            preferred_types=TRIGGER_REF_PREFERRED_TYPES
        )

    def _map_unqualified(name: str) -> Optional[str]:
        if not src_schema_u or not name:
            return None
        return _map_full_name(src_schema_u, name)

    masker = SqlMasker(ddl)
    working_sql = masker.masked_sql

    # CREATE TRIGGER 主对象名强制补 schema
    if tgt_schema_u and tgt_trigger_u:
        name_pattern = re.compile(
            r'(CREATE\s+(?:OR\s+REPLACE\s+)?TRIGGER\s+)(?P<name>"?[A-Z0-9_\$#]+"?(?:\s*\.\s*"?[A-Z0-9_\$#]+"?)?)',
            re.IGNORECASE
        )
        working_sql = name_pattern.sub(
            lambda m: f"{m.group(1)}{quote_qualified_parts(tgt_schema_u, tgt_trigger_u)}",
            working_sql,
            count=1
        )

    # ON 子句（仅匹配 CREATE TRIGGER 段落）
    if on_schema_u and on_table_u:
        on_pattern = re.compile(
            r'(CREATE\s+(?:OR\s+REPLACE\s+)?TRIGGER\b.*?\bON\s+)(?P<name>"?[A-Z0-9_\$#]+"?(?:\s*\.\s*"?[A-Z0-9_\$#]+"?)?)',
            re.IGNORECASE | re.DOTALL
        )
        working_sql = on_pattern.sub(
            lambda m: f"{m.group(1)}{quote_qualified_parts(on_schema_u, on_table_u)}",
            working_sql,
            count=1
        )

    # 替换已带 schema 的引用
    def _replace_qualified(match: re.Match) -> str:
        # 避免误匹配触发器伪记录 :NEW/:OLD 之类的字段引用
        idx = match.start() - 1
        text = match.string
        while idx >= 0 and text[idx].isspace():
            idx -= 1
        if idx >= 0 and text[idx] == ':':
            return match.group(0)
        schema = _strip_quotes(match.group("schema"))
        obj = _strip_quotes(match.group("object"))
        if schema in ("NEW", "OLD"):
            return match.group(0)
        tgt_full = _map_full_name(schema, obj)
        return ensure_quoted_qualified(tgt_full) if tgt_full else match.group(0)

    working_sql = TRIGGER_QUALIFIED_REF_PATTERN.sub(_replace_qualified, working_sql)

    # 补全触发器体内 DML 对象引用
    def _replace_dml(match: re.Match) -> str:
        prefix = match.group(1)
        name_raw = match.group("name")
        name_clean = _strip_quotes(name_raw)
        if not name_clean or "." in name_clean:
            return match.group(0)
        text = match.string
        pos = match.end()
        while pos < len(text) and text[pos].isspace():
            pos += 1
        if pos < len(text) and text[pos] == ".":
            return match.group(0)
        tgt_full = _map_unqualified(name_clean)
        return f"{prefix}{ensure_quoted_qualified(tgt_full)}" if tgt_full else match.group(0)

    for pattern in TRIGGER_DML_PATTERNS:
        working_sql = pattern.sub(_replace_dml, working_sql)

    # 补全序列 NEXTVAL/CURRVAL
    def _replace_seq(match: re.Match) -> str:
        name_raw = match.group("name")
        suffix = match.group("suffix")
        name_clean = _strip_quotes(name_raw)
        if not name_clean:
            return match.group(0)
        text = match.string
        pos = match.start() - 1
        while pos >= 0 and text[pos].isspace():
            pos -= 1
        if pos >= 0 and text[pos] == '"':
            pos -= 1
            while pos >= 0 and text[pos].isspace():
                pos -= 1
        if pos >= 0 and text[pos] == ".":
            return match.group(0)
        tgt_full = find_mapped_target_any_type(
            full_object_mapping,
            f"{src_schema_u}.{name_clean}" if src_schema_u else name_clean,
            preferred_types=("SEQUENCE",)
        )
        if not tgt_full:
            return match.group(0)
        return f"{ensure_quoted_qualified(tgt_full)}.{suffix}"

    working_sql = TRIGGER_SEQ_UNQUALIFIED_PATTERN.sub(_replace_seq, working_sql)

    return masker.unmask(working_sql)


def remap_plsql_object_references(
    ddl: str,
    obj_type: str,
    full_object_mapping: FullObjectMapping,
    source_schema: Optional[str] = None,
    *,
    trigger_qualify_schema: bool = True,
    trigger_on_target: Optional[Tuple[str, str]] = None,
    trigger_tgt_schema: Optional[str] = None,
    trigger_tgt_name: Optional[str] = None
) -> str:
    """
    重映射PL/SQL对象（PROCEDURE、FUNCTION、PACKAGE等）中的对象引用
    改进：
    - 支持 source_schema 以解析本地未限定引用
    - 使用 SqlMasker 保护
    - TRIGGER 可选强制补全 schema 前缀
    """
    if not ddl:
        return ddl
    
    obj_type_upper = obj_type.upper()
    
    # 触发器需要特殊处理表引用与 schema 补全
    if obj_type_upper == 'TRIGGER':
        return remap_trigger_object_references(
            ddl,
            full_object_mapping,
            source_schema,
            trigger_tgt_schema,
            trigger_tgt_name,
            on_target=trigger_on_target,
            qualify_schema=trigger_qualify_schema
        )
    
    masker = SqlMasker(ddl)
    working_sql = masker.masked_sql
    
    # 收集需要替换的引用
    replacements = {}
    preferred_types = (
        "TABLE", "VIEW", "MATERIALIZED VIEW", "SEQUENCE",
        "SYNONYM", "PACKAGE", "PACKAGE BODY", "FUNCTION",
        "PROCEDURE", "TYPE", "TYPE BODY", "TRIGGER"
    )

    # 1. 查找 SCHEMA.OBJECT 格式的引用
    ref_pattern = r'\b([A-Z_][A-Z0-9_]*\.[A-Z_][A-Z0-9_]*)\b'
    matches = re.findall(ref_pattern, working_sql, re.IGNORECASE)
    for match in matches:
        ref_name = match.strip().strip('"').upper()
        if '.' in ref_name:
            tgt_name = find_mapped_target_any_type(
                full_object_mapping,
                ref_name,
                preferred_types=preferred_types
            )
            if tgt_name and tgt_name.upper() != ref_name:
                replacements[ref_name] = tgt_name.upper()

    # 2. 查找未限定引用 (如果提供了 source_schema)
    if source_schema:
        # 查找所有可能的标识符
        ident_pattern = r'\b([A-Z_][A-Z0-9_\$#]*)\b'
        candidates = set(re.findall(ident_pattern, working_sql, re.IGNORECASE))
        
        # 排除保留字 (简单列表)
        reserved = {'BEGIN', 'END', 'IF', 'THEN', 'ELSE', 'LOOP', 'COMMIT', 'ROLLBACK', 'SELECT', 'FROM', 'WHERE', 'AND', 'OR'}
        
        for cand in candidates:
            cand_u = cand.upper()
            if cand_u in reserved:
                continue
                
            # 假设它是 source_schema 下的对象
            full_src = f"{source_schema.upper()}.{cand_u}"
            tgt_name = find_mapped_target_any_type(
                full_object_mapping,
                full_src,
                preferred_types=preferred_types
            )
            
            if tgt_name:
                # 仅当目标全名与当前不一致时替换
                # 例如 TAB -> TGT.TAB
                tgt_u = tgt_name.upper()
                if tgt_u != cand_u:
                     replacements[cand_u] = tgt_u

    # 执行替换
    if replacements:
        for src_ref in sorted(replacements.keys(), key=len, reverse=True):
            tgt_ref = replacements[src_ref]
            
            if '.' in src_ref:
                pattern = r'\b' + re.escape(src_ref) + r'\b'
                working_sql = re.sub(pattern, tgt_ref, working_sql, flags=re.IGNORECASE)
            else:
                 # 未限定引用，需确保不匹配已限定引用的尾部
                 pattern = r'(?<![A-Z0-9_\$#"\.])\b' + re.escape(src_ref) + r'\b'
                 working_sql = re.sub(pattern, tgt_ref, working_sql, flags=re.IGNORECASE)
            
            log.debug("[%s] 重映射对象引用: %s -> %s", obj_type_upper, src_ref, tgt_ref)
    
    return masker.unmask(working_sql)


_HINT_KEYWORD_RE = re.compile(r'([A-Z_][A-Z0-9_$#]*)(?:@[\w$#]+)?', re.IGNORECASE)
_Q_QUOTE_PAIRS = {
    "[": "]",
    "{": "}",
    "(": ")",
    "<": ">",
}
HINT_SAMPLE_LIMIT = 6


def _normalize_hint_sample(token: str, max_len: int = 60) -> str:
    text = re.sub(r'\s+', ' ', (token or '').strip())
    if len(text) > max_len:
        return text[:max_len - 3] + "..."
    return text


def _append_hint_sample(samples: List[str], value: str, limit: int = HINT_SAMPLE_LIMIT) -> None:
    if not value or value in samples or len(samples) >= limit:
        return
    samples.append(value)


def _extract_hint_keyword(token: str) -> str:
    if not token:
        return ""
    match = _HINT_KEYWORD_RE.match(token.strip())
    return match.group(1).upper() if match else ""


def _split_hint_tokens(content: str) -> List[str]:
    tokens: List[str] = []
    buf: List[str] = []
    depth = 0
    in_single = False
    in_double = False
    i = 0
    n = len(content)

    def flush() -> None:
        token = "".join(buf).strip()
        if token:
            tokens.append(token)
        buf.clear()

    while i < n:
        ch = content[i]
        nxt = content[i + 1] if i + 1 < n else ""

        if in_single:
            buf.append(ch)
            if ch == "'" and nxt == "'":
                buf.append(nxt)
                i += 2
                continue
            if ch == "'":
                in_single = False
            i += 1
            continue

        if in_double:
            buf.append(ch)
            if ch == '"' and nxt == '"':
                buf.append(nxt)
                i += 2
                continue
            if ch == '"':
                in_double = False
            i += 1
            continue

        if ch == "'":
            in_single = True
            buf.append(ch)
            i += 1
            continue
        if ch == '"':
            in_double = True
            buf.append(ch)
            i += 1
            continue
        if ch == "(":
            depth += 1
            buf.append(ch)
            i += 1
            continue
        if ch == ")" and depth > 0:
            depth -= 1
            buf.append(ch)
            i += 1
            continue

        if depth == 0 and (ch.isspace() or ch == ","):
            flush()
            i += 1
            continue

        buf.append(ch)
        i += 1

    flush()
    return tokens


def _consume_q_quote(sql: str, start: int) -> Optional[int]:
    n = len(sql)
    if start + 2 >= n or sql[start + 1] != "'":
        return None
    delimiter = sql[start + 2]
    end_delim = _Q_QUOTE_PAIRS.get(delimiter, delimiter)
    i = start + 3
    while i + 1 < n:
        if sql[i] == end_delim and sql[i + 1] == "'":
            return i + 2
        i += 1
    return n


def filter_oracle_hints(
    ddl: str,
    policy: str,
    allowlist: Set[str],
    denylist: Set[str],
    sample_limit: int = HINT_SAMPLE_LIMIT
) -> HintFilterResult:
    if not ddl:
        return HintFilterResult(ddl, 0, 0, 0, 0, [], [], [])
    if "/*+" not in ddl:
        return HintFilterResult(ddl, 0, 0, 0, 0, [], [], [])

    policy = policy if policy in DDL_HINT_POLICY_VALUES else DDL_HINT_POLICY_DEFAULT
    allowset = {h.upper() for h in (allowlist or set())}
    denyset = {h.upper() for h in (denylist or set())}

    kept_samples: List[str] = []
    removed_samples: List[str] = []
    unknown_samples: List[str] = []
    total = kept = removed = unknown = 0

    out: List[str] = []
    i = 0
    n = len(ddl)
    in_single = False
    in_double = False
    in_line_comment = False
    in_block_comment = False

    while i < n:
        ch = ddl[i]
        nxt = ddl[i + 1] if i + 1 < n else ""

        if in_line_comment:
            out.append(ch)
            if ch == "\n":
                in_line_comment = False
            i += 1
            continue

        if in_block_comment:
            out.append(ch)
            if ch == "*" and nxt == "/":
                out.append(nxt)
                in_block_comment = False
                i += 2
            else:
                i += 1
            continue

        if not in_single and not in_double:
            if ch in ("q", "Q") and nxt == "'":
                if i == 0 or not (ddl[i - 1].isalnum() or ddl[i - 1] in ("_", "$", "#")):
                    end = _consume_q_quote(ddl, i)
                    if end:
                        out.append(ddl[i:end])
                        i = end
                        continue
            if ch == "-" and nxt == "-":
                in_line_comment = True
                out.append(ch)
                out.append(nxt)
                i += 2
                continue
            if ch == "/" and nxt == "*":
                if i + 2 < n and ddl[i + 2] == "+":
                    end = ddl.find("*/", i + 3)
                    if end == -1:
                        out.append(ddl[i:])
                        break
                    content = ddl[i + 3:end]
                    tokens = _split_hint_tokens(content)
                    kept_tokens: List[str] = []
                    for token in tokens:
                        keyword = _extract_hint_keyword(token)
                        supported = bool(keyword) and keyword in allowset
                        denied = bool(keyword) and keyword in denyset
                        is_unknown = not supported
                        total += 1
                        if is_unknown:
                            unknown += 1
                            sample = _normalize_hint_sample(keyword or token)
                            _append_hint_sample(unknown_samples, sample, sample_limit)
                        if denied or policy == DDL_HINT_POLICY_DROP_ALL:
                            removed += 1
                            sample = _normalize_hint_sample(keyword or token)
                            _append_hint_sample(removed_samples, sample, sample_limit)
                            continue
                        if policy == DDL_HINT_POLICY_KEEP_SUPPORTED and not supported:
                            removed += 1
                            sample = _normalize_hint_sample(keyword or token)
                            _append_hint_sample(removed_samples, sample, sample_limit)
                            continue
                        kept += 1
                        kept_tokens.append(token)
                        sample = _normalize_hint_sample(keyword or token)
                        _append_hint_sample(kept_samples, sample, sample_limit)
                    if kept_tokens:
                        out.append("/*+ " + " ".join(kept_tokens) + " */")
                    else:
                        out.append(" ")
                    i = end + 2
                    continue
                in_block_comment = True
                out.append(ch)
                out.append(nxt)
                i += 2
                continue

        if not in_double and ch == "'":
            out.append(ch)
            if in_single and nxt == "'":
                out.append(nxt)
                i += 2
                continue
            in_single = not in_single
            i += 1
            continue

        if not in_single and ch == '"':
            out.append(ch)
            if in_double and nxt == '"':
                out.append(nxt)
                i += 2
                continue
            in_double = not in_double
            i += 1
            continue

        out.append(ch)
        i += 1

    return HintFilterResult(
        "".join(out),
        total,
        kept,
        removed,
        unknown,
        kept_samples,
        removed_samples,
        unknown_samples
    )


def clean_oracle_hints(ddl: str) -> str:
    """移除Oracle特有的Hint语法"""
    if not ddl:
        return ddl
    return re.sub(r'/\*\+[^*]*\*/', '', ddl, flags=re.DOTALL)


def clean_storage_clauses(ddl: str) -> str:
    """移除Oracle特有的存储子句"""
    if not ddl:
        return ddl
    
    # 移除STORAGE子句
    cleaned = re.sub(r'\s+STORAGE\s*\([^)]+\)', '', ddl, flags=re.IGNORECASE)
    
    # 移除TABLESPACE子句（OceanBase可能不完全兼容）
    cleaned = re.sub(r'\s+TABLESPACE\s+\w+', '', cleaned, flags=re.IGNORECASE)
    
    return cleaned


def clean_pragma_statements(ddl: str) -> str:
    """移除OceanBase不支持的PRAGMA语句"""
    if not ddl:
        return ddl
    
    # 移除PRAGMA AUTONOMOUS_TRANSACTION
    cleaned = re.sub(r'\s*PRAGMA\s+AUTONOMOUS_TRANSACTION\s*;', '', ddl, flags=re.IGNORECASE)
    
    # 移除其他可能的PRAGMA语句
    cleaned = re.sub(r'\s*PRAGMA\s+\w+[^;]*;', '', cleaned, flags=re.IGNORECASE)
    
    return cleaned


def clean_oracle_specific_syntax(ddl: str) -> str:
    """清理Oracle特有语法"""
    if not ddl:
        return ddl
    
    # 移除BFILE数据类型引用
    cleaned = re.sub(r'\bBFILE\b', 'BLOB', ddl, flags=re.IGNORECASE)
    
    # 移除XMLTYPE特殊语法
    cleaned = re.sub(r'\s+XMLTYPE\s+COLUMN\s+\w+\s+XMLSCHEMA[^;]*', '', cleaned, flags=re.IGNORECASE)
    
    return cleaned


def clean_interval_partition_clause(ddl: str) -> str:
    """移除 TABLE DDL 中的 INTERVAL 分区子句（OceanBase 不支持）。"""
    if not ddl:
        return ddl

    upper = ddl.upper()
    if "INTERVAL" not in upper:
        return ddl

    cleaned = ddl
    search_pos = 0
    while True:
        match = re.search(r"\bINTERVAL\b", cleaned[search_pos:], flags=re.IGNORECASE)
        if not match:
            break
        start = search_pos + match.start()
        paren_start = cleaned.find("(", start + len(match.group(0)))
        if paren_start == -1:
            break
        i = paren_start
        depth = 0
        in_single = False
        in_double = False
        while i < len(cleaned):
            ch = cleaned[i]
            if ch == "'" and not in_double:
                if in_single and i + 1 < len(cleaned) and cleaned[i + 1] == "'":
                    i += 2
                    continue
                in_single = not in_single
                i += 1
                continue
            if ch == '"' and not in_single:
                in_double = not in_double
                i += 1
                continue
            if in_single or in_double:
                i += 1
                continue
            if ch == "(":
                depth += 1
            elif ch == ")":
                depth -= 1
                if depth == 0:
                    cleaned = cleaned[:start] + cleaned[i + 1:]
                    search_pos = start
                    break
            i += 1
        else:
            break

    cleaned = re.sub(r"[ \t]+", " ", cleaned)
    cleaned = re.sub(r"\s+\(", " (", cleaned)
    return cleaned


def clean_editionable_flags(ddl: str) -> str:
    """移除 EDITIONABLE / NONEDITIONABLE 关键字（OceanBase 不需要）。"""
    if not ddl:
        return ddl
    cleaned = re.sub(r'\b(?:NON)?EDITIONABLE\b', ' ', ddl, flags=re.IGNORECASE)
    return re.sub(r'[ \t]+', ' ', cleaned)


def clean_sequence_unsupported_options(ddl: str) -> str:
    """移除 OceanBase 不支持的 SEQUENCE 选项"""
    if not ddl:
        return ddl
    masked = mask_sql_for_scan(ddl)
    cleaned_chars = list(ddl)
    token_pattern = re.compile(r"\b(?:NOKEEP|NOSCALE|GLOBAL)\b", re.IGNORECASE)
    for match in token_pattern.finditer(masked):
        for idx in range(match.start(), match.end()):
            cleaned_chars[idx] = " "
    cleaned = "".join(cleaned_chars)
    cleaned = re.sub(r"[ \t]+", " ", cleaned)
    cleaned = re.sub(r" \n", "\n", cleaned)
    return cleaned


def clean_semicolon_before_slash(ddl: str) -> str:
    """
    移除单独一行的分号，其后紧跟 / 的情况（常见于 PL/SQL 导出）。
    """
    if not ddl:
        return ddl
    lines = ddl.splitlines()
    cleaned: List[str] = []
    i = 0
    while i < len(lines):
        stripped = lines[i].strip()
        if stripped == ';':
            j = i + 1
            while j < len(lines) and lines[j].strip() == '':
                j += 1
            if j < len(lines) and lines[j].strip() == '/':
                i += 1
                continue
        cleaned.append(lines[i])
        i += 1
    return "\n".join(cleaned)


def clean_long_types_in_table_ddl(ddl: str) -> str:
    """
    将 TABLE DDL 中的 LONG/LONG RAW 转换为 CLOB/BLOB。
    """
    if not ddl:
        return ddl
    masked = mask_sql_for_scan(ddl)
    edits: List[Tuple[int, int, str]] = []
    occupied: List[Tuple[int, int]] = []

    def _occupied(start: int, end: int) -> bool:
        for s, e in occupied:
            if start < e and end > s:
                return True
        return False

    for m in re.finditer(r'\bLONG\s+RAW\b', masked, flags=re.IGNORECASE):
        edits.append((m.start(), m.end(), 'BLOB'))
        occupied.append((m.start(), m.end()))
    for m in re.finditer(r'\bLONG\b', masked, flags=re.IGNORECASE):
        if _occupied(m.start(), m.end()):
            continue
        edits.append((m.start(), m.end(), 'CLOB'))

    if not edits:
        return ddl

    result = ddl
    for start, end, replacement in sorted(edits, key=lambda x: x[0], reverse=True):
        result = result[:start] + replacement + result[end:]
    return result


# DDL清理规则配置（更新为包含生产环境规则）
DDL_CLEANUP_RULES = {
    # PL/SQL对象需要特殊的结尾处理和PRAGMA清理
    'PLSQL_OBJECTS': {
        'types': ['PROCEDURE', 'FUNCTION', 'PACKAGE', 'PACKAGE BODY', 'TYPE', 'TYPE BODY', 'TRIGGER'],
        'rules': [
            clean_end_schema_prefix,
            clean_editionable_flags,
            clean_for_loop_single_dot_range,
            clean_for_loop_collection_attr_range,
            clean_plsql_ending,
            clean_semicolon_before_slash,
            clean_pragma_statements,
            clean_oracle_specific_syntax,
            clean_extra_semicolons,
            clean_extra_dots,
            clean_trailing_whitespace,
            clean_empty_lines,
        ]
    },
    
    # 表对象需要存储子句清理
    'TABLE_OBJECTS': {
        'types': ['TABLE'],
        'rules': [
            clean_long_types_in_table_ddl,
            clean_interval_partition_clause,
            clean_storage_clauses,
            clean_oracle_specific_syntax,
            clean_extra_semicolons,
            clean_extra_dots,
            clean_trailing_whitespace,
            clean_empty_lines,
        ]
    },
    
    # SEQUENCE 对象需要移除 OceanBase 不支持的选项
    'SEQUENCE_OBJECTS': {
        'types': ['SEQUENCE'],
        'rules': [
            clean_sequence_unsupported_options,
            clean_oracle_specific_syntax,
            clean_extra_semicolons,
            clean_extra_dots,
            clean_trailing_whitespace,
            clean_empty_lines,
        ]
    },
    
    # 其他对象的通用清理
    'GENERAL_OBJECTS': {
        'types': ['VIEW', 'MATERIALIZED VIEW', 'SYNONYM'],
        'rules': [
            clean_editionable_flags,
            clean_oracle_specific_syntax,
            clean_extra_semicolons,
            clean_extra_dots,
            clean_trailing_whitespace,
            clean_empty_lines,
        ]
    }
}
DDL_CLEANUP_RULES_LOCK = threading.RLock()


def apply_ddl_cleanup_rules(ddl: str, obj_type: str) -> str:
    """
    根据对象类型应用相应的DDL清理规则
    
    Args:
        ddl: 原始DDL
        obj_type: 对象类型
    
    Returns:
        清理后的DDL
    """
    if not ddl:
        return ddl
    
    obj_type_upper = obj_type.upper()
    
    # 确定使用哪套规则（先快照，避免并发修改字典导致迭代异常）
    rules_to_apply: List[Callable[[str], str]] = []
    with DDL_CLEANUP_RULES_LOCK:
        rule_sets_snapshot = list(DDL_CLEANUP_RULES.items())
        for _rule_set_name, rule_set in rule_sets_snapshot:
            if obj_type_upper in rule_set['types']:
                rules_to_apply.extend(list(rule_set['rules']))
        # 如果没有匹配的规则，使用通用规则
        if not rules_to_apply:
            rules_to_apply = list(DDL_CLEANUP_RULES['GENERAL_OBJECTS']['rules'])
    
    # 依次应用所有规则
    cleaned_ddl = ddl
    for rule_func in rules_to_apply:
        previous = cleaned_ddl
        try:
            cleaned_ddl = rule_func(cleaned_ddl)
        except Exception as exc:
            cleaned_ddl = previous
            log.warning("DDL清理规则 %s 执行失败: %s", rule_func.__name__, exc)
    
    return cleaned_ddl


def add_custom_cleanup_rule(rule_name: str, obj_types: List[str], rule_func: Callable[[str], str]):
    """
    动态添加自定义清理规则
    
    Args:
        rule_name: 规则名称
        obj_types: 适用的对象类型列表
        rule_func: 清理函数，接受DDL字符串，返回清理后的DDL
    
    Example:
        def my_custom_rule(ddl: str) -> str:
            return ddl.replace("OLD_SYNTAX", "NEW_SYNTAX")
        
        add_custom_cleanup_rule("my_rule", ["PROCEDURE", "FUNCTION"], my_custom_rule)
    """
    # 创建新的规则集或更新现有规则集
    rule_set_name = f"CUSTOM_{rule_name.upper()}"
    
    with DDL_CLEANUP_RULES_LOCK:
        if rule_set_name not in DDL_CLEANUP_RULES:
            DDL_CLEANUP_RULES[rule_set_name] = {
                'types': [t.upper() for t in obj_types],
                'rules': []
            }
        # 添加规则函数
        DDL_CLEANUP_RULES[rule_set_name]['rules'].append(rule_func)
    
    log.info("已添加自定义DDL清理规则: %s，适用于对象类型: %s", rule_name, obj_types)


def normalize_ddl_for_ob(ddl: str) -> str:
    """
    清理 DBMS_METADATA 的输出，使其更适合在 OceanBase (Oracle 模式) 上执行：
      - 移除 "USING INDEX ... ENABLE/DISABLE" 之类 Oracle 专有语法
    未来如有更多不兼容语法，可在此扩展。
    """
    ddl = USING_INDEX_PATTERN_WITH_OPTIONS.sub(lambda m: m.group(1), ddl)
    ddl = USING_INDEX_PATTERN_SIMPLE.sub(lambda m: m.group(1), ddl)
    ddl = USING_INDEX_PATTERN_NAME.sub("", ddl)
    ddl = MV_REFRESH_ON_DEMAND_PATTERN.sub('', ddl)
    return ddl


def enforce_schema_for_ddl(ddl: str, schema: str, obj_type: str) -> str:
    obj_type_u = obj_type.upper()
    if obj_type_u not in DDL_OBJECT_TYPE_OVERRIDE:
        return ddl

    schema_u = schema.upper()
    existing_pattern = re.compile(
        rf'^\s*ALTER\s+SESSION\s+SET\s+CURRENT_SCHEMA\s*=\s*"?{re.escape(schema_u)}"?\s*;?\s*$',
        re.IGNORECASE | re.MULTILINE
    )
    if existing_pattern.search(ddl):
        return ddl

    set_stmt = f"ALTER SESSION SET CURRENT_SCHEMA = {schema_u};"
    lines = ddl.splitlines()
    insert_idx = 0

    if lines and lines[0].strip().upper().startswith('DELIMITER'):
        insert_idx = 1
        while insert_idx < len(lines) and not lines[insert_idx].strip():
            insert_idx += 1

    lines.insert(insert_idx, set_stmt)
    return "\n".join(lines)


CONSTRAINT_ENABLE_VALIDATE_PATTERN = re.compile(
    r'\s+ENABLE\s+VALIDATE',
    re.IGNORECASE
)
CONSTRAINT_ENABLE_PATTERN = re.compile(
    r'\s+ENABLE(?=\s*;)',
    re.IGNORECASE
)

ENABLE_NOVALIDATE_PATTERN = re.compile(
    r'\s*\bENABLE\s+NOVALIDATE\b',
    re.IGNORECASE
)
TRAILING_VALIDATE_TOKEN_PATTERN = re.compile(
    r'\s+(?:ENABLE\s+)?(?:NO)?VALIDATE\s*$',
    re.IGNORECASE
)
ALTER_TABLE_ADD_CONSTRAINT_PATTERN = re.compile(
    r'^\s*ALTER\s+TABLE\b.*\bADD\s+CONSTRAINT\b',
    re.IGNORECASE | re.DOTALL
)


def strip_constraint_enable(ddl: str) -> str:
    ddl = CONSTRAINT_ENABLE_VALIDATE_PATTERN.sub(' VALIDATE', ddl)
    ddl = CONSTRAINT_ENABLE_PATTERN.sub('', ddl)
    return ddl


def strip_enable_novalidate(ddl: str) -> str:
    """
    移除行内的 ENABLE NOVALIDATE 关键字组合，以适配 OB 的 CREATE TABLE。
    """
    cleaned_lines: List[str] = []
    for line in ddl.splitlines():
        cleaned = ENABLE_NOVALIDATE_PATTERN.sub('', line)
        cleaned_lines.append(cleaned.rstrip())
    return "\n".join(cleaned_lines)


def resolve_constraint_missing_validate_keyword(
    mode: str,
    src_validated: Optional[str]
) -> Tuple[str, str]:
    """
    返回缺失约束 fixup 应采用的 VALIDATE 关键字及原因标签。
    """
    mode_n = normalize_constraint_missing_fixup_validate_mode(mode)
    src_valid_n = normalize_constraint_validated_status(src_validated)
    if mode_n == "force_validate":
        return "VALIDATE", "force_validate"
    if mode_n == "source":
        if src_valid_n == "VALIDATED":
            return "VALIDATE", "source_validated"
        if src_valid_n == "NOT VALIDATED":
            return "NOVALIDATE", "source_not_validated"
        return "NOVALIDATE", "source_unknown_fallback"
    return "NOVALIDATE", "safe_novalidate"


def apply_constraint_missing_validate_mode_to_ddl(
    ddl: str,
    mode: str,
    src_validated: Optional[str],
    src_status: Optional[str] = None
) -> Tuple[str, Optional[str], Optional[str]]:
    """
    对缺失约束 DDL 应用 VALIDATE/NOVALIDATE 策略。

    Returns:
        (new_ddl, applied_keyword, reason_tag)
    """
    if not ddl:
        return ddl, None, None
    if not ALTER_TABLE_ADD_CONSTRAINT_PATTERN.search(ddl):
        return ddl, None, None

    src_status_n = normalize_constraint_enabled_status(src_status)
    keyword, reason = resolve_constraint_missing_validate_keyword(mode, src_validated)
    stmt = ddl.strip()
    had_semicolon = stmt.endswith(";")
    if had_semicolon:
        stmt = stmt[:-1].rstrip()
    while True:
        stripped = TRAILING_VALIDATE_TOKEN_PATTERN.sub("", stmt)
        if stripped == stmt:
            break
        stmt = stripped.rstrip()
    if src_status_n == "DISABLED":
        stmt = f"{stmt} DISABLE"
        keyword = "DISABLE"
        reason = "source_disabled"
    else:
        stmt = f"{stmt} ENABLE {keyword}"
    if had_semicolon:
        stmt += ";"
    return stmt, keyword, reason


def split_ddl_statements(ddl: str) -> List[str]:
    """
    以较稳健的方式按顶层分号切分 DDL：
    - 忽略字符串/注释内的分号
    - 简单识别 BEGIN/END 块，在块内不切分
    """
    if not ddl:
        return []

    statements: List[str] = []
    current: List[str] = []
    in_single = False
    in_double = False
    in_line_comment = False
    in_block_comment = False
    begin_depth = 0
    token_buf: List[str] = []

    i = 0
    n = len(ddl)
    while i < n:
        ch = ddl[i]
        nxt = ddl[i + 1] if i + 1 < n else ''

        # 处理行注释
        if in_line_comment:
            current.append(ch)
            if ch == '\n':
                in_line_comment = False
            i += 1
            continue

        # 处理块注释
        if in_block_comment:
            current.append(ch)
            if ch == '*' and nxt == '/':
                current.append(nxt)
                in_block_comment = False
                i += 2
            else:
                i += 1
            continue

        # 非字符串时识别注释起始
        if not in_single and not in_double:
            if ch == '-' and nxt == '-':
                in_line_comment = True
                current.append(ch)
                current.append(nxt)
                i += 2
                continue
            if ch == '/' and nxt == '*':
                in_block_comment = True
                current.append(ch)
                current.append(nxt)
                i += 2
                continue

        # 处理 Oracle q'...'-style 引用（如 q'[a;b]' / q'!a;b!'）
        if not in_single and not in_double and (ch == 'q' or ch == 'Q') and nxt == "'" and i + 2 < n:
            prev = ddl[i - 1] if i > 0 else ''
            if not (prev.isalnum() or prev in ('_', '$', '#')):
                open_delim = ddl[i + 2]
                close_delim = {
                    '[': ']',
                    '{': '}',
                    '(': ')',
                    '<': '>',
                }.get(open_delim, open_delim)
                j = i + 3
                found = False
                while j < n - 1:
                    if ddl[j] == close_delim and ddl[j + 1] == "'":
                        current.append(ddl[i:j + 2])
                        i = j + 2
                        found = True
                        break
                    j += 1
                if not found:
                    current.append(ddl[i:])
                    i = n
                continue

        # 处理字符串（单/双引号）
        if not in_double and ch == "'":
            current.append(ch)
            if in_single and nxt == "'":
                # Oracle 单引号转义 ''
                current.append(nxt)
                i += 2
                continue
            in_single = not in_single
            i += 1
            continue

        if not in_single and ch == '"':
            current.append(ch)
            if in_double and nxt == '"':
                current.append(nxt)
                i += 2
                continue
            in_double = not in_double
            i += 1
            continue

        # 累积 token 用于 BEGIN/END 深度探测
        if not in_single and not in_double:
            if ch.isalnum() or ch in ('_', '$', '#'):
                token_buf.append(ch)
            else:
                if token_buf:
                    token = ''.join(token_buf).upper()
                    if token == 'BEGIN':
                        begin_depth += 1
                    elif token == 'END' and begin_depth > 0:
                        begin_depth -= 1
                    token_buf = []

        current.append(ch)

        # 顶层分号切分
        if (not in_single and not in_double and begin_depth == 0 and ch == ';'):
            stmt = ''.join(current).strip()
            if stmt:
                statements.append(stmt)
            current = []

        i += 1

    if token_buf:
        token = ''.join(token_buf).upper()
        if token == 'BEGIN':
            begin_depth += 1
        elif token == 'END' and begin_depth > 0:
            begin_depth -= 1

    tail = ''.join(current).strip()
    if tail:
        statements.append(tail)
    return statements


def _split_set_schema_prefix(ddl: str) -> Tuple[str, str]:
    if not ddl:
        return "", ""
    lines = ddl.splitlines()
    idx = 0
    prefix_lines: List[str] = []
    if lines and lines[0].strip().upper().startswith("ALTER SESSION SET CURRENT_SCHEMA"):
        prefix_lines.append(lines[0])
        idx = 1
        while idx < len(lines) and not lines[idx].strip():
            prefix_lines.append(lines[idx])
            idx += 1
    if not prefix_lines:
        return "", ddl
    prefix = "\n".join(prefix_lines).rstrip()
    body = "\n".join(lines[idx:]).lstrip("\n")
    return prefix, body


def _strip_trailing_ddl_delimiters(ddl: str) -> str:
    if not ddl:
        return ""
    lines = ddl.strip().splitlines()
    while lines and lines[-1].strip() == "/":
        lines.pop()
    text = "\n".join(lines).rstrip()
    while text.endswith(";"):
        text = text[:-1].rstrip()
    return text.strip()


def _ensure_statement_terminated(ddl: str) -> str:
    if not ddl:
        return ddl
    stripped = ddl.rstrip()
    if stripped.endswith(";") or stripped.endswith("/"):
        return stripped
    return stripped + ";"


def _ensure_create_or_replace(ddl: str) -> str:
    if not ddl:
        return ddl
    return re.sub(
        r'^\s*CREATE\s+(?!OR\s+REPLACE\b)',
        'CREATE OR REPLACE ',
        ddl,
        count=1,
        flags=re.IGNORECASE
    )


def _escape_sql_literal(value: str) -> str:
    return value.replace("'", "''")


def _build_q_quote_literal(text: str) -> str:
    if text is None:
        return "''"
    for delim in ("~", "^", "|", "!", "#", "%", "@", "+", "?", "`", ":"):
        if delim not in text:
            return f"q'{delim}{text}{delim}'"
    return "'" + text.replace("'", "''") + "'"


def _build_exist_check_sql(obj_type: str, schema: str, name: str) -> Optional[str]:
    obj_type_u = (obj_type or "").upper()
    schema_u = _escape_sql_literal(schema.upper())
    name_u = _escape_sql_literal(name.upper())
    if obj_type_u == "TABLE":
        return f"SELECT COUNT(*) INTO v_count FROM ALL_TABLES WHERE OWNER='{schema_u}' AND TABLE_NAME='{name_u}'"
    if obj_type_u == "VIEW":
        return f"SELECT COUNT(*) INTO v_count FROM ALL_VIEWS WHERE OWNER='{schema_u}' AND VIEW_NAME='{name_u}'"
    if obj_type_u == "MATERIALIZED VIEW":
        return (
            "SELECT COUNT(*) INTO v_count FROM ALL_OBJECTS "
            f"WHERE OWNER='{schema_u}' AND OBJECT_NAME='{name_u}' AND OBJECT_TYPE='MATERIALIZED VIEW'"
        )
    if obj_type_u == "SEQUENCE":
        return f"SELECT COUNT(*) INTO v_count FROM ALL_SEQUENCES WHERE SEQUENCE_OWNER='{schema_u}' AND SEQUENCE_NAME='{name_u}'"
    if obj_type_u == "INDEX":
        return f"SELECT COUNT(*) INTO v_count FROM ALL_INDEXES WHERE OWNER='{schema_u}' AND INDEX_NAME='{name_u}'"
    if obj_type_u == "CONSTRAINT":
        return f"SELECT COUNT(*) INTO v_count FROM ALL_CONSTRAINTS WHERE OWNER='{schema_u}' AND CONSTRAINT_NAME='{name_u}'"
    if obj_type_u == "TRIGGER":
        return f"SELECT COUNT(*) INTO v_count FROM ALL_TRIGGERS WHERE OWNER='{schema_u}' AND TRIGGER_NAME='{name_u}'"
    if obj_type_u == "SYNONYM":
        return f"SELECT COUNT(*) INTO v_count FROM ALL_SYNONYMS WHERE OWNER='{schema_u}' AND SYNONYM_NAME='{name_u}'"
    if obj_type_u == "JOB":
        return f"SELECT COUNT(*) INTO v_count FROM ALL_SCHEDULER_JOBS WHERE OWNER='{schema_u}' AND JOB_NAME='{name_u}'"
    if obj_type_u == "SCHEDULE":
        return f"SELECT COUNT(*) INTO v_count FROM ALL_SCHEDULER_SCHEDULES WHERE OWNER='{schema_u}' AND SCHEDULE_NAME='{name_u}'"
    if obj_type_u in ("PROCEDURE", "FUNCTION", "PACKAGE", "PACKAGE BODY", "TYPE", "TYPE BODY"):
        return (
            "SELECT COUNT(*) INTO v_count FROM ALL_OBJECTS "
            f"WHERE OWNER='{schema_u}' AND OBJECT_NAME='{name_u}' AND OBJECT_TYPE='{obj_type_u}'"
        )
    return None


def _build_drop_statement(obj_type: str, schema: str, name: str, parent_table: Optional[str] = None) -> Optional[str]:
    obj_type_u = (obj_type or "").upper()
    schema_u = schema.upper()
    name_u = name.upper()
    if obj_type_u == "CONSTRAINT":
        if not parent_table:
            return None
        table_full = quote_qualified_parts(schema_u, parent_table.upper())
        return f"ALTER TABLE {table_full} DROP CONSTRAINT {name_u}"
    if obj_type_u == "PACKAGE BODY":
        return None
    if obj_type_u == "TYPE BODY":
        return None
    if obj_type_u in ("TABLE", "VIEW", "MATERIALIZED VIEW", "SEQUENCE", "INDEX", "TRIGGER",
                      "PROCEDURE", "FUNCTION", "PACKAGE", "TYPE", "SYNONYM", "JOB", "SCHEDULE"):
        obj_full = quote_qualified_parts(schema_u, name_u)
        return f"DROP {obj_type_u} {obj_full}"
    return None


def _build_extra_cleanup_drop_statement(
    obj_type: str,
    schema: str,
    name: str,
    parent_table: Optional[str] = None
) -> Optional[str]:
    obj_type_u = (obj_type or "").upper()
    schema_u = (schema or "").strip().upper()
    name_u = (name or "").strip().upper()
    if not obj_type_u or not schema_u or not name_u:
        return None
    if obj_type_u in PACKAGE_OBJECT_TYPES:
        return None
    if obj_type_u == "SYNONYM" and schema_u in ("PUBLIC", "__PUBLIC"):
        return f"DROP PUBLIC SYNONYM {name_u}"
    return _build_drop_statement(obj_type_u, schema_u, name_u, parent_table=parent_table)


def collect_extra_cleanup_candidates(
    tv_results: Optional[ReportResults],
    extra_results: Optional[ExtraCheckResults]
) -> List[Tuple[str, str, str, str]]:
    """
    汇总目标端“多余对象”的清理候选语句（默认仅输出注释候选，不直接执行）。
    返回字段: (OBJECT_TYPE, TARGET_FULL, SOURCE, SQL)
    """
    candidates: Dict[Tuple[str, str], Tuple[str, str, str, str]] = {}

    def _add_candidate(obj_type: str, target_full: str, sql_stmt: Optional[str], source: str) -> None:
        obj_type_u = (obj_type or "").upper()
        target_u = (target_full or "").upper()
        if not obj_type_u or not target_u or not sql_stmt:
            return
        key = (obj_type_u, target_u)
        if key not in candidates:
            candidates[key] = (obj_type_u, target_u, source, sql_stmt.rstrip(";") + ";")

    if tv_results:
        for obj_type, tgt_name in (tv_results.get("extra_targets", []) or []):
            parsed = parse_full_object_name(tgt_name)
            if not parsed:
                continue
            schema_u, name_u = parsed
            stmt = _build_extra_cleanup_drop_statement(obj_type, schema_u, name_u)
            _add_candidate(obj_type, f"{schema_u}.{name_u}", stmt, "PRIMARY_EXTRA")

    if extra_results:
        for item in (extra_results.get("index_mismatched", []) or []):
            parsed = parse_full_object_name(item.table)
            if not parsed:
                continue
            schema_u, _table_u = parsed
            for idx_name in sorted(item.extra_indexes or set()):
                idx_u = (idx_name or "").upper()
                stmt = _build_extra_cleanup_drop_statement("INDEX", schema_u, idx_u)
                _add_candidate("INDEX", f"{schema_u}.{idx_u}", stmt, "EXTRA_INDEX")

        for item in (extra_results.get("constraint_mismatched", []) or []):
            parsed = parse_full_object_name(item.table)
            if not parsed:
                continue
            schema_u, table_u = parsed
            for cons_name in sorted(item.extra_constraints or set()):
                cons_u = (cons_name or "").upper()
                stmt = _build_extra_cleanup_drop_statement(
                    "CONSTRAINT",
                    schema_u,
                    cons_u,
                    parent_table=table_u
                )
                _add_candidate("CONSTRAINT", f"{schema_u}.{cons_u}", stmt, "EXTRA_CONSTRAINT")

        for item in (extra_results.get("sequence_mismatched", []) or []):
            schema_u = (item.tgt_schema or "").upper()
            if not schema_u:
                continue
            for seq_name in sorted(item.extra_sequences or set()):
                seq_u = (seq_name or "").upper()
                stmt = _build_extra_cleanup_drop_statement("SEQUENCE", schema_u, seq_u)
                _add_candidate("SEQUENCE", f"{schema_u}.{seq_u}", stmt, "EXTRA_SEQUENCE")

        for item in (extra_results.get("trigger_mismatched", []) or []):
            for trg_full in sorted(item.extra_triggers or set()):
                parsed = parse_full_object_name(trg_full)
                if not parsed:
                    continue
                schema_u, trg_u = parsed
                stmt = _build_extra_cleanup_drop_statement("TRIGGER", schema_u, trg_u)
                _add_candidate("TRIGGER", f"{schema_u}.{trg_u}", stmt, "EXTRA_TRIGGER")

    return [candidates[key] for key in sorted(candidates.keys())]


def export_extra_cleanup_candidates(
    base_dir: Path,
    candidates: List[Tuple[str, str, str, str]]
) -> Optional[Path]:
    if not base_dir or not candidates:
        return None
    output_dir = Path(base_dir) / "cleanup_candidates"
    ensure_dir(output_dir)
    output_path = output_dir / "extra_cleanup_candidates.txt"
    lines: List[str] = [
        "# EXTRA 清理候选脚本（仅供人工审核）",
        "# 说明：以下 SQL 默认以注释形式输出，不会被 run_fixup 自动执行。",
        "# 字段分隔符: |",
        "# OBJECT_TYPE|TARGET_FULL|SOURCE|CANDIDATE_SQL",
        "",
        "DETAIL"
    ]
    for obj_type_u, target_u, source, sql_stmt in candidates:
        lines.append(
            "|".join([
                sanitize_pipe_field(obj_type_u),
                sanitize_pipe_field(target_u),
                sanitize_pipe_field(source),
                sanitize_pipe_field(sql_stmt),
            ])
        )

    lines.append("")
    lines.append("CANDIDATE_SQL_COMMENTS")
    for _obj_type_u, target_u, _source, sql_stmt in candidates:
        lines.append(f"-- {target_u}")
        lines.append(f"-- {sql_stmt}")

    output_path.write_text("\n".join(lines).rstrip() + "\n", encoding="utf-8")
    return output_path


def _build_guard_block(obj_type: str, schema: str, name: str, ddl: str) -> str:
    exist_sql = _build_exist_check_sql(obj_type, schema, name)
    ddl_body = _strip_trailing_ddl_delimiters(ddl)
    if not exist_sql or not ddl_body:
        return ddl
    ddl_literal = _build_q_quote_literal(ddl_body)
    return (
        "DECLARE\n"
        "  v_count NUMBER := 0;\n"
        "BEGIN\n"
        "  BEGIN\n"
        f"    {exist_sql};\n"
        "  EXCEPTION WHEN OTHERS THEN\n"
        "    v_count := 0;\n"
        "  END;\n"
        "  IF v_count = 0 THEN\n"
        f"    EXECUTE IMMEDIATE {ddl_literal};\n"
        "  END IF;\n"
        "END;\n"
        "/"
    )


def _build_drop_block(
    obj_type: str,
    schema: str,
    name: str,
    *,
    parent_table: Optional[str] = None
) -> Optional[str]:
    drop_stmt = _build_drop_statement(obj_type, schema, name, parent_table=parent_table)
    exist_sql = _build_exist_check_sql(obj_type, schema, name)
    if not drop_stmt or not exist_sql:
        return None
    drop_literal = _build_q_quote_literal(drop_stmt)
    return (
        "DECLARE\n"
        "  v_count NUMBER := 0;\n"
        "BEGIN\n"
        "  BEGIN\n"
        f"    {exist_sql};\n"
        "  EXCEPTION WHEN OTHERS THEN\n"
        "    v_count := 0;\n"
        "  END;\n"
        "  IF v_count > 0 THEN\n"
        f"    EXECUTE IMMEDIATE {drop_literal};\n"
        "  END IF;\n"
        "END;\n"
        "/"
    )


def apply_fixup_idempotency(
    ddl: str,
    obj_type: str,
    schema: str,
    name: str,
    settings: Dict,
    stats: Optional[Dict[str, int]] = None,
    *,
    parent_table: Optional[str] = None
) -> str:
    if not ddl:
        return ddl
    obj_type_u = (obj_type or "").upper()
    mode = normalize_fixup_idempotent_mode(settings.get("fixup_idempotent_mode", "replace"))
    types_set = settings.get("fixup_idempotent_types_set") or set()
    if mode == "off" or obj_type_u not in types_set:
        return ddl

    prefix, body = _split_set_schema_prefix(ddl)
    if obj_type_u in FIXUP_CREATE_REPLACE_TYPES:
        body = _ensure_create_or_replace(body)

    if mode == "replace":
        if stats is not None:
            stats["replaced"] = stats.get("replaced", 0) + 1
        parts = [p for p in (prefix, body) if p]
        return "\n".join(parts)

    if mode == "guard" and obj_type_u not in FIXUP_CREATE_REPLACE_TYPES:
        statements = split_ddl_statements(body)
        if statements:
            guarded = _build_guard_block(obj_type_u, schema, name, statements[0])
            remainder = "\n\n".join(stmt for stmt in statements[1:] if stmt.strip())
            body = guarded + (f"\n\n{remainder}" if remainder else "")
        else:
            body = _build_guard_block(obj_type_u, schema, name, body)
        if stats is not None:
            stats["guarded"] = stats.get("guarded", 0) + 1

    if mode == "drop_create":
        drop_block = _build_drop_block(obj_type_u, schema, name, parent_table=parent_table)
        if drop_block and stats is not None:
            stats["drop_create"] = stats.get("drop_create", 0) + 1
        parts = [p for p in (prefix, drop_block, body) if p]
        return "\n".join(parts)

    parts = [p for p in (prefix, body) if p]
    return "\n".join(parts)


def extract_statements_for_names(
    ddl: str,
    names: Set[str],
    predicate: Callable[[str], bool]
) -> Dict[str, List[str]]:
    result: Dict[str, List[str]] = {name.upper(): [] for name in names}
    if not ddl:
        return result

    statements = split_ddl_statements(ddl)
    for stmt in statements:
        stmt_upper = stmt.upper()
        if not predicate(stmt_upper):
            continue
        for name in names:
            name_u = name.upper()
            if (
                f'"{name_u}"' in stmt_upper
                or re.search(rf'\b{re.escape(name_u)}\b', stmt_upper)
            ):
                result.setdefault(name_u, []).append(stmt.strip())
    return result


def ensure_dir(p: Path):
    p.mkdir(parents=True, exist_ok=True)


def write_fixup_file(
    base_dir: Path,
    subdir: str,
    filename: str,
    content: str,
    header_comment: str,
    grants_to_add: Optional[List[str]] = None,
    extra_comments: Optional[List[str]] = None
):
    target_dir = base_dir / subdir
    ensure_dir(target_dir)
    file_path = target_dir / filename
    with open(file_path, 'w', encoding='utf-8') as f:
        f.write(f"-- {header_comment}\n")
        f.write("-- 本文件由校验工具自动生成，请在 OceanBase 执行前仔细审核。\n\n")
        if extra_comments:
            for line in extra_comments:
                if line:
                    f.write(f"-- {line}\n")
            f.write("\n")
        
        body = content.strip()
        f.write(body)
        f.write('\n')
        tail = body.rstrip()
        if tail and not tail.endswith((';', '/')):
            f.write(';\n')

        if grants_to_add:
            f.write('\n-- 自动追加相关授权语句\n')
            for grant_stmt in sorted(grants_to_add):
                f.write(f"{grant_stmt}\n")

    log.info(f"[FIXUP] 生成目标端订正 SQL: {file_path}")


def count_lines_with_limit(path: Path, max_lines: int) -> Tuple[int, bool]:
    if max_lines <= 0:
        return 0, False
    count = 0
    try:
        with open(path, 'r', encoding='utf-8', errors='ignore') as f:
            for line in f:
                count += 1
                if count > max_lines:
                    return count, True
    except OSError as exc:
        log.warning("读取行数失败 %s: %s", path, exc)
        return 0, False
    return count, False


def export_ddl_format_report(
    rows: List[DdlFormatReportRow],
    summary_by_type: Dict[str, Dict[str, int]],
    report_dir: Path,
    report_timestamp: Optional[str],
    scanned_total: int,
    skip_reasons: Dict[str, int]
) -> Optional[Path]:
    if not report_dir or not report_timestamp:
        return None
    output_path = Path(report_dir) / f"ddl_format_report_{report_timestamp}.txt"
    formatted_total = sum(int(v.get("formatted", 0) or 0) for v in summary_by_type.values())
    skipped_total = sum(int(v.get("skipped", 0) or 0) for v in summary_by_type.values())
    failed_total = sum(int(v.get("failed", 0) or 0) for v in summary_by_type.values())
    known_total = sum(int(v.get("total", 0) or 0) for v in summary_by_type.values())
    delimiter = "|"
    detail_header = delimiter.join(["TYPE", "STATUS", "REASON", "SIZE_BYTES", "LINES", "PATH"])
    lines: List[str] = [
        "# DDL 格式化报告",
        f"# timestamp={report_timestamp}",
        f"# scanned_files={scanned_total}",
        f"# known_type_files={known_total}",
        f"# formatted={formatted_total} skipped={skipped_total} failed={failed_total}",
        f"# 分隔符: {delimiter}",
        f"# 字段说明: {detail_header}"
    ]

    if summary_by_type:
        lines.append("")
        lines.append("SUMMARY_BY_TYPE")
        lines.append(delimiter.join(["TYPE", "TOTAL", "FORMATTED", "SKIPPED", "FAILED"]))
        for obj_type, counts in sorted(summary_by_type.items()):
            lines.append(
                delimiter.join([
                    obj_type,
                    str(counts.get('total', 0)),
                    str(counts.get('formatted', 0)),
                    str(counts.get('skipped', 0)),
                    str(counts.get('failed', 0))
                ])
            )

    if skip_reasons:
        lines.append("")
        lines.append("SKIP_REASONS")
        lines.append(delimiter.join(["REASON", "COUNT"]))
        for reason, count in sorted(skip_reasons.items()):
            lines.append(delimiter.join([reason, str(count)]))

    if rows:
        lines.append("")
        lines.append("DETAIL")
        lines.append(detail_header)
        for row in sorted(rows, key=lambda r: (r.obj_type, r.status, r.path)):
            lines.append(
                delimiter.join([
                    sanitize_pipe_field(row.obj_type),
                    sanitize_pipe_field(row.status),
                    sanitize_pipe_field(row.reason),
                    str(row.size_bytes),
                    str(row.line_count),
                    sanitize_pipe_field(row.path)
                ])
            )
    try:
        output_path.parent.mkdir(parents=True, exist_ok=True)
        output_path.write_text("\n".join(lines).rstrip() + "\n", encoding="utf-8")
        return output_path
    except OSError as exc:
        log.warning("写入 ddl_format 报告失败 %s: %s", output_path, exc)
        return None


def format_fixup_outputs(
    settings: Dict,
    fixup_dir: Path,
    report_dir: Optional[Path],
    report_timestamp: Optional[str]
) -> Optional[Path]:
    if not settings.get('ddl_format_enable', False):
        return None
    if settings.get('ddl_formatter') != DDL_FORMATTER_SQLCL:
        return None
    ddl_format_types = set(settings.get('ddl_format_type_set', set()) or set())
    if not ddl_format_types:
        log.info("[DDL_FORMAT] ddl_format_enable=true 但 ddl_format_types 为空，已跳过格式化。")
        return None
    if not fixup_dir.exists():
        log.warning("[DDL_FORMAT] fixup_dir 不存在，跳过格式化: %s", fixup_dir)
        return None

    fail_policy = settings.get('ddl_format_fail_policy', DDL_FORMAT_FAIL_FALLBACK)
    sqlcl_raw = settings.get('sqlcl_bin', '').strip()
    sqlcl_exec = resolve_sqlcl_executable(sqlcl_raw)
    if not sqlcl_exec or not sqlcl_exec.exists():
        msg = f"[DDL_FORMAT] SQLcl 路径不可用: {sqlcl_raw}"
        if fail_policy == DDL_FORMAT_FAIL_ERROR:
            raise RuntimeError(msg)
        log.warning(msg)
        return None
    if not os.access(sqlcl_exec, os.X_OK):
        log.warning("[DDL_FORMAT] SQLcl 路径不可执行: %s", sqlcl_exec)

    profile_path = (settings.get('sqlcl_profile_path') or "").strip()
    profile_file: Optional[Path] = None
    if profile_path:
        profile_file = Path(profile_path).expanduser()
        if not profile_file.exists():
            msg = f"[DDL_FORMAT] SQLcl profile 不存在: {profile_path}，将忽略该 profile。"
            if fail_policy == DDL_FORMAT_FAIL_ERROR:
                raise RuntimeError(msg)
            log.warning(msg)
            profile_file = None

    max_lines = int(settings.get('ddl_format_max_lines', 0) or 0)
    max_bytes = int(settings.get('ddl_format_max_bytes', 0) or 0)
    batch_size = int(settings.get('ddl_format_batch_size', 200) or 200)
    timeout_val = int(settings.get('ddl_format_timeout', 60) or 60)
    timeout = timeout_val if timeout_val > 0 else None

    items_to_format: List[DdlFormatItem] = []
    report_rows: List[DdlFormatReportRow] = []
    summary_by_type: Dict[str, Dict[str, int]] = defaultdict(lambda: {
        "total": 0, "formatted": 0, "skipped": 0, "failed": 0
    })
    skip_reasons: Dict[str, int] = defaultdict(int)
    scanned_total = 0

    for path in sorted(fixup_dir.rglob("*.sql")):
        if not path.is_file():
            continue
        scanned_total += 1
        rel_dir = str(path.relative_to(fixup_dir).parent)
        obj_type = infer_format_type_from_subdir(rel_dir) or ""
        size_bytes = path.stat().st_size
        line_count = 0
        if not obj_type:
            skip_reasons["unknown_type"] += 1
            report_rows.append(DdlFormatReportRow(
                "UNKNOWN",
                "skipped",
                "unknown_type",
                size_bytes,
                line_count,
                str(path)
            ))
            continue
        summary = summary_by_type[obj_type]
        summary["total"] += 1
        if obj_type not in ddl_format_types:
            summary["skipped"] += 1
            skip_reasons["type_filtered"] += 1
            continue
        if max_bytes > 0 and size_bytes > max_bytes:
            summary["skipped"] += 1
            skip_reasons["size_bytes"] += 1
            report_rows.append(DdlFormatReportRow(
                obj_type,
                "skipped",
                f"size_bytes>{max_bytes}",
                size_bytes,
                line_count,
                str(path)
            ))
            continue
        exceeded = False
        if max_lines > 0:
            line_count, exceeded = count_lines_with_limit(path, max_lines)
        if exceeded:
            summary["skipped"] += 1
            skip_reasons["size_lines"] += 1
            report_rows.append(DdlFormatReportRow(
                obj_type,
                "skipped",
                f"size_lines>{max_lines}",
                size_bytes,
                line_count,
                str(path)
            ))
            continue
        items_to_format.append(DdlFormatItem(
            path=path,
            obj_type=obj_type,
            rel_dir=rel_dir,
            size_bytes=size_bytes,
            line_count=line_count
        ))

    if not items_to_format:
        if report_dir and report_timestamp:
            return export_ddl_format_report(
                report_rows,
                summary_by_type,
                report_dir,
                report_timestamp,
                scanned_total,
                skip_reasons
            )
        return None

    log.info(
        "[DDL_FORMAT] 准备格式化 %d 个 DDL (types=%s)",
        len(items_to_format),
        ", ".join(sorted(ddl_format_types))
    )
    error_message = ""

    def _short_sqlcl_error(proc: subprocess.CompletedProcess) -> str:
        msg = proc.stderr or proc.stdout or ""
        msg = normalize_error_text(msg)
        if len(msg) > 240:
            msg = msg[:240] + "..."
        return msg

    for idx in range(0, len(items_to_format), max(1, batch_size)):
        batch = items_to_format[idx: idx + max(1, batch_size)]
        with tempfile.TemporaryDirectory(prefix="sqlcl_fmt_") as tmp_dir:
            tmp_path = Path(tmp_dir)
            script_lines: List[str] = []
            if profile_file:
                script_lines.append(f'FORMAT RULES "{profile_file}"')
            io_map: List[Tuple[DdlFormatItem, Path, bool]] = []
            for b_idx, item in enumerate(batch):
                content = item.path.read_text(encoding="utf-8")
                if item.line_count <= 0:
                    item.line_count = content.count("\n") + (1 if content else 0)
                needs_slash = False
                if item.obj_type in DDL_FORMAT_PLSQL_TYPES:
                    content, needs_slash = strip_plsql_trailing_slash(content)
                input_path = tmp_path / f"in_{b_idx}.sql"
                output_path = tmp_path / f"out_{b_idx}.sql"
                input_path.write_text(content.rstrip() + "\n", encoding="utf-8")
                script_lines.append(f'FORMAT FILE "{input_path}" "{output_path}"')
                io_map.append((item, output_path, needs_slash))

            script_lines.append("EXIT")
            script_path = tmp_path / "format.sql"
            script_path.write_text("\n".join(script_lines) + "\n", encoding="utf-8")

            env = os.environ.copy()
            env["JAVA_TOOL_OPTIONS"] = f"-Duser.home={tmp_path}"
            try:
                result = subprocess.run(
                    [str(sqlcl_exec), "-S", "/nolog", f"@{script_path}"],
                    capture_output=True,
                    text=True,
                    timeout=timeout,
                    env=env
                )
            except subprocess.TimeoutExpired:
                error_message = "timeout"
                for item, _out_path, _slash in io_map:
                    summary_by_type[item.obj_type]["failed"] += 1
                    report_rows.append(DdlFormatReportRow(
                        item.obj_type,
                        "failed",
                        "timeout",
                        item.size_bytes,
                        item.line_count,
                        str(item.path)
                    ))
                if fail_policy == DDL_FORMAT_FAIL_ERROR:
                    raise RuntimeError("[DDL_FORMAT] SQLcl 超时，已停止格式化。")
                continue

            if result.returncode != 0:
                error_message = _short_sqlcl_error(result) or "sqlcl_error"
                log.warning("[DDL_FORMAT] SQLcl 返回非零状态: %s", error_message)

            for item, out_path, needs_slash in io_map:
                if not out_path.exists():
                    summary_by_type[item.obj_type]["failed"] += 1
                    report_rows.append(DdlFormatReportRow(
                        item.obj_type,
                        "failed",
                        error_message or "no_output",
                        item.size_bytes,
                        item.line_count,
                        str(item.path)
                    ))
                    continue
                formatted = out_path.read_text(encoding="utf-8")
                if needs_slash:
                    formatted = formatted.rstrip() + "\n/\n"
                item.path.write_text(formatted.rstrip() + "\n", encoding="utf-8")
                summary_by_type[item.obj_type]["formatted"] += 1

    failed_total = sum(int(v.get("failed", 0) or 0) for v in summary_by_type.values())
    report_path = None
    if report_dir and report_timestamp:
        report_path = export_ddl_format_report(
            report_rows,
            summary_by_type,
            report_dir,
            report_timestamp,
            scanned_total,
            skip_reasons
        )
    if failed_total and fail_policy == DDL_FORMAT_FAIL_ERROR:
        raise RuntimeError("[DDL_FORMAT] DDL 格式化失败，已按配置中断运行。")
    return report_path


def format_oracle_column_type(
    info: Dict,
    *,
    override_length: Optional[int] = None,
    prefer_ob_varchar: bool = False
) -> str:
    """
    Render an Oracle column definition using available metadata without dropping
    precision/scale/length/semantics.
    """
    raw_dt = (info.get("data_type") or "").strip()
    dt = raw_dt.upper()
    prec = info.get("data_precision")
    scale = info.get("data_scale")
    data_length = info.get("data_length")
    char_length = info.get("char_length")
    char_used = (info.get("char_used") or "").strip().upper()

    def strip_byte_suffix(type_literal: str, base_type: str) -> str:
        if base_type not in ("VARCHAR2", "VARCHAR"):
            return type_literal
        return re.sub(r'\s+BYTE\b', '', type_literal, flags=re.IGNORECASE)

    def apply_varchar_pref(type_literal: str) -> str:
        literal = type_literal
        if prefer_ob_varchar and literal.startswith("VARCHAR2"):
            literal = "VARCHAR" + literal[len("VARCHAR2"):]
        literal = strip_byte_suffix(literal, "VARCHAR2")
        literal = strip_byte_suffix(literal, "VARCHAR")
        return literal

    if is_long_type(dt):
        return map_long_type_to_ob(dt)

    # If data_type already carries explicit precision/length (e.g., TIMESTAMP(6)), respect it.
    if '(' in dt and override_length is None:
        return apply_varchar_pref(dt)

    # Length semantics suffix for VARCHAR/VARCHAR2 (CHAR vs BYTE)
    def _char_suffix(base_type: str) -> str:
        if base_type not in ("VARCHAR", "VARCHAR2"):
            return ""
        if char_used == "C":
            return " CHAR"
        # BYTE 语义不需要显式指定（OceanBase 默认就是 BYTE）
        return ""

    # Choose effective length with optional override
    def _pick_length(default_len: Optional[int]) -> Optional[int]:
        return override_length if override_length is not None else default_len

    # NUMBER-like
    if dt in ("NUMBER", "DECIMAL", "NUMERIC"):
        base_dt = "NUMBER" if dt in ("DECIMAL", "NUMERIC") else dt
        if prec is not None:
            if scale is not None:
                return f"{dt}({int(prec)},{int(scale)})"
            return f"{dt}({int(prec)})"
        if scale is not None:
            if int(scale) == 0:
                return f"{base_dt}(*)"
            return f"{base_dt}(*,{int(scale)})"
        return dt

    # FLOAT
    if dt == "FLOAT":
        if prec is not None:
            return f"{dt}({int(prec)})"
        return dt

    # TIMESTAMP family
    if dt.startswith("TIMESTAMP"):
        if "WITH LOCAL TIME ZONE" in dt:
            suffix = " WITH LOCAL TIME ZONE"
        elif "WITH TIME ZONE" in dt:
            suffix = " WITH TIME ZONE"
        else:
            suffix = ""
        base = "TIMESTAMP"
        if scale is not None:
            return f"{base}({int(scale)}){suffix}"
        return f"{base}{suffix}"

    # INTERVAL family
    if dt.startswith("INTERVAL YEAR"):
        if prec is not None or scale is not None:
            year_prec = int(prec) if prec is not None else 2
            return f"INTERVAL YEAR({year_prec}) TO MONTH"
        return "INTERVAL YEAR TO MONTH"
    if dt.startswith("INTERVAL DAY"):
        if prec is not None or scale is not None:
            day_prec = int(prec) if prec is not None else 2
            frac_prec = int(scale) if scale is not None else 6
            return f"INTERVAL DAY({day_prec}) TO SECOND({frac_prec})"
        return "INTERVAL DAY TO SECOND"

    # VARCHAR/VARCHAR2 with length semantics (CHAR vs BYTE)
    if dt in ("VARCHAR", "VARCHAR2"):
        ln = _pick_length(char_length if char_used == "C" else (char_length or data_length))
        if ln is not None:
            return apply_varchar_pref(f"{dt}({int(ln)}){_char_suffix(dt)}")
        return apply_varchar_pref(dt)
    
    # CHAR type (separate from VARCHAR length semantics)
    if dt == "CHAR":
        ln = _pick_length(char_length if char_used == "C" else (char_length or data_length))
        if ln is not None:
            suffix = " CHAR" if char_used == "C" else ""
            return f"{dt}({int(ln)}){suffix}"
        return dt

    # National character types (length is character-based; no CHAR/BYTE suffix)
    if dt in ("NCHAR", "NVARCHAR2"):
        ln = _pick_length(char_length or data_length)
        if ln is not None:
            return f"{dt}({int(ln)})"
        return dt

    # Binary/ROWID with length
    if dt in ("RAW", "VARBINARY"):
        ln = _pick_length(data_length)
        if ln is not None:
            return f"{dt}({int(ln)})"
        return dt

    if dt == "UROWID":
        ln = _pick_length(data_length)
        if ln is not None:
            return f"{dt}({int(ln)})"
        return dt

    # Fallback
    return apply_varchar_pref(dt)


def inflate_table_varchar_lengths(
    ddl: str,
    src_schema: str,
    src_table: str,
    oracle_meta: OracleMetadata
) -> str:
    """
    将表 DDL 中 VARCHAR/VARCHAR2 列的长度放大到 ceil(src*1.5)，避免修补后再次被长度校验拦截。
    仅在元数据可用且实际长度不足时修改。
    """
    col_meta = oracle_meta.table_columns.get((src_schema.upper(), src_table.upper()))
    if not col_meta:
        return ddl

    replacements = 0
    updated = ddl

    for col_name, info in col_meta.items():
        dtype = (info.get("data_type") or "").strip().upper()
        if dtype not in ("VARCHAR2", "VARCHAR"):
            continue
        
        # 只对BYTE语义的列进行放大，CHAR语义保持原样
        char_used = (info.get("char_used") or "").strip().upper()
        if char_used == 'C':
            continue
        
        src_len = info.get("char_length") or info.get("data_length")
        try:
            src_len_int = int(src_len)
        except (TypeError, ValueError):
            continue
        min_len = int(math.ceil(src_len_int * VARCHAR_LEN_MIN_MULTIPLIER))

        col_pat = re.escape(col_name)
        pattern = re.compile(
            rf'(?P<prefix>"{col_pat}"\s+|{col_pat}\s+)'
            rf'(?P<dtype>VARCHAR2|VARCHAR)\s*'
            rf'\(\s*(?P<len>\d+)\s*(?P<inner_sem>(?:BYTE|CHAR)?)\s*\)'
            rf'(?P<outer_sem>\s*(?:BYTE|CHAR)?)',
            re.IGNORECASE
        )

        def _repl(match: re.Match) -> str:
            nonlocal replacements
            current_len = int(match.group("len"))
            if current_len >= min_len:
                return match.group(0)
            inner_sem_raw = (match.group("inner_sem") or "").strip()
            outer_sem_raw = (match.group("outer_sem") or "").strip()
            effective_sem = (inner_sem_raw or outer_sem_raw).upper()
            # DDL 中显式 CHAR 语义时不放大，避免改变语义
            if effective_sem == "CHAR":
                return match.group(0)
            replacements += 1
            prefix = match.group("prefix")
            dtype_literal = match.group("dtype")
            if inner_sem_raw:
                return f"{prefix}{dtype_literal}({min_len} {inner_sem_raw})"
            if outer_sem_raw:
                return f"{prefix}{dtype_literal}({min_len}) {outer_sem_raw}"
            return f"{prefix}{dtype_literal}({min_len})"

        updated, _ = pattern.subn(_repl, updated, count=1)

    if replacements:
        log.info("[DDL] 放大 %s.%s 中 %d 个 VARCHAR 列至校验下限。", src_schema, src_table, replacements)
    return updated


def generate_alter_for_table_columns(
    oracle_meta: OracleMetadata,
    src_schema: str,
    src_table: str,
    tgt_schema: str,
    tgt_table: str,
    missing_cols: Set[str],
    extra_cols: Set[str],
    length_mismatches: List[ColumnLengthIssue],
    type_mismatches: List[ColumnTypeIssue],
    drop_sys_c_columns: bool = False
) -> Optional[str]:
    """
    为一个具体的表生成 ALTER TABLE 脚本：
      - 对 missing_cols 生成 ADD COLUMN
      - 对 extra_cols 生成注释掉的 DROP COLUMN 建议（可选对 SYS_C* 生成 FORCE）
      - 对 length_mismatches 生成 MODIFY COLUMN
      - 对 type_mismatches (LONG/LONG RAW、NUMBER 精度等) 生成 MODIFY COLUMN
    """
    if not missing_cols and not extra_cols and not length_mismatches and not type_mismatches:
        return None

    col_details = oracle_meta.table_columns.get((src_schema.upper(), src_table.upper()))
    if col_details is None:
        log.warning(f"[ALTER] 未找到 {src_schema}.{src_table} 的列元数据，跳过 ALTER 生成。")
        return None

    lines: List[str] = []
    tgt_schema_u = tgt_schema.upper()
    tgt_table_u = tgt_table.upper()
    table_full = quote_qualified_parts(tgt_schema_u, tgt_table_u)

    # 缺失列：ADD
    if missing_cols:
        lines.append(f"-- 源端存在而目标端缺失的列，将通过 ALTER TABLE ADD 补齐：")
        for col in sorted(missing_cols):
            info = col_details.get(col)
            if not info:
                lines.append(f"-- WARNING: 源端未找到列 {col} 的详细定义，需手工补充。")
                continue

            col_u = col.upper()
            is_virtual = bool(info.get("virtual"))
            virtual_expr = info.get("virtual_expr") if is_virtual else None
            override_len = None
            dtype = (info.get("data_type") or "").upper()
            if dtype in ("VARCHAR2", "VARCHAR"):
                src_len = info.get("char_length") or info.get("data_length")
                try:
                    src_len_int = int(src_len)
                    override_len = int(math.ceil(src_len_int * VARCHAR_LEN_MIN_MULTIPLIER))
                except (TypeError, ValueError):
                    override_len = None

            col_type = format_oracle_column_type(
                info,
                override_length=override_len,
                prefer_ob_varchar=True
            )
            default_clause = ""
            default_val = info.get("data_default")
            if not is_virtual and default_val is not None:
                default_str = str(default_val).strip()
                if default_str:
                    default_clause = f" DEFAULT {default_str}"

            nullable_clause = ""
            if not is_virtual:
                nullable_clause = " NOT NULL" if (info.get("nullable") == "N") else ""

            virtual_clause = ""
            if is_virtual:
                if virtual_expr:
                    virtual_clause = f" GENERATED ALWAYS AS ({virtual_expr}) VIRTUAL"
                else:
                    lines.append(f"-- WARNING: 虚拟列 {col_u} 未获取表达式，需手工补充。")

            lines.append(
                f"ALTER TABLE {table_full} "
                f"ADD ({col_u} {col_type}{virtual_clause}{default_clause}{nullable_clause});"
            )

    # 为新增列生成注释语句（过滤OMS列）
    col_comments = oracle_meta.column_comments.get((src_schema.upper(), src_table.upper()), {})
    added_col_comments: List[str] = []
    for col in sorted(missing_cols):
        col_u = col.upper()
        # 跳过OMS列的注释
        if is_ignored_oms_column(col_u):
            continue
        comment = col_comments.get(col_u)
        if comment:
            # 转义单引号
            comment_escaped = comment.replace("'", "''")
            added_col_comments.append(
                f"COMMENT ON COLUMN {table_full}.{col_u} IS '{comment_escaped}';"
            )
    if added_col_comments:
        lines.append("")
        lines.append("-- 新增列的注释：")
        lines.extend(added_col_comments)

    # 长度不匹配：MODIFY
    if length_mismatches:
        lines.append("")
        lines.append("-- 列长度不匹配：")
        for issue in length_mismatches:
            col_name, src_len, tgt_len, limit_len, issue_type = issue
            info = col_details.get(col_name)
            if not info:
                continue

            if issue_type == 'char_mismatch':
                # CHAR语义：要求长度完全一致
                modified_type = format_oracle_column_type(
                    info,
                    override_length=src_len,
                    prefer_ob_varchar=True
                )
                lines.append(
                    f"ALTER TABLE {table_full} "
                    f"MODIFY ({col_name.upper()} {modified_type}); "
                    f"-- CHAR语义，源长度: {src_len}, 目标长度: {tgt_len}, 要求一致"
                )
            elif issue_type == 'short':
                # BYTE语义：放大到校验下限
                modified_type = format_oracle_column_type(
                    info,
                    override_length=limit_len,
                    prefer_ob_varchar=True
                )
                lines.append(
                    f"ALTER TABLE {table_full} "
                    f"MODIFY ({col_name.upper()} {modified_type}); "
                    f"-- BYTE语义，源长度: {src_len}, 目标长度: {tgt_len}, 期望下限: {limit_len}"
                )
            else:
                lines.append(
                    f"-- WARNING: {col_name.upper()} 长度过大 (源={src_len}, 目标={tgt_len}, "
                    f"建议上限={limit_len})，请人工评估是否需要收敛。"
                )

    # 类型不匹配：MODIFY (LONG/LONG RAW、NUMBER 精度等)
    if type_mismatches:
        lines.append("")
        lines.append("-- 列类型不匹配：")
        for issue in type_mismatches:
            col_name, src_type, tgt_type, expected_type, issue_type = issue
            info = col_details.get(col_name)
            if not info:
                continue
            if issue_type in ("long_type", "number_precision"):
                lines.append(
                    f"ALTER TABLE {table_full} "
                    f"MODIFY ({col_name.upper()} {expected_type}); "
                    f"-- 源类型: {src_type}, 目标类型: {tgt_type}"
                )
            elif issue_type.startswith("virtual"):
                lines.append(
                    f"-- WARNING: {col_name.upper()} 为虚拟列差异 ({issue_type})，请人工处理。"
                )
            elif issue_type in ("identity_missing", "default_on_null_missing"):
                lines.append(
                    f"-- WARNING: {col_name.upper()} 缺失 {expected_type} 特性，请人工处理。"
                )
            elif issue_type == "visibility_mismatch":
                if expected_type.upper() == "INVISIBLE":
                    lines.append(
                        f"ALTER TABLE {table_full} "
                        f"MODIFY {col_name.upper()} INVISIBLE; "
                        f"-- 列可见性需要与源端一致"
                    )
                elif expected_type.upper() == "VISIBLE":
                    lines.append(
                        f"ALTER TABLE {table_full} "
                        f"MODIFY {col_name.upper()} VISIBLE; "
                        f"-- 列可见性需要与源端一致"
                    )
                else:
                    lines.append(
                        f"-- WARNING: {col_name.upper()} 列可见性差异，请人工处理。"
                    )
            else:
                lines.append(
                    f"-- WARNING: {col_name.upper()} 类型差异未自动修复 (源={src_type}, 目标={tgt_type})。"
                )

    # 多余列：DROP（默认注释掉，供人工评估；SYS_C* 可选 FORCE）
    if extra_cols:
        lines.append("")
        sys_c_cols = {
            normalize_identifier_name(col)
            for col in extra_cols
            if is_sys_c_column_name(col)
        }
        if drop_sys_c_columns and sys_c_cols:
            lines.append("-- 目标端存在而源端不存在的列：SYS_C* 无法 DROP，将通过 FORCE 清理。")
            lines.append(f"ALTER TABLE {table_full} FORCE;")
        else:
            lines.append("-- 目标端存在而源端不存在的列，以下 DROP COLUMN 为建议操作，请谨慎执行：")
        for col in sorted(extra_cols):
            col_u = col.upper()
            if drop_sys_c_columns and normalize_identifier_name(col) in sys_c_cols:
                continue
            lines.append(
                f"-- ALTER TABLE {table_full} "
                f"DROP COLUMN {col_u};"
            )

    return "\n".join(lines) if lines else None


def build_invisible_column_alter_sql(
    oracle_meta: OracleMetadata,
    src_schema: str,
    src_table: str,
    tgt_schema: str,
    tgt_table: str,
    visibility_enabled: bool
) -> Optional[str]:
    if not visibility_enabled:
        return None
    src_schema_u = src_schema.upper()
    src_table_u = src_table.upper()
    tgt_schema_u = tgt_schema.upper()
    tgt_table_u = tgt_table.upper()
    table_full = quote_qualified_parts(tgt_schema_u, tgt_table_u)
    col_details = oracle_meta.table_columns.get((src_schema_u, src_table_u), {})
    if not col_details:
        return None
    lines: List[str] = []
    for col_name, meta in col_details.items():
        if is_ignored_source_column(col_name, meta):
            continue
        if meta.get("invisible") is True:
            lines.append(
                f"ALTER TABLE {table_full} "
                f"MODIFY {col_name.upper()} INVISIBLE;"
            )
    return "\n".join(lines) if lines else None


def generate_interval_partition_fixup_scripts(
    oracle_meta: OracleMetadata,
    table_target_map: Dict[Tuple[str, str], Tuple[str, str]],
    settings: Dict,
    base_dir: Path,
    allow_fixup_fn: Callable[[str, str, Optional[str]], bool],
    unsupported_table_keys: Optional[Set[Tuple[str, str]]] = None
) -> Optional[Path]:
    if not settings.get("generate_interval_partition_fixup"):
        return None

    cutoff_date = settings.get("interval_partition_cutoff_date")
    cutoff_str = (settings.get("interval_partition_cutoff") or "").strip()
    cutoff_numeric = settings.get("interval_partition_cutoff_numeric_value")
    cutoff_numeric_raw = (settings.get("interval_partition_cutoff_numeric") or "").strip()
    if not cutoff_date and not cutoff_numeric:
        log.warning(
            "[FIXUP] interval_partition_cutoff/interval_partition_cutoff_numeric 未配置或非法，跳过 interval 分区补齐脚本。"
        )
        return None

    interval_map = oracle_meta.interval_partitions or {}
    if not interval_map:
        log.info("[FIXUP] 未检测到 interval 分区表，跳过分区补齐脚本。")
        return None
    unsupported_table_keys = {
        (s.upper(), t.upper()) for s, t in (unsupported_table_keys or set())
    }

    def _sanitize_cutoff_label(label: str) -> str:
        sanitized = re.sub(r"[^A-Za-z0-9_]+", "_", (label or "").strip())
        return sanitized or "numeric"

    written = 0
    output_dirs: Set[Path] = set()
    for (src_schema, src_table), info in interval_map.items():
        key = (src_schema.upper(), src_table.upper())
        tgt_schema, tgt_table = table_target_map.get(key, key)
        if key in unsupported_table_keys:
            log.info(
                "[FIXUP][INTERVAL] %s.%s 属于不支持表，跳过 interval 分区补齐脚本。",
                src_schema,
                src_table
            )
            continue
        if not allow_fixup_fn("TABLE", tgt_schema, src_schema):
            continue

        if len(info.partition_key_columns) != 1:
            log.warning(
                "[FIXUP][INTERVAL] %s.%s 分区键列数量不为 1，已跳过。",
                src_schema,
                src_table
            )
            continue
        part_col = info.partition_key_columns[0]
        col_meta = oracle_meta.table_columns.get(key, {}).get(part_col, {})
        data_type = (col_meta.get("data_type") or "").strip()
        table_full = quote_qualified_parts(tgt_schema.upper(), tgt_table.upper())
        statements, warnings, kind = generate_interval_partition_statements(
            info,
            cutoff_date,
            cutoff_numeric,
            data_type,
            table_full
        )
        if not statements:
            msg = "; ".join(warnings) if warnings else "无可生成分区"
            log.warning(
                "[FIXUP][INTERVAL] %s.%s 未生成分区：%s",
                src_schema,
                src_table,
                msg
            )
            continue

        cutoff_label = cutoff_str if kind != "numeric" else _sanitize_cutoff_label(cutoff_numeric_raw)
        subdir = (
            f"table_alter/interval_add_{cutoff_label}"
            if kind != "numeric"
            else f"table_alter/interval_add_numeric_{cutoff_label}"
        )
        output_dir = base_dir / subdir
        ensure_dir(output_dir)
        output_dirs.add(output_dir)

        header = f"基于 interval 分区规则补齐 {table_full} 的分区 (截止 {cutoff_label})"
        extra_comments = [
            f"source_table: {src_schema.upper()}.{src_table.upper()}",
            f"interval_expr: {info.interval_expr}",
            f"last_high_value: {info.last_high_value}",
            f"partition_key: {part_col}",
        ]
        if kind == "numeric" and cutoff_numeric_raw:
            extra_comments.append(f"numeric_cutoff: {cutoff_numeric_raw}")
        if warnings:
            extra_comments.append("warnings: " + "; ".join(warnings))

        filename = f"{tgt_schema.upper()}.{tgt_table.upper()}.interval_add.sql"
        write_fixup_file(
            base_dir,
            subdir,
            filename,
            "\n".join(statements),
            header,
            extra_comments=extra_comments
        )
        written += 1

    if written:
        dir_list = ", ".join(sorted({str(p) for p in output_dirs})) if output_dirs else "-"
        log.info(
            "[FIXUP] interval 分区补齐脚本已生成 %d 份，目录: %s",
            written,
            dir_list
        )
        return next(iter(output_dirs)) if output_dirs else None

    log.info("[FIXUP] interval 分区补齐脚本未生成任何文件。")
    return None


def generate_fixup_scripts(
    ora_cfg: OraConfig,
    ob_cfg: ObConfig,
    settings: Dict,
    tv_results: ReportResults,
    extra_results: ExtraCheckResults,
    master_list: MasterCheckList,
    oracle_meta: OracleMetadata,
    full_object_mapping: FullObjectMapping,
    remap_rules: RemapRules,
    grant_plan: Optional[GrantPlan] = None,
    enable_grant_generation: bool = True,
    dependency_report: Optional[DependencyReport] = None,
    ob_meta: Optional[ObMetadata] = None,
    expected_dependency_pairs: Optional[Set[Tuple[str, str, str, str]]] = None,
    synonym_metadata: Optional[Dict[Tuple[str, str], SynonymMeta]] = None,
    trigger_filter_entries: Optional[Set[str]] = None,
    trigger_filter_enabled: bool = False,
    package_results: Optional[PackageCompareResults] = None,
    report_dir: Optional[Path] = None,
    report_timestamp: Optional[str] = None,
    fixup_skip_summary: Optional[Dict[str, Dict[str, object]]] = None,
    support_state_map: Optional[Dict[Tuple[str, str], ObjectSupportReportRow]] = None,
    unsupported_table_keys: Optional[Set[Tuple[str, str]]] = None,
    view_compat_map: Optional[Dict[Tuple[str, str], ViewCompatResult]] = None,
    view_dependency_map: Optional[Dict[Tuple[str, str], Set[str]]] = None,
    trigger_status_rows: Optional[List[TriggerStatusReportRow]] = None,
    constraint_status_rows: Optional[List[ConstraintStatusDriftRow]] = None,
):
    """
    基于校验结果生成 fixup_scripts DDL 脚本，并按依赖顺序排列：
      1. SEQUENCE
      2. TABLE (CREATE)
      3. TABLE (ALTER - for column diffs)
      4. TABLE (INTERVAL 分区补齐)
      5. VIEW / MATERIALIZED VIEW 等代码对象
      6. INDEX
      7. CONSTRAINT
      8. TRIGGER
      9. 依赖重编译 (ALTER ... COMPILE)
     10. 授权脚本 (对象/角色/系统)

    如果配置了 trigger_list，则仅生成清单中列出的触发器脚本。
    """
    try:
        progress_log_interval = float(settings.get('progress_log_interval', 10))
    except (TypeError, ValueError):
        progress_log_interval = 10.0
    progress_log_interval = max(1.0, progress_log_interval)
    synonym_meta_map = synonym_metadata or {}
    view_dependency_map = view_dependency_map or {}
    trigger_status_rows = trigger_status_rows or []
    constraint_status_rows = constraint_status_rows or []
    trigger_filter_set = {t.upper() for t in (trigger_filter_entries or set())}
    support_state_map = support_state_map or {}
    unsupported_table_keys = {(s.upper(), t.upper()) for s, t in (unsupported_table_keys or set())}
    view_compat_map = view_compat_map or {}
    check_status_drift_types = set(settings.get("check_status_drift_type_set", set()) or set())
    status_fixup_types = set(settings.get("status_fixup_type_set", set()) or set())
    generate_status_fixup = parse_bool_flag(settings.get("generate_status_fixup", "true"), True)
    generate_extra_cleanup = parse_bool_flag(settings.get("generate_extra_cleanup", "false"), False)
    constraint_status_sync_mode = settings.get("constraint_status_sync_mode", "enabled_only")
    constraint_missing_validate_mode = normalize_constraint_missing_fixup_validate_mode(
        settings.get("constraint_missing_fixup_validate_mode", "safe_novalidate")
    )
    trigger_validity_sync_mode = settings.get("trigger_validity_sync_mode", "compile")
    deferred_validation_rows: List[ConstraintValidateDeferredRow] = []
    deferred_validation_lock = threading.Lock()
    invalid_view_keys: Set[Tuple[str, str]] = set()
    invalid_trigger_keys: Set[Tuple[str, str]] = set()
    for (owner, name, obj_type), status in (oracle_meta.object_statuses or {}).items():
        if normalize_object_status(status) != "INVALID":
            continue
        owner_u = (owner or "").upper()
        name_u = (name or "").upper()
        obj_type_u = (obj_type or "").upper()
        if not owner_u or not name_u:
            continue
        if obj_type_u == "VIEW":
            invalid_view_keys.add((owner_u, name_u))
        elif obj_type_u == "TRIGGER":
            invalid_trigger_keys.add((owner_u, name_u))
    if trigger_filter_enabled:
        log.info("[FIXUP] 已启用 trigger_list 过滤，清单条目数=%d。", len(trigger_filter_set))
        if not trigger_filter_set:
            log.warning("[FIXUP] trigger_list 为空，已回退全量 TRIGGER 生成。")
            trigger_filter_enabled = False
    allowed_synonym_targets = {s.upper() for s in settings.get('source_schemas_list', [])}

    base_dir = Path(settings.get('fixup_dir', 'fixup_scripts')).expanduser()
    log.info("[FIXUP] 准备生成修补脚本，目标目录=%s", base_dir.resolve())
    fixup_force_clean = parse_bool_flag(settings.get("fixup_force_clean", "true"), True)
    idempotent_stats: Dict[str, int] = {}
    idempotent_mode = settings.get("fixup_idempotent_mode", "replace")
    idempotent_types = settings.get("fixup_idempotent_types_set") or set()
    if idempotent_mode and normalize_fixup_idempotent_mode(idempotent_mode) != "off":
        log.info(
            "[FIXUP] 幂等模式=%s, types=%s",
            normalize_fixup_idempotent_mode(idempotent_mode),
            ",".join(sorted(idempotent_types)) if idempotent_types else "-"
        )
    log.info(
        "[FIXUP] 缺失约束 VALIDATE 策略: %s",
        constraint_missing_validate_mode
    )
    view_chain_file: Optional[Path] = None
    visibility_policy = normalize_column_visibility_policy(
        settings.get("column_visibility_policy", "auto")
    )
    visibility_fixup_enabled = False
    ob_visibility_supported = bool(ob_meta and ob_meta.invisible_column_supported)
    if visibility_policy == "enforce":
        if oracle_meta.invisible_column_supported:
            visibility_fixup_enabled = True
            if not ob_visibility_supported:
                log.warning(
                    "[FIXUP] column_visibility_policy=enforce，但目标端未暴露 INVISIBLE_COLUMN 元数据，"
                    "生成的 INVISIBLE DDL 可能无法执行。"
                )
        else:
            log.warning(
                "[FIXUP] column_visibility_policy=enforce，但源端未提供 INVISIBLE_COLUMN 元数据，"
                "已跳过 INVISIBLE 修复。"
            )
    elif visibility_policy == "auto":
        if oracle_meta.invisible_column_supported and ob_visibility_supported:
            visibility_fixup_enabled = True
        else:
            log.info(
                "[FIXUP] column_visibility_policy=auto，INVISIBLE_COLUMN 元数据不完整，"
                "已跳过 INVISIBLE 修复。"
            )

    ensure_dir(base_dir)
    safe_to_clean = False
    try:
        base_resolved = base_dir.resolve()
        run_root = Path.cwd().resolve()
        safe_to_clean = (not base_dir.is_absolute()) or (run_root == base_resolved or run_root in base_resolved.parents)
    except Exception:
        safe_to_clean = not base_dir.is_absolute()
    if fixup_force_clean:
        if not safe_to_clean:
            log.warning("[FIXUP] fixup_force_clean=true，将强制清理 %s。", base_dir.resolve())
        safe_to_clean = True

    if safe_to_clean:
        removed_files = 0
        removed_dirs = 0
        failed = 0
        log.info("[FIXUP] 正在清理旧脚本目录: %s", base_dir.resolve())
        for child in base_dir.iterdir():
            try:
                if child.is_file():
                    child.unlink()
                    removed_files += 1
                elif child.is_dir():
                    shutil.rmtree(child, ignore_errors=True)
                    removed_dirs += 1
            except OSError as exc:
                failed += 1
                log.warning("[FIXUP] 无法删除 %s: %s", child, exc)
        log.info(
            "[FIXUP] 旧脚本清理完成: files=%d, dirs=%d, failed=%d",
            removed_files,
            removed_dirs,
            failed
        )
    else:
        log.warning(
            "[FIXUP] fixup_dir=%s 位于运行目录之外，已跳过自动清理以避免误删。",
            base_dir.resolve()
        )

    if not master_list:
        log.info("[FIXUP] master_list 为空，目录已清理，无新增订正 SQL。")
        return None

    log.info(f"[FIXUP] 目标端订正 SQL 将生成到目录: {base_dir.resolve()}")
    if not settings.get('enable_ddl_punct_sanitize', True):
        log.info("[DDL_CLEAN] 已关闭 PL/SQL 全角标点清洗。")
    hint_policy = settings.get('ddl_hint_policy', DDL_HINT_POLICY_DEFAULT)
    hint_allowlist: Set[str] = set(OB_ORACLE_HINT_ALLOWLIST)
    hint_allowlist.update(settings.get('ddl_hint_allowlist_set', set()))
    hint_allowlist.update(settings.get('ddl_hint_allowlist_file_set', set()))
    hint_denylist: Set[str] = set(settings.get('ddl_hint_denylist_set', set()))
    log.info(
        "[DDL_HINT] hint_policy=%s allowlist=%d denylist=%d",
        hint_policy,
        len(hint_allowlist),
        len(hint_denylist)
    )

    table_map: Dict[str, str] = {
        tgt_name: src_name
        for (src_name, tgt_name, obj_type) in master_list
        if obj_type.upper() == 'TABLE'
    }
    table_target_map = build_table_target_map(master_list)

    object_replacements: List[Tuple[Tuple[str, str], Tuple[str, str]]] = []
    replacement_set: Set[Tuple[str, str, str, str]] = set()
    for src_name, type_map in full_object_mapping.items():
        for tgt_name in type_map.values():
            try:
                src_schema, src_object = src_name.split('.', 1)
                tgt_schema, tgt_object = tgt_name.split('.', 1)
            except ValueError:
                continue
            key = (src_schema.upper(), src_object.upper(), tgt_schema.upper(), tgt_object.upper())
            if key in replacement_set:
                continue
            object_replacements.append(((key[0], key[1]), (key[2], key[3])))
            replacement_set.add(key)

    all_replacements = list(object_replacements)
    # 预构建按schema索引的替换表，加速 lookup
    replacements_by_schema: Dict[str, List[Tuple[Tuple[str, str], Tuple[str, str]]]] = defaultdict(list)
    for (src_s, src_o), (tgt_s, tgt_o) in all_replacements:
        for sch in (src_s.upper(), tgt_s.upper()):
            replacements_by_schema[sch].append(((src_s, src_o), (tgt_s, tgt_o)))

    def get_relevant_replacements(src_schema: str) -> List[Tuple[Tuple[str, str], Tuple[str, str]]]:
        """
        返回与指定源schema相关的replacements（已按schema预索引，加速匹配）。
        相关规则包括：
        1. 源schema匹配的规则
        2. 目标schema匹配的规则（处理跨schema引用）
        """
        return replacements_by_schema.get(src_schema.upper(), [])

    obj_type_to_dir = {
        'TABLE': 'table',
        'VIEW': 'view',
        'MATERIALIZED VIEW': 'materialized_view',
        'PROCEDURE': 'procedure',
        'FUNCTION': 'function',
        'PACKAGE': 'package',
        'PACKAGE BODY': 'package_body',
        'SYNONYM': 'synonym',
        'JOB': 'job',
        'SCHEDULE': 'schedule',
        'TYPE': 'type',
        'TYPE BODY': 'type_body',
        'SEQUENCE': 'sequence',
        'TRIGGER': 'trigger'
    }

    fixup_schema_filter: Set[str] = set(settings.get('fixup_schema_list', []))
    fixup_type_filter: Set[str] = set(settings.get('fixup_type_set', []))
    fixup_schema_used_source_match = False
    synonym_fixup_scope = settings.get('synonym_fixup_scope', 'public_only')
    if synonym_fixup_scope == "public_only":
        log.info("[FIXUP] synonym_fixup_scope=public_only，仅生成 PUBLIC 同义词脚本。")

    def allow_synonym_scope(src_schema: str, tgt_schema: str) -> bool:
        if synonym_fixup_scope != "public_only":
            return True
        if (src_schema or "").upper() == "PUBLIC":
            return True
        if (tgt_schema or "").upper() == "PUBLIC":
            return True
        return False

    ddl_source_stats: Dict[str, Dict[str, int]] = defaultdict(lambda: defaultdict(int))
    ddl_source_lock = threading.Lock()

    def mark_source(obj_type: str, source: str) -> None:
        with ddl_source_lock:
            ddl_source_stats[obj_type.upper()][source] += 1

    ddl_clean_records: List[DdlCleanReportRow] = []
    ddl_clean_lock = threading.Lock()
    hint_clean_records: List[DdlHintCleanReportRow] = []
    hint_clean_lock = threading.Lock()

    def record_ddl_clean(
        obj_type: str,
        obj_full: str,
        replaced: int,
        samples: List[Tuple[str, str]]
    ) -> None:
        if replaced <= 0:
            return
        row = DdlCleanReportRow(
            obj_type=obj_type.upper(),
            obj_full=obj_full,
            replaced=replaced,
            samples=samples or []
        )
        with ddl_clean_lock:
            ddl_clean_records.append(row)

    def record_hint_clean(
        obj_type: str,
        obj_full: str,
        result: HintFilterResult
    ) -> None:
        if result.total <= 0:
            return
        if result.removed <= 0 and result.unknown <= 0:
            return
        row = DdlHintCleanReportRow(
            obj_type=obj_type.upper(),
            obj_full=obj_full,
            policy=hint_policy,
            total=result.total,
            kept=result.kept,
            removed=result.removed,
            unknown=result.unknown,
            kept_samples=result.kept_samples,
            removed_samples=result.removed_samples,
            unknown_samples=result.unknown_samples
        )
        with hint_clean_lock:
            hint_clean_records.append(row)

    def apply_hint_filter(obj_type: str, obj_full: str, ddl_text: str) -> str:
        result = filter_oracle_hints(ddl_text, hint_policy, hint_allowlist, hint_denylist)
        record_hint_clean(obj_type, obj_full, result)
        return result.ddl

    def source_tag(label: str) -> str:
        return f"[{label}]"

    def build_support_comments(row: Optional[ObjectSupportReportRow]) -> List[str]:
        if not row:
            return []
        return [
            f"support_state: {row.support_state}",
            f"reason_code: {row.reason_code or '-'}",
            f"reason: {row.reason or '-'}",
            f"dependency: {row.dependency or '-'}",
            f"action: {row.action or '-'}",
            f"detail: {row.detail or '-'}",
        ]

    def allow_fixup(obj_type: str, tgt_schema: str, src_schema: Optional[str] = None) -> bool:
        nonlocal fixup_schema_used_source_match
        obj_type_u = obj_type.upper()
        schema_u = tgt_schema.upper() if tgt_schema else ""
        if fixup_type_filter and obj_type_u not in fixup_type_filter:
            return False
        if not fixup_schema_filter:
            return True
        if schema_u in fixup_schema_filter:
            return True
        src_schema_u = (src_schema or "").upper()
        if src_schema_u and src_schema_u in fixup_schema_filter:
            if not fixup_schema_used_source_match:
                log.info(
                    "[FIXUP] fixup_schemas 命中源 schema=%s (目标=%s)，已放行生成。",
                    src_schema_u,
                    schema_u or "-"
                )
                fixup_schema_used_source_match = True
            return True
        return False

    def classify_fixup_skip(obj_type: str, tgt_schema: str, src_schema: Optional[str] = None) -> Optional[str]:
        obj_type_u = obj_type.upper()
        schema_u = tgt_schema.upper() if tgt_schema else ""
        if fixup_type_filter and obj_type_u not in fixup_type_filter:
            return "type_filter"
        if not fixup_schema_filter:
            return None
        if schema_u in fixup_schema_filter:
            return None
        src_schema_u = (src_schema or "").upper()
        if src_schema_u and src_schema_u in fixup_schema_filter:
            return None
        return "schema_filter"

    def fetch_ddl_with_timing(schema: str, obj_type: str, obj_name: str) -> Tuple[Optional[str], str, float]:
        """
        返回 (DDL, 来源标签, 耗时秒)，来源标签为:
          - DBCAT_CACHE / DBCAT_RUN / DBCAT_UNKNOWN
          - DBMS_METADATA / DBA_VIEWS
          - MISSING
        """
        key = (schema.upper(), obj_type.upper(), obj_name.upper())
        start_time = time.time()

        # 快速路径：同义词使用缓存元数据直接生成，避免逐个 DBMS_METADATA
        if obj_type.upper() == 'SYNONYM':
            syn_meta = synonym_meta_map.get((schema.upper(), obj_name.upper()))
            if syn_meta and syn_meta.table_name:
                syn_name = syn_meta.name.split('.', 1)[-1]
                target = syn_meta.table_owner
                if target:
                    target = f"{target}.{syn_meta.table_name}"
                else:
                    target = syn_meta.table_name
                if syn_meta.db_link:
                    target = f"{target}@{syn_meta.db_link}"
                if syn_meta.owner == 'PUBLIC':
                    syn_name_quoted = quote_identifier(syn_name)
                    ddl = f"CREATE OR REPLACE PUBLIC SYNONYM {syn_name_quoted} FOR {target};"
                else:
                    syn_owner = (syn_meta.owner or "").upper()
                    syn_full = quote_qualified_parts(syn_owner, syn_name)
                    ddl = f"CREATE OR REPLACE SYNONYM {syn_full} FOR {target};"
                elapsed = time.time() - start_time
                log.info(
                    "[DDL_FETCH] %s.%s (%s) 来源=META_SYN 耗时=%.3fs",
                    schema, obj_name, obj_type, elapsed
                )
                return ddl, "META_SYN", elapsed

        # VIEW 固定使用 DBMS_METADATA，忽略 dbcat 输出
        if obj_type.upper() == 'VIEW':
            ddl, source_label = get_fallback_ddl(schema, obj_type, obj_name)
            if not ddl:
                source_label = "MISSING"
            elapsed = time.time() - start_time
            log.info(
                "[DDL_FETCH] %s.%s (%s) 来源=%s 耗时=%.3fs",
                schema, obj_name, obj_type, source_label, elapsed
            )
            return ddl, source_label, elapsed

        ddl = (
            dbcat_data
            .get(schema.upper(), {})
            .get(obj_type.upper(), {})
            .get(obj_name.upper())
        )
        source_label = "MISSING"
        elapsed_hint = None

        if ddl:
            meta = ddl_source_meta.get(key)
            if meta:
                source_label = "DBCAT_CACHE" if meta[0] == "cache" else "DBCAT_RUN"
                # 对于缓存，使用实际读取耗时；对于dbcat_run，使用记录的平均耗时
                if meta[0] == "cache":
                    elapsed_hint = time.time() - start_time
                else:
                    elapsed_hint = meta[1]
            else:
                # DDL存在但无元数据，可能是缓存部分加载
                source_label = "DBCAT"
                elapsed_hint = time.time() - start_time
        else:
            ddl, fallback_source = get_fallback_ddl(schema, obj_type, obj_name)
            if ddl:
                source_label = fallback_source
            else:
                source_label = "MISSING"

        elapsed = elapsed_hint if elapsed_hint is not None else (time.time() - start_time)
        
        # 只在非缓存或耗时较长时输出详细日志
        if source_label != "DBCAT_CACHE" or elapsed > 0.1:
            log.info(
                "[DDL_FETCH] %s.%s (%s) 来源=%s 耗时=%.3fs",
                schema, obj_name, obj_type, source_label, elapsed
            )
        return ddl, source_label, elapsed

    def build_progress_tracker(total: int, label: str) -> Callable[[Optional[str]], None]:
        state = {"done": 0, "last": time.time()}
        state_lock = threading.Lock()
        if total:
            log.info("%s 总计 %d 个。", label, total)

        def _tick(extra: Optional[str] = None) -> None:
            if total <= 0:
                return
            with state_lock:
                state["done"] += 1
                now = time.time()
                should_log = state["done"] == total or (now - state["last"]) >= progress_log_interval
                if should_log:
                    state["last"] = now
                    suffix = f" [{extra}]" if extra else ""
                    pct = state["done"] * 100.0 / total if total else 100.0
                    log.info("%s 进度 %d/%d (%.1f%%)%s", label, state["done"], total, pct, suffix)

        return _tick

    def build_compile_order(
        tasks: Dict[Tuple[str, str, str], Set[str]],
        expected_pairs: Optional[Set[Tuple[str, str, str, str]]]
    ) -> List[Tuple[str, str, str]]:
        if not tasks:
            return []
        node_lookup: Dict[Tuple[str, str], Tuple[str, str, str]] = {}
        for key in tasks.keys():
            node_lookup[(f"{key[0]}.{key[1]}", key[2])] = key

        graph: Dict[Tuple[str, str, str], Set[Tuple[str, str, str]]] = defaultdict(set)
        if expected_pairs:
            for dep_name, dep_type, ref_name, ref_type in expected_pairs:
                dep_key = node_lookup.get((dep_name, dep_type))
                ref_key = node_lookup.get((ref_name, ref_type))
                if dep_key and ref_key and dep_key != ref_key:
                    graph[dep_key].add(ref_key)

        order: List[Tuple[str, str, str]] = []
        visiting: Set[Tuple[str, str, str]] = set()
        visited: Set[Tuple[str, str, str]] = set()
        cycles: List[str] = []

        def dfs(node: Tuple[str, str, str], stack: List[str]) -> None:
            if node in visited:
                return
            if node in visiting:
                cycles.append(" -> ".join(stack + [f"{node[0]}.{node[1]} ({node[2]})"]))
                return
            visiting.add(node)
            for dep in graph.get(node, set()):
                dfs(dep, stack + [f"{node[0]}.{node[1]} ({node[2]})"])
            visiting.remove(node)
            visited.add(node)
            order.append(node)

        for node in sorted(tasks.keys()):
            dfs(node, [])

        if cycles:
            log.warning("[FIXUP] 发现依赖环，编译顺序将按拓扑结果执行: %s", " | ".join(cycles))

        return order if order else list(sorted(tasks.keys()))

    grant_enabled = bool(enable_grant_generation)
    if grant_plan is None:
        grant_plan = GrantPlan(
            object_grants={},
            sys_privs={},
            role_privs={},
            role_ddls=[],
            filtered_grants=[],
            view_grant_targets=set()
        )
    if not grant_enabled:
        grant_plan = GrantPlan(
            object_grants={},
            sys_privs={},
            role_privs={},
            role_ddls=[],
            filtered_grants=[],
            view_grant_targets=set()
        )

    object_grants_by_grantee: Dict[str, Set[ObjectGrantEntry]] = defaultdict(set)
    sys_privs_by_grantee: Dict[str, Set[SystemGrantEntry]] = defaultdict(set)
    role_privs_by_grantee: Dict[str, Set[RoleGrantEntry]] = defaultdict(set)

    if grant_enabled:
        for grantee, entries in grant_plan.object_grants.items():
            if entries:
                object_grants_by_grantee[(grantee or "").upper()].update(entries)
        for grantee, entries in grant_plan.sys_privs.items():
            if entries:
                sys_privs_by_grantee[(grantee or "").upper()].update(entries)
        for grantee, entries in grant_plan.role_privs.items():
            if entries:
                role_privs_by_grantee[(grantee or "").upper()].update(entries)

    object_grant_lookup: Dict[str, List[str]] = {}
    grants_by_owner: Dict[str, Set[str]] = {}
    merge_privileges = parse_bool_flag(settings.get('grant_merge_privileges', 'true'), True)
    merge_grantees = parse_bool_flag(settings.get('grant_merge_grantees', 'true'), True)
    source_schema_set = {s.upper() for s in settings.get('source_schemas_list', []) if s}
    target_schema_set: Set[str] = set()
    for type_map in full_object_mapping.values():
        for tgt_name in type_map.values():
            if '.' in tgt_name:
                target_schema_set.add(tgt_name.split('.', 1)[0].upper())
    for tgt_full in remap_rules.values():
        if '.' in tgt_full:
            target_schema_set.add(tgt_full.split('.', 1)[0].upper())
    grant_owner_allowlist: Set[str] = set(source_schema_set) | target_schema_set
    grant_owner_denylist: Set[str] = {"SYS", "PUBLIC"}

    def is_allowed_grant_owner(owner: str) -> bool:
        owner_u = (owner or "").upper()
        if not owner_u:
            return False
        if owner_u in grant_owner_denylist:
            return False
        if not grant_owner_allowlist:
            return False
        return owner_u in grant_owner_allowlist

    def format_object_grant(grantee: str, entry: ObjectGrantEntry) -> str:
        stmt = f"GRANT {entry.privilege.upper()} ON {entry.object_full.upper()} TO {grantee.upper()}"
        if entry.grantable:
            stmt += " WITH GRANT OPTION"
        return stmt + ";"

    def format_object_grant_stmt(
        privileges: List[str],
        object_full: str,
        grantees: List[str],
        grantable: bool
    ) -> str:
        priv_part = ", ".join(privileges)
        grantee_part = ", ".join(grantees)
        stmt = f"GRANT {priv_part} ON {object_full.upper()} TO {grantee_part}"
        if grantable:
            stmt += " WITH GRANT OPTION"
        return stmt + ";"

    def format_sys_grant(grantee: str, entry: SystemGrantEntry) -> str:
        stmt = f"GRANT {entry.privilege.upper()} TO {grantee.upper()}"
        if entry.admin_option:
            stmt += " WITH ADMIN OPTION"
        return stmt + ";"

    def format_role_grant(grantee: str, entry: RoleGrantEntry) -> str:
        stmt = f"GRANT {entry.role.upper()} TO {grantee.upper()}"
        if entry.admin_option:
            stmt += " WITH ADMIN OPTION"
        return stmt + ";"

    def add_object_grant(grantee: str, privilege: str, object_full: str, grantable: bool = False) -> None:
        if not grant_enabled:
            return
        grantee_u = (grantee or "").upper()
        object_u = (object_full or "").upper()
        privilege_u = (privilege or "").upper()
        if not grantee_u or not object_u or not privilege_u:
            return
        owner_u = object_u.split('.', 1)[0] if '.' in object_u else ""
        if not is_allowed_grant_owner(owner_u):
            return
        if grantable:
            object_grants_by_grantee[grantee_u].discard(ObjectGrantEntry(
                privilege=privilege_u,
                object_full=object_u,
                grantable=False
            ))
            object_grants_by_grantee[grantee_u].add(ObjectGrantEntry(
                privilege=privilege_u,
                object_full=object_u,
                grantable=True
            ))
            return
        if ObjectGrantEntry(privilege_u, object_u, True) in object_grants_by_grantee[grantee_u]:
            return
        object_grants_by_grantee[grantee_u].add(ObjectGrantEntry(
            privilege=privilege_u,
            object_full=object_u,
            grantable=False
        ))

    def collect_grants_for_object(target_full: str) -> List[str]:
        if not grant_enabled:
            return []
        return sorted(set(object_grant_lookup.get(target_full.upper(), [])))

    def build_object_grant_statements_for(
        grants_by_grantee: Dict[str, Set[ObjectGrantEntry]]
    ) -> Tuple[int, int, Dict[str, List[str]], Dict[str, Set[str]]]:
        object_grant_lookup_local: Dict[str, List[str]] = defaultdict(list)
        grants_by_owner_local: Dict[str, Set[str]] = defaultdict(set)
        raw_count = sum(len(v) for v in grants_by_grantee.values())
        if not raw_count:
            return 0, 0, dict(object_grant_lookup_local), dict(grants_by_owner_local)
        object_index: Dict[str, Dict[bool, Dict[str, Set[str]]]] = defaultdict(lambda: defaultdict(lambda: defaultdict(set)))
        filtered = 0
        for grantee, entries in grants_by_grantee.items():
            grantee_u = (grantee or "").upper()
            if not grantee_u:
                continue
            for entry in entries:
                if not entry or not entry.object_full:
                    continue
                obj_full_u = entry.object_full.upper()
                owner_u = obj_full_u.split('.', 1)[0] if '.' in obj_full_u else ""
                if not is_allowed_grant_owner(owner_u):
                    filtered += 1
                    continue
                object_index[obj_full_u][bool(entry.grantable)][grantee_u].add(entry.privilege.upper())

        if filtered:
            log.info("[GRANT] 已过滤 %d 条对象授权（owner 不在允许范围）。", filtered)

        merged_count = 0
        for obj_full, grantable_map in object_index.items():
            owner = obj_full.split('.', 1)[0] if '.' in obj_full else obj_full
            for grantable, grantee_map in grantable_map.items():
                if merge_grantees:
                    if merge_privileges:
                        group_by_privs: Dict[Tuple[str, ...], Set[str]] = defaultdict(set)
                        for grantee_u, privs in grantee_map.items():
                            priv_key = tuple(sorted(privs))
                            group_by_privs[priv_key].add(grantee_u)
                        for priv_key, grantees in group_by_privs.items():
                            stmt = format_object_grant_stmt(
                                list(priv_key),
                                obj_full,
                                sorted(grantees),
                                grantable
                            )
                            object_grant_lookup_local[obj_full].append(stmt)
                            grants_by_owner_local[owner].add(stmt)
                            merged_count += 1
                    else:
                        group_by_priv: Dict[str, Set[str]] = defaultdict(set)
                        for grantee_u, privs in grantee_map.items():
                            for priv in privs:
                                group_by_priv[priv].add(grantee_u)
                        for priv, grantees in group_by_priv.items():
                            stmt = format_object_grant_stmt(
                                [priv],
                                obj_full,
                                sorted(grantees),
                                grantable
                            )
                            object_grant_lookup_local[obj_full].append(stmt)
                            grants_by_owner_local[owner].add(stmt)
                            merged_count += 1
                else:
                    for grantee_u, privs in grantee_map.items():
                        if merge_privileges:
                            stmt = format_object_grant_stmt(
                                sorted(privs),
                                obj_full,
                                [grantee_u],
                                grantable
                            )
                            object_grant_lookup_local[obj_full].append(stmt)
                            grants_by_owner_local[owner].add(stmt)
                            merged_count += 1
                        else:
                            for priv in sorted(privs):
                                stmt = format_object_grant_stmt(
                                    [priv],
                                    obj_full,
                                    [grantee_u],
                                    grantable
                                )
                                object_grant_lookup_local[obj_full].append(stmt)
                                grants_by_owner_local[owner].add(stmt)
                                merged_count += 1
        return raw_count, merged_count, dict(object_grant_lookup_local), dict(grants_by_owner_local)

    def pre_add_cross_schema_grants(
        constraint_tasks: List[Tuple[ConstraintMismatch, str, str, str, str]],
        trigger_tasks: List[Tuple[str, str, str, str, str, str, str]]
    ) -> int:
        if not grant_enabled:
            return 0
        added = 0
        for item, src_schema, src_table, tgt_schema, _tgt_table in constraint_tasks:
            cons_map = oracle_meta.constraints.get((src_schema.upper(), src_table.upper()), {})
            for cons_name in item.missing_constraints:
                cons_meta = cons_map.get(cons_name.upper())
                if not cons_meta:
                    continue
                ctype = (cons_meta.get("type") or "").upper()
                if ctype != "R":
                    continue
                ref_owner = cons_meta.get("ref_table_owner") or cons_meta.get("r_owner")
                ref_table = cons_meta.get("ref_table_name")
                if not ref_owner or not ref_table:
                    continue
                ref_owner_u = ref_owner.upper()
                ref_table_u = ref_table.upper()
                ref_src_full = f"{ref_owner_u}.{ref_table_u}"
                ref_tgt_full = get_mapped_target(full_object_mapping, ref_src_full, 'TABLE') or ref_src_full
                if '.' in ref_tgt_full:
                    ref_tgt_schema, ref_tgt_table = ref_tgt_full.split('.', 1)
                else:
                    ref_tgt_schema, ref_tgt_table = ref_owner_u, ref_table_u
                if ref_tgt_schema.upper() != tgt_schema.upper():
                    add_object_grant(tgt_schema, "REFERENCES", f"{ref_tgt_schema}.{ref_tgt_table}")
                    added += 1

        for _ss, _tn, ts, _to, _st, tts, tt in trigger_tasks:
            if tts and tt and ts and tts.upper() != ts.upper():
                required_priv = GRANT_PRIVILEGE_BY_TYPE.get('TABLE', 'SELECT')
                add_object_grant(ts, required_priv, f"{tts}.{tt}")
                added += 1

        return added

    schema_requests: Dict[str, Dict[str, Set[str]]] = defaultdict(lambda: defaultdict(set))
    unsupported_types: Set[str] = set()
    public_synonym_fallback: Set[Tuple[str, str]] = set()

    def _trigger_allowed(src_full: Optional[str], tgt_full: Optional[str]) -> bool:
        if not trigger_filter_enabled:
            return True
        if not trigger_filter_set:
            return False
        src_u = (src_full or "").upper()
        tgt_u = (tgt_full or "").upper()
        return (src_u in trigger_filter_set) or (tgt_u in trigger_filter_set)

    def queue_request(schema: str, obj_type: str, obj_name: str) -> None:
        obj_type_u = obj_type.upper()
        schema_u = schema.upper()
        obj_name_u = obj_name.upper()
        if obj_type_u == 'VIEW':
            # VIEW 固定走 DBMS_METADATA，不加入 dbcat 导出队列
            return
        if schema_u == 'PUBLIC' and obj_type_u == 'SYNONYM':
            public_synonym_fallback.add((schema_u, obj_name_u))
            return
        if obj_type_u not in DBCAT_OPTION_MAP:
            unsupported_types.add(obj_type_u)
            return
        schema_requests[schema_u][obj_type_u].add(obj_name_u)

    missing_tables_supported: List[Tuple[str, str, str, str]] = []
    missing_tables_unsupported: List[Tuple[str, str, str, str, ObjectSupportReportRow]] = []
    other_missing_supported: List[Tuple[str, str, str, str, str]] = []
    other_missing_unsupported: List[Tuple[str, str, str, str, str, ObjectSupportReportRow]] = []
    missing_total_by_type: Dict[str, int] = defaultdict(int)
    missing_allowed_by_type: Dict[str, int] = defaultdict(int)
    synonym_scope_skipped = 0
    index_missing_total = sum(len(item.missing_indexes) for item in extra_results.get('index_mismatched', []))
    index_skip_counts: Dict[str, int] = defaultdict(int)

    def is_temporary_support_row(row: Optional[ObjectSupportReportRow]) -> bool:
        if not row:
            return False
        code = (row.reason_code or "").upper()
        if "TEMPORARY_TABLE" in code or "TEMP_TABLE" in code:
            return True
        return False

    for (obj_type, tgt_name, src_name) in tv_results.get('missing', []):
        obj_type_u = obj_type.upper()
        if '.' not in src_name or '.' not in tgt_name:
            continue
        src_schema, src_obj = src_name.split('.', 1)
        tgt_schema, tgt_obj = tgt_name.split('.', 1)
        if obj_type_u == 'SYNONYM':
            src_schema_u = src_schema.upper()
            src_obj_u = src_obj.upper()
            src_key = (src_schema_u, src_obj_u)
            src_full = f"{src_schema_u}.{src_obj_u}"
            syn_meta = synonym_meta_map.get(src_key)
            if syn_meta and allowed_synonym_targets and syn_meta.table_owner:
                table_owner_u = syn_meta.table_owner.upper()
                if table_owner_u not in allowed_synonym_targets and src_full not in remap_rules:
                    log.info(
                        "[FIXUP] 跳过同义词 %s.%s（table_owner=%s 不在 source_schemas 范围内）。",
                        src_schema, src_obj, table_owner_u
                    )
                    continue
            if src_schema_u == 'PUBLIC' and not syn_meta and src_full not in remap_rules:
                log.info(
                    "[FIXUP] 跳过 PUBLIC 同义词 %s.%s（table_owner 不在 source_schemas 范围内）。",
                    src_schema, src_obj
                )
                continue
        missing_total_by_type[obj_type_u] += 1
        support_row = support_state_map.get((obj_type_u, f"{src_schema.upper()}.{src_obj.upper()}"))
        support_state = support_row.support_state if support_row else SUPPORT_STATE_SUPPORTED
        if not allow_fixup(obj_type_u, tgt_schema, src_schema):
            continue
        if obj_type_u == 'SYNONYM' and not allow_synonym_scope(src_schema, tgt_schema):
            synonym_scope_skipped += 1
            continue
        missing_allowed_by_type[obj_type_u] += 1
        queue_request(src_schema, obj_type_u, src_obj)
        if obj_type_u == 'TABLE':
            if support_state == SUPPORT_STATE_SUPPORTED:
                missing_tables_supported.append((src_schema, src_obj, tgt_schema, tgt_obj))
            else:
                missing_tables_unsupported.append((src_schema, src_obj, tgt_schema, tgt_obj, support_row))
        else:
            if support_state == SUPPORT_STATE_SUPPORTED:
                other_missing_supported.append((obj_type_u, src_schema, src_obj, tgt_schema, tgt_obj))
            else:
                other_missing_unsupported.append((obj_type_u, src_schema, src_obj, tgt_schema, tgt_obj, support_row))

    if fixup_schema_filter or fixup_type_filter:
        filtered = []
        for obj_type_u in sorted(missing_total_by_type.keys()):
            total = missing_total_by_type[obj_type_u]
            allowed = missing_allowed_by_type.get(obj_type_u, 0)
            if allowed != total:
                filtered.append(f"{obj_type_u}={allowed}/{total}")
        if filtered:
            log.info("[FIXUP] fixup_types/fixup_schemas 生效: %s", ", ".join(filtered))
    if synonym_scope_skipped:
        log.info(
            "[FIXUP] synonym_fixup_scope=%s，已跳过非 PUBLIC 同义词 %d 个。",
            synonym_fixup_scope,
            synonym_scope_skipped
        )

    if package_results:
        for row in package_results.get("rows", []):
            if row.result != "MISSING_TARGET":
                continue
            if row.src_status == "INVALID":
                log.info(
                    "[FIXUP] 跳过源端 INVALID 的 %s %s (不生成 DDL)。",
                    row.obj_type, row.src_full
                )
                continue
            if "." not in row.src_full or "." not in row.tgt_full:
                continue
            src_schema, src_obj = row.src_full.split(".", 1)
            tgt_schema, tgt_obj = row.tgt_full.split(".", 1)
            obj_type_u = row.obj_type.upper()
            if not allow_fixup(obj_type_u, tgt_schema, src_schema):
                continue
            support_row = support_state_map.get((obj_type_u, row.src_full.upper()))
            support_state = support_row.support_state if support_row else SUPPORT_STATE_SUPPORTED
            queue_request(src_schema, obj_type_u, src_obj)
            if support_state == SUPPORT_STATE_SUPPORTED:
                other_missing_supported.append((obj_type_u, src_schema, src_obj, tgt_schema, tgt_obj))
            else:
                other_missing_unsupported.append((obj_type_u, src_schema, src_obj, tgt_schema, tgt_obj, support_row))

    sequence_tasks: List[Tuple[str, str, str, str]] = []
    for seq_mis in extra_results.get('sequence_mismatched', []):
        src_schema = seq_mis.src_schema.upper()
        for seq_name in sorted(seq_mis.missing_sequences):
            seq_name_u = seq_name.upper()
            src_full = f"{src_schema}.{seq_name_u}"
            mapped = get_mapped_target(full_object_mapping, src_full, 'SEQUENCE')
            if mapped and '.' in mapped:
                tgt_schema, tgt_name = mapped.split('.')
            else:
                tgt_schema = seq_mis.tgt_schema.upper()
                tgt_name = seq_name_u
            if not allow_fixup('SEQUENCE', tgt_schema, src_schema):
                continue
            queue_request(src_schema, 'SEQUENCE', seq_name_u)
            sequence_tasks.append((src_schema, seq_name_u, tgt_schema, tgt_name))

    index_tasks: List[Tuple[IndexMismatch, str, str, str, str]] = []
    for item in extra_results.get('index_mismatched', []):
        table_str = item.table.split()[0]
        if '.' not in table_str:
            index_skip_counts["table_unmapped"] += len(item.missing_indexes)
            continue
        src_name = table_map.get(table_str)
        if not src_name or '.' not in src_name:
            index_skip_counts["table_unmapped"] += len(item.missing_indexes)
            continue
        src_schema, src_table = src_name.split('.')
        tgt_schema, tgt_table = table_str.split('.')
        if (src_schema.upper(), src_table.upper()) in unsupported_table_keys:
            index_skip_counts["unsupported_table"] += len(item.missing_indexes)
            continue
        if not allow_fixup('INDEX', tgt_schema, src_schema):
            reason = classify_fixup_skip('INDEX', tgt_schema, src_schema) or "filtered"
            index_skip_counts[reason] += len(item.missing_indexes)
            continue
        queue_request(src_schema, 'TABLE', src_table)
        index_tasks.append((item, src_schema, src_table, tgt_schema.upper(), tgt_table.upper()))

    constraint_tasks: List[Tuple[ConstraintMismatch, str, str, str, str]] = []
    for item in extra_results.get('constraint_mismatched', []):
        table_str = item.table.split()[0]
        if '.' not in table_str:
            continue
        src_name = table_map.get(table_str)
        if not src_name or '.' not in src_name:
            continue
        src_schema, src_table = src_name.split('.')
        tgt_schema, tgt_table = table_str.split('.')
        if (src_schema.upper(), src_table.upper()) in unsupported_table_keys:
            continue
        if not allow_fixup('CONSTRAINT', tgt_schema, src_schema):
            continue
        queue_request(src_schema, 'TABLE', src_table)
        constraint_tasks.append((item, src_schema, src_table, tgt_schema.upper(), tgt_table.upper()))

    trigger_tasks: List[Tuple[str, str, str, str, str, str, str]] = []
    for item in extra_results.get('trigger_mismatched', []):
        table_str = item.table.split()[0]
        if '.' not in table_str:
            continue
        src_name = table_map.get(table_str)
        if not src_name or '.' not in src_name:
            continue
        src_schema, src_table = src_name.split('.', 1)
        tgt_schema, tgt_table = table_str.split('.', 1)
        if (src_schema.upper(), src_table.upper()) in unsupported_table_keys:
            continue
        # 优先使用缺失映射对（源->目标），确保 dbcat 按源名导出
        if item.missing_mappings:
            for src_full, tgt_full in item.missing_mappings:
                if '.' not in src_full or '.' not in tgt_full:
                    continue
                src_schema_u, src_trg = src_full.split('.', 1)
                tgt_schema_final, tgt_obj = tgt_full.split('.', 1)
                if not _trigger_allowed(src_full, tgt_full):
                    continue
                if not allow_fixup('TRIGGER', tgt_schema_final, src_schema_u):
                    continue
                queue_request(src_schema_u, 'TRIGGER', src_trg)
                trigger_tasks.append((src_schema_u, src_trg, tgt_schema_final, tgt_obj, src_table, tgt_schema, tgt_table))
        else:
            for trg_name in sorted(item.missing_triggers):
                src_owner_u, src_trg_u = normalize_trigger_identity(trg_name, None, src_schema)
                if not src_trg_u:
                    continue
                src_owner_u = (src_owner_u or src_schema).upper()
                src_full = f"{src_owner_u}.{src_trg_u}"
                mapped = get_mapped_target(full_object_mapping, src_full, 'TRIGGER')
                if mapped and '.' in mapped:
                    tgt_schema_final, tgt_obj = mapped.split('.')
                else:
                    tgt_schema_final = src_owner_u
                    tgt_obj = src_trg_u
                tgt_full = f"{tgt_schema_final}.{tgt_obj}"
                if not _trigger_allowed(src_full, tgt_full):
                    continue
                if not allow_fixup('TRIGGER', tgt_schema_final, src_owner_u):
                    continue
                queue_request(src_owner_u, 'TRIGGER', src_trg_u)
                trigger_tasks.append((src_owner_u, src_trg_u, tgt_schema_final, tgt_obj, src_table, tgt_schema, tgt_table))

    if invalid_trigger_keys:
        before_count = len(trigger_tasks)
        trigger_tasks = [
            task for task in trigger_tasks
            if (task[0].upper(), task[1].upper()) not in invalid_trigger_keys
        ]
        skipped = before_count - len(trigger_tasks)
        if skipped:
            log.info("[FIXUP] 跳过源端 INVALID 的 TRIGGER %d 个。", skipped)

    other_missing_summary: Dict[str, int] = defaultdict(int)
    for ot, _, _, _, _ in other_missing_supported:
        other_missing_summary[ot] += 1
    for ot, _, _, _, _, _ in other_missing_unsupported:
        other_missing_summary[ot] += 1
    index_total = sum(len(item.missing_indexes) for item, _, _, _, _ in index_tasks)
    constraint_total = sum(len(item.missing_constraints) for item, _, _, _, _ in constraint_tasks)
    supported_table_cnt = len(missing_tables_supported)
    unsupported_table_cnt = len(missing_tables_unsupported)
    supported_other_cnt = len(other_missing_supported)
    unsupported_other_cnt = len(other_missing_unsupported)
    total_missing_scripts = (
        len(sequence_tasks)
        + supported_table_cnt
        + supported_other_cnt
        + index_total
        + constraint_total
        + len(trigger_tasks)
    )
    unsupported_total = unsupported_table_cnt + unsupported_other_cnt
    other_summary = ", ".join(f"{k}={v}" for k, v in sorted(other_missing_summary.items())) or "无"
    log.info(
        "[FIXUP] 待生成缺失对象/脚本总计 %d 个 (unsupported=%d): TABLE=%d(+%d), 其他=%d(+%d) (%s), INDEX=%d, CONSTRAINT=%d, SEQUENCE=%d, TRIGGER=%d",
        total_missing_scripts,
        unsupported_total,
        supported_table_cnt,
        unsupported_table_cnt,
        supported_other_cnt,
        unsupported_other_cnt,
        other_summary,
        index_total,
        constraint_total,
        len(sequence_tasks),
        len(trigger_tasks)
    )
    log.info("[FIXUP] 进度日志间隔 %.0f 秒，可通过 progress_log_interval 配置。", progress_log_interval)
    if public_synonym_fallback:
        log.info(
            "[FIXUP] 检测到 %d 个 PUBLIC 同义词，将跳过 dbcat，优先使用同义词元数据；缺失时再用 DBMS_METADATA 兜底。",
            len(public_synonym_fallback)
        )

    missing_tables = list(missing_tables_supported)

    ob_grant_catalog: Optional[ObGrantCatalog] = None
    object_grants_missing_by_grantee: Dict[str, Set[ObjectGrantEntry]] = {}
    sys_privs_missing_by_grantee: Dict[str, Set[SystemGrantEntry]] = {}
    role_privs_missing_by_grantee: Dict[str, Set[RoleGrantEntry]] = {}

    if grant_enabled:
        pre_added = pre_add_cross_schema_grants(constraint_tasks, trigger_tasks)
        if pre_added:
            log.info("[GRANT] 预追加跨 schema 授权 %d 条 (FK/Trigger)。", pre_added)
        expected_grantees = set(object_grants_by_grantee.keys())
        expected_grantees.update(sys_privs_by_grantee.keys())
        expected_grantees.update(role_privs_by_grantee.keys())
        ob_grant_catalog = load_ob_grant_catalog(ob_cfg, expected_grantees)
        if ob_grant_catalog is None:
            log.warning("[GRANT_MISS] OB 权限读取失败，缺失授权将退化为全量输出。")
        (
            object_grants_missing_by_grantee,
            sys_privs_missing_by_grantee,
            role_privs_missing_by_grantee
        ) = filter_missing_grant_entries(
            object_grants_by_grantee,
            sys_privs_by_grantee,
            role_privs_by_grantee,
            ob_grant_catalog
        )
        if object_grants_by_grantee:
            raw_count, merged_count, object_grant_lookup, grants_by_owner = build_object_grant_statements_for(
                object_grants_by_grantee
            )
            log.info(
                "[GRANT] 对象授权合并: 原始=%d, 合并后=%d, merge_privileges=%s, merge_grantees=%s",
                raw_count,
                merged_count,
                "true" if merge_privileges else "false",
                "true" if merge_grantees else "false"
            )

    # 分离 VIEW 对象和其他对象（支持/不支持）
    view_missing_supported: List[Tuple[str, str, str, str]] = []
    view_missing_unsupported: List[Tuple[str, str, str, str, ObjectSupportReportRow]] = []
    non_view_missing_supported: List[Tuple[str, str, str, str, str]] = []
    non_view_missing_unsupported: List[Tuple[str, str, str, str, str, ObjectSupportReportRow]] = []

    for (obj_type, src_schema, src_obj, tgt_schema, tgt_obj) in other_missing_supported:
        if obj_type.upper() == 'VIEW':
            view_missing_supported.append((src_schema, src_obj, tgt_schema, tgt_obj))
        else:
            non_view_missing_supported.append((obj_type, src_schema, src_obj, tgt_schema, tgt_obj))

    for (obj_type, src_schema, src_obj, tgt_schema, tgt_obj, support_row) in other_missing_unsupported:
        if obj_type.upper() == 'VIEW':
            view_missing_unsupported.append((src_schema, src_obj, tgt_schema, tgt_obj, support_row))
        else:
            non_view_missing_unsupported.append((obj_type, src_schema, src_obj, tgt_schema, tgt_obj, support_row))

    if invalid_view_keys:
        before_supported = len(view_missing_supported)
        before_unsupported = len(view_missing_unsupported)
        view_missing_supported = [
            entry for entry in view_missing_supported
            if (entry[0].upper(), entry[1].upper()) not in invalid_view_keys
        ]
        view_missing_unsupported = [
            entry for entry in view_missing_unsupported
            if (entry[0].upper(), entry[1].upper()) not in invalid_view_keys
        ]
        skipped = (before_supported - len(view_missing_supported)) + (before_unsupported - len(view_missing_unsupported))
        if skipped:
            log.info("[FIXUP] 跳过源端 INVALID 的 VIEW %d 个。", skipped)

    view_missing_objects = list(view_missing_supported)
    non_view_missing_objects = list(non_view_missing_supported)

    view_target_set: Set[str] = {
        f"{tgt_schema}.{tgt_obj}".upper()
        for _src_schema, _src_obj, tgt_schema, tgt_obj in view_missing_supported
    }
    view_prereq_grants_by_grantee: Dict[str, Set[ObjectGrantEntry]] = {}
    view_post_grants_by_grantee: Dict[str, Set[ObjectGrantEntry]] = {}
    if grant_enabled and object_grants_missing_by_grantee and view_target_set:
        (
            view_prereq_grants_by_grantee,
            view_post_grants_by_grantee,
            object_grants_missing_by_grantee
        ) = split_view_grants(
            view_target_set,
            expected_dependency_pairs,
            object_grants_missing_by_grantee
        )
    view_grants_split_enabled = bool(view_prereq_grants_by_grantee or view_post_grants_by_grantee)

    def _order_plsql_fixups(
        items: List[Tuple[str, str, str, str, str]]
    ) -> List[Tuple[str, str, str, str, str]]:
        plsql_items = [item for item in items if item[0].upper() in PLSQL_ORDER_TYPES]
        if len(plsql_items) <= 1:
            return items
        node_lookup: Dict[Tuple[str, str], Tuple[str, str, str, str, str]] = {}
        for obj_type, _src_schema, _src_obj, tgt_schema, tgt_obj in plsql_items:
            node_lookup[(f"{tgt_schema}.{tgt_obj}".upper(), obj_type.upper())] = (
                obj_type, _src_schema, _src_obj, tgt_schema, tgt_obj
            )
        graph: Dict[Tuple[str, str], Set[Tuple[str, str]]] = defaultdict(set)
        if expected_dependency_pairs:
            for dep_full, dep_type, ref_full, ref_type in expected_dependency_pairs:
                dep_key = (dep_full.upper(), dep_type.upper())
                ref_key = (ref_full.upper(), ref_type.upper())
                if dep_key in node_lookup and ref_key in node_lookup and dep_key != ref_key:
                    graph[dep_key].add(ref_key)
        # 强制 BODY 依赖 SPEC
        for obj_type, _src_schema, _src_obj, tgt_schema, tgt_obj in plsql_items:
            obj_type_u = obj_type.upper()
            if obj_type_u in ("PACKAGE BODY", "TYPE BODY"):
                tgt_full = f"{tgt_schema}.{tgt_obj}".upper()
                body_key = (tgt_full, obj_type_u)
                spec_type = "PACKAGE" if obj_type_u == "PACKAGE BODY" else "TYPE"
                spec_key = (tgt_full, spec_type)
                if spec_key in node_lookup and body_key in node_lookup:
                    graph[body_key].add(spec_key)

        def _node_sort_key(node: Tuple[str, str]) -> Tuple[int, str, str]:
            return (PLSQL_ORDER_PRIORITY.get(node[1], 99), node[0], node[1])

        order: List[Tuple[str, str]] = []
        visiting: Set[Tuple[str, str]] = set()
        visited: Set[Tuple[str, str]] = set()
        cycles: List[str] = []

        def _dfs(node: Tuple[str, str], stack: List[str]) -> None:
            if node in visited:
                return
            if node in visiting:
                cycles.append(" -> ".join(stack + [f"{node[0]} ({node[1]})"]))
                return
            visiting.add(node)
            for dep in sorted(graph.get(node, set()), key=_node_sort_key):
                _dfs(dep, stack + [f"{node[0]} ({node[1]})"])
            visiting.remove(node)
            visited.add(node)
            order.append(node)

        for node in sorted(node_lookup.keys(), key=_node_sort_key):
            _dfs(node, [])

        if cycles:
            log.warning("[FIXUP] 发现 PLSQL 依赖环，将使用稳定顺序: %s", " | ".join(cycles))

        ordered_items = [node_lookup[key] for key in order if key in node_lookup]
        if len(ordered_items) != len(plsql_items):
            return items

        plsql_iter = iter(ordered_items)
        reordered: List[Tuple[str, str, str, str, str]] = []
        for entry in items:
            if entry[0].upper() in PLSQL_ORDER_TYPES:
                try:
                    reordered.append(next(plsql_iter))
                except StopIteration:
                    reordered.append(entry)
            else:
                reordered.append(entry)
        return reordered

    non_view_missing_objects = _order_plsql_fixups(non_view_missing_objects)
    if non_view_missing_unsupported:
        support_lookup = {
            (t.upper(), s.upper(), o.upper(), ts.upper(), to.upper()): row
            for t, s, o, ts, to, row in non_view_missing_unsupported
        }
        ordered_items = _order_plsql_fixups(
            [(t, s, o, ts, to) for (t, s, o, ts, to, _row) in non_view_missing_unsupported]
        )
        reordered_with_support: List[Tuple[str, str, str, str, str, ObjectSupportReportRow]] = []
        for obj_type, src_schema, src_obj, tgt_schema, tgt_obj in ordered_items:
            key = (obj_type.upper(), src_schema.upper(), src_obj.upper(), tgt_schema.upper(), tgt_obj.upper())
            support_row = support_lookup.get(key)
            if support_row:
                reordered_with_support.append((obj_type, src_schema, src_obj, tgt_schema, tgt_obj, support_row))
        if reordered_with_support:
            non_view_missing_unsupported = reordered_with_support

    if report_dir and report_timestamp and view_missing_supported:
        view_targets = [f"{tgt_schema}.{tgt_obj}" for _, _, tgt_schema, tgt_obj in view_missing_supported]
        dep_pairs = expected_dependency_pairs or set()
        view_chain_path = Path(report_dir) / f"VIEWs_chain_{report_timestamp}.txt"
        view_chain_file = export_view_fixup_chains(
            view_targets,
            dep_pairs,
            view_chain_path,
            full_object_mapping,
            remap_rules,
            synonym_meta=synonym_meta_map,
            ob_meta=ob_meta,
            ob_grant_catalog=ob_grant_catalog,
            view_grant_targets=grant_plan.view_grant_targets if grant_plan else None
        )
        if view_chain_file:
            log.info("VIEW fixup 依赖链已输出: %s", view_chain_file)
        else:
            log.info("VIEW fixup 依赖链输出已跳过（无链路或写入失败）。")

    dbcat_data, ddl_source_meta = fetch_dbcat_schema_objects(ora_cfg, settings, schema_requests)

    # 预取所有可能需要 fallback 的 DDL（dbcat 未命中的对象）
    fallback_ddl_cache: Dict[Tuple[str, str, str], str] = {}
    
    # 收集需要 fallback 的对象（dbcat 缺失时由 DBMS_METADATA 兜底）
    fallback_needed: List[Tuple[str, str, str]] = []
    for obj_type, src_schema, src_obj, tgt_schema, tgt_obj in non_view_missing_supported:
        # 检查 dbcat_data 中是否有
        if not dbcat_data.get(src_schema.upper(), {}).get(obj_type.upper(), {}).get(src_obj.upper()):
            fallback_needed.append((src_schema, obj_type, src_obj))
    for obj_type, src_schema, src_obj, tgt_schema, tgt_obj, _support_row in non_view_missing_unsupported:
        if not dbcat_data.get(src_schema.upper(), {}).get(obj_type.upper(), {}).get(src_obj.upper()):
            fallback_needed.append((src_schema, obj_type, src_obj))

    # 视图任务（VIEW 固定使用 DBMS_METADATA）
    for src_schema, src_obj, _, _ in view_missing_supported:
        fallback_needed.append((src_schema, 'VIEW', src_obj))
    for src_schema, src_obj, _, _, _support_row in view_missing_unsupported:
        fallback_needed.append((src_schema, 'VIEW', src_obj))
    
    # 序列任务
    for src_schema, src_seq, tgt_schema, tgt_seq in sequence_tasks:
        if not dbcat_data.get(src_schema.upper(), {}).get('SEQUENCE', {}).get(src_seq.upper()):
            fallback_needed.append((src_schema, 'SEQUENCE', src_seq))
    
    # 触发器任务
    for src_schema, trg_name, tgt_schema, tgt_obj, _src_table, _tgt_schema, _tgt_table in trigger_tasks:
        if not dbcat_data.get(src_schema.upper(), {}).get('TRIGGER', {}).get(trg_name.upper()):
            fallback_needed.append((src_schema, 'TRIGGER', trg_name))
    
    # 批量预取
    if fallback_needed:
        log.info("[FIXUP] 预取 %d 个可能需要 DBMS_METADATA 兜底的对象...", len(fallback_needed))
        fallback_ddl_cache = oracle_get_ddl_batch(ora_cfg, fallback_needed)

    oracle_conn = None
    oracle_conn_lock = threading.Lock()

    def _is_not_found_error(exc: Exception) -> bool:
        msg = str(exc).upper()
        return any(code in msg for code in ("ORA-31603", "ORA-04043", "ORA-00942", "ORA-06512"))

    def get_fallback_ddl(schema: str, obj_type: str, obj_name: str) -> Tuple[Optional[str], str]:
        """当 dbcat 缺失 DDL 时尝试使用 DBMS_METADATA 兜底，优先使用预取缓存。"""
        nonlocal oracle_conn
        allowed_types = BATCH_DDL_ALLOWED_TYPES
        if obj_type.upper() not in allowed_types:
            return None, "MISSING"

        # 优先使用预取缓存 (DBMS_METADATA)
        cache_key = (schema.upper(), obj_type.upper(), obj_name.upper())
        if cache_key in fallback_ddl_cache:
            return fallback_ddl_cache[cache_key], "DBMS_METADATA"

        # 缓存未命中，单独获取（兜底）
        with oracle_conn_lock:
            try:
                if oracle_conn is None:
                    oracle_conn = oracledb.connect(
                        user=ora_cfg['user'],
                        password=ora_cfg['password'],
                        dsn=ora_cfg['dsn']
                    )
                    setup_metadata_session(oracle_conn)
                ddl = oracle_get_ddl(oracle_conn, obj_type, schema, obj_name)
                if ddl:
                    return ddl, "DBMS_METADATA"
                if obj_type.upper() == "VIEW":
                    view_text = oracle_get_view_text(oracle_conn, schema, obj_name)
                    if view_text:
                        ddl = build_view_ddl_from_text(schema, obj_name, *view_text)
                        if ddl:
                            log.info("[DDL] VIEW %s.%s 使用 DBA_VIEWS 兜底生成 DDL。", schema, obj_name)
                            return ddl, "DBA_VIEWS"
                return None, "MISSING"
            except Exception as exc:
                if _is_not_found_error(exc):
                    log.info("[DDL] DBMS_METADATA 未找到 %s.%s (%s)，跳过兜底。", schema, obj_name, obj_type)
                    return None, "MISSING"
                log.warning("[DDL] DBMS_METADATA 获取 %s.%s (%s) 失败: %s", schema, obj_name, obj_type, exc)
                return None, "MISSING"

    def _is_dbcat_unsupported_table(ddl: str) -> bool:
        """判断 dbcat 抓取的 TABLE DDL 是否包含 unsupported 提示。"""
        if not ddl:
            return False
        return "UNSUPPORTED" in ddl.upper()

    table_ddl_cache: Dict[Tuple[str, str], str] = {}
    for schema, type_map in dbcat_data.items():
        for table_name, ddl in type_map.get('TABLE', {}).items():
            table_ddl_cache[(schema, table_name)] = ddl

    def build_index_from_meta(
        src_schema: str,
        src_table: str,
        tgt_schema: str,
        tgt_table: str,
        idx_name: str
    ) -> Optional[str]:
        meta = oracle_meta.indexes.get((src_schema.upper(), src_table.upper()), {}).get(idx_name.upper())
        if not meta:
            return None
        cols = meta.get("columns") or []
        expr_map = meta.get("expressions") or {}
        if not cols:
            return None
        uniq = (meta.get("uniqueness") or "").upper() == "UNIQUE"
        ddl_cols: List[str] = []
        for pos, col in enumerate(cols, start=1):
            expr = expr_map.get(pos)
            ddl_cols.append(expr if expr else col)
        col_list = ", ".join(ddl_cols)
        prefix = "UNIQUE " if uniq else ""
        idx_full = quote_qualified_parts(tgt_schema.upper(), idx_name.upper())
        table_full = quote_qualified_parts(tgt_schema.upper(), tgt_table.upper())
        return f"CREATE {prefix}INDEX {idx_full} ON {table_full} ({col_list});"

    worker_count = max(1, int(settings.get('fixup_workers', 1)))
    if worker_count == 1:
        log.info("[FIXUP] 并发未启用 (worker=1)，可通过 fixup_workers 配置提高到 8 或 12。")
    else:
        log.info("[FIXUP] 启用并发生成 (worker=%d)，可通过 fixup_workers 配置调整。", worker_count)

    def run_tasks(tasks: List[Callable[[], None]], label: str) -> None:
        if not tasks:
            return
        if worker_count <= 1:
            for task in tasks:
                try:
                    task()
                except Exception as exc:
                    log.error("[FIXUP] 任务 %s 失败: %s", label, exc)
            return

        with ThreadPoolExecutor(max_workers=worker_count) as executor:
            futures = [executor.submit(task) for task in tasks]
            for future in as_completed(futures):
                try:
                    future.result()
                except Exception as exc:
                    log.error("[FIXUP] 任务 %s 失败: %s", label, exc)

    log.info("[FIXUP] (1/9) 正在生成 SEQUENCE 脚本...")
    seq_progress = build_progress_tracker(len(sequence_tasks), "[FIXUP] (1/9) SEQUENCE")
    seq_jobs: List[Callable[[], None]] = []
    for src_schema, seq_name, tgt_schema, tgt_name in sequence_tasks:
        def _job(ss=src_schema, sn=seq_name, ts=tgt_schema, tn=tgt_name):
            try:
                fetch_result = fetch_ddl_with_timing(ss, 'SEQUENCE', sn)
                if len(fetch_result) != 3:
                    log.error("[FIXUP] SEQUENCE fetch_ddl_with_timing 返回了 %d 个值，期望 3 个: %s", len(fetch_result), fetch_result)
                    return
                ddl, ddl_source_label, _elapsed = fetch_result
                if not ddl:
                    log.warning("[FIXUP] 未找到 SEQUENCE %s.%s 的 dbcat DDL。", ss, sn)
                    mark_source('SEQUENCE', 'missing')
                    return
                if ddl_source_label.startswith("DBCAT"):
                    mark_source('SEQUENCE', 'dbcat')
                elif ddl_source_label in ("DBMS_METADATA", "DBA_VIEWS"):
                    mark_source('SEQUENCE', 'fallback')
                else:
                    mark_source('SEQUENCE', 'missing')
                ddl_adj = adjust_ddl_for_object(
                    ddl,
                    ss,
                    sn,
                    ts,
                    tn,
                    extra_identifiers=get_relevant_replacements(ss),
                    obj_type='SEQUENCE'
                )
                ddl_adj = cleanup_dbcat_wrappers(ddl_adj)
                ddl_adj = prepend_set_schema(ddl_adj, ts)
                ddl_adj = normalize_ddl_for_ob(ddl_adj)
                obj_full = f"{ts}.{tn}"
                ddl_adj = apply_hint_filter('SEQUENCE', obj_full, ddl_adj)
                ddl_adj = apply_ddl_cleanup_rules(ddl_adj, 'SEQUENCE')
                ddl_adj = strip_constraint_enable(ddl_adj)
                ddl_adj = apply_fixup_idempotency(
                    ddl_adj,
                    'SEQUENCE',
                    ts,
                    tn,
                    settings,
                    idempotent_stats
                )
                filename = f"{ts}.{tn}.sql"
                header = f"修补缺失的 SEQUENCE {obj_full} (源: {ss}.{sn})"
                grants_for_seq = collect_grants_for_object(obj_full)
                log.info("[FIXUP]%s 写入 SEQUENCE 脚本: %s", source_tag(ddl_source_label), filename)
                write_fixup_file(
                    base_dir,
                    'sequence',
                    filename,
                    ddl_adj,
                    header,
                    grants_to_add=grants_for_seq if grants_for_seq else None
                )
            finally:
                seq_progress()
        seq_jobs.append(_job)
    run_tasks(seq_jobs, "SEQUENCE")

    log.info("[FIXUP] (2/9) 正在生成缺失的 TABLE CREATE 脚本...")
    table_jobs: List[Callable[[], None]] = []
    table_total = len(missing_tables)
    table_progress_tick = build_progress_tracker(table_total, "[FIXUP] (2/9) TABLE")
    table_counts: Dict[str, int] = defaultdict(int)
    table_progress = {"done": 0}
    table_lock = threading.Lock()

    def _record_table_progress(source_label: str) -> None:
        with table_lock:
            table_counts[source_label] += 1
            table_progress["done"] += 1
            stats = ", ".join(f"{k}={v}" for k, v in sorted(table_counts.items()))
        table_progress_tick(stats or "no scripts")

    for src_schema, src_table, tgt_schema, tgt_table in missing_tables:
        def _job(ss=src_schema, st=src_table, ts=tgt_schema, tt=tgt_table):
            progress_label = "unknown"
            try:
                fetch_result = fetch_ddl_with_timing(ss, 'TABLE', st)
                if len(fetch_result) != 3:
                    log.error("[FIXUP] TABLE fetch_ddl_with_timing 返回了 %d 个值，期望 3 个: %s", len(fetch_result), fetch_result)
                    return
                ddl, ddl_source_label, _elapsed = fetch_result
                if not ddl:
                    log.warning("[FIXUP] 未找到 TABLE %s.%s 的 dbcat DDL。", ss, st)
                    mark_source('TABLE', 'missing')
                    progress_label = "missing"
                    return
                # 如果 dbcat 返回 unsupported，尝试 DBMS_METADATA 兜底，直接暴露给用户
                if _is_dbcat_unsupported_table(ddl):
                    fallback_ddl, fallback_label = get_fallback_ddl(ss, 'TABLE', st)
                    if fallback_ddl:
                        ddl = fallback_ddl
                        ddl_source_label = fallback_label
                        mark_source('TABLE', 'fallback')
                        log.info("[FIXUP] TABLE %s.%s 的 dbcat DDL 为 unsupported，已使用 %s 兜底。", ss, st, fallback_label)
                    else:
                        log.warning("[FIXUP] TABLE %s.%s 的 dbcat DDL 为 unsupported，DBMS_METADATA 兜底失败，仍输出原始 DDL 供人工处理。", ss, st)
                if ddl_source_label.startswith("DBCAT"):
                    mark_source('TABLE', 'dbcat')
                elif ddl_source_label in ("DBMS_METADATA", "DBA_VIEWS"):
                    mark_source('TABLE', 'fallback')
                else:
                    mark_source('TABLE', 'missing')
                ddl_adj = adjust_ddl_for_object(
                    ddl,
                    ss,
                    st,
                    ts,
                    tt,
                    extra_identifiers=get_relevant_replacements(ss),
                    obj_type='TABLE'
                )
                ddl_adj = inflate_table_varchar_lengths(ddl_adj, ss, st, oracle_meta)
                ddl_adj = cleanup_dbcat_wrappers(ddl_adj)
                ddl_adj = prepend_set_schema(ddl_adj, ts)
                ddl_adj = normalize_ddl_for_ob(ddl_adj)
                obj_full = f"{ts}.{tt}"
                ddl_adj = apply_hint_filter('TABLE', obj_full, ddl_adj)
                ddl_adj = apply_ddl_cleanup_rules(ddl_adj, 'TABLE')
                ddl_adj = strip_constraint_enable(ddl_adj)
                ddl_adj = strip_enable_novalidate(ddl_adj)
                invisible_sql = build_invisible_column_alter_sql(
                    oracle_meta,
                    ss,
                    st,
                    ts,
                    tt,
                    visibility_fixup_enabled
                )
                if invisible_sql:
                    ddl_adj = _ensure_statement_terminated(ddl_adj)
                    ddl_adj = (
                        ddl_adj.rstrip()
                        + "\n\n-- APPLY INVISIBLE COLUMNS\n"
                        + invisible_sql
                    )
                ddl_adj = apply_fixup_idempotency(
                    ddl_adj,
                    'TABLE',
                    ts,
                    tt,
                    settings,
                    idempotent_stats
                )
                filename = f"{ts}.{tt}.sql"
                header = f"修补缺失的 TABLE {obj_full} (源: {ss}.{st})"
                grants_for_table = collect_grants_for_object(obj_full)
                log.info("[FIXUP]%s 写入 TABLE 脚本: %s", source_tag(ddl_source_label), filename)
                write_fixup_file(
                    base_dir,
                    'table',
                    filename,
                    ddl_adj,
                    header,
                    grants_to_add=grants_for_table if grants_for_table else None
                )
                progress_label = ddl_source_label.lower()
            finally:
                _record_table_progress(progress_label)
        table_jobs.append(_job)
    run_tasks(table_jobs, "TABLE")

    if missing_tables_unsupported:
        log.info("[FIXUP] (2b/9) 正在生成不支持 TABLE 的 DDL（仅输出，默认不执行）...")
        table_unsup_jobs: List[Callable[[], None]] = []
        table_unsup_progress = build_progress_tracker(
            len(missing_tables_unsupported),
            "[FIXUP] (2b/9) TABLE_UNSUPPORTED"
        )
        for src_schema, src_table, tgt_schema, tgt_table, support_row in missing_tables_unsupported:
            def _job(ss=src_schema, st=src_table, ts=tgt_schema, tt=tgt_table, sr=support_row):
                try:
                    fetch_result = fetch_ddl_with_timing(ss, 'TABLE', st)
                    if len(fetch_result) != 3:
                        log.error("[FIXUP] TABLE fetch_ddl_with_timing 返回了 %d 个值，期望 3 个: %s", len(fetch_result), fetch_result)
                        return
                    ddl, ddl_source_label, _elapsed = fetch_result
                    if not ddl:
                        log.warning("[FIXUP] 未找到 TABLE %s.%s 的 dbcat DDL。", ss, st)
                        mark_source('TABLE', 'missing')
                        return
                    if _is_dbcat_unsupported_table(ddl):
                        fallback_ddl, fallback_label = get_fallback_ddl(ss, 'TABLE', st)
                        if fallback_ddl:
                            ddl = fallback_ddl
                            ddl_source_label = fallback_label
                            mark_source('TABLE', 'fallback')
                            log.info("[FIXUP] TABLE %s.%s 的 dbcat DDL 为 unsupported，已使用 %s 兜底。", ss, st, fallback_label)
                        else:
                            log.warning("[FIXUP] TABLE %s.%s 的 dbcat DDL 为 unsupported，DBMS_METADATA 兜底失败，仍输出原始 DDL 供人工处理。", ss, st)
                    if ddl_source_label.startswith("DBCAT"):
                        mark_source('TABLE', 'dbcat')
                    elif ddl_source_label in ("DBMS_METADATA", "DBA_VIEWS"):
                        mark_source('TABLE', 'fallback')
                    else:
                        mark_source('TABLE', 'missing')
                    ddl_adj = adjust_ddl_for_object(
                        ddl,
                        ss,
                        st,
                        ts,
                        tt,
                        extra_identifiers=get_relevant_replacements(ss),
                        obj_type='TABLE'
                    )
                    ddl_adj = inflate_table_varchar_lengths(ddl_adj, ss, st, oracle_meta)
                    ddl_adj = cleanup_dbcat_wrappers(ddl_adj)
                    ddl_adj = prepend_set_schema(ddl_adj, ts)
                    ddl_adj = normalize_ddl_for_ob(ddl_adj)
                    obj_full = f"{ts}.{tt}"
                    ddl_adj = apply_hint_filter('TABLE', obj_full, ddl_adj)
                    ddl_adj = apply_ddl_cleanup_rules(ddl_adj, 'TABLE')
                    ddl_adj = strip_constraint_enable(ddl_adj)
                    ddl_adj = strip_enable_novalidate(ddl_adj)
                    invisible_sql = build_invisible_column_alter_sql(
                        oracle_meta,
                        ss,
                        st,
                        ts,
                        tt,
                        visibility_fixup_enabled
                    )
                    if invisible_sql:
                        ddl_adj = _ensure_statement_terminated(ddl_adj)
                        ddl_adj = (
                            ddl_adj.rstrip()
                            + "\n\n-- APPLY INVISIBLE COLUMNS\n"
                            + invisible_sql
                        )
                    filename = f"{ts}.{tt}.sql"
                    header = f"不支持 TABLE DDL {obj_full} (源: {ss}.{st})"
                    subdir = "tables_unsupported/temporary" if is_temporary_support_row(sr) else "tables_unsupported"
                    extra_comments = build_support_comments(sr)
                    log.info("[FIXUP]%s 写入不支持 TABLE 脚本: %s", source_tag(ddl_source_label), filename)
                    write_fixup_file(
                        base_dir,
                        subdir,
                        filename,
                        ddl_adj,
                        header,
                        extra_comments=extra_comments
                    )
                finally:
                    table_unsup_progress()
            table_unsup_jobs.append(_job)
        run_tasks(table_unsup_jobs, "TABLE_UNSUPPORTED")

    log.info("[FIXUP] (3/9) 正在生成 TABLE ALTER 脚本...")
    for (obj_type, tgt_name, missing_cols, extra_cols, length_mismatches, type_mismatches) in tv_results.get('mismatched', []):
        if obj_type.upper() != 'TABLE' or "获取失败" in tgt_name:
            continue
        src_name = table_map.get(tgt_name)
        if not src_name:
            continue
        src_schema, src_table = src_name.split('.')
        tgt_schema, tgt_table = tgt_name.split('.')
        if (src_schema.upper(), src_table.upper()) in unsupported_table_keys:
            continue
        alter_sql = generate_alter_for_table_columns(
            oracle_meta,
            src_schema,
            src_table,
            tgt_schema,
            tgt_table,
            missing_cols,
            extra_cols,
            length_mismatches,
            type_mismatches,
            drop_sys_c_columns=bool(settings.get("fixup_drop_sys_c_columns", False)),
        )
        if alter_sql:
            alter_sql = prepend_set_schema(alter_sql, tgt_schema)
            filename = f"{tgt_schema}.{tgt_table}.alter_columns.sql"
            header = f"基于列差异的 ALTER TABLE 订正 SQL: {tgt_schema}.{tgt_table} (源: {src_schema}.{src_table})"
            write_fixup_file(base_dir, 'table_alter', filename, alter_sql, header)

    if settings.get("generate_interval_partition_fixup"):
        log.info("[FIXUP] (3b/9) 正在生成 interval 分区补齐脚本...")
        generate_interval_partition_fixup_scripts(
            oracle_meta,
            table_target_map,
            settings,
            base_dir,
            allow_fixup,
            unsupported_table_keys
        )

    # 获取OceanBase版本
    ob_version = get_oceanbase_version(ob_cfg)
    if ob_version:
        log.info("[VIEW] 检测到OceanBase版本: %s", ob_version)
    else:
        log.warning("[VIEW] 无法获取OceanBase版本，将使用保守的DDL清理策略")

    log.info("[FIXUP] (4/9) 正在生成 VIEW / MATERIALIZED VIEW / 其他对象脚本...")

    def get_view_cached_ddl(
        schema: str,
        view_name: str
    ) -> Tuple[Optional[str], Optional[ViewCompatResult]]:
        compat = view_compat_map.get((schema.upper(), view_name.upper()))
        if compat and compat.cleaned_ddl:
            return compat.cleaned_ddl, compat
        return None, compat

    # 处理VIEW对象 - 使用简化的拓扑排序
    if view_missing_objects:
        log.info("[FIXUP] (4a/9) 正在排序 %d 个VIEW依赖关系...", len(view_missing_objects))
        
        # Simple topological sort using already-fetched DDLs
        try:
            # Step 1: Fetch all VIEW DDLs first
            view_ddl_map = {}  # (src_schema, src_obj) -> DDL
            for src_schema, src_obj, tgt_schema, tgt_obj in view_missing_objects:
                cached_ddl, _compat = get_view_cached_ddl(src_schema, src_obj)
                if cached_ddl:
                    view_ddl_map[(src_schema, src_obj)] = cached_ddl
                    continue
                fetch_result = fetch_ddl_with_timing(src_schema, 'VIEW', src_obj)
                if len(fetch_result) == 3:
                    raw_ddl, _, _ = fetch_result
                    if raw_ddl:
                        view_ddl_map[(src_schema, src_obj)] = raw_ddl
            
            # Step 2: Build dependency graph
            view_deps = {}  # (tgt_schema, tgt_obj) -> set of (tgt_schema, tgt_obj) dependencies
            src_to_tgt = {(s_sch, s_obj): (t_sch, t_obj) 
                         for s_sch, s_obj, t_sch, t_obj in view_missing_objects}
            tgtfull_to_tuple = {f"{t_sch}.{t_obj}".upper(): (t_sch, t_obj)
                               for _, _, t_sch, t_obj in view_missing_objects}
            
            for src_schema, src_obj, tgt_schema, tgt_obj in view_missing_objects:
                tgt_key = (tgt_schema, tgt_obj)
                view_deps[tgt_key] = set()
                
                ddl = view_ddl_map.get((src_schema, src_obj), "")
                if ddl:
                    # Extract dependencies
                    try:
                        dependencies = extract_view_dependencies(ddl, src_schema)
                        if view_dependency_map:
                            dep_key = (src_schema.upper(), src_obj.upper())
                            fallback_deps = view_dependency_map.get(dep_key)
                            if fallback_deps:
                                dependencies |= {d.upper() for d in fallback_deps}
                        for dep in dependencies:
                            dep_upper = dep.upper()
                            # Check if this dependency is another VIEW in our list
                            for (s_sch_d, s_obj_d), (t_sch_d, t_obj_d) in src_to_tgt.items():
                                if f"{s_sch_d}.{s_obj_d}".upper() == dep_upper:
                                    view_deps[tgt_key].add((t_sch_d, t_obj_d))
                                    break
                    except Exception as e:
                        log.debug(f"[FIXUP] 提取 VIEW {src_schema}.{src_obj} 依赖失败: {e}")
            
            # Step 3: Topological sort using Kahn's algorithm
            from collections import deque
            in_degree = defaultdict(int)
            dep_graph = defaultdict(set)  # dependency -> [dependents]
            
            for view_key, deps in view_deps.items():
                if view_key not in in_degree:
                    in_degree[view_key] = 0
                for dep_key in deps:
                    in_degree[view_key] += 1
                    dep_graph[dep_key].add(view_key)
            
            queue = deque([v for v in view_deps.keys() if in_degree[v] == 0])
            sorted_view_tuples = []
            
            while queue:
                current = queue.popleft()
                sorted_view_tuples.append(current)
                for dependent in dep_graph.get(current, set()):
                    in_degree[dependent] -= 1
                    if in_degree[dependent] == 0:
                        queue.append(dependent)
            
            # Check for cycles
            if len(sorted_view_tuples) < len(view_deps):
                circular = [v for v, d in in_degree.items() if d > 0]
                log.warning(f"[FIXUP] 发现 {len(circular)} 个循环依赖的VIEW，将最后创建")
                sorted_view_tuples.extend(circular)
            
            # Map back to original format
            tgt_to_orig = {(t_sch, t_obj): (s_sch, s_obj, t_sch, t_obj)
                          for s_sch, s_obj, t_sch, t_obj in view_missing_objects}
            view_missing_objects = [tgt_to_orig[v] for v in sorted_view_tuples if v in tgt_to_orig]
            
            log.info("[FIXUP] VIEW拓扑排序完成：依赖对象将优先创建")
            
        except Exception as e:
            log.warning(f"[FIXUP] VIEW拓扑排序失败: {e}, 使用原始顺序")
        
        log.info("[FIXUP] 正在生成 %d 个VIEW脚本...", len(view_missing_objects))
        for src_schema, src_obj, tgt_schema, tgt_obj in view_missing_objects:
            try:
                if (src_schema.upper(), src_obj.upper()) in invalid_view_keys:
                    log.info("[FIXUP] 跳过源端 INVALID 的 VIEW %s.%s。", src_schema, src_obj)
                    mark_source('VIEW', 'invalid')
                    continue
                raw_ddl = None
                ddl_source_label = "MISSING"
                compat = view_compat_map.get((src_schema.upper(), src_obj.upper()))
                if compat and compat.cleaned_ddl:
                    raw_ddl = compat.cleaned_ddl
                    ddl_source_label = "VIEW_CACHE"
                else:
                    fetch_result = fetch_ddl_with_timing(src_schema, 'VIEW', src_obj)
                    if len(fetch_result) != 3:
                        log.error("[FIXUP] VIEW fetch_ddl_with_timing 返回了 %d 个值，期望 3 个: %s", len(fetch_result), fetch_result)
                        continue
                    raw_ddl, ddl_source_label, _elapsed = fetch_result
                if not raw_ddl:
                    log.warning("[FIXUP] 未找到 VIEW %s.%s 的 DDL。", src_schema, src_obj)
                    mark_source('VIEW', 'missing')
                    continue
                if ddl_source_label.startswith("DBCAT"):
                    mark_source('VIEW', 'dbcat')
                elif ddl_source_label in ("DBMS_METADATA", "DBA_VIEWS"):
                    mark_source('VIEW', 'fallback')
                elif ddl_source_label == "VIEW_CACHE":
                    mark_source('VIEW', 'fallback')
                else:
                    mark_source('VIEW', 'missing')
                
                # 清理DDL使其兼容OceanBase
                cleaned_ddl = clean_view_ddl_for_oceanbase(raw_ddl, ob_version)

                # 修复 dbcat DDL 的注释/列名异常
                col_meta = oracle_meta.table_columns.get((src_schema.upper(), src_obj.upper()), {}) or {}
                cleaned_ddl = sanitize_view_ddl(cleaned_ddl, set(col_meta.keys()))
                
                # 重写依赖对象引用
                remapped_ddl = remap_view_dependencies(
                    cleaned_ddl,
                    src_schema,
                    src_obj,
                    remap_rules,
                    full_object_mapping,
                    synonym_meta=synonym_meta_map,
                    view_dependency_map=view_dependency_map
                )
                
                # 调整schema和对象名
                final_ddl = adjust_ddl_for_object(
                    remapped_ddl,
                    src_schema,
                    src_obj,
                    tgt_schema,
                    tgt_obj,
                    extra_identifiers=get_relevant_replacements(src_schema),
                    obj_type='VIEW'
                )
                
                # 最终清理
                final_ddl = cleanup_dbcat_wrappers(final_ddl)
                final_ddl = prepend_set_schema(final_ddl, tgt_schema)
                final_ddl = normalize_ddl_for_ob(final_ddl)
                obj_full = f"{tgt_schema}.{tgt_obj}"
                final_ddl = apply_hint_filter('VIEW', obj_full, final_ddl)
                final_ddl = apply_ddl_cleanup_rules(final_ddl, 'VIEW')
                final_ddl = strip_constraint_enable(final_ddl)
                final_ddl = enforce_schema_for_ddl(final_ddl, tgt_schema, 'VIEW')
                
                # 确保DDL以分号结尾
                if not final_ddl.rstrip().endswith(';'):
                    final_ddl = final_ddl.rstrip() + ';'
                final_ddl = apply_fixup_idempotency(
                    final_ddl,
                    'VIEW',
                    tgt_schema,
                    tgt_obj,
                    settings,
                    idempotent_stats
                )
                
                # 写入文件
                filename = f"{tgt_schema}.{tgt_obj}.sql"
                header = f"修补缺失的 VIEW {tgt_obj} (源: {src_schema}.{src_obj})"
                extra_comments = []
                if compat and compat.rewrite_notes:
                    extra_comments.append("rewrite_notes: " + "; ".join(compat.rewrite_notes))
                grants_for_view = [] if view_grants_split_enabled else collect_grants_for_object(obj_full)
                log.info("[FIXUP]%s 写入 VIEW 脚本: %s", source_tag(ddl_source_label), filename)
                write_fixup_file(
                    base_dir,
                    'view',
                    filename,
                    final_ddl,
                    header,
                    grants_to_add=grants_for_view if grants_for_view else None,
                    extra_comments=extra_comments if extra_comments else None
                )
                
            except Exception as exc:
                log.error("[FIXUP] 处理 VIEW %s.%s 时出错: %s", src_schema, src_obj, exc)

    if view_missing_unsupported:
        log.info("[FIXUP] (4a/9) 正在生成不支持 VIEW 的 DDL（仅输出，默认不执行）...")
        view_unsup_progress = build_progress_tracker(
            len(view_missing_unsupported),
            "[FIXUP] (4a/9) VIEW_UNSUPPORTED"
        )
        for src_schema, src_obj, tgt_schema, tgt_obj, support_row in view_missing_unsupported:
            try:
                if (src_schema.upper(), src_obj.upper()) in invalid_view_keys:
                    log.info("[FIXUP] 跳过源端 INVALID 的 VIEW %s.%s。", src_schema, src_obj)
                    mark_source('VIEW', 'invalid')
                    continue
                raw_ddl = None
                ddl_source_label = "MISSING"
                compat = view_compat_map.get((src_schema.upper(), src_obj.upper()))
                if compat and compat.cleaned_ddl:
                    raw_ddl = compat.cleaned_ddl
                    ddl_source_label = "VIEW_CACHE"
                else:
                    fetch_result = fetch_ddl_with_timing(src_schema, 'VIEW', src_obj)
                    if len(fetch_result) != 3:
                        log.error("[FIXUP] VIEW fetch_ddl_with_timing 返回了 %d 个值，期望 3 个: %s", len(fetch_result), fetch_result)
                        continue
                    raw_ddl, ddl_source_label, _elapsed = fetch_result
                if not raw_ddl:
                    log.warning("[FIXUP] 未找到 VIEW %s.%s 的 DDL。", src_schema, src_obj)
                    mark_source('VIEW', 'missing')
                    continue
                if ddl_source_label.startswith("DBCAT"):
                    mark_source('VIEW', 'dbcat')
                elif ddl_source_label in ("DBMS_METADATA", "DBA_VIEWS"):
                    mark_source('VIEW', 'fallback')
                elif ddl_source_label == "VIEW_CACHE":
                    mark_source('VIEW', 'fallback')
                else:
                    mark_source('VIEW', 'missing')

                cleaned_ddl = clean_view_ddl_for_oceanbase(raw_ddl, ob_version)
                col_meta = oracle_meta.table_columns.get((src_schema.upper(), src_obj.upper()), {}) or {}
                cleaned_ddl = sanitize_view_ddl(cleaned_ddl, set(col_meta.keys()))
                remapped_ddl = remap_view_dependencies(
                    cleaned_ddl,
                    src_schema,
                    src_obj,
                    remap_rules,
                    full_object_mapping,
                    synonym_meta=synonym_meta_map,
                    view_dependency_map=view_dependency_map
                )
                final_ddl = adjust_ddl_for_object(
                    remapped_ddl,
                    src_schema,
                    src_obj,
                    tgt_schema,
                    tgt_obj,
                    extra_identifiers=get_relevant_replacements(src_schema),
                    obj_type='VIEW'
                )
                final_ddl = cleanup_dbcat_wrappers(final_ddl)
                final_ddl = prepend_set_schema(final_ddl, tgt_schema)
                final_ddl = normalize_ddl_for_ob(final_ddl)
                obj_full = f"{tgt_schema}.{tgt_obj}"
                final_ddl = apply_hint_filter('VIEW', obj_full, final_ddl)
                final_ddl = apply_ddl_cleanup_rules(final_ddl, 'VIEW')
                final_ddl = strip_constraint_enable(final_ddl)
                final_ddl = enforce_schema_for_ddl(final_ddl, tgt_schema, 'VIEW')
                if not final_ddl.rstrip().endswith(';'):
                    final_ddl = final_ddl.rstrip() + ';'

                filename = f"{tgt_schema}.{tgt_obj}.sql"
                header = f"不支持 VIEW DDL {tgt_obj} (源: {src_schema}.{src_obj})"
                extra_comments = build_support_comments(support_row)
                if compat and compat.rewrite_notes:
                    extra_comments.append("rewrite_notes: " + "; ".join(compat.rewrite_notes))
                log.info("[FIXUP]%s 写入不支持 VIEW 脚本: %s", source_tag(ddl_source_label), filename)
                write_fixup_file(
                    base_dir,
                    "unsupported/view",
                    filename,
                    final_ddl,
                    header,
                    extra_comments=extra_comments
                )
            except Exception as exc:
                log.error("[FIXUP] 处理不支持 VIEW %s.%s 时出错: %s", src_schema, src_obj, exc)
            finally:
                view_unsup_progress()

    # 处理非VIEW对象
    other_progress = build_progress_tracker(len(non_view_missing_objects), "[FIXUP] (4b/9) 其他对象")
    other_jobs: List[Callable[[], None]] = []
    for (obj_type, src_schema, src_obj, tgt_schema, tgt_obj) in non_view_missing_objects:
        def _job(ot=obj_type, ss=src_schema, so=src_obj, ts=tgt_schema, to=tgt_obj):
            try:
                fetch_result = fetch_ddl_with_timing(ss, ot, so)
                if len(fetch_result) != 3:
                    log.error("[FIXUP] fetch_ddl_with_timing 返回了 %d 个值，期望 3 个: %s", len(fetch_result), fetch_result)
                    return
                ddl, ddl_source_label, _elapsed = fetch_result
                if not ddl:
                    log.warning("[FIXUP] 未找到 %s %s.%s 的 dbcat DDL。", ot, ss, so)
                    mark_source(ot, 'missing')
                    return
                if ddl_source_label.startswith("DBCAT"):
                    mark_source(ot, 'dbcat')
                elif ddl_source_label in ("DBMS_METADATA", "DBA_VIEWS"):
                    mark_source(ot, 'fallback')
                else:
                    mark_source(ot, 'missing')
                ddl_adj = adjust_ddl_for_object(
                    ddl,
                    ss,
                    so,
                    ts,
                    to,
                    extra_identifiers=get_relevant_replacements(ss),
                    obj_type=ot
                )
                if ot.upper() == 'SYNONYM':
                    ddl_adj = remap_synonym_target(
                        ddl_adj,
                        remap_rules,
                        full_object_mapping
                    )
                    if ts.upper() == 'PUBLIC':
                        ddl_adj = normalize_public_synonym_name(ddl_adj, to)
                # 重映射PL/SQL对象中的对象引用
                if ot.upper() in ['PROCEDURE', 'FUNCTION', 'PACKAGE', 'PACKAGE BODY', 'TYPE', 'TYPE BODY']:
                    ddl_adj = remap_plsql_object_references(ddl_adj, ot, full_object_mapping, source_schema=ss)
                ddl_adj = cleanup_dbcat_wrappers(ddl_adj)
                
                # PUBLIC SYNONYM 不需要 ALTER SESSION SET CURRENT_SCHEMA = PUBLIC（多余）
                # 但用户schema的SYNONYM需要ALTER SESSION来确保对象在正确的schema创建
                if not (ot.upper() == 'SYNONYM' and ts.upper() == 'PUBLIC'):
                    ddl_adj = prepend_set_schema(ddl_adj, ts)
                
                ddl_adj = normalize_ddl_for_ob(ddl_adj)
                obj_full = f"{ts}.{to}"
                ddl_adj = apply_hint_filter(ot, obj_full, ddl_adj)
                ddl_adj = apply_ddl_cleanup_rules(ddl_adj, ot)
                clean_notes = None
                if settings.get('enable_ddl_punct_sanitize', True):
                    ddl_adj, replaced, samples = sanitize_plsql_punctuation(ddl_adj, ot)
                    if replaced:
                        sample_text = ", ".join(f"{src}->{dst}" for src, dst in samples)
                        suffix = f" 示例: {sample_text}" if sample_text else ""
                        log.info(
                            "[DDL_CLEAN] %s %s 全角标点清洗 %d 处。%s",
                            ot,
                            obj_full,
                            replaced,
                            suffix
                        )
                        record_ddl_clean(ot, obj_full, replaced, samples)
                        clean_notes = [f"DDL_CLEAN: 全角标点清洗 {replaced} 处。{suffix}"]
                ddl_adj = strip_constraint_enable(ddl_adj)
                ddl_adj = enforce_schema_for_ddl(ddl_adj, ts, ot)
                ddl_adj = apply_fixup_idempotency(
                    ddl_adj,
                    ot,
                    ts,
                    to,
                    settings,
                    idempotent_stats
                )
                
                # --- Find and prepare grants for this object ---
                grants_for_this_object = collect_grants_for_object(obj_full)

                subdir = obj_type_to_dir.get(ot, ot.lower())
                filename = f"{ts}.{to}.sql"
                header = f"修补缺失的 {ot} {obj_full} (源: {ss}.{so})"
                log.info("[FIXUP]%s 写入 %s 脚本: %s", source_tag(ddl_source_label), ot, filename)
                write_fixup_file(
                    base_dir,
                    subdir,
                    filename,
                    ddl_adj,
                    header,
                    grants_to_add=grants_for_this_object,
                    extra_comments=clean_notes
                )
            finally:
                other_progress()
        other_jobs.append(_job)
    run_tasks(other_jobs, "OTHER_OBJECTS")

    if non_view_missing_unsupported:
        other_unsup_progress = build_progress_tracker(
            len(non_view_missing_unsupported),
            "[FIXUP] (4c/9) 不支持对象"
        )
        other_unsup_jobs: List[Callable[[], None]] = []
        for (obj_type, src_schema, src_obj, tgt_schema, tgt_obj, support_row) in non_view_missing_unsupported:
            def _job(ot=obj_type, ss=src_schema, so=src_obj, ts=tgt_schema, to=tgt_obj, sr=support_row):
                try:
                    fetch_result = fetch_ddl_with_timing(ss, ot, so)
                    if len(fetch_result) != 3:
                        log.error("[FIXUP] fetch_ddl_with_timing 返回了 %d 个值，期望 3 个: %s", len(fetch_result), fetch_result)
                        return
                    ddl, ddl_source_label, _elapsed = fetch_result
                    if not ddl:
                        log.warning("[FIXUP] 未找到 %s %s.%s 的 dbcat DDL。", ot, ss, so)
                        mark_source(ot, 'missing')
                        return
                    if ddl_source_label.startswith("DBCAT"):
                        mark_source(ot, 'dbcat')
                    elif ddl_source_label in ("DBMS_METADATA", "DBA_VIEWS"):
                        mark_source(ot, 'fallback')
                    else:
                        mark_source(ot, 'missing')
                    ddl_adj = adjust_ddl_for_object(
                        ddl,
                        ss,
                        so,
                        ts,
                        to,
                        extra_identifiers=get_relevant_replacements(ss),
                        obj_type=ot
                    )
                    if ot.upper() == 'SYNONYM':
                        ddl_adj = remap_synonym_target(
                            ddl_adj,
                            remap_rules,
                            full_object_mapping
                        )
                        if ts.upper() == 'PUBLIC':
                            ddl_adj = normalize_public_synonym_name(ddl_adj, to)
                    if ot.upper() in ['PROCEDURE', 'FUNCTION', 'PACKAGE', 'PACKAGE BODY', 'TYPE', 'TYPE BODY']:
                        ddl_adj = remap_plsql_object_references(ddl_adj, ot, full_object_mapping, source_schema=ss)
                    ddl_adj = cleanup_dbcat_wrappers(ddl_adj)
                    if not (ot.upper() == 'SYNONYM' and ts.upper() == 'PUBLIC'):
                        ddl_adj = prepend_set_schema(ddl_adj, ts)
                    ddl_adj = normalize_ddl_for_ob(ddl_adj)
                    obj_full = f"{ts}.{to}"
                    ddl_adj = apply_hint_filter(ot, obj_full, ddl_adj)
                    ddl_adj = apply_ddl_cleanup_rules(ddl_adj, ot)
                    clean_notes = None
                    if settings.get('enable_ddl_punct_sanitize', True):
                        ddl_adj, replaced, samples = sanitize_plsql_punctuation(ddl_adj, ot)
                        if replaced:
                            sample_text = ", ".join(f"{src}->{dst}" for src, dst in samples)
                            suffix = f" 示例: {sample_text}" if sample_text else ""
                            log.info(
                                "[DDL_CLEAN] %s %s 全角标点清洗 %d 处。%s",
                                ot,
                                obj_full,
                                replaced,
                                suffix
                            )
                            record_ddl_clean(ot, obj_full, replaced, samples)
                            clean_notes = [f"DDL_CLEAN: 全角标点清洗 {replaced} 处。{suffix}"]
                    ddl_adj = strip_constraint_enable(ddl_adj)
                    ddl_adj = enforce_schema_for_ddl(ddl_adj, ts, ot)

                    subdir = f"unsupported/{obj_type_to_dir.get(ot, ot.lower())}"
                    filename = f"{ts}.{to}.sql"
                    header = f"不支持对象 DDL {obj_full} (源: {ss}.{so})"
                    extra_comments = build_support_comments(sr)
                    if clean_notes:
                        extra_comments.extend(clean_notes)
                    log.info("[FIXUP]%s 写入不支持 %s 脚本: %s", source_tag(ddl_source_label), ot, filename)
                    write_fixup_file(
                        base_dir,
                        subdir,
                        filename,
                        ddl_adj,
                        header,
                        extra_comments=extra_comments
                    )
                finally:
                    other_unsup_progress()
            other_unsup_jobs.append(_job)
        run_tasks(other_unsup_jobs, "OTHER_UNSUPPORTED")

    index_generated = 0
    index_skip_lock = threading.Lock()
    log.info("[FIXUP] (5/9) 正在生成 INDEX 脚本...")
    index_progress = build_progress_tracker(index_total, "[FIXUP] (5/9) INDEX")
    index_jobs: List[Callable[[], None]] = []
    for item, src_schema, src_table, tgt_schema, tgt_table in index_tasks:
        def _job(it=item, ss=src_schema, st=src_table, ts=tgt_schema, tt=tgt_table):
            nonlocal index_generated
            table_ddl = table_ddl_cache.get((ss.upper(), st.upper()))
            if not table_ddl:
                log.warning("[FIXUP] 未找到 TABLE %s.%s 的 dbcat DDL，将尝试基于元数据重建索引。", ss, st)

            def index_predicate(stmt_upper: str) -> bool:
                return 'CREATE' in stmt_upper and ' INDEX ' in stmt_upper

            extracted = extract_statements_for_names(table_ddl, it.missing_indexes, index_predicate) if table_ddl else {}
            for idx_name in sorted(it.missing_indexes):
                idx_name_u = idx_name.upper()
                try:
                    statements = extracted.get(idx_name_u) or []
                    source_label = "DBCAT" if table_ddl else "META"
                    meta_entry = oracle_meta.indexes.get((ss.upper(), st.upper()), {}).get(idx_name_u)
                    if not statements:
                        fallback_stmt = build_index_from_meta(ss, st, ts, tt, idx_name_u)
                        if fallback_stmt:
                            statements = [fallback_stmt]
                            log.info("[FIXUP][META] 使用元数据重建索引 %s.%s。", ts, idx_name_u)
                            source_label = "META"
                            mark_source('INDEX', 'fallback')
                        else:
                            mark_source('INDEX', 'missing')
                            with index_skip_lock:
                                if meta_entry:
                                    index_skip_counts["meta_incomplete"] += 1
                                else:
                                    index_skip_counts["meta_missing"] += 1
                            log.warning("[FIXUP] 未在 TABLE %s.%s 的 DDL 中找到索引 %s，且无元数据可重建。", ss, st, idx_name_u)
                            continue
                    else:
                        mark_source('INDEX', 'dbcat')
                    ddl_lines: List[str] = []
                    for stmt in statements:
                        ddl_adj = adjust_ddl_for_object(
                            stmt,
                            ss,
                            st,
                            ts,
                            tt,
                            extra_identifiers=get_relevant_replacements(ss),
                            obj_type='INDEX'
                        )
                        ddl_adj = normalize_ddl_for_ob(ddl_adj)
                        ddl_adj = apply_fixup_idempotency(
                            ddl_adj,
                            'INDEX',
                            ts,
                            idx_name_u,
                            settings,
                            idempotent_stats
                        )
                        ddl_lines.append(_ensure_statement_terminated(ddl_adj))
                    content = prepend_set_schema("\n".join(ddl_lines), ts)
                    filename = f"{ts}.{idx_name_u}.sql"
                    header = f"修补缺失的 INDEX {idx_name_u} (表: {ts}.{tt})"
                    log.info("[FIXUP]%s 写入 INDEX 脚本: %s", source_tag(source_label), filename)
                    write_fixup_file(base_dir, 'index', filename, content, header)
                    with index_skip_lock:
                        index_generated += 1
                finally:
                    index_progress()
        index_jobs.append(_job)
    run_tasks(index_jobs, "INDEX")
    if fixup_skip_summary is not None:
        fixup_skip_summary["INDEX"] = {
            "missing_total": index_missing_total,
            "task_total": index_total,
            "generated": index_generated,
            "skipped": dict(index_skip_counts)
        }

    log.info("[FIXUP] (6/9) 正在生成 CONSTRAINT 脚本...")
    constraint_progress = build_progress_tracker(constraint_total, "[FIXUP] (6/9) CONSTRAINT")
    constraint_jobs: List[Callable[[], None]] = []
    for item, src_schema, src_table, tgt_schema, tgt_table in constraint_tasks:
        def _job(it=item, ss=src_schema, st=src_table, ts=tgt_schema, tt=tgt_table):
            table_ddl = table_ddl_cache.get((ss.upper(), st.upper()))
            if not table_ddl:
                log.warning("[FIXUP] 未找到 TABLE %s.%s 的 dbcat DDL，将尝试基于元数据重建约束。", ss, st)

            def constraint_predicate(stmt_upper: str) -> bool:
                return 'ALTER TABLE' in stmt_upper and 'CONSTRAINT' in stmt_upper

            extracted = extract_statements_for_names(table_ddl, it.missing_constraints, constraint_predicate) if table_ddl else {}
            for cons_name in sorted(it.missing_constraints):
                cons_name_u = cons_name.upper()
                table_full = quote_qualified_parts(ts, tt)
                grants_for_constraint: Set[str] = set()
                try:
                    statements = extracted.get(cons_name_u) or []
                    source_label = "DBCAT" if statements else "META"
                    cons_meta = oracle_meta.constraints.get((ss.upper(), st.upper()), {}).get(cons_name_u)
                    ctype = (cons_meta or {}).get("type", "").upper()
                    src_validated = (cons_meta or {}).get("validated")
                    src_status = (cons_meta or {}).get("status")
                    downgrade_pk = cons_name_u in it.downgraded_pk_constraints
                    cols = cons_meta.get("columns") if cons_meta else []
                    # FK 引用表的 remap 映射，用于替换 DDL 中的 REFERENCES 子句
                    fk_ref_replacements: List[Tuple[Tuple[str, str], Tuple[str, str]]] = []
                    # 针对外键，准备 REFERENCES 授权并收集引用表的 remap 映射
                    if cons_meta and ctype == 'R':
                        ref_owner = cons_meta.get("ref_table_owner") or cons_meta.get("r_owner")
                        ref_table = cons_meta.get("ref_table_name")
                        if ref_owner and ref_table:
                            ref_owner_u = ref_owner.upper()
                            ref_table_u = ref_table.upper()
                            ref_src_full = f"{ref_owner_u}.{ref_table_u}"
                            ref_tgt_full = get_mapped_target(full_object_mapping, ref_src_full, 'TABLE') or ref_src_full
                            # 如果引用表被 remap，添加到替换列表
                            if '.' in ref_tgt_full:
                                ref_tgt_schema, ref_tgt_table = ref_tgt_full.split('.', 1)
                                ref_tgt_schema_u = ref_tgt_schema.upper()
                                ref_tgt_table_u = ref_tgt_table.upper()
                                if (ref_owner_u, ref_table_u) != (ref_tgt_schema_u, ref_tgt_table_u):
                                    fk_ref_replacements.append(
                                        ((ref_owner_u, ref_table_u), (ref_tgt_schema_u, ref_tgt_table_u))
                                    )
                                    log.info(
                                        "[FIXUP] FK 约束 %s 引用表 %s.%s -> %s.%s",
                                        cons_name_u, ref_owner_u, ref_table_u, ref_tgt_schema_u, ref_tgt_table_u
                                    )
                            # 跨 schema 外键需要 REFERENCES 授权
                            if ref_tgt_full.upper().split('.')[0] != ts.upper():
                                add_object_grant(ts, "REFERENCES", ref_tgt_full)
                                if grant_enabled:
                                    grants_for_constraint.add(format_object_grant(
                                        ts,
                                        ObjectGrantEntry("REFERENCES", ref_tgt_full.upper(), False)
                                    ))
                    # Fallback: PK/UK 可能内联在 CREATE TABLE 中，尝试用元数据重建
                    if not statements:
                        cols_join = ", ".join(c for c in cols if c)
                        if cols_join and ctype in ('P', 'U'):
                            add_clause = "PRIMARY KEY" if ctype == 'P' and not downgrade_pk else "UNIQUE"
                            stmt = (
                                f"ALTER TABLE {table_full} "
                                f"ADD CONSTRAINT {cons_name_u} {add_clause} ({cols_join})"
                            )
                            statements = [stmt]
                            mark_source('CONSTRAINT', 'fallback')
                            source_label = "META"
                        elif cons_meta and ctype == 'R':
                            # 尝试基于元数据重建外键
                            ref_owner = cons_meta.get("ref_table_owner") or cons_meta.get("r_owner")
                            ref_table = cons_meta.get("ref_table_name")
                            ref_cons = cons_meta.get("r_constraint")
                            ref_cols: List[str] = []
                            if ref_owner and ref_table:
                                ref_owner_u = ref_owner.upper()
                                ref_table_u = ref_table.upper()
                                ref_key = (ref_owner_u, ref_table_u)
                                # 优先使用引用的约束列，否则取该表的 PK/UK
                                if ref_cons:
                                    ref_cons_u = ref_cons.upper()
                                    ref_info = oracle_meta.constraints.get(ref_key, {}).get(ref_cons_u, {})
                                    ref_cols = ref_info.get("columns") or []
                                if not ref_cols:
                                    ref_tbl_cons = oracle_meta.constraints.get(ref_key, {})
                                    for cand in ref_tbl_cons.values():
                                        ctype_cand = (cand.get("type") or "").upper()
                                        if ctype_cand in ("P", "U"):
                                            ref_cols = cand.get("columns") or []
                                            if ref_cols:
                                                break
                            if cols and ref_owner and ref_table and ref_cols:
                                ref_tgt_full = get_mapped_target(
                                    full_object_mapping,
                                    f"{ref_owner}.{ref_table}",
                                    'TABLE'
                                ) or f"{ref_owner}.{ref_table}"
                                if '.' in ref_tgt_full:
                                    ref_tgt_schema, ref_tgt_table = ref_tgt_full.split('.', 1)
                                else:
                                    ref_tgt_schema, ref_tgt_table = ref_owner.upper(), ref_table.upper()
                                delete_rule = normalize_delete_rule(cons_meta.get("delete_rule"))
                                delete_clause = f" ON DELETE {delete_rule}" if delete_rule else ""
                                stmt = (
                                    f"ALTER TABLE {table_full} "
                                    f"ADD CONSTRAINT {cons_name_u} FOREIGN KEY ({', '.join(cols)}) "
                                    f"REFERENCES {quote_qualified_parts(ref_tgt_schema, ref_tgt_table)} "
                                    f"({', '.join(ref_cols)})"
                                    f"{delete_clause}"
                                )
                                statements = [stmt]
                                mark_source('CONSTRAINT', 'fallback')
                                source_label = "META"
                                # 跨 schema FK 需要 grant
                                if ref_tgt_schema != ts.upper():
                                    ref_full = f"{ref_tgt_schema}.{ref_tgt_table}"
                                    add_object_grant(ts, "REFERENCES", ref_full)
                                    if grant_enabled:
                                        grants_for_constraint.add(format_object_grant(
                                            ts,
                                            ObjectGrantEntry("REFERENCES", ref_full.upper(), False)
                                        ))
                        elif cons_meta and ctype == 'C':
                            search_condition = cons_meta.get("search_condition")
                            if search_condition and not is_system_notnull_check(cons_name_u, search_condition):
                                stmt = (
                                    f"ALTER TABLE {table_full} "
                                    f"ADD CONSTRAINT {cons_name_u} CHECK ({search_condition})"
                                )
                                statements = [stmt]
                                mark_source('CONSTRAINT', 'fallback')
                                source_label = "META"
                            elif not search_condition:
                                ddl_fallback, ddl_source = get_fallback_ddl(
                                    ss,
                                    "CONSTRAINT",
                                    cons_name_u
                                )
                                if ddl_fallback:
                                    statements = [ddl_fallback]
                                    mark_source('CONSTRAINT', 'fallback')
                                    source_label = ddl_source
                        elif cons_meta:
                            log.warning(
                                "[FIXUP] 约束 %s 类型为 %s，无内联 DDL 可用，无法自动重建。",
                                cons_name_u, ctype or "UNKNOWN"
                            )
                    else:
                        mark_source('CONSTRAINT', 'dbcat')
                    if not statements:
                        mark_source('CONSTRAINT', 'missing')
                        log.warning("[FIXUP] 未在 TABLE %s.%s 的 DDL 中找到约束 %s。", ss, st, cons_name_u)
                        continue
                    ddl_lines: List[str] = []
                    deferred_recorded = False
                    # 合并相关的 replacements 和 FK 引用表的映射
                    constraint_replacements = get_relevant_replacements(ss) + fk_ref_replacements
                    for stmt in statements:
                        ddl_adj = adjust_ddl_for_object(
                            stmt,
                            ss,
                            st,
                            ts,
                            tt,
                            extra_identifiers=constraint_replacements
                        )
                        if downgrade_pk and ctype == 'P':
                            ddl_adj = re.sub(r'\bPRIMARY\s+KEY\b', 'UNIQUE', ddl_adj, flags=re.IGNORECASE)
                        ddl_adj = normalize_ddl_for_ob(ddl_adj)
                        ddl_adj = strip_constraint_enable(ddl_adj)
                        ddl_adj = strip_enable_novalidate(ddl_adj)
                        ddl_adj, applied_keyword, applied_reason = apply_constraint_missing_validate_mode_to_ddl(
                            ddl_adj,
                            constraint_missing_validate_mode,
                            src_validated,
                            src_status
                        )
                        if (
                            applied_keyword == "NOVALIDATE"
                            and not deferred_recorded
                        ):
                            validate_sql = (
                                f"ALTER TABLE {table_full} ENABLE VALIDATE CONSTRAINT "
                                f"{quote_identifier(cons_name_u)};"
                            )
                            with deferred_validation_lock:
                                deferred_validation_rows.append(
                                    ConstraintValidateDeferredRow(
                                        schema_name=ts.upper(),
                                        table_name=tt.upper(),
                                        constraint_name=cons_name_u,
                                        constraint_type=ctype or "UNKNOWN",
                                        src_validated=normalize_constraint_validated_status(src_validated),
                                        applied_mode=constraint_missing_validate_mode,
                                        reason=applied_reason or "safe_novalidate",
                                        validate_sql=validate_sql,
                                    )
                                )
                            deferred_recorded = True
                        ddl_adj = apply_fixup_idempotency(
                            ddl_adj,
                            'CONSTRAINT',
                            ts,
                            cons_name_u,
                            settings,
                            idempotent_stats,
                            parent_table=tt
                        )
                        ddl_lines.append(_ensure_statement_terminated(ddl_adj))
                    content = prepend_set_schema("\n".join(ddl_lines), ts)
                    filename = f"{ts}.{cons_name_u}.sql"
                    header = f"修补缺失的约束 {cons_name_u} (表: {ts}.{tt})"
                    extra_comments = None
                    if downgrade_pk and ctype == 'P':
                        extra_comments = ["NOTE: 源端为分区表且主键未包含分区键，已降级为 UNIQUE。"]
                    log.info("[FIXUP]%s 写入 CONSTRAINT 脚本: %s", source_tag(source_label), filename)
                    write_fixup_file(
                        base_dir,
                        'constraint',
                        filename,
                        content,
                        header,
                        grants_to_add=sorted(grants_for_constraint) if grants_for_constraint else None,
                        extra_comments=extra_comments
                    )
                finally:
                    constraint_progress()
        constraint_jobs.append(_job)
    run_tasks(constraint_jobs, "CONSTRAINT")

    deferred_validate_report_path: Optional[Path] = None
    if deferred_validation_rows:
        validate_dir = "constraint_validate_later"
        grouped_validate_sql: Dict[str, Set[str]] = defaultdict(set)
        for row in deferred_validation_rows:
            grouped_validate_sql[row.schema_name].add(row.validate_sql)
        for schema_u, sql_set in sorted(grouped_validate_sql.items()):
            stmts = sorted(sql_set)
            content = prepend_set_schema("\n".join(stmts), schema_u)
            write_fixup_file(
                base_dir,
                validate_dir,
                f"{schema_u}.constraint_validate.sql",
                content,
                f"缺失约束后置校验（清理脏数据后执行） {schema_u}",
                extra_comments=[
                    "NOTE: 本目录脚本用于数据清理后的二次收敛，默认不建议在首次 fixup 执行。"
                ]
            )
        log.info(
            "[FIXUP] 已生成后置约束 VALIDATE 脚本: %d 条, 目录=%s/%s",
            len(deferred_validation_rows),
            base_dir,
            validate_dir
        )
        if report_dir and report_timestamp:
            deferred_validate_report_path = export_constraint_validate_deferred_detail(
                deferred_validation_rows,
                report_dir,
                report_timestamp
            )
            if deferred_validate_report_path:
                log.info(
                    "[FIXUP] 后置约束 VALIDATE 明细已输出: %s",
                    deferred_validate_report_path
                )
    settings["_constraint_validate_deferred_count"] = len(deferred_validation_rows)
    if deferred_validate_report_path:
        settings["_constraint_validate_deferred_report_path"] = str(deferred_validate_report_path)
    else:
        settings.pop("_constraint_validate_deferred_report_path", None)

    log.info("[FIXUP] (7/9) 正在生成 TRIGGER 脚本...")
    trigger_progress = build_progress_tracker(len(trigger_tasks), "[FIXUP] (7/9) TRIGGER")
    trigger_jobs: List[Callable[[], None]] = []
    for src_schema, trg_name, tgt_schema, tgt_obj, src_table, tgt_table_schema, tgt_table in trigger_tasks:
        def _job(
            ss=src_schema,
            tn=trg_name,
            ts=tgt_schema,
            to=tgt_obj,
            st=src_table,
            tts=tgt_table_schema,
            tt=tgt_table
        ):
            try:
                if (ss.upper(), tn.upper()) in invalid_trigger_keys:
                    log.info("[FIXUP] 跳过源端 INVALID 的 TRIGGER %s.%s。", ss, tn)
                    mark_source('TRIGGER', 'invalid')
                    return
                fetch_result = fetch_ddl_with_timing(ss, 'TRIGGER', tn)
                if len(fetch_result) != 3:
                    log.error("[FIXUP] TRIGGER fetch_ddl_with_timing 返回了 %d 个值，期望 3 个: %s", len(fetch_result), fetch_result)
                    return
                ddl, ddl_source_label, _elapsed = fetch_result
                if not ddl:
                    log.warning("[FIXUP] 未找到 TRIGGER %s.%s 的 dbcat DDL。", ss, tn)
                    mark_source('TRIGGER', 'missing')
                    return
                if ddl_source_label.startswith("DBCAT"):
                    mark_source('TRIGGER', 'dbcat')
                elif ddl_source_label in ("DBMS_METADATA", "DBA_VIEWS"):
                    mark_source('TRIGGER', 'fallback')
                else:
                    mark_source('TRIGGER', 'missing')
                extra_ids = get_relevant_replacements(ss)
                if st and tt and tts:
                    extra_ids = extra_ids + [((ss.upper(), st.upper()), (tts.upper(), tt.upper()))]
                ddl_adj = adjust_ddl_for_object(
                    ddl,
                    ss,
                    tn,
                    ts,
                    to,
                    extra_identifiers=extra_ids,
                    obj_type='TRIGGER'
                )
                # 重映射触发器中的对象引用并补全 schema
                trigger_qualify_schema = bool(settings.get('trigger_qualify_schema', True))
                ddl_adj = remap_plsql_object_references(
                    ddl_adj,
                    'TRIGGER',
                    full_object_mapping,
                    source_schema=ss,
                    trigger_qualify_schema=trigger_qualify_schema,
                    trigger_on_target=(tts, tt) if tts and tt else None,
                    trigger_tgt_schema=ts,
                    trigger_tgt_name=to
                )
                if not trigger_qualify_schema:
                    # 保持旧逻辑：仅补全 CREATE TRIGGER 与 ON 子句
                    def _rewrite_trigger_name_and_on(text: str) -> str:
                        name_pattern = re.compile(
                            rf'(CREATE\s+(?:OR\s+REPLACE\s+)?TRIGGER\s+)"?{re.escape(ss)}"?\s*\.\s*"?{re.escape(tn)}"?',
                            re.IGNORECASE
                        )
                        text = name_pattern.sub(
                            rf'\1{quote_qualified_parts(ts, to)}',
                            text,
                            count=1
                        )
                        if st and tt and tts:
                            on_pattern = re.compile(
                                rf'(\bON\s+)("?\s*{re.escape(ss)}\s*"?\s*\.\s*)?"?{re.escape(st)}"?',
                                re.IGNORECASE
                            )
                            text = on_pattern.sub(
                                rf'\1{quote_qualified_parts(tts, tt)}',
                                text,
                                count=1
                            )
                        return text
                    ddl_adj = _rewrite_trigger_name_and_on(ddl_adj)
                ddl_adj = cleanup_dbcat_wrappers(ddl_adj)
                ddl_adj = prepend_set_schema(ddl_adj, ts)
                obj_full = f"{ts}.{to}"
                ddl_adj = apply_hint_filter('TRIGGER', obj_full, ddl_adj)
                ddl_adj = apply_ddl_cleanup_rules(ddl_adj, 'TRIGGER')
                clean_notes = None
                if settings.get('enable_ddl_punct_sanitize', True):
                    ddl_adj, replaced, samples = sanitize_plsql_punctuation(ddl_adj, 'TRIGGER')
                    if replaced:
                        sample_text = ", ".join(f"{src}->{dst}" for src, dst in samples)
                        suffix = f" 示例: {sample_text}" if sample_text else ""
                        log.info(
                            "[DDL_CLEAN] TRIGGER %s 全角标点清洗 %d 处。%s",
                            obj_full,
                            replaced,
                            suffix
                        )
                        record_ddl_clean('TRIGGER', obj_full, replaced, samples)
                        clean_notes = [f"DDL_CLEAN: 全角标点清洗 {replaced} 处。{suffix}"]
                ddl_adj = strip_constraint_enable(ddl_adj)
                ddl_adj = enforce_schema_for_ddl(ddl_adj, ts, 'TRIGGER')
                ddl_adj = apply_fixup_idempotency(
                    ddl_adj,
                    'TRIGGER',
                    ts,
                    to,
                    settings,
                    idempotent_stats
                )
                grants_for_trigger: Set[str] = set()
                if tts and tt and ts and tts.upper() != ts.upper():
                    table_full = f"{tts}.{tt}"
                    required_priv = GRANT_PRIVILEGE_BY_TYPE.get('TABLE', 'SELECT')
                    add_object_grant(ts, required_priv, table_full)
                    if grant_enabled:
                        grants_for_trigger.add(format_object_grant(
                            ts,
                            ObjectGrantEntry(required_priv.upper(), table_full.upper(), False)
                        ))
                if grant_enabled and ts and tts and tt:
                    table_full_u = f"{tts}.{tt}".upper()
                    for entry in object_grants_by_grantee.get(ts.upper(), set()):
                        if entry.object_full.upper() == table_full_u:
                            grants_for_trigger.add(format_object_grant(ts, entry))
                filename = f"{ts}.{to}.sql"
                header = f"修补缺失的触发器 {to} (源: {ss}.{tn})"
                log.info("[FIXUP]%s 写入 TRIGGER 脚本: %s", source_tag(ddl_source_label), filename)
                write_fixup_file(
                    base_dir,
                    'trigger',
                    filename,
                    ddl_adj,
                    header,
                    grants_to_add=sorted(grants_for_trigger) if grants_for_trigger else None,
                    extra_comments=clean_notes
                )
            finally:
                trigger_progress()
        trigger_jobs.append(_job)
    run_tasks(trigger_jobs, "TRIGGER")

    if (
        generate_status_fixup
        and parse_bool_flag(settings.get("generate_fixup", "true"), True)
        and status_fixup_types
    ):
        status_jobs: List[Callable[[], None]] = []
        trigger_status_jobs: List[Tuple[TriggerStatusReportRow, List[str]]] = []
        constraint_status_jobs: List[ConstraintStatusDriftRow] = []
        if "TRIGGER" in status_fixup_types and "TRIGGER" in check_status_drift_types:
            for row in trigger_status_rows:
                sqls = build_trigger_status_fixup_sqls(row, trigger_validity_sync_mode)
                if sqls:
                    trigger_status_jobs.append((row, sqls))
        if "CONSTRAINT" in status_fixup_types and "CONSTRAINT" in check_status_drift_types:
            for row in constraint_status_rows:
                action_sql = (row.action_sql or "").strip()
                if action_sql and action_sql != "-":
                    constraint_status_jobs.append(row)

        total_status_jobs = len(trigger_status_jobs) + len(constraint_status_jobs)
        log.info("[FIXUP] (7.5/9) 正在生成状态修复脚本...")
        status_progress = build_progress_tracker(total_status_jobs, "[FIXUP] (7.5/9) STATUS")

        for row, sqls in trigger_status_jobs:
            def _job(_row=row, _sqls=sqls):
                try:
                    parsed = parse_full_object_name(_row.trigger_full or "")
                    if not parsed:
                        return
                    schema_u, trigger_u = parsed
                    if not allow_fixup("TRIGGER", schema_u, schema_u):
                        return
                    content = prepend_set_schema("\n".join(_sqls), schema_u)
                    filename = f"{schema_u}.{trigger_u}.status.sql"
                    header = f"状态修复 TRIGGER {schema_u}.{trigger_u}"
                    extra_comments = [
                        f"src_enabled={_row.src_enabled}",
                        f"tgt_enabled={_row.tgt_enabled}",
                        f"src_valid={_row.src_valid}",
                        f"tgt_valid={_row.tgt_valid}",
                        f"detail={_row.detail or '-'}",
                    ]
                    write_fixup_file(
                        base_dir,
                        "status/trigger",
                        filename,
                        content,
                        header,
                        extra_comments=extra_comments
                    )
                finally:
                    status_progress()
            status_jobs.append(_job)

        for row in constraint_status_jobs:
            def _job(_row=row):
                try:
                    table_parsed = parse_full_object_name(_row.table_full or "")
                    if not table_parsed:
                        return
                    schema_u, table_u = table_parsed
                    if not allow_fixup("CONSTRAINT", schema_u, schema_u):
                        return
                    content = prepend_set_schema(_row.action_sql, schema_u)
                    filename = f"{schema_u}.{_row.tgt_constraint}.status.sql"
                    header = f"状态修复 CONSTRAINT {_row.tgt_constraint} (表: {schema_u}.{table_u})"
                    extra_comments = [
                        f"src_constraint={_row.src_constraint}",
                        f"tgt_constraint={_row.tgt_constraint}",
                        f"constraint_type={_row.constraint_type}",
                        f"src_status={_row.src_status}",
                        f"tgt_status={_row.tgt_status}",
                        f"src_validated={_row.src_validated}",
                        f"tgt_validated={_row.tgt_validated}",
                        f"detail={_row.detail or '-'}",
                    ]
                    write_fixup_file(
                        base_dir,
                        "status/constraint",
                        filename,
                        content,
                        header,
                        extra_comments=extra_comments
                    )
                finally:
                    status_progress()
            status_jobs.append(_job)

        run_tasks(status_jobs, "STATUS_FIXUP")
    elif generate_status_fixup:
        log.info("[FIXUP] (7.5/9) 状态修复脚本已开启，但当前无可生成的状态差异。")

    dep_report = dependency_report or {}
    compile_tasks: Dict[Tuple[str, str, str], Set[str]] = defaultdict(set)

    def _ob_object_exists(full_name: str, obj_type: str) -> bool:
        if ob_meta is None:
            return True
        return full_name.upper() in ob_meta.objects_by_type.get(obj_type.upper(), set())

    def _compile_statements(obj_type: str, obj_name: str) -> List[str]:
        obj_type_u = obj_type.upper()
        obj_name_u = obj_name.upper()
        if obj_type_u in ("VIEW", "MATERIALIZED VIEW"):
            return []
        if obj_type_u in ("FUNCTION", "PROCEDURE"):
            return [f"ALTER {obj_type_u} {obj_name_u} COMPILE;"]
        if obj_type_u in ("PACKAGE", "PACKAGE BODY"):
            return [
                f"ALTER PACKAGE {obj_name_u} COMPILE;",
                f"ALTER PACKAGE {obj_name_u} COMPILE BODY;"
            ]
        if obj_type_u == "TRIGGER":
            return [f"ALTER TRIGGER {obj_name_u} COMPILE;"]
        if obj_type_u == "TYPE":
            return [f"ALTER TYPE {obj_name_u} COMPILE;"]
        if obj_type_u == "TYPE BODY":
            return [f"ALTER TYPE {obj_name_u} COMPILE BODY;"]
        return []

    log.info("[FIXUP] (8/9) 正在生成依赖重编译脚本...")
    for issue in dep_report.get("missing", []):
        dep_name = (issue.dependent or "").upper()
        dep_type = (issue.dependent_type or "").upper()
        if not dep_name or not dep_type:
            continue
        if not _ob_object_exists(dep_name, dep_type):
            continue
        parts = dep_name.split('.', 1)
        if len(parts) != 2:
            continue
        schema_u, obj_u = parts[0], parts[1]
        if not allow_fixup(dep_type, schema_u):
            continue
        stmts = _compile_statements(dep_type, obj_u)
        if not stmts:
            continue
        compile_tasks[(schema_u, obj_u, dep_type)].update(stmts)

    if compile_tasks:
        compile_order = build_compile_order(compile_tasks, expected_dependency_pairs)
        for (schema_u, obj_u, dep_type) in compile_order:
            stmts = compile_tasks.get((schema_u, obj_u, dep_type)) or set()
            if not stmts:
                continue
            content = "\n".join(sorted(stmts))
            content = prepend_set_schema(content, schema_u)
            filename = f"{schema_u}.{obj_u}.compile.sql"
            header = f"依赖重编译 {dep_type} {schema_u}.{obj_u}"
            write_fixup_file(base_dir, 'compile', filename, content, header)
    else:
        log.info("[FIXUP] (8/9) 无需生成依赖重编译脚本。")

    if grant_enabled:
        grant_dir_all = 'grants_all'
        grant_dir_miss = 'grants_miss'
        grant_dir_view_prereq = 'view_prereq_grants'
        grant_dir_view_post = 'view_post_grants'

        if grant_plan.role_ddls:
            role_content = "\n".join(grant_plan.role_ddls).strip()
            if role_content:
                log.info("[FIXUP] (9/9) 正在生成角色 DDL 脚本...")
                write_fixup_file(
                    base_dir,
                    grant_dir_all,
                    "roles.sql",
                    role_content,
                    "角色 DDL (来自 Oracle 授权引用)"
                )
                write_fixup_file(
                    base_dir,
                    grant_dir_miss,
                    "roles.sql",
                    role_content,
                    "角色 DDL (来自 Oracle 授权引用)"
                )

        if view_prereq_grants_by_grantee:
            _raw_pre, _merged_pre, _obj_lookup_pre, prereq_by_owner = (
                build_object_grant_statements_for(view_prereq_grants_by_grantee)
            )
            for owner, stmts in sorted(prereq_by_owner.items()):
                if not stmts:
                    continue
                content = "\n".join(sorted(stmts))
                header = f"{owner} VIEW 前置授权 (依赖对象)"
                write_fixup_file(
                    base_dir,
                    grant_dir_view_prereq,
                    f"{owner}.grants.sql",
                    content,
                    header
                )

        if view_post_grants_by_grantee:
            _raw_post, _merged_post, _obj_lookup_post, post_by_owner = (
                build_object_grant_statements_for(view_post_grants_by_grantee)
            )
            for owner, stmts in sorted(post_by_owner.items()):
                if not stmts:
                    continue
                content = "\n".join(sorted(stmts))
                header = f"{owner} VIEW 创建后授权"
                write_fixup_file(
                    base_dir,
                    grant_dir_view_post,
                    f"{owner}.grants.sql",
                    content,
                    header
                )

        if object_grants_by_grantee or sys_privs_by_grantee or role_privs_by_grantee:
            log.info("[FIXUP] (9/9) 正在生成授权脚本...")
            # --- grants_all ---
            if grants_by_owner:
                for owner, stmts in sorted(grants_by_owner.items()):
                    if not stmts:
                        continue
                    content = "\n".join(sorted(stmts))
                    header = f"{owner} 对象权限授权"
                    write_fixup_file(
                        base_dir,
                        grant_dir_all,
                        f"{owner}.grants.sql",
                        content,
                        header
                    )

            privs_by_grantee: Dict[str, Set[str]] = defaultdict(set)
            for grantee, entries in sys_privs_by_grantee.items():
                for entry in entries:
                    privs_by_grantee[grantee.upper()].add(format_sys_grant(grantee, entry))
            for grantee, entries in role_privs_by_grantee.items():
                for entry in entries:
                    privs_by_grantee[grantee.upper()].add(format_role_grant(grantee, entry))

            for grantee, stmts in sorted(privs_by_grantee.items()):
                if not stmts:
                    continue
                content = "\n".join(sorted(stmts))
                header = f"{grantee} 系统/角色权限授权"
                write_fixup_file(
                    base_dir,
                    grant_dir_all,
                    f"{grantee}.privs.sql",
                    content,
                    header
                )

            # --- grants_miss ---
            miss_grants_by_owner: Dict[str, Set[str]] = {}
            if object_grants_missing_by_grantee:
                _raw_miss, _merged_miss, _obj_lookup_miss, miss_grants_by_owner = (
                    build_object_grant_statements_for(object_grants_missing_by_grantee)
                )

            if miss_grants_by_owner:
                for owner, stmts in sorted(miss_grants_by_owner.items()):
                    if not stmts:
                        continue
                    content = "\n".join(sorted(stmts))
                    header = f"{owner} 对象权限授权"
                    write_fixup_file(
                        base_dir,
                        grant_dir_miss,
                        f"{owner}.grants.sql",
                        content,
                        header
                    )

            miss_privs_by_grantee: Dict[str, Set[str]] = defaultdict(set)
            for grantee, entries in sys_privs_missing_by_grantee.items():
                for entry in entries:
                    miss_privs_by_grantee[grantee.upper()].add(format_sys_grant(grantee, entry))
            for grantee, entries in role_privs_missing_by_grantee.items():
                for entry in entries:
                    miss_privs_by_grantee[grantee.upper()].add(format_role_grant(grantee, entry))

            for grantee, stmts in sorted(miss_privs_by_grantee.items()):
                if not stmts:
                    continue
                content = "\n".join(sorted(stmts))
                header = f"{grantee} 系统/角色权限授权"
                write_fixup_file(
                    base_dir,
                    grant_dir_miss,
                    f"{grantee}.privs.sql",
                    content,
                    header
                )

            if (
                not grants_by_owner
                and not privs_by_grantee
                and not grant_plan.role_ddls
                and not miss_grants_by_owner
                and not miss_privs_by_grantee
                and not view_prereq_grants_by_grantee
                and not view_post_grants_by_grantee
            ):
                log.info("[FIXUP] (9/9) 无需生成授权脚本。")
        else:
            if not grant_plan.role_ddls:
                log.info("[FIXUP] (9/9) 无需生成授权脚本。")
    else:
        log.info("[FIXUP] (9/9) 授权脚本生成已关闭。")

    if generate_extra_cleanup:
        cleanup_candidates = collect_extra_cleanup_candidates(tv_results, extra_results)
        cleanup_path = export_extra_cleanup_candidates(base_dir, cleanup_candidates)
        if cleanup_path:
            log.info(
                "[FIXUP] 额外对象清理候选已输出: %s (count=%d, 注释候选不自动执行)",
                cleanup_path,
                len(cleanup_candidates)
            )
        else:
            log.info("[FIXUP] 未发现可输出的额外对象清理候选。")

    if oracle_conn:
        try:
            oracle_conn.close()
        except Exception:
            pass

    if ddl_source_stats:
        summary_lines: List[str] = []
        for obj_type, src_map in sorted(ddl_source_stats.items()):
            parts = []
            for label in ("dbcat", "fallback", "missing"):
                val = src_map.get(label, 0)
                if val:
                    parts.append(f"{label}={val}")
            if parts:
                summary_lines.append(f"{obj_type}: " + ", ".join(parts))
        if summary_lines:
            log.info("[FIXUP] DDL 来源统计: %s", " | ".join(summary_lines))

    if idempotent_stats and normalize_fixup_idempotent_mode(idempotent_mode) != "off":
        log.info(
            "[FIXUP] 幂等处理统计: replace=%d guard=%d drop_create=%d",
            idempotent_stats.get("replaced", 0),
            idempotent_stats.get("guarded", 0),
            idempotent_stats.get("drop_create", 0)
        )

    if ddl_clean_records and report_dir and report_timestamp:
        clean_report_path = export_ddl_clean_report(ddl_clean_records, report_dir, report_timestamp)
        if clean_report_path:
            log.info("[DDL_CLEAN] 全角标点清洗报告已输出: %s", clean_report_path)

    if hint_clean_records:
        total_removed = sum(row.removed for row in hint_clean_records)
        total_unknown = sum(row.unknown for row in hint_clean_records)
        log.info(
            "[DDL_HINT] hint 清洗汇总: objects=%d removed=%d unknown=%d policy=%s",
            len(hint_clean_records),
            total_removed,
            total_unknown,
            hint_policy
        )
    if hint_clean_records and report_dir and report_timestamp:
        hint_report_path = export_ddl_hint_clean_report(hint_clean_records, report_dir, report_timestamp)
        if hint_report_path:
            log.info("[DDL_HINT] hint 清洗报告已输出: %s", hint_report_path)

    format_report_path = format_fixup_outputs(settings, base_dir, report_dir, report_timestamp)
    if format_report_path:
        log.info("[DDL_FORMAT] DDL 格式化报告已输出: %s", format_report_path)

    if unsupported_types:
        log.warning(
            "[dbcat] 以下对象类型当前未集成自动导出，需人工处理: %s",
            ", ".join(sorted(unsupported_types))
        )
    return view_chain_file


# ====================== 报告输出 (Rich) ======================
try:
    from rich.console import Console
    from rich.table import Table
    from rich.panel import Panel
    from rich.text import Text
    from rich.theme import Theme
except ImportError:
    print("错误: 未找到 'rich' 库。", file=sys.stderr)
    print("请先安装: pip install rich", file=sys.stderr)
    sys.exit(1)


def format_missing_mapping(src_name: str, tgt_name: str) -> str:
    """
    返回缺失对象的展示字符串：如需 remap 则展示 src=tgt，否则仅展示源名。
    """
    src_clean = (src_name or "").strip()
    tgt_clean = (tgt_name or "").strip()
    if not src_clean and not tgt_clean:
        return ""
    if not tgt_clean or src_clean.upper() == tgt_clean.upper():
        return src_clean or tgt_clean
    return f"{src_clean}={tgt_clean}"


def sanitize_pipe_field(value: Optional[str]) -> str:
    if value is None:
        return "-"
    cleaned = normalize_error_text(str(value))
    cleaned = cleaned.replace("|", "/")
    return cleaned if cleaned else "-"


def parse_full_object_name(name: str) -> Optional[Tuple[str, str]]:
    if not name or '.' not in name:
        return None
    parts = name.split('.', 1)
    schema = parts[0].strip().strip('"').upper()
    obj = parts[1].strip().strip('"').upper()
    if not schema or not obj:
        return None
    return schema, obj


def export_full_object_mapping(
    full_object_mapping: FullObjectMapping,
    output_path: Path
) -> Optional[Path]:
    """
    将最终推导的全量对象映射输出为纯文本，便于人工审计。
    每行格式：SRC_FULL<TAB>OBJECT_TYPE<TAB>TGT_FULL
    """
    if not full_object_mapping:
        return None
    try:
        output_path.parent.mkdir(parents=True, exist_ok=True)
        lines: List[str] = []
        for src_full in sorted(full_object_mapping.keys()):
            type_map = full_object_mapping.get(src_full, {})
            for obj_type in sorted(type_map.keys()):
                tgt_full = type_map[obj_type]
                lines.append(f"{src_full}\t{obj_type}\t{tgt_full}")
        output_path.write_text("\n".join(lines) + "\n", encoding="utf-8")
        return output_path
    except OSError as exc:
        log.warning("写入全量对象映射文件失败 %s: %s", output_path, exc)
        return None


def export_remap_conflicts(
    remap_conflicts: RemapConflictMap,
    output_path: Path
) -> Optional[Path]:
    """
    输出无法自动推导的对象列表，便于提醒显式 remap。
    每行格式：SRC_FULL<TAB>OBJECT_TYPE<TAB>REASON
    """
    if not remap_conflicts:
        return None
    try:
        output_path.parent.mkdir(parents=True, exist_ok=True)
        lines: List[str] = ["# 无法自动推导的对象，请在 remap_rules.txt 中显式配置"]
        for (src_full, obj_type), reason in sorted(remap_conflicts.items()):
            lines.append(f"{src_full}\t{obj_type}\t{reason}")
        output_path.write_text("\n".join(lines) + "\n", encoding="utf-8")
        return output_path
    except OSError as exc:
        log.warning("写入 remap_conflicts 文件失败 %s: %s", output_path, exc)
        return None


def export_missing_table_view_mappings(
    tv_results: ReportResults,
    report_dir: Path,
    blacklisted_tables: Optional[Set[Tuple[str, str]]] = None,
    support_state_map: Optional[Dict[Tuple[str, str], ObjectSupportReportRow]] = None
) -> Optional[Path]:
    """
    将缺失的 TABLE/VIEW 映射按目标 schema 输出为文本，便于迁移工具直接消费。
    输出文件示例：SCHEMA_T.txt / SCHEMA_V.txt（目录: missed_tables_views_for_OMS）
    若提供 blacklisted_tables，则跳过黑名单 TABLE。
    若提供 support_state_map，则跳过不支持/阻断的 TABLE/VIEW。
    """
    if not report_dir:
        return None

    output_dir = Path(report_dir) / "missed_tables_views_for_OMS"
    if output_dir.exists():
        shutil.rmtree(output_dir, ignore_errors=True)

    grouped: Dict[str, Dict[str, List[str]]] = defaultdict(lambda: {"TABLE": [], "VIEW": []})
    for obj_type, tgt_name, src_name in tv_results.get("missing", []):
        obj_type_u = obj_type.upper()
        if obj_type_u not in ("TABLE", "VIEW"):
            continue
        if support_state_map and "." in src_name:
            src_full = src_name.upper()
            support_row = support_state_map.get((obj_type_u, src_full))
            if support_row and support_row.support_state != SUPPORT_STATE_SUPPORTED:
                continue
        if obj_type_u == "TABLE" and blacklisted_tables:
            src_key = parse_full_object_name(src_name)
            if src_key and src_key in blacklisted_tables:
                continue
        if "." not in tgt_name or "." not in src_name:
            continue
        tgt_schema = tgt_name.split(".")[0].upper()
        formatted = format_missing_mapping(src_name, tgt_name)
        if formatted:
            grouped[tgt_schema][obj_type_u].append(formatted)

    if not grouped:
        return None

    try:
        output_dir.mkdir(parents=True, exist_ok=True)
    except OSError as exc:
        log.warning("无法创建缺失 TABLE/VIEW 映射目录 %s: %s", output_dir, exc)
        return None

    for tgt_schema, type_map in sorted(grouped.items()):
        for obj_type, mappings in sorted(type_map.items()):
            if not mappings:
                continue
            suffix = "T" if obj_type == "TABLE" else "V"
            file_path = output_dir / f"{tgt_schema}_{suffix}.txt"
            lines = sorted(set(mappings))
            try:
                file_path.write_text("\n".join(lines) + "\n", encoding="utf-8")
            except OSError as exc:
                log.warning("写入缺失映射文件失败 %s: %s", file_path, exc)
    return output_dir


def write_pipe_report(
    title: str,
    header_fields: List[str],
    rows: List[List[str]],
    output_path: Path
) -> Optional[Path]:
    """
    写入 pipe 分隔报表，便于直接导入 Excel。
    """
    if not rows or not output_path:
        return None
    delimiter = "|"
    header = delimiter.join(header_fields)
    lines: List[str] = [
        f"# {title}",
        f"# total={len(rows)}",
        f"# 分隔符: {delimiter}",
        f"# 字段说明: {header}",
        header
    ]
    for row in rows:
        lines.append(delimiter.join(sanitize_pipe_field(item) for item in row))
    try:
        output_path.parent.mkdir(parents=True, exist_ok=True)
        output_path.write_text("\n".join(lines).rstrip() + "\n", encoding="utf-8")
        return output_path
    except OSError as exc:
        log.warning("写入报告失败 %s: %s", output_path, exc)
        return None


def _report_index_relpath(report_dir: Path, target: Path) -> str:
    try:
        return str(target.relative_to(report_dir))
    except ValueError:
        return str(target)


def _infer_report_index_meta(path: Path) -> Tuple[str, str]:
    name = path.name
    if path.is_dir():
        if name == "missed_tables_views_for_OMS":
            return "DIR", "缺失 TABLE/VIEW 规则目录"
        return "DIR", "报告输出目录"
    if name.startswith("report_") and name.endswith(".txt"):
        return "REPORT", "主报告"
    if name.startswith("package_compare_"):
        return "DETAIL", "PACKAGE/PKG BODY 对比明细"
    if name.startswith("object_mapping_"):
        return "AUX", "全量对象映射"
    if name.startswith("remap_conflicts_"):
        return "AUX", "无法自动推导对象"
    if name.startswith("dependency_chains_"):
        return "AUX", "依赖链条导出"
    if name.startswith("VIEWs_chain_"):
        return "AUX", "VIEW 依赖链"
    if name == "blacklist_tables.txt":
        return "AUX", "黑名单表清单"
    if name == "trigger_status_report.txt":
        return "AUX", "触发器状态/清单报告"
    if name == "filtered_grants.txt":
        return "AUX", "过滤授权清单"
    if name.startswith("fixup_skip_summary_"):
        return "AUX", "Fixup 跳过汇总"
    if name.startswith("ddl_format_report_"):
        return "AUX", "DDL 格式化报告"
    if name.startswith("ddl_punct_clean_"):
        return "AUX", "全角标点清洗报告"
    if name.startswith("ddl_hint_clean_"):
        return "AUX", "DDL hint 清洗报告"
    if name.startswith("missing_objects_detail_"):
        return "DETAIL", "缺失对象支持性明细"
    if name.startswith("unsupported_objects_detail_"):
        return "DETAIL", "不支持/阻断对象明细"
    if name.startswith("view_constraint_cleaned_detail_"):
        return "DETAIL", "VIEW 列清单约束清洗明细"
    if name.startswith("view_constraint_uncleanable_detail_"):
        return "DETAIL", "VIEW 列清单约束无法清洗明细"
    if name.startswith("indexes_unsupported_detail_"):
        return "DETAIL", "索引语法不支持明细(仅DESC)"
    if name.startswith("indexes_blocked_detail_"):
        return "DETAIL", "索引依赖阻断明细"
    if name.startswith("constraints_unsupported_detail_"):
        return "DETAIL", "约束语法不支持明细(DEFERRABLE/自引用外键等)"
    if name.startswith("constraint_validate_deferred_detail_"):
        return "DETAIL", "缺失约束后置 VALIDATE 明细"
    if name.startswith("constraints_blocked_detail_"):
        return "DETAIL", "约束依赖阻断明细"
    if name.startswith("triggers_blocked_detail_"):
        return "DETAIL", "触发器依赖阻断明细"
    if name.startswith("migration_focus_"):
        return "DETAIL", "迁移聚焦清单"
    if name.startswith("extra_targets_detail_"):
        return "DETAIL", "目标端多余对象明细"
    if name.startswith("skipped_objects_detail_"):
        return "DETAIL", "仅打印未校验对象明细"
    if name.startswith("mismatched_tables_detail_"):
        return "DETAIL", "表列不匹配明细"
    if name.startswith("column_order_mismatch_detail_"):
        return "DETAIL", "列顺序差异明细"
    if name.startswith("comment_mismatch_detail_"):
        return "DETAIL", "注释差异明细"
    if name.startswith("extra_mismatch_detail_"):
        return "DETAIL", "扩展对象差异明细"
    if name.startswith("status_drift_detail_"):
        return "DETAIL", "触发器/约束状态漂移明细"
    if name.startswith("noise_suppressed_detail_"):
        return "DETAIL", "降噪明细"
    if name.startswith("dependency_detail_"):
        return "DETAIL", "依赖关系差异明细"
    if name.startswith("usability_check_detail_"):
        return "DETAIL", "对象可用性校验明细"
    if name.startswith("missing_") and "_detail_" in name:
        match = re.match(r"missing_(.+)_detail_", name)
        if match:
            obj_type = match.group(1).replace("_", " ").upper()
            return "DETAIL", f"缺失 {obj_type} 明细"
    if name.startswith("unsupported_") and "_detail_" in name:
        match = re.match(r"unsupported_(.+)_detail_", name)
        if match:
            obj_type = match.group(1).replace("_", " ").upper()
            return "DETAIL", f"不支持/阻断 {obj_type} 明细"
    if name.startswith("report_index_"):
        return "AUX", "报告索引"
    return "AUX", "其他报告输出"


def export_report_index(
    entries: List[ReportIndexEntry],
    report_dir: Path,
    report_timestamp: Optional[str],
    report_detail_mode: str
) -> Optional[Path]:
    if not report_dir or not report_timestamp:
        return None
    if not entries:
        entries = []
    report_detail_mode = normalize_report_detail_mode(report_detail_mode)
    rows: List[List[str]] = [
        [entry.category, entry.path, entry.rows, entry.description]
        for entry in entries
    ]
    if report_detail_mode == "summary":
        rows.append(["NOTE", "-", "-", "report_detail_mode=summary，明细文件未输出"])
    elif report_detail_mode == "full":
        rows.append(["NOTE", "-", "-", "report_detail_mode=full，明细已内嵌在主报告"])
    if not rows:
        return None
    output_path = Path(report_dir) / f"report_index_{report_timestamp}.txt"
    header_fields = ["CATEGORY", "PATH", "ROWS", "DESCRIPTION"]
    return write_pipe_report("报告索引", header_fields, rows, output_path)


def export_missing_objects_detail(
    rows: List[ObjectSupportReportRow],
    report_dir: Path,
    report_timestamp: Optional[str]
) -> Optional[Path]:
    """
    输出缺失对象支持性明细，包含支持状态与原因。
    """
    if not report_dir or not rows or not report_timestamp:
        return None
    output_path = Path(report_dir) / f"missing_objects_detail_{report_timestamp}.txt"
    rows_sorted = sorted(rows, key=lambda r: (r.obj_type, r.src_full, r.tgt_full))
    header_fields = [
        "SRC_FULL",
        "TYPE",
        "TGT_FULL",
        "STATE",
        "REASON_CODE",
        "REASON",
        "DEPENDENCY",
        "ACTION",
        "DETAIL"
    ]
    data_rows = [
        [
            row.src_full,
            row.obj_type,
            row.tgt_full,
            row.support_state,
            row.reason_code,
            row.reason,
            row.dependency,
            row.action,
            row.detail
        ]
        for row in rows_sorted
    ]
    return write_pipe_report("缺失对象支持性明细", header_fields, data_rows, output_path)


def export_unsupported_objects_detail(
    rows: List[ObjectSupportReportRow],
    report_dir: Path,
    report_timestamp: Optional[str]
) -> Optional[Path]:
    """
    输出不支持/阻断对象明细。
    """
    if not report_dir or not rows or not report_timestamp:
        return None
    output_path = Path(report_dir) / f"unsupported_objects_detail_{report_timestamp}.txt"
    rows_sorted = sorted(rows, key=lambda r: (r.obj_type, r.src_full, r.tgt_full))
    header_fields = [
        "SRC_FULL",
        "TYPE",
        "TGT_FULL",
        "STATE",
        "REASON_CODE",
        "REASON",
        "DEPENDENCY",
        "ACTION",
        "DETAIL"
    ]
    data_rows = [
        [
            row.src_full,
            row.obj_type,
            row.tgt_full,
            row.support_state,
            row.reason_code,
            row.reason,
            row.dependency,
            row.action,
            row.detail
        ]
        for row in rows_sorted
    ]
    return write_pipe_report("不支持/阻断对象明细", header_fields, data_rows, output_path)


def convert_constraint_unsupported_rows(
    rows: List[ConstraintUnsupportedDetail]
) -> List[ObjectSupportReportRow]:
    """
    将约束不支持明细转换为统一的支持性明细行，用于不支持总表输出。
    """
    if not rows:
        return []
    converted: List[ObjectSupportReportRow] = []
    for row in rows:
        table_full = row.table_full or "-"
        table_schema, _table_name = parse_full_object_name(table_full) or ("", "")
        schema_u = (table_schema or "").upper()
        cons_name = (row.constraint_name or "").upper()
        if schema_u and cons_name:
            src_full = f"{schema_u}.{cons_name}"
        else:
            src_full = cons_name or table_full
        reason_code = row.reason_code or "UNSUPPORTED"
        converted.append(ObjectSupportReportRow(
            obj_type="CONSTRAINT",
            src_full=src_full,
            tgt_full=src_full,
            support_state=SUPPORT_STATE_UNSUPPORTED,
            reason_code=reason_code,
            reason=row.reason or "不支持",
            dependency=table_full,
            action="改造/不迁移",
            detail=row.search_condition or "-",
            root_cause=f"{src_full}({reason_code})" if reason_code else "-"
        ))
    return converted


def export_view_constraint_cleaned_detail(
    rows: List[ViewConstraintReportRow],
    report_dir: Path,
    report_timestamp: Optional[str]
) -> Optional[Path]:
    if not report_dir or not rows or not report_timestamp:
        return None
    output_path = Path(report_dir) / f"view_constraint_cleaned_detail_{report_timestamp}.txt"
    rows_sorted = sorted(rows, key=lambda r: (r.view_full, r.mode))
    header_fields = ["VIEW", "MODE", "ACTION", "REASON", "CONSTRAINTS"]
    data_rows = [
        [row.view_full, row.mode, row.action, row.reason, row.constraints]
        for row in rows_sorted
    ]
    return write_pipe_report("VIEW 列清单约束清洗明细", header_fields, data_rows, output_path)


def export_view_constraint_uncleanable_detail(
    rows: List[ViewConstraintReportRow],
    report_dir: Path,
    report_timestamp: Optional[str]
) -> Optional[Path]:
    if not report_dir or not rows or not report_timestamp:
        return None
    output_path = Path(report_dir) / f"view_constraint_uncleanable_detail_{report_timestamp}.txt"
    rows_sorted = sorted(rows, key=lambda r: (r.view_full, r.mode))
    header_fields = ["VIEW", "MODE", "ACTION", "REASON", "CONSTRAINTS"]
    data_rows = [
        [row.view_full, row.mode, row.action, row.reason, row.constraints]
        for row in rows_sorted
    ]
    return write_pipe_report("VIEW 列清单约束无法清洗明细", header_fields, data_rows, output_path)


def export_missing_by_type(
    rows: List[ObjectSupportReportRow],
    report_dir: Path,
    report_timestamp: Optional[str]
) -> Dict[str, Optional[Path]]:
    """
    按对象类型输出缺失且可修补的对象明细（支持但缺失）。
    """
    if not report_dir or not report_timestamp or not rows:
        return {}
    grouped: Dict[str, List[ObjectSupportReportRow]] = defaultdict(list)
    for row in rows:
        if row.support_state != SUPPORT_STATE_SUPPORTED:
            continue
        grouped[row.obj_type.upper()].append(row)
    result: Dict[str, Optional[Path]] = {}
    header_fields = [
        "SRC_FULL",
        "TYPE",
        "TGT_FULL",
        "STATE",
        "REASON_CODE",
        "REASON",
        "DEPENDENCY",
        "ACTION",
        "DETAIL"
    ]
    for obj_type, items in sorted(grouped.items()):
        if not items:
            continue
        type_lower = obj_type.lower().replace(" ", "_")
        output_path = Path(report_dir) / f"missing_{type_lower}_detail_{report_timestamp}.txt"
        rows_sorted = sorted(items, key=lambda r: (r.src_full, r.tgt_full))
        data_rows = [
            [
                row.src_full,
                row.obj_type,
                row.tgt_full,
                row.support_state,
                row.reason_code,
                row.reason,
                row.dependency,
                row.action,
                row.detail
            ]
            for row in rows_sorted
        ]
        result[obj_type] = write_pipe_report(f"缺失 {obj_type} 明细", header_fields, data_rows, output_path)
    return result


def export_unsupported_by_type(
    rows: List[ObjectSupportReportRow],
    report_dir: Path,
    report_timestamp: Optional[str]
) -> Dict[str, Optional[Path]]:
    """
    按对象类型输出不支持/阻断对象明细（含 ROOT_CAUSE）。
    """
    if not report_dir or not report_timestamp or not rows:
        return {}
    grouped: Dict[str, List[ObjectSupportReportRow]] = defaultdict(list)
    for row in rows:
        if row.support_state not in {SUPPORT_STATE_UNSUPPORTED, SUPPORT_STATE_BLOCKED}:
            continue
        grouped[row.obj_type.upper()].append(row)
    result: Dict[str, Optional[Path]] = {}
    header_fields = [
        "SRC_FULL",
        "TYPE",
        "TGT_FULL",
        "STATE",
        "REASON_CODE",
        "REASON",
        "DEPENDENCY",
        "ACTION",
        "DETAIL",
        "ROOT_CAUSE"
    ]
    for obj_type, items in sorted(grouped.items()):
        if not items:
            continue
        type_lower = obj_type.lower().replace(" ", "_")
        output_path = Path(report_dir) / f"unsupported_{type_lower}_detail_{report_timestamp}.txt"
        rows_sorted = sorted(items, key=lambda r: (r.support_state, r.src_full, r.tgt_full))
        data_rows = [
            [
                row.src_full,
                row.obj_type,
                row.tgt_full,
                row.support_state,
                row.reason_code,
                row.reason,
                row.dependency,
                row.action,
                row.detail,
                row.root_cause or "-"
            ]
            for row in rows_sorted
        ]
        result[obj_type] = write_pipe_report(f"不支持/阻断 {obj_type} 明细", header_fields, data_rows, output_path)
    return result


def export_constraints_unsupported_detail(
    rows: List[ConstraintUnsupportedDetail],
    report_dir: Path,
    report_timestamp: Optional[str]
) -> Optional[Path]:
    """
    输出不支持的约束明细。
    """
    if not report_dir or not rows or not report_timestamp:
        return None
    output_path = Path(report_dir) / f"constraints_unsupported_detail_{report_timestamp}.txt"
    rows_sorted = sorted(rows, key=lambda r: (r.table_full, r.constraint_name))
    header_fields = [
        "TABLE",
        "CONSTRAINT_NAME",
        "SEARCH_CONDITION",
        "REASON_CODE",
        "REASON",
        "OB_ERROR_HINT"
    ]
    data_rows = [
        [
            row.table_full,
            row.constraint_name,
            row.search_condition or "-",
            row.reason_code,
            row.reason,
            row.ob_error_hint or "-"
        ]
        for row in rows_sorted
    ]
    return write_pipe_report("约束不支持明细", header_fields, data_rows, output_path)


def export_constraint_validate_deferred_detail(
    rows: List[ConstraintValidateDeferredRow],
    report_dir: Path,
    report_timestamp: Optional[str]
) -> Optional[Path]:
    """
    输出缺失约束采用 NOVALIDATE 的后置校验明细。
    """
    if not report_dir or not rows or not report_timestamp:
        return None
    output_path = Path(report_dir) / f"constraint_validate_deferred_detail_{report_timestamp}.txt"
    rows_sorted = sorted(
        rows,
        key=lambda r: (r.schema_name, r.table_name, r.constraint_name)
    )
    header_fields = [
        "SCHEMA",
        "TABLE",
        "CONSTRAINT_NAME",
        "CONSTRAINT_TYPE",
        "SRC_VALIDATED",
        "APPLIED_MODE",
        "REASON",
        "VALIDATE_SQL"
    ]
    data_rows = [
        [
            row.schema_name,
            row.table_name,
            row.constraint_name,
            row.constraint_type,
            row.src_validated,
            row.applied_mode,
            row.reason,
            row.validate_sql
        ]
        for row in rows_sorted
    ]
    return write_pipe_report("约束后置 VALIDATE 明细", header_fields, data_rows, output_path)


def export_indexes_unsupported_detail(
    rows: List[IndexUnsupportedDetail],
    report_dir: Path,
    report_timestamp: Optional[str]
) -> Optional[Path]:
    """
    输出不支持的索引明细。
    """
    if not report_dir or not rows or not report_timestamp:
        return None
    output_path = Path(report_dir) / f"indexes_unsupported_detail_{report_timestamp}.txt"
    rows_sorted = sorted(rows, key=lambda r: (r.table_full, r.index_name))
    header_fields = [
        "TABLE",
        "INDEX_NAME",
        "COLUMNS",
        "REASON_CODE",
        "REASON",
        "OB_ERROR_HINT"
    ]
    data_rows = [
        [
            row.table_full,
            row.index_name,
            row.columns or "-",
            row.reason_code,
            row.reason,
            row.ob_error_hint or "-"
        ]
        for row in rows_sorted
    ]
    return write_pipe_report("索引不支持明细", header_fields, data_rows, output_path)


def _filter_blocked_support_rows(
    rows: List[ObjectSupportReportRow],
    obj_type: str
) -> List[ObjectSupportReportRow]:
    obj_type_u = (obj_type or "").upper()
    return [
        row for row in rows
        if row.obj_type.upper() == obj_type_u
        and row.reason_code == "DEPENDENCY_UNSUPPORTED"
    ]


def _export_blocked_extra_detail(
    rows: List[ObjectSupportReportRow],
    report_dir: Path,
    report_timestamp: Optional[str],
    obj_type: str,
    file_prefix: str,
    title: str
) -> Optional[Path]:
    if not report_dir or not report_timestamp or not rows:
        return None
    filtered = _filter_blocked_support_rows(rows, obj_type)
    if not filtered:
        return None
    output_path = Path(report_dir) / f"{file_prefix}_blocked_detail_{report_timestamp}.txt"
    rows_sorted = sorted(
        filtered,
        key=lambda r: (r.dependency or "-", r.src_full or "-", r.tgt_full or "-")
    )
    header_fields = [
        "TABLE",
        "OBJECT_NAME",
        "SRC_FULL",
        "TGT_FULL",
        "REASON_CODE",
        "REASON",
        "DEPENDENCY",
        "ACTION",
        "DETAIL"
    ]
    data_rows = []
    for row in rows_sorted:
        src_full = row.src_full or "-"
        obj_name = src_full.split(".", 1)[1] if "." in src_full else src_full
        data_rows.append([
            row.dependency or "-",
            obj_name,
            src_full,
            row.tgt_full or "-",
            row.reason_code or "-",
            row.reason or "-",
            row.dependency or "-",
            row.action or "-",
            row.detail or "-"
        ])
    return write_pipe_report(title, header_fields, data_rows, output_path)


def export_indexes_blocked_detail(
    rows: List[ObjectSupportReportRow],
    report_dir: Path,
    report_timestamp: Optional[str]
) -> Optional[Path]:
    """
    输出索引依赖阻断明细（依赖不支持表）。
    """
    return _export_blocked_extra_detail(
        rows,
        report_dir,
        report_timestamp,
        "INDEX",
        "indexes",
        "索引阻断明细"
    )


def export_constraints_blocked_detail(
    rows: List[ObjectSupportReportRow],
    report_dir: Path,
    report_timestamp: Optional[str]
) -> Optional[Path]:
    """
    输出约束依赖阻断明细（依赖不支持表）。
    """
    return _export_blocked_extra_detail(
        rows,
        report_dir,
        report_timestamp,
        "CONSTRAINT",
        "constraints",
        "约束阻断明细"
    )


def export_triggers_blocked_detail(
    rows: List[ObjectSupportReportRow],
    report_dir: Path,
    report_timestamp: Optional[str]
) -> Optional[Path]:
    """
    输出触发器依赖阻断明细（依赖不支持表）。
    """
    return _export_blocked_extra_detail(
        rows,
        report_dir,
        report_timestamp,
        "TRIGGER",
        "triggers",
        "触发器阻断明细"
    )


def export_migration_focus_report(
    missing_rows: List[ObjectSupportReportRow],
    unsupported_rows: List[ObjectSupportReportRow],
    report_dir: Path,
    report_timestamp: Optional[str]
) -> Optional[Path]:
    """
    输出迁移聚焦清单：仅包含缺失但可修补的对象，以及不支持/阻断对象。
    """
    if not report_dir or not report_timestamp:
        return None
    missing_supported = [
        row for row in (missing_rows or [])
        if row.support_state == SUPPORT_STATE_SUPPORTED
    ]
    unsupported_or_blocked = [
        row for row in (unsupported_rows or [])
        if row.support_state in {SUPPORT_STATE_UNSUPPORTED, SUPPORT_STATE_BLOCKED}
    ]
    output_path = Path(report_dir) / f"migration_focus_{report_timestamp}.txt"
    lines: List[str] = [
        "# Migration focus report",
        f"# timestamp={report_timestamp}",
        f"# missing_supported={len(missing_supported)}",
        f"# unsupported_or_blocked={len(unsupported_or_blocked)}",
        "",
        "# section=MISSING_SUPPORTED",
        "# fields: SRC_FULL|TYPE|TGT_FULL|ACTION|DETAIL",
        "SRC_FULL|TYPE|TGT_FULL|ACTION|DETAIL"
    ]
    for row in missing_supported:
        lines.append("|".join([
            sanitize_pipe_field(row.src_full),
            sanitize_pipe_field(row.obj_type),
            sanitize_pipe_field(row.tgt_full),
            sanitize_pipe_field(row.action),
            sanitize_pipe_field(row.detail),
        ]))
    lines.extend([
        "",
        "# section=UNSUPPORTED_OR_BLOCKED",
        "# fields: SRC_FULL|TYPE|TGT_FULL|STATE|REASON_CODE|REASON|DEPENDENCY|ACTION|DETAIL",
        "SRC_FULL|TYPE|TGT_FULL|STATE|REASON_CODE|REASON|DEPENDENCY|ACTION|DETAIL"
    ])
    for row in unsupported_or_blocked:
        lines.append("|".join([
            sanitize_pipe_field(row.src_full),
            sanitize_pipe_field(row.obj_type),
            sanitize_pipe_field(row.tgt_full),
            sanitize_pipe_field(row.support_state),
            sanitize_pipe_field(row.reason_code),
            sanitize_pipe_field(row.reason),
            sanitize_pipe_field(row.dependency),
            sanitize_pipe_field(row.action),
            sanitize_pipe_field(row.detail),
        ]))
    try:
        output_path.parent.mkdir(parents=True, exist_ok=True)
        output_path.write_text("\n".join(lines).rstrip() + "\n", encoding="utf-8")
        return output_path
    except OSError as exc:
        log.warning("写入迁移聚焦清单失败 %s: %s", output_path, exc)
        return None


def export_extra_targets_detail(
    extra_targets: List[Tuple[str, str]],
    report_dir: Path,
    report_timestamp: Optional[str]
) -> Optional[Path]:
    if not report_dir or not extra_targets or not report_timestamp:
        return None
    output_path = Path(report_dir) / f"extra_targets_detail_{report_timestamp}.txt"
    header_fields = ["TYPE", "TARGET_FULL"]
    rows = [[obj_type, tgt_name] for obj_type, tgt_name in extra_targets]
    return write_pipe_report("目标端多余对象明细", header_fields, rows, output_path)


def export_skipped_objects_detail(
    skipped_items: List[Tuple[str, str, str, str]],
    report_dir: Path,
    report_timestamp: Optional[str]
) -> Optional[Path]:
    if not report_dir or not skipped_items or not report_timestamp:
        return None
    output_path = Path(report_dir) / f"skipped_objects_detail_{report_timestamp}.txt"
    header_fields = ["TYPE", "SRC_FULL", "TGT_FULL", "REASON"]
    rows = [
        [obj_type, src_name, tgt_name, reason or "-"]
        for obj_type, tgt_name, src_name, reason in skipped_items
    ]
    return write_pipe_report("仅打印未校验对象明细", header_fields, rows, output_path)


def export_mismatched_tables_detail(
    mismatched_items: List[Tuple[str, str, Set[str], Set[str], List[Tuple[str, int, int, int, str]], List[Tuple[str, str, str, str]]]],
    report_dir: Path,
    report_timestamp: Optional[str]
) -> Optional[Path]:
    if not report_dir or not mismatched_items or not report_timestamp:
        return None
    output_path = Path(report_dir) / f"mismatched_tables_detail_{report_timestamp}.txt"
    header_fields = ["TABLE", "MISSING_COLS", "EXTRA_COLS", "LENGTH_MISMATCHES", "TYPE_MISMATCHES"]
    rows: List[List[str]] = []
    for obj_type, tgt_name, missing, extra, length_mismatches, type_mismatches in mismatched_items:
        if (obj_type or "").upper() != "TABLE":
            continue
        if "获取失败" in tgt_name:
            rows.append([tgt_name, "SOURCE_META_FAILED", "-", "-", "-"])
            continue
        length_parts = []
        for col, src_len, tgt_len, limit_len, issue_type in length_mismatches:
            length_parts.append(f"{col}:{src_len}->{tgt_len}({issue_type}:{limit_len})")
        type_parts = []
        for col, src_type, tgt_type, expected_type, issue_type in type_mismatches:
            type_parts.append(f"{col}:{src_type}->{tgt_type}({issue_type}:{expected_type})")
        rows.append([
            tgt_name,
            ",".join(sorted(missing)) if missing else "-",
            ",".join(sorted(extra)) if extra else "-",
            ";".join(length_parts) if length_parts else "-",
            ";".join(type_parts) if type_parts else "-"
        ])
    return write_pipe_report("表列不匹配明细", header_fields, rows, output_path)


def export_column_order_mismatch_detail(
    order_items: List[ColumnOrderMismatch],
    report_dir: Path,
    report_timestamp: Optional[str]
) -> Optional[Path]:
    if not report_dir or not order_items or not report_timestamp:
        return None
    output_path = Path(report_dir) / f"column_order_mismatch_detail_{report_timestamp}.txt"
    header_fields = ["TABLE", "SRC_ORDER", "TGT_ORDER"]
    rows: List[List[str]] = []
    for item in order_items:
        src_seq = ",".join(item.src_order) if item.src_order else "-"
        tgt_seq = ",".join(item.tgt_order) if item.tgt_order else "-"
        rows.append([item.table, src_seq, tgt_seq])
    return write_pipe_report("表列顺序差异明细", header_fields, rows, output_path)


def export_comment_mismatch_detail(
    comment_items: List[CommentMismatch],
    report_dir: Path,
    report_timestamp: Optional[str]
) -> Optional[Path]:
    if not report_dir or not comment_items or not report_timestamp:
        return None
    output_path = Path(report_dir) / f"comment_mismatch_detail_{report_timestamp}.txt"
    header_fields = ["TABLE", "TABLE_COMMENT_DIFF", "MISSING_COLS", "EXTRA_COLS", "COLUMN_COMMENT_DIFFS"]
    rows: List[List[str]] = []
    for item in comment_items:
        table_comment = "-"
        if item.table_comment:
            table_comment = f"{item.table_comment[0]} -> {item.table_comment[1]}"
        col_comment_parts = []
        for col, src_cmt, tgt_cmt in item.column_comment_diffs:
            col_comment_parts.append(f"{col}:{src_cmt}->{tgt_cmt}")
        rows.append([
            item.table,
            table_comment,
            ",".join(sorted(item.missing_columns)) if item.missing_columns else "-",
            ",".join(sorted(item.extra_columns)) if item.extra_columns else "-",
            ";".join(col_comment_parts) if col_comment_parts else "-"
        ])
    return write_pipe_report("注释差异明细", header_fields, rows, output_path)


def export_usability_check_detail(
    summary: UsabilitySummary,
    report_dir: Path,
    report_timestamp: Optional[str]
) -> Optional[Path]:
    if not summary or not report_dir or not report_timestamp:
        return None
    output_path = Path(report_dir) / f"usability_check_detail_{report_timestamp}.txt"
    header_fields = [
        "SCHEMA",
        "OBJECT_NAME",
        "OBJECT_TYPE",
        "SRC_EXISTS",
        "SRC_USABLE",
        "TGT_EXISTS",
        "TGT_USABLE",
        "STATUS",
        "SRC_ERROR",
        "TGT_ERROR",
        "ROOT_CAUSE",
        "RECOMMENDATION",
        "SRC_TIME_MS",
        "TGT_TIME_MS"
    ]
    rows: List[List[str]] = []
    for item in summary.results:
        rows.append([
            item.schema,
            item.object_name,
            item.object_type,
            "YES" if item.src_exists else "NO",
            "-" if item.src_usable is None else ("YES" if item.src_usable else "NO"),
            "YES" if item.tgt_exists else "NO",
            "-" if item.tgt_usable is None else ("YES" if item.tgt_usable else "NO"),
            item.status,
            item.src_error or "-",
            item.tgt_error or "-",
            item.root_cause or "-",
            item.recommendation or "-",
            str(item.src_time_ms),
            str(item.tgt_time_ms)
        ])
    return write_pipe_report("对象可用性校验明细", header_fields, rows, output_path)


def export_extra_mismatch_detail(
    extra_results: ExtraCheckResults,
    report_dir: Path,
    report_timestamp: Optional[str]
) -> Optional[Path]:
    if not report_dir or not extra_results or not report_timestamp:
        return None
    output_path = Path(report_dir) / f"extra_mismatch_detail_{report_timestamp}.txt"
    header_fields = ["TYPE", "OBJECT", "MISSING", "EXTRA", "DETAIL"]
    rows: List[List[str]] = []
    def _csv(values) -> str:
        if not values:
            return "-"
        names = [extract_name_value(v) for v in values if extract_name_value(v)]
        if not names:
            return "-"
        return ",".join(sorted(names))
    for item in extra_results.get("index_mismatched", []):
        rows.append([
            "INDEX",
            item.table,
            _csv(item.missing_indexes),
            _csv(item.extra_indexes),
            ";".join(item.detail_mismatch) if item.detail_mismatch else "-"
        ])
    for item in extra_results.get("constraint_mismatched", []):
        rows.append([
            "CONSTRAINT",
            item.table,
            _csv(item.missing_constraints),
            _csv(item.extra_constraints),
            ";".join(item.detail_mismatch) if item.detail_mismatch else "-"
        ])
    for item in extra_results.get("sequence_mismatched", []):
        missing = _csv(item.missing_sequences)
        extra = _csv(item.extra_sequences)
        detail_parts: List[str] = []
        if item.note:
            detail_parts.append(item.note)
        if item.detail_mismatch:
            detail_parts.extend(item.detail_mismatch)
        note = ";".join(detail_parts) if detail_parts else "-"
        rows.append([
            "SEQUENCE",
            f"{item.src_schema}->{item.tgt_schema}",
            missing,
            extra,
            note
        ])
    for item in extra_results.get("trigger_mismatched", []):
        rows.append([
            "TRIGGER",
            item.table,
            _csv(item.missing_triggers),
            _csv(item.extra_triggers),
            ";".join(item.detail_mismatch) if item.detail_mismatch else "-"
        ])
    if not rows:
        return None
    return write_pipe_report("扩展对象差异明细", header_fields, rows, output_path)


def export_status_drift_detail(
    trigger_rows: List[TriggerStatusReportRow],
    constraint_rows: List[ConstraintStatusDriftRow],
    report_dir: Path,
    report_timestamp: Optional[str],
    trigger_validity_mode: str = "off"
) -> Optional[Path]:
    if not report_dir or not report_timestamp:
        return None
    if not trigger_rows and not constraint_rows:
        return None
    output_path = Path(report_dir) / f"status_drift_detail_{report_timestamp}.txt"
    header_fields = [
        "OBJECT_TYPE",
        "OBJECT",
        "SRC_ENABLED",
        "TGT_ENABLED",
        "SRC_VALIDATED",
        "TGT_VALIDATED",
        "DETAIL",
        "ACTION_SQL"
    ]
    rows: List[List[str]] = []
    for row in sorted(trigger_rows, key=lambda r: (r.trigger_full, r.detail)):
        action_sql = " ".join(build_trigger_status_fixup_sqls(row, trigger_validity_mode)) or "-"
        rows.append([
            "TRIGGER",
            row.trigger_full,
            row.src_enabled,
            row.tgt_enabled,
            row.src_valid,
            row.tgt_valid,
            row.detail or "-",
            action_sql
        ])
    for row in sorted(constraint_rows, key=lambda r: (r.table_full, r.tgt_constraint)):
        rows.append([
            "CONSTRAINT",
            f"{row.table_full}.{row.tgt_constraint}",
            row.src_status,
            row.tgt_status,
            row.src_validated,
            row.tgt_validated,
            row.detail or "-",
            row.action_sql or "-"
        ])
    return write_pipe_report("触发器/约束状态漂移明细", header_fields, rows, output_path)


def export_noise_suppressed_detail(
    rows: List[NoiseSuppressedDetail],
    report_dir: Path,
    report_timestamp: Optional[str]
) -> Optional[Path]:
    if not report_dir or not rows or not report_timestamp:
        return None
    output_path = Path(report_dir) / f"noise_suppressed_detail_{report_timestamp}.txt"
    header_fields = ["TYPE", "SCOPE", "REASON", "IDENTIFIERS", "DETAIL"]
    data_rows = [
        [
            row.category,
            row.scope,
            row.reason,
            row.identifiers or "-",
            row.detail or "-"
        ]
        for row in rows
    ]
    return write_pipe_report("降噪明细", header_fields, data_rows, output_path)


def export_dependency_detail(
    dependency_report: DependencyReport,
    report_dir: Path,
    report_timestamp: Optional[str]
) -> Optional[Path]:
    if not report_dir or not dependency_report or not report_timestamp:
        return None
    rows: List[List[str]] = []
    for issue in dependency_report.get("missing", []):
        rows.append(["MISSING", issue.dependent, issue.dependent_type, issue.referenced, issue.referenced_type, issue.reason])
    for issue in dependency_report.get("unexpected", []):
        rows.append(["EXTRA", issue.dependent, issue.dependent_type, issue.referenced, issue.referenced_type, issue.reason])
    for issue in dependency_report.get("skipped", []):
        rows.append(["SKIPPED", issue.dependent, issue.dependent_type, issue.referenced, issue.referenced_type, issue.reason])
    if not rows:
        return None
    output_path = Path(report_dir) / f"dependency_detail_{report_timestamp}.txt"
    header_fields = ["CATEGORY", "DEPENDENT", "DEPENDENT_TYPE", "REFERENCED", "REFERENCED_TYPE", "REASON"]
    return write_pipe_report("依赖关系差异明细", header_fields, rows, output_path)


def derive_package_report_path(report_file: Path) -> Path:
    name = report_file.name
    if name.startswith("report_"):
        suffix = name[len("report_"):]
        return report_file.parent / f"package_compare_{suffix}"
    return report_file.parent / "package_compare.txt"


def export_package_compare_report(
    rows: List[PackageCompareRow],
    output_path: Path
) -> Optional[Path]:
    """
    输出 PACKAGE / PACKAGE BODY 校验明细。
    """
    if not rows or not output_path:
        return None
    def _package_sort_key(row: PackageCompareRow) -> Tuple[str, str, int, str, str]:
        parsed = parse_full_object_name(row.src_full) or parse_full_object_name(row.tgt_full)
        owner, name = parsed if parsed else ("", "")
        type_u = (row.obj_type or "").upper()
        type_rank = 0 if type_u == "PACKAGE" else 1 if type_u == "PACKAGE BODY" else 2
        return (owner, name, type_rank, row.src_full, row.tgt_full)

    rows_sorted = sorted(rows, key=_package_sort_key)
    delimiter = "|"
    header = delimiter.join([
        "SRC_FULL",
        "TYPE",
        "SRC_STATUS",
        "TGT_FULL",
        "TGT_STATUS",
        "RESULT",
        "ERROR_COUNT",
        "FIRST_ERROR"
    ])
    lines: List[str] = [
        "# PACKAGE/PACKAGE BODY 对比明细",
        f"# total={len(rows_sorted)}",
        f"# 分隔符: {delimiter}",
        f"# 字段说明: {header}",
        header
    ]
    for row in rows_sorted:
        first_error = normalize_error_text(row.first_error)
        lines.append(delimiter.join([
            row.src_full,
            row.obj_type,
            row.src_status,
            row.tgt_full,
            row.tgt_status,
            row.result,
            str(row.error_count),
            first_error
        ]))
    try:
        output_path.parent.mkdir(parents=True, exist_ok=True)
        output_path.write_text("\n".join(lines).rstrip() + "\n", encoding="utf-8")
        return output_path
    except OSError as exc:
        log.warning("写入 package_compare 报告失败 %s: %s", output_path, exc)
        return None


def export_ddl_clean_report(
    rows: List[DdlCleanReportRow],
    report_dir: Path,
    report_timestamp: Optional[str]
) -> Optional[Path]:
    """
    输出 PL/SQL 全角标点清洗报告。
    """
    if not report_dir or not rows or not report_timestamp:
        return None

    output_path = Path(report_dir) / f"ddl_punct_clean_{report_timestamp}.txt"
    rows_sorted = sorted(rows, key=lambda r: (r.obj_type, r.obj_full))
    total_replaced = sum(r.replaced for r in rows_sorted)
    lines: List[str] = [
        "# PL/SQL 全角标点清洗报告",
        f"# total_objects={len(rows_sorted)} total_replacements={total_replaced}",
        "# 字段说明: TYPE | OBJECT | REPLACED | SAMPLES"
    ]

    sample_texts: List[str] = []
    for row in rows_sorted:
        if row.samples:
            sample_texts.append(", ".join(f"{src}->{dst}" for src, dst in row.samples))
        else:
            sample_texts.append("-")

    type_w = max(len("TYPE"), max((len(r.obj_type) for r in rows_sorted), default=0))
    obj_w = max(len("OBJECT"), max((len(r.obj_full) for r in rows_sorted), default=0))
    replaced_w = max(len("REPLACED"), max((len(str(r.replaced)) for r in rows_sorted), default=0))
    sample_w = max(len("SAMPLES"), max((len(text) for text in sample_texts), default=0))

    header = (
        f"{'TYPE'.ljust(type_w)}  "
        f"{'OBJECT'.ljust(obj_w)}  "
        f"{'REPLACED'.rjust(replaced_w)}  "
        f"{'SAMPLES'.ljust(sample_w)}"
    )
    lines.append(header)
    lines.append(
        f"{'-' * type_w}  "
        f"{'-' * obj_w}  "
        f"{'-' * replaced_w}  "
        f"{'-' * sample_w}"
    )

    for row, sample in zip(rows_sorted, sample_texts):
        lines.append(
            f"{row.obj_type.ljust(type_w)}  "
            f"{row.obj_full.ljust(obj_w)}  "
            f"{str(row.replaced).rjust(replaced_w)}  "
            f"{sample.ljust(sample_w)}"
        )

    try:
        output_path.parent.mkdir(parents=True, exist_ok=True)
        content = "\n".join(lines).rstrip() + "\n"
        output_path.write_text(content, encoding="utf-8")
        return output_path
    except OSError as exc:
        log.warning("写入全角标点清洗报告失败 %s: %s", output_path, exc)
        return None


def export_ddl_hint_clean_report(
    rows: List[DdlHintCleanReportRow],
    report_dir: Path,
    report_timestamp: Optional[str]
) -> Optional[Path]:
    """
    输出 DDL hint 清洗报告。
    """
    if not report_dir or not rows or not report_timestamp:
        return None

    output_path = Path(report_dir) / f"ddl_hint_clean_{report_timestamp}.txt"
    rows_sorted = sorted(rows, key=lambda r: (r.obj_type, r.obj_full))
    total_tokens = sum(r.total for r in rows_sorted)
    total_kept = sum(r.kept for r in rows_sorted)
    total_removed = sum(r.removed for r in rows_sorted)
    total_unknown = sum(r.unknown for r in rows_sorted)

    def join_samples(samples: List[str]) -> str:
        return ", ".join(samples) if samples else "-"

    kept_texts = [join_samples(row.kept_samples) for row in rows_sorted]
    removed_texts = [join_samples(row.removed_samples) for row in rows_sorted]
    unknown_texts = [join_samples(row.unknown_samples) for row in rows_sorted]

    type_w = max(len("TYPE"), max((len(r.obj_type) for r in rows_sorted), default=0))
    obj_w = max(len("OBJECT"), max((len(r.obj_full) for r in rows_sorted), default=0))
    policy_w = max(len("POLICY"), max((len(r.policy) for r in rows_sorted), default=0))
    total_w = max(len("TOKENS"), max((len(str(r.total)) for r in rows_sorted), default=0))
    kept_w = max(len("KEPT"), max((len(str(r.kept)) for r in rows_sorted), default=0))
    removed_w = max(len("REMOVED"), max((len(str(r.removed)) for r in rows_sorted), default=0))
    unknown_w = max(len("UNKNOWN"), max((len(str(r.unknown)) for r in rows_sorted), default=0))
    kept_s_w = max(len("KEPT_SAMPLES"), max((len(text) for text in kept_texts), default=0))
    removed_s_w = max(len("REMOVED_SAMPLES"), max((len(text) for text in removed_texts), default=0))
    unknown_s_w = max(len("UNKNOWN_SAMPLES"), max((len(text) for text in unknown_texts), default=0))

    lines: List[str] = [
        "# DDL hint clean report",
        (
            f"# total_objects={len(rows_sorted)} total_tokens={total_tokens} "
            f"kept={total_kept} removed={total_removed} unknown={total_unknown}"
        ),
        "# 字段说明: TYPE | OBJECT | POLICY | TOKENS | KEPT | REMOVED | UNKNOWN | KEPT_SAMPLES | REMOVED_SAMPLES | UNKNOWN_SAMPLES"
    ]

    header = (
        f"{'TYPE'.ljust(type_w)}  "
        f"{'OBJECT'.ljust(obj_w)}  "
        f"{'POLICY'.ljust(policy_w)}  "
        f"{'TOKENS'.rjust(total_w)}  "
        f"{'KEPT'.rjust(kept_w)}  "
        f"{'REMOVED'.rjust(removed_w)}  "
        f"{'UNKNOWN'.rjust(unknown_w)}  "
        f"{'KEPT_SAMPLES'.ljust(kept_s_w)}  "
        f"{'REMOVED_SAMPLES'.ljust(removed_s_w)}  "
        f"{'UNKNOWN_SAMPLES'.ljust(unknown_s_w)}"
    )
    lines.append(header)
    lines.append(
        f"{'-' * type_w}  "
        f"{'-' * obj_w}  "
        f"{'-' * policy_w}  "
        f"{'-' * total_w}  "
        f"{'-' * kept_w}  "
        f"{'-' * removed_w}  "
        f"{'-' * unknown_w}  "
        f"{'-' * kept_s_w}  "
        f"{'-' * removed_s_w}  "
        f"{'-' * unknown_s_w}"
    )

    for row, kept_text, removed_text, unknown_text in zip(rows_sorted, kept_texts, removed_texts, unknown_texts):
        lines.append(
            f"{row.obj_type.ljust(type_w)}  "
            f"{row.obj_full.ljust(obj_w)}  "
            f"{row.policy.ljust(policy_w)}  "
            f"{str(row.total).rjust(total_w)}  "
            f"{str(row.kept).rjust(kept_w)}  "
            f"{str(row.removed).rjust(removed_w)}  "
            f"{str(row.unknown).rjust(unknown_w)}  "
            f"{kept_text.ljust(kept_s_w)}  "
            f"{removed_text.ljust(removed_s_w)}  "
            f"{unknown_text.ljust(unknown_s_w)}"
        )

    try:
        output_path.parent.mkdir(parents=True, exist_ok=True)
        output_path.write_text("\n".join(lines).rstrip() + "\n", encoding="utf-8")
        return output_path
    except OSError as exc:
        log.warning("写入 ddl_hint_clean 报告失败 %s: %s", output_path, exc)
        return None


def evaluate_long_conversion_status(
    oracle_meta: OracleMetadata,
    ob_meta: ObMetadata,
    src_schema: str,
    src_table: str,
    tgt_schema: str,
    tgt_table: str
) -> Tuple[str, str, bool]:
    """
    校验 LONG/LONG RAW 列在目标端是否已转换为 CLOB/BLOB。
    返回 (status, detail, verified)。
    """
    tgt_full = f"{tgt_schema.upper()}.{tgt_table.upper()}"
    src_key = (src_schema.upper(), src_table.upper())
    tgt_key = (tgt_schema.upper(), tgt_table.upper())

    if not ob_meta or not ob_meta.objects_by_type:
        return "UNKNOWN", "目标端元数据缺失", False

    tgt_tables = ob_meta.objects_by_type.get("TABLE")
    if tgt_tables is None:
        return "UNKNOWN", "目标端未加载 TABLE 元数据", False

    if tgt_full not in tgt_tables:
        return "MISSING_TABLE", "目标端表不存在", False

    src_cols = oracle_meta.table_columns.get(src_key)
    if src_cols is None:
        return "UNKNOWN", "源端列元数据缺失", False

    long_cols: Dict[str, str] = {}
    for col, info in src_cols.items():
        src_type = (info.get("data_type") or "").strip().upper()
        if is_long_type(src_type):
            long_cols[col.upper()] = src_type

    if not long_cols:
        return "NO_LONG_COLUMNS", "源端未发现 LONG/LONG RAW 列", False

    if not ob_meta.tab_columns:
        return "UNKNOWN", "目标端未加载列元数据", False

    tgt_cols = ob_meta.tab_columns.get(tgt_key)
    if not tgt_cols:
        return "UNKNOWN", "目标端列元数据缺失", False

    missing_cols: List[str] = []
    mismatch_cols: List[str] = []
    for col, src_type in long_cols.items():
        tgt_info = tgt_cols.get(col)
        if not tgt_info:
            missing_cols.append(col)
            continue
        tgt_type = (tgt_info.get("data_type") or "").strip().upper() or "UNKNOWN"
        expected = map_long_type_to_ob(src_type)
        if tgt_type != expected:
            mismatch_cols.append(f"{col}({tgt_type}->{expected})")

    if missing_cols:
        return "MISSING_COLUMN", f"缺失列: {sorted(missing_cols)}", False
    if mismatch_cols:
        return "TYPE_MISMATCH", f"类型不匹配: {sorted(mismatch_cols)}", False

    return "VERIFIED", "已校验: LONG/LONG RAW 已转换为 CLOB/BLOB", True


def build_blacklist_report_rows(
    blacklist_tables: BlacklistTableMap,
    table_target_map: Dict[Tuple[str, str], Tuple[str, str]],
    oracle_meta: OracleMetadata,
    ob_meta: ObMetadata
) -> List[BlacklistReportRow]:
    """
    生成黑名单表报告行，LONG/LONG RAW 额外校验目标端转换情况。
    """
    rows: List[BlacklistReportRow] = []
    for (schema, table), entries in blacklist_tables.items():
        schema_u = schema.upper()
        table_u = table.upper()
        tgt_schema, tgt_table = table_target_map.get((schema_u, table_u), (schema_u, table_u))
        mapped_full = f"{tgt_schema.upper()}.{tgt_table.upper()}"
        src_full = f"{schema_u}.{table_u}"
        mapping_hint = ""
        if mapped_full != src_full:
            mapping_hint = f"目标端: {mapped_full}"

        for entry in sorted(entries.values(), key=lambda e: (e.black_type, e.data_type, e.source)):
            black_type_u = normalize_black_type(entry.black_type) or "UNKNOWN"
            data_type_u = normalize_black_data_type(entry.data_type) or "-"
            reason = blacklist_reason(black_type_u)
            status = "BLACKLISTED"
            source_detail = format_blacklist_source(entry.source)
            detail = source_detail or "-"

            if black_type_u == "LONG" or is_long_type(data_type_u):
                status, detail, verified = evaluate_long_conversion_status(
                    oracle_meta,
                    ob_meta,
                    schema_u,
                    table_u,
                    tgt_schema,
                    tgt_table
                )
                detail = merge_detail_parts(source_detail, mapping_hint, detail)
                if verified:
                    reason = "已校验: LONG/LONG RAW 已转换为 CLOB/BLOB"

            rows.append(
                BlacklistReportRow(
                    schema=schema_u,
                    table=table_u,
                    black_type=black_type_u,
                    data_type=data_type_u,
                    reason=reason,
                    status=status,
                    detail=detail
                )
            )
    return rows


def export_blacklist_tables(
    rows: List[BlacklistReportRow],
    report_dir: Path
) -> Optional[Path]:
    """
    将黑名单表输出为文本，按 schema 分组并标注原因与 LONG 校验状态。
    """
    if not report_dir or not rows:
        return None

    output_path = Path(report_dir) / "blacklist_tables.txt"
    grouped: Dict[str, List[BlacklistReportRow]] = defaultdict(list)
    for row in rows:
        grouped[row.schema.upper()].append(row)

    lines: List[str] = [
        "# 黑名单表清单（LONG/LONG RAW 将校验目标端转换情况）",
        "# 说明: 黑名单缺失表不会生成 missed_tables_views_for_OMS 规则",
        "# DETAIL 可能包含 RULE=<id> 作为规则来源标记",
        "# 字段说明: TABLE_FULL | BLACK_TYPE | DATA_TYPE | STATUS | DETAIL | REASON"
    ]
    for schema in sorted(grouped.keys()):
        schema_rows = sorted(
            grouped[schema],
            key=lambda r: (r.table, r.black_type, r.data_type)
        )
        formatted_rows: List[Tuple[str, str, str, str, str, str]] = []
        for row in schema_rows:
            formatted_rows.append((
                f"{schema}.{row.table}",
                row.black_type,
                row.data_type,
                row.status,
                row.detail,
                row.reason
            ))

        table_count = len({r[0] for r in formatted_rows})
        entry_count = len(formatted_rows)
        lines.append(f"[{schema}] (tables={table_count}, entries={entry_count})")

        table_w = max(len("TABLE_FULL"), max((len(r[0]) for r in formatted_rows), default=0))
        type_w = max(len("BLACK_TYPE"), max((len(r[1]) for r in formatted_rows), default=0))
        data_w = max(len("DATA_TYPE"), max((len(r[2]) for r in formatted_rows), default=0))
        status_w = max(len("STATUS"), max((len(r[3]) for r in formatted_rows), default=0))
        detail_w = max(len("DETAIL"), max((len(r[4]) for r in formatted_rows), default=0))

        header = (
            f"{'TABLE_FULL'.ljust(table_w)}  "
            f"{'BLACK_TYPE'.ljust(type_w)}  "
            f"{'DATA_TYPE'.ljust(data_w)}  "
            f"{'STATUS'.ljust(status_w)}  "
            f"{'DETAIL'.ljust(detail_w)}  "
            f"REASON"
        )
        sep = (
            f"{'-' * table_w}  "
            f"{'-' * type_w}  "
            f"{'-' * data_w}  "
            f"{'-' * status_w}  "
            f"{'-' * detail_w}  "
            f"{'-' * 6}"
        )
        lines.append(header)
        lines.append(sep)

        for table_full, black_type, data_type, status, detail, reason in formatted_rows:
            lines.append(
                f"{table_full.ljust(table_w)}  "
                f"{black_type.ljust(type_w)}  "
                f"{data_type.ljust(data_w)}  "
                f"{status.ljust(status_w)}  "
                f"{detail.ljust(detail_w)}  "
                f"{reason}"
            )
        lines.append("")

    try:
        output_path.parent.mkdir(parents=True, exist_ok=True)
        content = "\n".join(lines).rstrip() + "\n"
        output_path.write_text(content, encoding="utf-8")
        return output_path
    except OSError as exc:
        log.warning("写入黑名单表清单失败 %s: %s", output_path, exc)
        return None


def export_trigger_status_report(
    trigger_list_rows: List[TriggerListReportRow],
    trigger_list_summary: Optional[Dict[str, object]],
    status_rows: List[TriggerStatusReportRow],
    report_dir: Path
) -> Optional[Path]:
    """
    输出触发器清单筛选 + 状态差异报告。
    """
    if not report_dir:
        return None
    has_list = bool(trigger_list_summary and trigger_list_summary.get("enabled"))
    has_status = bool(status_rows)
    if not has_list and not has_status:
        return None

    output_path = Path(report_dir) / "trigger_status_report.txt"
    lines: List[str] = ["# 触发器状态与清单报告"]

    def append_pipe_section(
        title: str,
        header_fields: List[str],
        rows: List[List[str]],
        extra_notes: Optional[List[str]] = None
    ) -> None:
        lines.append("")
        lines.append(f"# [Section] {title}")
        if extra_notes:
            for note in extra_notes:
                if note:
                    lines.append(f"# {note}")
        lines.append(f"# total={len(rows)}")
        lines.append("# 分隔符: |")
        header = "|".join(header_fields)
        lines.append(f"# 字段说明: {header}")
        lines.append(header)
        for row in rows:
            lines.append("|".join(sanitize_pipe_field(item) for item in row))

    if has_list and trigger_list_summary:
        summary = trigger_list_summary
        notes = [
            f"trigger_list: {summary.get('path', '')}",
            (
                "汇总: total_lines={total_lines}, valid={valid_entries}, invalid={invalid_entries}, "
                "duplicate={duplicate_entries}, selected_missing={selected_missing}, "
                "missing_not_listed={missing_not_listed}, not_found={not_found}, not_missing={not_missing}"
            ).format(**{k: summary.get(k, 0) for k in [
                "total_lines", "valid_entries", "invalid_entries", "duplicate_entries",
                "selected_missing", "missing_not_listed", "not_found", "not_missing"
            ]})
        ]
        if summary.get("error"):
            notes.append(f"ERROR: {summary.get('error')}")
        if summary.get("fallback_full"):
            reason = summary.get("fallback_reason") or "unknown"
            note = "清单不可用，已回退全量触发器"
            if reason == "empty_list":
                note = "清单为空，已回退全量触发器"
            elif reason == "read_error":
                note = "清单读取失败，已回退全量触发器"
            notes.append(f"NOTE: {note}")
        if summary.get("check_disabled"):
            notes.append("NOTE: TRIGGER 未启用检查，清单仅做格式校验。")
        list_rows = [
            [row.entry, row.status, row.detail]
            for row in trigger_list_rows
        ]
        append_pipe_section(
            "trigger_list 清单筛选",
            ["ENTRY", "STATUS", "DETAIL"],
            list_rows,
            extra_notes=notes
        )

    if has_status:
        status_data = [
            [
                row.trigger_full,
                row.src_event,
                row.tgt_event,
                row.src_enabled,
                row.tgt_enabled,
                row.src_valid,
                row.tgt_valid,
                row.detail
            ]
            for row in status_rows
        ]
        append_pipe_section(
            "触发器状态差异",
            ["TRIGGER", "SRC_EVENT", "TGT_EVENT", "SRC_ENABLED", "TGT_ENABLED", "SRC_VALID", "TGT_VALID", "DETAIL"],
            status_data
        )

    try:
        output_path.parent.mkdir(parents=True, exist_ok=True)
        content = "\n".join(lines).rstrip() + "\n"
        output_path.write_text(content, encoding="utf-8")
        return output_path
    except OSError as exc:
        log.warning("写入 trigger_status_report 报告失败 %s: %s", output_path, exc)
        return None


def export_filtered_grants(
    filtered_grants: List[FilteredGrantEntry],
    report_dir: Path
) -> Optional[Path]:
    """
    输出被过滤掉的 GRANT 权限清单。
    """
    if not report_dir or not filtered_grants:
        return None

    output_path = Path(report_dir) / "filtered_grants.txt"
    rows = sorted(
        filtered_grants,
        key=lambda r: (r.category, r.grantee, r.privilege, r.object_full, r.reason)
    )
    lines: List[str] = [
        "# 被过滤掉的 GRANT 权限 (Oracle/OB 不兼容或未知权限)",
        f"# total={len(rows)}",
        "# 字段说明: CATEGORY | GRANTEE | PRIVILEGE | OBJECT | REASON"
    ]

    cat_w = max(len("CATEGORY"), max((len(r.category) for r in rows), default=0))
    grantee_w = max(len("GRANTEE"), max((len(r.grantee) for r in rows), default=0))
    priv_w = max(len("PRIVILEGE"), max((len(r.privilege) for r in rows), default=0))
    obj_w = max(len("OBJECT"), max((len(r.object_full or "-") for r in rows), default=0))

    header = (
        f"{'CATEGORY'.ljust(cat_w)}  "
        f"{'GRANTEE'.ljust(grantee_w)}  "
        f"{'PRIVILEGE'.ljust(priv_w)}  "
        f"{'OBJECT'.ljust(obj_w)}  "
        f"REASON"
    )
    lines.append(header)
    lines.append("-" * len(header))

    for row in rows:
        obj = row.object_full or "-"
        lines.append(
            f"{row.category.ljust(cat_w)}  "
            f"{row.grantee.ljust(grantee_w)}  "
            f"{row.privilege.ljust(priv_w)}  "
            f"{obj.ljust(obj_w)}  "
            f"{row.reason}"
        )

    try:
        output_path.parent.mkdir(parents=True, exist_ok=True)
        content = "\n".join(lines).rstrip() + "\n"
        output_path.write_text(content, encoding="utf-8")
        return output_path
    except OSError as exc:
        log.warning("写入 filtered_grants 报告失败 %s: %s", output_path, exc)
        return None


def export_fixup_skip_summary(
    summary: Dict[str, Dict[str, object]],
    report_dir: Path,
    report_timestamp: Optional[str]
) -> Optional[Path]:
    """
    输出 fixup 跳过原因汇总，用于解释缺失对象数量与实际生成脚本数量差异。
    """
    if not summary or not report_dir or not report_timestamp:
        return None

    output_path = Path(report_dir) / f"fixup_skip_summary_{report_timestamp}.txt"
    lines: List[str] = [
        "# Fixup 跳过原因汇总",
        f"# timestamp={report_timestamp}",
    ]
    for obj_type, data in sorted(summary.items()):
        missing_total = int(data.get("missing_total", 0) or 0)
        task_total = int(data.get("task_total", 0) or 0)
        generated = int(data.get("generated", 0) or 0)
        skipped = data.get("skipped", {}) or {}
        lines.append("")
        lines.append(f"[{obj_type}] missing_total={missing_total} task_total={task_total} generated={generated}")
        if skipped:
            for reason, count in sorted(skipped.items()):
                lines.append(f"  - {reason}: {count}")
        else:
            lines.append("  - skipped: 0")

    try:
        output_path.parent.mkdir(parents=True, exist_ok=True)
        output_path.write_text("\n".join(lines).rstrip() + "\n", encoding="utf-8")
        return output_path
    except OSError as exc:
        log.warning("写入 fixup 跳过汇总失败 %s: %s", output_path, exc)
        return None


REPORT_DB_TABLES = {
    "summary": "DIFF_REPORT_SUMMARY",
    "detail": "DIFF_REPORT_DETAIL",
    "detail_item": "DIFF_REPORT_DETAIL_ITEM",
    "grants": "DIFF_REPORT_GRANT",
    "counts": "DIFF_REPORT_COUNTS",
    "usability": "DIFF_REPORT_USABILITY",
    "package_compare": "DIFF_REPORT_PACKAGE_COMPARE",
    "trigger_status": "DIFF_REPORT_TRIGGER_STATUS",
    "artifact": "DIFF_REPORT_ARTIFACT",
    "artifact_line": "DIFF_REPORT_ARTIFACT_LINE",
    "dependency": "DIFF_REPORT_DEPENDENCY",
    "view_chain": "DIFF_REPORT_VIEW_CHAIN",
    "remap_conflict": "DIFF_REPORT_REMAP_CONFLICT",
    "object_mapping": "DIFF_REPORT_OBJECT_MAPPING",
    "blacklist": "DIFF_REPORT_BLACKLIST",
    "fixup_skip": "DIFF_REPORT_FIXUP_SKIP",
    "oms_missing": "DIFF_REPORT_OMS_MISSING",
    "write_errors": "DIFF_REPORT_WRITE_ERRORS",
    "resolution": "DIFF_REPORT_RESOLUTION",
}
REPORT_DB_VIEWS = {
    "actions": "DIFF_REPORT_ACTIONS_V",
    "object_profile": "DIFF_REPORT_OBJECT_PROFILE_V",
    "trends": "DIFF_REPORT_TRENDS_V",
    "pending_actions": "DIFF_REPORT_PENDING_ACTIONS_V",
    "grant_class": "DIFF_REPORT_GRANT_CLASS_V",
    "usability_class": "DIFF_REPORT_USABILITY_CLASS_V",
}
REPORT_DB_CLOB_CHUNK_SIZE = 2000
REPORT_DB_WRITE_ERROR_SNIPPET_MAX = 4000


def _sanitize_sql_literal_text(value: str) -> str:
    """
    Sanitize SQL literal text for obclient INSERT ALL statements.
    - Drop NUL bytes.
    - Drop non-printable control chars except CR/LF/TAB.
    """
    if not value:
        return ""
    value = value.replace("\x00", "")
    return "".join(ch for ch in value if ch.isprintable() or ch in "\r\n\t")


def sql_quote_literal(value: Optional[object]) -> str:
    if value is None:
        return "NULL"
    text = _sanitize_sql_literal_text(str(value))
    return "'" + text.replace("'", "''") + "'"


def sql_clob_literal(value: Optional[object], chunk_size: int = REPORT_DB_CLOB_CHUNK_SIZE) -> str:
    if value is None:
        return "NULL"
    text = _sanitize_sql_literal_text(str(value))
    if text == "":
        return "TO_CLOB('')"
    chunks = [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]
    clob_parts = [f"TO_CLOB({sql_quote_literal(chunk)})" for chunk in chunks]
    return " || ".join(clob_parts)


def _truncate_report_db_text(value: Optional[object], max_len: int = REPORT_DB_WRITE_ERROR_SNIPPET_MAX) -> Optional[str]:
    if value is None:
        return None
    text = str(value)
    if max_len <= 0 or len(text) <= max_len:
        return text
    if max_len <= 3:
        return text[:max_len]
    return text[:max_len - 3] + "..."


def _record_report_db_write_error(
    ob_cfg: ObConfig,
    schema_prefix: str,
    report_id: str,
    table_name: str,
    sql_snippet: Optional[str],
    error_message: Optional[str]
) -> None:
    if not report_id:
        return
    error_id = f"{report_id}_{uuid.uuid4().hex[:12]}"
    snippet = _truncate_report_db_text(sql_snippet)
    err_text = _truncate_report_db_text(error_message, max_len=4000)
    insert_sql = (
        f"INSERT INTO {schema_prefix}{REPORT_DB_TABLES['write_errors']} "
        "(ERROR_ID, REPORT_ID, TABLE_NAME, SQL_SNIPPET, ERROR_MESSAGE) "
        "VALUES ({error_id}, {report_id}, {table_name}, {sql_snippet}, {error_message})".format(
            error_id=sql_quote_literal(error_id),
            report_id=sql_quote_literal(report_id),
            table_name=sql_quote_literal(table_name) if table_name else "NULL",
            sql_snippet=sql_clob_literal(snippet) if snippet else "NULL",
            error_message=sql_clob_literal(err_text) if err_text else "NULL",
        )
    )
    ok, _out, err = obclient_run_sql_commit(ob_cfg, insert_sql)
    if not ok:
        log.debug("[REPORT_DB] WRITE_ERRORS 写入失败: %s", err)


def build_report_db_schema_prefix(settings: Dict) -> str:
    schema = (settings.get("report_db_schema") or "").strip()
    return f"{schema}." if schema else ""


def generate_report_id(timestamp: str) -> str:
    short_uuid = uuid.uuid4().hex[:8]
    return f"{timestamp}_{short_uuid}"


def ensure_report_db_tables_exist(ob_cfg: ObConfig, settings: Dict) -> Tuple[bool, str]:
    schema = (settings.get("report_db_schema") or "").strip().upper()
    schema_prefix = f"{schema}." if schema else ""
    owner_clause = f"OWNER = '{schema}'" if schema else "OWNER = USER"
    table_names = "', '".join(REPORT_DB_TABLES.values())
    check_sql = (
        "SELECT TABLE_NAME FROM ALL_TABLES "
        f"WHERE {owner_clause} AND TABLE_NAME IN ('{table_names}')"
    )
    ok, out, err = obclient_run_sql(ob_cfg, check_sql)
    if not ok:
        return False, f"检查报告表失败: {err}"
    existing = {line.strip().upper() for line in (out.splitlines() if out else []) if line.strip()}

    ddl_summary = f"""
CREATE TABLE {schema_prefix}{REPORT_DB_TABLES['summary']} (
    REPORT_ID           VARCHAR2(64) NOT NULL,
    RUN_TIMESTAMP       TIMESTAMP NOT NULL,
    RUN_DATE            DATE GENERATED ALWAYS AS (TRUNC(RUN_TIMESTAMP)) VIRTUAL,
    DURATION_SECONDS    NUMBER(10,2),
    SOURCE_HOST         VARCHAR2(256),
    SOURCE_PORT         NUMBER,
    SOURCE_SERVICE      VARCHAR2(128),
    SOURCE_USER         VARCHAR2(128),
    SOURCE_SCHEMAS      CLOB,
    TARGET_HOST         VARCHAR2(256),
    TARGET_PORT         NUMBER,
    TARGET_TENANT       VARCHAR2(128),
    TARGET_USER         VARCHAR2(128),
    TARGET_SCHEMAS      CLOB,
    CHECK_PRIMARY_TYPES VARCHAR2(512),
    CHECK_EXTRA_TYPES   VARCHAR2(512),
    FIXUP_ENABLED       NUMBER(1) DEFAULT 0,
    GRANT_ENABLED       NUMBER(1) DEFAULT 0,
    TOTAL_CHECKED       NUMBER DEFAULT 0,
    MISSING_COUNT       NUMBER DEFAULT 0,
    MISMATCHED_COUNT    NUMBER DEFAULT 0,
    OK_COUNT            NUMBER DEFAULT 0,
    SKIPPED_COUNT       NUMBER DEFAULT 0,
    UNSUPPORTED_COUNT   NUMBER DEFAULT 0,
    INDEX_MISSING       NUMBER DEFAULT 0,
    INDEX_MISMATCHED    NUMBER DEFAULT 0,
    CONSTRAINT_MISSING  NUMBER DEFAULT 0,
    CONSTRAINT_MISMATCH NUMBER DEFAULT 0,
    TRIGGER_MISSING     NUMBER DEFAULT 0,
    SEQUENCE_MISSING    NUMBER DEFAULT 0,
    DETAIL_TRUNCATED    NUMBER(1) DEFAULT 0,
    DETAIL_TRUNCATED_COUNT NUMBER DEFAULT 0,
    CONCLUSION          VARCHAR2(32),
    CONCLUSION_DETAIL   VARCHAR2(1000),
    FULL_REPORT_JSON    CLOB,
    TOOL_VERSION        VARCHAR2(64),
    HOSTNAME            VARCHAR2(256),
    RUN_DIR             VARCHAR2(512),
    CREATED_AT          TIMESTAMP DEFAULT SYSTIMESTAMP,
    CONSTRAINT PK_DIFF_REPORT_SUMMARY PRIMARY KEY (REPORT_ID)
)
"""
    ddl_detail = f"""
CREATE TABLE {schema_prefix}{REPORT_DB_TABLES['detail']} (
    DETAIL_ID           VARCHAR2(64) NOT NULL,
    REPORT_ID           VARCHAR2(64) NOT NULL,
    REPORT_TYPE         VARCHAR2(64) NOT NULL,
    OBJECT_TYPE         VARCHAR2(32) NOT NULL,
    SUB_TYPE            VARCHAR2(64),
    SOURCE_SCHEMA       VARCHAR2(128),
    SOURCE_NAME         VARCHAR2(128),
    TARGET_SCHEMA       VARCHAR2(128),
    TARGET_NAME         VARCHAR2(128),
    STATUS              VARCHAR2(64),
    REASON              VARCHAR2(2000),
    BLACKLIST_REASON    VARCHAR2(256),
    DETAIL_JSON         CLOB,
    CREATED_AT          TIMESTAMP DEFAULT SYSTIMESTAMP,
    CONSTRAINT PK_DIFF_REPORT_DETAIL PRIMARY KEY (DETAIL_ID)
)
"""
    ddl_detail_item = f"""
CREATE TABLE {schema_prefix}{REPORT_DB_TABLES['detail_item']} (
    ITEM_ID            VARCHAR2(64) NOT NULL,
    REPORT_ID          VARCHAR2(64) NOT NULL,
    REPORT_TYPE        VARCHAR2(64),
    OBJECT_TYPE        VARCHAR2(32),
    SOURCE_SCHEMA      VARCHAR2(128),
    SOURCE_NAME        VARCHAR2(128),
    TARGET_SCHEMA      VARCHAR2(128),
    TARGET_NAME        VARCHAR2(128),
    ITEM_TYPE          VARCHAR2(64),
    ITEM_KEY           VARCHAR2(256),
    SRC_VALUE          VARCHAR2(1000),
    TGT_VALUE          VARCHAR2(1000),
    ITEM_VALUE         CLOB,
    STATUS             VARCHAR2(64),
    CREATED_AT         TIMESTAMP DEFAULT SYSTIMESTAMP,
    CONSTRAINT PK_DIFF_REPORT_DETAIL_ITEM PRIMARY KEY (ITEM_ID)
)
"""
    ddl_grants = f"""
CREATE TABLE {schema_prefix}{REPORT_DB_TABLES['grants']} (
    GRANT_ID            VARCHAR2(64) NOT NULL,
    REPORT_ID           VARCHAR2(64) NOT NULL,
    GRANT_TYPE          VARCHAR2(32) NOT NULL,
    GRANTEE             VARCHAR2(128) NOT NULL,
    GRANTOR             VARCHAR2(128),
    PRIVILEGE           VARCHAR2(64),
    TARGET_SCHEMA       VARCHAR2(128),
    TARGET_NAME         VARCHAR2(128),
    TARGET_TYPE         VARCHAR2(32),
    WITH_GRANT_OPTION   NUMBER(1) DEFAULT 0,
    STATUS              VARCHAR2(32),
    FILTER_REASON       VARCHAR2(256),
    CREATED_AT          TIMESTAMP DEFAULT SYSTIMESTAMP,
    CONSTRAINT PK_DIFF_REPORT_GRANT PRIMARY KEY (GRANT_ID)
)
"""
    ddl_counts = f"""
CREATE TABLE {schema_prefix}{REPORT_DB_TABLES['counts']} (
    REPORT_ID           VARCHAR2(64) NOT NULL,
    OBJECT_TYPE         VARCHAR2(32) NOT NULL,
    ORACLE_COUNT        NUMBER DEFAULT 0,
    OCEANBASE_COUNT     NUMBER DEFAULT 0,
    MISSING_COUNT       NUMBER DEFAULT 0,
    UNSUPPORTED_COUNT   NUMBER DEFAULT 0,
    EXTRA_COUNT         NUMBER DEFAULT 0,
    CREATED_AT          TIMESTAMP DEFAULT SYSTIMESTAMP,
    CONSTRAINT PK_DIFF_REPORT_COUNTS PRIMARY KEY (REPORT_ID, OBJECT_TYPE)
)
"""
    ddl_usability = f"""
CREATE TABLE {schema_prefix}{REPORT_DB_TABLES['usability']} (
    REPORT_ID           VARCHAR2(64) NOT NULL,
    OBJECT_TYPE         VARCHAR2(32) NOT NULL,
    SCHEMA_NAME         VARCHAR2(128),
    OBJECT_NAME         VARCHAR2(256),
    USABLE              NUMBER(1),
    STATUS              VARCHAR2(32),
    REASON              VARCHAR2(1024),
    DETAIL_JSON         CLOB,
    CREATED_AT          TIMESTAMP DEFAULT SYSTIMESTAMP
)
"""
    ddl_package_compare = f"""
CREATE TABLE {schema_prefix}{REPORT_DB_TABLES['package_compare']} (
    REPORT_ID           VARCHAR2(64) NOT NULL,
    SCHEMA_NAME         VARCHAR2(128),
    OBJECT_NAME         VARCHAR2(256),
    OBJECT_TYPE         VARCHAR2(32),
    SRC_STATUS          VARCHAR2(32),
    TGT_STATUS          VARCHAR2(32),
    DIFF_STATUS         VARCHAR2(32),
    DIFF_HASH           VARCHAR2(64),
    DIFF_SUMMARY        VARCHAR2(4000),
    DIFF_PATH           VARCHAR2(512),
    CREATED_AT          TIMESTAMP DEFAULT SYSTIMESTAMP
)
"""
    ddl_trigger_status = f"""
CREATE TABLE {schema_prefix}{REPORT_DB_TABLES['trigger_status']} (
    REPORT_ID           VARCHAR2(64) NOT NULL,
    SCHEMA_NAME         VARCHAR2(128),
    TRIGGER_NAME        VARCHAR2(256),
    SRC_EVENT           VARCHAR2(256),
    TGT_EVENT           VARCHAR2(256),
    SRC_ENABLED         VARCHAR2(16),
    TGT_ENABLED         VARCHAR2(16),
    SRC_VALID           VARCHAR2(16),
    TGT_VALID           VARCHAR2(16),
    DIFF_STATUS         VARCHAR2(64),
    REASON              VARCHAR2(1024),
    CREATED_AT          TIMESTAMP DEFAULT SYSTIMESTAMP
)
"""
    ddl_artifact = f"""
CREATE TABLE {schema_prefix}{REPORT_DB_TABLES['artifact']} (
    REPORT_ID           VARCHAR2(64) NOT NULL,
    ARTIFACT_TYPE       VARCHAR2(64) NOT NULL,
    FILE_PATH           VARCHAR2(512),
    FILE_HASH           VARCHAR2(64),
    ROW_COUNT           NUMBER DEFAULT 0,
    FIELD_LIST          VARCHAR2(2000),
    STATUS              VARCHAR2(32),
    NOTE                VARCHAR2(1000),
    CREATED_AT          TIMESTAMP DEFAULT SYSTIMESTAMP
)
"""
    ddl_artifact_line = f"""
CREATE TABLE {schema_prefix}{REPORT_DB_TABLES['artifact_line']} (
    REPORT_ID           VARCHAR2(64) NOT NULL,
    ARTIFACT_TYPE       VARCHAR2(64) NOT NULL,
    FILE_PATH           VARCHAR2(512) NOT NULL,
    LINE_NO             NUMBER NOT NULL,
    LINE_TEXT           CLOB,
    CREATED_AT          TIMESTAMP DEFAULT SYSTIMESTAMP
)
"""
    ddl_dependency = f"""
CREATE TABLE {schema_prefix}{REPORT_DB_TABLES['dependency']} (
    REPORT_ID           VARCHAR2(64) NOT NULL,
    DEP_SCHEMA          VARCHAR2(128),
    DEP_NAME            VARCHAR2(256),
    DEP_TYPE            VARCHAR2(32),
    REF_SCHEMA          VARCHAR2(128),
    REF_NAME            VARCHAR2(256),
    REF_TYPE            VARCHAR2(32),
    EDGE_STATUS         VARCHAR2(32),
    REASON              VARCHAR2(2000),
    CREATED_AT          TIMESTAMP DEFAULT SYSTIMESTAMP
)
"""
    ddl_view_chain = f"""
CREATE TABLE {schema_prefix}{REPORT_DB_TABLES['view_chain']} (
    REPORT_ID           VARCHAR2(64) NOT NULL,
    VIEW_SCHEMA         VARCHAR2(128),
    VIEW_NAME           VARCHAR2(256),
    CHAIN_ID            NUMBER,
    NODE_INDEX          NUMBER,
    NODE_SCHEMA         VARCHAR2(128),
    NODE_NAME           VARCHAR2(256),
    NODE_TYPE           VARCHAR2(32),
    NODE_EXISTS         VARCHAR2(16),
    GRANT_STATUS        VARCHAR2(32),
    CYCLE_FLAG          NUMBER(1) DEFAULT 0,
    CREATED_AT          TIMESTAMP DEFAULT SYSTIMESTAMP
)
"""
    ddl_remap_conflict = f"""
CREATE TABLE {schema_prefix}{REPORT_DB_TABLES['remap_conflict']} (
    REPORT_ID           VARCHAR2(64) NOT NULL,
    SOURCE_SCHEMA       VARCHAR2(128),
    SOURCE_NAME         VARCHAR2(256),
    OBJECT_TYPE         VARCHAR2(32),
    REASON              VARCHAR2(2000),
    CANDIDATES          CLOB,
    CREATED_AT          TIMESTAMP DEFAULT SYSTIMESTAMP
)
"""
    ddl_object_mapping = f"""
CREATE TABLE {schema_prefix}{REPORT_DB_TABLES['object_mapping']} (
    REPORT_ID           VARCHAR2(64) NOT NULL,
    SRC_SCHEMA          VARCHAR2(128),
    SRC_NAME            VARCHAR2(256),
    OBJECT_TYPE         VARCHAR2(32),
    TGT_SCHEMA          VARCHAR2(128),
    TGT_NAME            VARCHAR2(256),
    MAP_SOURCE          VARCHAR2(32),
    CREATED_AT          TIMESTAMP DEFAULT SYSTIMESTAMP
)
"""
    ddl_blacklist = f"""
CREATE TABLE {schema_prefix}{REPORT_DB_TABLES['blacklist']} (
    REPORT_ID           VARCHAR2(64) NOT NULL,
    SCHEMA_NAME         VARCHAR2(128),
    TABLE_NAME          VARCHAR2(256),
    BLACK_TYPE          VARCHAR2(64),
    DATA_TYPE           VARCHAR2(128),
    STATUS              VARCHAR2(32),
    REASON              VARCHAR2(1000),
    DETAIL              VARCHAR2(2000),
    CREATED_AT          TIMESTAMP DEFAULT SYSTIMESTAMP
)
"""
    ddl_fixup_skip = f"""
CREATE TABLE {schema_prefix}{REPORT_DB_TABLES['fixup_skip']} (
    REPORT_ID           VARCHAR2(64) NOT NULL,
    OBJECT_TYPE         VARCHAR2(32),
    MISSING_TOTAL       NUMBER DEFAULT 0,
    TASK_TOTAL          NUMBER DEFAULT 0,
    GENERATED_TOTAL     NUMBER DEFAULT 0,
    SKIP_REASON         VARCHAR2(128),
    SKIP_COUNT          NUMBER DEFAULT 0,
    CREATED_AT          TIMESTAMP DEFAULT SYSTIMESTAMP
)
"""
    ddl_oms_missing = f"""
CREATE TABLE {schema_prefix}{REPORT_DB_TABLES['oms_missing']} (
    REPORT_ID           VARCHAR2(64) NOT NULL,
    TARGET_SCHEMA       VARCHAR2(128),
    OBJECT_TYPE         VARCHAR2(32),
    SRC_SCHEMA          VARCHAR2(128),
    SRC_NAME            VARCHAR2(256),
    TGT_SCHEMA          VARCHAR2(128),
    TGT_NAME            VARCHAR2(256),
    CREATED_AT          TIMESTAMP DEFAULT SYSTIMESTAMP
)
"""
    ddl_write_errors = f"""
CREATE TABLE {schema_prefix}{REPORT_DB_TABLES['write_errors']} (
    ERROR_ID            VARCHAR2(64) NOT NULL,
    REPORT_ID           VARCHAR2(64),
    TABLE_NAME          VARCHAR2(128),
    SQL_SNIPPET         CLOB,
    ERROR_MESSAGE       CLOB,
    CREATED_AT          TIMESTAMP DEFAULT SYSTIMESTAMP,
    CONSTRAINT PK_DIFF_REPORT_WRITE_ERRORS PRIMARY KEY (ERROR_ID)
)
"""
    ddl_resolution = f"""
CREATE TABLE {schema_prefix}{REPORT_DB_TABLES['resolution']} (
    REPORT_ID           VARCHAR2(64) NOT NULL,
    OBJECT_TYPE         VARCHAR2(32) NOT NULL,
    SCHEMA_NAME         VARCHAR2(128) NOT NULL,
    OBJECT_NAME         VARCHAR2(256) NOT NULL,
    ACTION_TYPE         VARCHAR2(64) NOT NULL,
    RESOLUTION_STATUS   VARCHAR2(32) DEFAULT 'OPEN',
    RESOLVED_BY         VARCHAR2(128),
    RESOLVED_AT         TIMESTAMP,
    NOTE                VARCHAR2(1000),
    CREATED_AT          TIMESTAMP DEFAULT SYSTIMESTAMP,
    CONSTRAINT PK_DIFF_REPORT_RESOLUTION PRIMARY KEY (REPORT_ID, OBJECT_TYPE, SCHEMA_NAME, OBJECT_NAME, ACTION_TYPE)
)
"""

    ddl_index_sqls = [
        ("IDX_DIFF_REPORT_TS",
         f"CREATE INDEX IDX_DIFF_REPORT_TS ON {schema_prefix}{REPORT_DB_TABLES['summary']}(RUN_TIMESTAMP)"),
        ("IDX_DIFF_REPORT_DATE",
         f"CREATE INDEX IDX_DIFF_REPORT_DATE ON {schema_prefix}{REPORT_DB_TABLES['summary']}(RUN_DATE)"),
        ("IDX_DIFF_REPORT_CONCLUSION",
         f"CREATE INDEX IDX_DIFF_REPORT_CONCLUSION ON {schema_prefix}{REPORT_DB_TABLES['summary']}(CONCLUSION)"),
        ("IDX_DIFF_DETAIL_REPORT_ID",
         f"CREATE INDEX IDX_DIFF_DETAIL_REPORT_ID ON {schema_prefix}{REPORT_DB_TABLES['detail']}(REPORT_ID)"),
        ("IDX_DIFF_DETAIL_TYPE",
         f"CREATE INDEX IDX_DIFF_DETAIL_TYPE ON {schema_prefix}{REPORT_DB_TABLES['detail']}(REPORT_TYPE, OBJECT_TYPE)"),
        ("IDX_DIFF_DETAIL_ITEM_REPORT_ID",
         f"CREATE INDEX IDX_DIFF_DETAIL_ITEM_REPORT_ID ON {schema_prefix}{REPORT_DB_TABLES['detail_item']}(REPORT_ID)"),
        ("IDX_DIFF_DETAIL_ITEM_TYPE",
         f"CREATE INDEX IDX_DIFF_DETAIL_ITEM_TYPE ON {schema_prefix}{REPORT_DB_TABLES['detail_item']}(REPORT_TYPE, OBJECT_TYPE, ITEM_TYPE)"),
        ("IDX_DIFF_GRANT_REPORT_ID",
         f"CREATE INDEX IDX_DIFF_GRANT_REPORT_ID ON {schema_prefix}{REPORT_DB_TABLES['grants']}(REPORT_ID)"),
        ("IDX_DIFF_COUNTS_REPORT_ID",
         f"CREATE INDEX IDX_DIFF_COUNTS_REPORT_ID ON {schema_prefix}{REPORT_DB_TABLES['counts']}(REPORT_ID)"),
        ("IDX_DIFF_USABILITY_REPORT_ID",
         f"CREATE INDEX IDX_DIFF_USABILITY_REPORT_ID ON {schema_prefix}{REPORT_DB_TABLES['usability']}(REPORT_ID)"),
        ("IDX_DIFF_USABILITY_TYPE",
         f"CREATE INDEX IDX_DIFF_USABILITY_TYPE ON {schema_prefix}{REPORT_DB_TABLES['usability']}(OBJECT_TYPE)"),
        ("IDX_DIFF_PKG_REPORT_ID",
         f"CREATE INDEX IDX_DIFF_PKG_REPORT_ID ON {schema_prefix}{REPORT_DB_TABLES['package_compare']}(REPORT_ID)"),
        ("IDX_DIFF_PKG_OBJ",
         f"CREATE INDEX IDX_DIFF_PKG_OBJ ON {schema_prefix}{REPORT_DB_TABLES['package_compare']}(OBJECT_TYPE, SCHEMA_NAME)"),
        ("IDX_DIFF_TRG_REPORT_ID",
         f"CREATE INDEX IDX_DIFF_TRG_REPORT_ID ON {schema_prefix}{REPORT_DB_TABLES['trigger_status']}(REPORT_ID)"),
        ("IDX_DIFF_ARTIFACT_REPORT_ID",
         f"CREATE INDEX IDX_DIFF_ARTIFACT_REPORT_ID ON {schema_prefix}{REPORT_DB_TABLES['artifact']}(REPORT_ID)"),
        ("IDX_DIFF_ARTIFACT_TYPE",
         f"CREATE INDEX IDX_DIFF_ARTIFACT_TYPE ON {schema_prefix}{REPORT_DB_TABLES['artifact']}(ARTIFACT_TYPE)"),
        ("IDX_DIFF_ART_LINE_REPORT_ID",
         f"CREATE INDEX IDX_DIFF_ART_LINE_REPORT_ID ON {schema_prefix}{REPORT_DB_TABLES['artifact_line']}(REPORT_ID)"),
        ("IDX_DIFF_ART_LINE_FILE",
         f"CREATE INDEX IDX_DIFF_ART_LINE_FILE ON {schema_prefix}{REPORT_DB_TABLES['artifact_line']}(REPORT_ID, FILE_PATH, LINE_NO)"),
        ("IDX_DIFF_DEP_REPORT_ID",
         f"CREATE INDEX IDX_DIFF_DEP_REPORT_ID ON {schema_prefix}{REPORT_DB_TABLES['dependency']}(REPORT_ID)"),
        ("IDX_DIFF_DEP_TYPE",
         f"CREATE INDEX IDX_DIFF_DEP_TYPE ON {schema_prefix}{REPORT_DB_TABLES['dependency']}(DEP_TYPE, REF_TYPE)"),
        ("IDX_DIFF_VIEW_CHAIN_REPORT_ID",
         f"CREATE INDEX IDX_DIFF_VIEW_CHAIN_REPORT_ID ON {schema_prefix}{REPORT_DB_TABLES['view_chain']}(REPORT_ID)"),
        ("IDX_DIFF_VIEW_CHAIN_VIEW",
         f"CREATE INDEX IDX_DIFF_VIEW_CHAIN_VIEW ON {schema_prefix}{REPORT_DB_TABLES['view_chain']}(VIEW_SCHEMA, VIEW_NAME)"),
        ("IDX_DIFF_REMAP_REPORT_ID",
         f"CREATE INDEX IDX_DIFF_REMAP_REPORT_ID ON {schema_prefix}{REPORT_DB_TABLES['remap_conflict']}(REPORT_ID)"),
        ("IDX_DIFF_REMAP_SRC",
         f"CREATE INDEX IDX_DIFF_REMAP_SRC ON {schema_prefix}{REPORT_DB_TABLES['remap_conflict']}(SOURCE_SCHEMA, SOURCE_NAME)"),
        ("IDX_DIFF_MAP_REPORT_ID",
         f"CREATE INDEX IDX_DIFF_MAP_REPORT_ID ON {schema_prefix}{REPORT_DB_TABLES['object_mapping']}(REPORT_ID)"),
        ("IDX_DIFF_MAP_SRC",
         f"CREATE INDEX IDX_DIFF_MAP_SRC ON {schema_prefix}{REPORT_DB_TABLES['object_mapping']}(SRC_SCHEMA, SRC_NAME)"),
        ("IDX_DIFF_BLACKLIST_REPORT_ID",
         f"CREATE INDEX IDX_DIFF_BLACKLIST_REPORT_ID ON {schema_prefix}{REPORT_DB_TABLES['blacklist']}(REPORT_ID)"),
        ("IDX_DIFF_BLACKLIST_TABLE",
         f"CREATE INDEX IDX_DIFF_BLACKLIST_TABLE ON {schema_prefix}{REPORT_DB_TABLES['blacklist']}(SCHEMA_NAME, TABLE_NAME)"),
        ("IDX_DIFF_FIXUP_SKIP_REPORT_ID",
         f"CREATE INDEX IDX_DIFF_FIXUP_SKIP_REPORT_ID ON {schema_prefix}{REPORT_DB_TABLES['fixup_skip']}(REPORT_ID)"),
        ("IDX_DIFF_OMS_MISSING_REPORT_ID",
         f"CREATE INDEX IDX_DIFF_OMS_MISSING_REPORT_ID ON {schema_prefix}{REPORT_DB_TABLES['oms_missing']}(REPORT_ID)"),
        ("IDX_DIFF_WRITE_ERRORS_REPORT_ID",
         f"CREATE INDEX IDX_DIFF_WRITE_ERRORS_REPORT_ID ON {schema_prefix}{REPORT_DB_TABLES['write_errors']}(REPORT_ID)"),
        ("IDX_DIFF_WRITE_ERRORS_TABLE",
         f"CREATE INDEX IDX_DIFF_WRITE_ERRORS_TABLE ON {schema_prefix}{REPORT_DB_TABLES['write_errors']}(TABLE_NAME)"),
        ("IDX_DIFF_RESOLUTION_REPORT_ID",
         f"CREATE INDEX IDX_DIFF_RESOLUTION_REPORT_ID ON {schema_prefix}{REPORT_DB_TABLES['resolution']}(REPORT_ID)"),
        ("IDX_DIFF_RESOLUTION_OBJECT",
         f"CREATE INDEX IDX_DIFF_RESOLUTION_OBJECT ON {schema_prefix}{REPORT_DB_TABLES['resolution']}(OBJECT_TYPE, SCHEMA_NAME, OBJECT_NAME)"),
    ]

    to_create: List[Tuple[str, str]] = []
    if REPORT_DB_TABLES["summary"] not in existing:
        to_create.append(("summary", ddl_summary))
    if REPORT_DB_TABLES["detail"] not in existing:
        to_create.append(("detail", ddl_detail))
    if REPORT_DB_TABLES["detail_item"] not in existing:
        to_create.append(("detail_item", ddl_detail_item))
    if REPORT_DB_TABLES["grants"] not in existing:
        to_create.append(("grants", ddl_grants))
    if REPORT_DB_TABLES["counts"] not in existing:
        to_create.append(("counts", ddl_counts))
    if REPORT_DB_TABLES["usability"] not in existing:
        to_create.append(("usability", ddl_usability))
    if REPORT_DB_TABLES["package_compare"] not in existing:
        to_create.append(("package_compare", ddl_package_compare))
    if REPORT_DB_TABLES["trigger_status"] not in existing:
        to_create.append(("trigger_status", ddl_trigger_status))
    if REPORT_DB_TABLES["artifact"] not in existing:
        to_create.append(("artifact", ddl_artifact))
    if REPORT_DB_TABLES["artifact_line"] not in existing:
        to_create.append(("artifact_line", ddl_artifact_line))
    if REPORT_DB_TABLES["dependency"] not in existing:
        to_create.append(("dependency", ddl_dependency))
    if REPORT_DB_TABLES["view_chain"] not in existing:
        to_create.append(("view_chain", ddl_view_chain))
    if REPORT_DB_TABLES["remap_conflict"] not in existing:
        to_create.append(("remap_conflict", ddl_remap_conflict))
    if REPORT_DB_TABLES["object_mapping"] not in existing:
        to_create.append(("object_mapping", ddl_object_mapping))
    if REPORT_DB_TABLES["blacklist"] not in existing:
        to_create.append(("blacklist", ddl_blacklist))
    if REPORT_DB_TABLES["fixup_skip"] not in existing:
        to_create.append(("fixup_skip", ddl_fixup_skip))
    if REPORT_DB_TABLES["oms_missing"] not in existing:
        to_create.append(("oms_missing", ddl_oms_missing))
    if REPORT_DB_TABLES["write_errors"] not in existing:
        to_create.append(("write_errors", ddl_write_errors))
    if REPORT_DB_TABLES["resolution"] not in existing:
        to_create.append(("resolution", ddl_resolution))

    fk_constraints = [
        (REPORT_DB_TABLES["detail"], "FK_DIFF_REPORT_DETAIL_SUMMARY"),
        (REPORT_DB_TABLES["grants"], "FK_DIFF_REPORT_GRANT_SUMMARY"),
        (REPORT_DB_TABLES["counts"], "FK_DIFF_REPORT_COUNTS_SUMMARY"),
        (REPORT_DB_TABLES["usability"], "FK_DIFF_REPORT_USABILITY_SUMMARY"),
        (REPORT_DB_TABLES["package_compare"], "FK_DIFF_REPORT_PKG_SUMMARY"),
        (REPORT_DB_TABLES["trigger_status"], "FK_DIFF_REPORT_TRG_SUMMARY"),
    ]
    constraint_owner_clause = f"OWNER = '{schema}'" if schema else "OWNER = USER"
    for table_name, constraint_name in fk_constraints:
        if table_name not in existing:
            continue
        check_sql = (
            "SELECT CONSTRAINT_NAME FROM ALL_CONSTRAINTS "
            f"WHERE {constraint_owner_clause} "
            f"AND TABLE_NAME = '{table_name}' "
            f"AND CONSTRAINT_NAME = '{constraint_name}'"
        )
        ok_check, out_check, err_check = obclient_run_sql(ob_cfg, check_sql)
        if not ok_check:
            log.warning("[REPORT_DB] 检查外键 %s.%s 失败: %s", table_name, constraint_name, err_check)
            continue
        exists = any(line.strip() for line in (out_check or "").splitlines())
        if not exists:
            continue
        drop_sql = (
            f"ALTER TABLE {schema_prefix}{table_name} "
            f"DROP CONSTRAINT {constraint_name}"
        )
        ok_drop, _o, err_drop = obclient_run_sql(ob_cfg, drop_sql)
        if not ok_drop:
            log.warning("[REPORT_DB] 删除外键 %s.%s 失败: %s", table_name, constraint_name, err_drop)

    if not to_create:
        return True, ""

    for name, ddl in to_create:
        ok, _out, err = obclient_run_sql(ob_cfg, ddl)
        if not ok:
            return False, f"创建报告表失败({name}): {err}"

    if ddl_index_sqls:
        idx_names = "', '".join(name for name, _ in ddl_index_sqls)
        idx_check_sql = (
            "SELECT INDEX_NAME FROM ALL_INDEXES "
            f"WHERE {owner_clause} AND INDEX_NAME IN ('{idx_names}')"
        )
        ok_idx, idx_out, _idx_err = obclient_run_sql(ob_cfg, idx_check_sql)
        existing_indexes = {
            line.strip().upper() for line in (idx_out.splitlines() if idx_out else []) if line.strip()
        } if ok_idx else set()
        for idx_name, idx_sql in ddl_index_sqls:
            if idx_name.upper() in existing_indexes:
                continue
            obclient_run_sql(ob_cfg, idx_sql)

    return True, ""


def _build_report_detail_rows(
    settings: Dict,
    tv_results: ReportResults,
    extra_results: ExtraCheckResults,
    support_summary: Optional[SupportClassificationResult],
    table_target_map: Optional[Dict[Tuple[str, str], Tuple[str, str]]] = None,
    trigger_status_rows: Optional[List[TriggerStatusReportRow]] = None,
    constraint_status_rows: Optional[List[ConstraintStatusDriftRow]] = None,
    trigger_validity_mode: str = "off",
) -> Tuple[List[Dict[str, object]], bool, int]:
    detail_modes = settings.get("report_db_detail_mode_set") or set(DEFAULT_REPORT_DB_DETAIL_MODES)
    max_rows = int(settings.get("report_db_detail_max_rows", 0) or 0)
    rows: List[Dict[str, object]] = []

    missing_supported: List[ObjectSupportReportRow] = []
    unsupported_rows: List[ObjectSupportReportRow] = []
    if support_summary:
        missing_supported = [
            row for row in support_summary.missing_detail_rows
            if row.support_state == SUPPORT_STATE_SUPPORTED
        ]
        unsupported_rows = [
            row for row in support_summary.unsupported_rows
            if row.support_state in {SUPPORT_STATE_UNSUPPORTED, SUPPORT_STATE_BLOCKED}
        ]

    mismatch_rows = tv_results.get("mismatched", []) or []
    ok_rows = tv_results.get("ok", []) or []
    skipped_rows = tv_results.get("skipped", []) or []

    extra_index_unsupported = extra_results.get("index_unsupported", []) or []
    extra_constraint_unsupported = extra_results.get("constraint_unsupported", []) or []
    extra_index_mismatched = extra_results.get("index_mismatched", []) or []
    extra_constraint_mismatched = extra_results.get("constraint_mismatched", []) or []
    extra_sequence_mismatched = extra_results.get("sequence_mismatched", []) or []
    extra_trigger_mismatched = extra_results.get("trigger_mismatched", []) or []
    if trigger_status_rows is None:
        trigger_status_rows = list(extra_results.get("trigger_status_drift", []) or [])
    if constraint_status_rows is None:
        constraint_status_rows = list(extra_results.get("constraint_status_drift", []) or [])

    total_count = 0
    if "missing" in detail_modes:
        total_count += len(missing_supported) or 0
    if "unsupported" in detail_modes:
        total_count += len(unsupported_rows) or 0
        total_count += len(extra_index_unsupported) + len(extra_constraint_unsupported)
    if "mismatched" in detail_modes:
        total_count += len(mismatch_rows)
        total_count += len(extra_index_mismatched) + len(extra_constraint_mismatched)
        total_count += len(extra_sequence_mismatched) + len(extra_trigger_mismatched)
        total_count += len(trigger_status_rows or []) + len(constraint_status_rows or [])
    if "ok" in detail_modes:
        total_count += len(ok_rows)
    if "skipped" in detail_modes:
        total_count += len(skipped_rows)

    truncated = False
    truncated_count = 0

    def _push(row: Dict[str, object]) -> None:
        nonlocal truncated, truncated_count
        if max_rows and len(rows) >= max_rows:
            truncated = True
            truncated_count += 1
            return
        rows.append(row)

    if "missing" in detail_modes:
        for row in missing_supported:
            src_schema, src_name = parse_full_object_name(row.src_full) or ("", "")
            tgt_schema, tgt_name = parse_full_object_name(row.tgt_full) or ("", "")
            _push({
                "report_type": "MISSING",
                "object_type": row.obj_type,
                "source_schema": src_schema,
                "source_name": src_name,
                "target_schema": tgt_schema,
                "target_name": tgt_name,
                "status": row.support_state,
                "reason": row.reason,
                "detail_json": json.dumps({
                    "reason_code": row.reason_code,
                    "dependency": row.dependency,
                    "action": row.action,
                    "detail": row.detail,
                    "root_cause": row.root_cause,
                }, ensure_ascii=False)
            })

    if "unsupported" in detail_modes:
        for row in unsupported_rows:
            src_schema, src_name = parse_full_object_name(row.src_full) or ("", "")
            tgt_schema, tgt_name = parse_full_object_name(row.tgt_full) or ("", "")
            _push({
                "report_type": "UNSUPPORTED",
                "object_type": row.obj_type,
                "source_schema": src_schema,
                "source_name": src_name,
                "target_schema": tgt_schema,
                "target_name": tgt_name,
                "status": row.support_state,
                "reason": row.reason,
                "detail_json": json.dumps({
                    "reason_code": row.reason_code,
                    "dependency": row.dependency,
                    "action": row.action,
                    "detail": row.detail,
                    "root_cause": row.root_cause,
                }, ensure_ascii=False)
            })
        for row in extra_index_unsupported:
            table_schema, table_name = parse_full_object_name(row.table_full) or ("", row.table_full)
            _push({
                "report_type": "UNSUPPORTED",
                "object_type": "INDEX",
                "source_schema": table_schema,
                "source_name": row.index_name,
                "target_schema": table_schema,
                "target_name": row.index_name,
                "status": row.reason_code,
                "reason": row.reason,
                "detail_json": json.dumps({
                    "table": row.table_full,
                    "columns": row.columns,
                    "ob_error_hint": row.ob_error_hint,
                }, ensure_ascii=False)
            })
        for row in extra_constraint_unsupported:
            table_schema, table_name = parse_full_object_name(row.table_full) or ("", row.table_full)
            _push({
                "report_type": "UNSUPPORTED",
                "object_type": "CONSTRAINT",
                "source_schema": table_schema,
                "source_name": row.constraint_name,
                "target_schema": table_schema,
                "target_name": row.constraint_name,
                "status": row.reason_code,
                "reason": row.reason,
                "detail_json": json.dumps({
                    "table": row.table_full,
                    "search_condition": row.search_condition,
                    "ob_error_hint": row.ob_error_hint,
                }, ensure_ascii=False)
            })

    if "mismatched" in detail_modes:
        target_to_source: Dict[str, Tuple[str, str]] = {}
        if table_target_map:
            target_to_source = {
                f"{tgt_schema}.{tgt_name}".upper(): (src_schema, src_name)
                for (src_schema, src_name), (tgt_schema, tgt_name) in table_target_map.items()
            }
        for obj_type, tgt_name, missing, extra, length_mismatches, type_mismatches in mismatch_rows:
            src_schema = ""
            src_name = ""
            if tgt_name:
                src_pair = target_to_source.get(tgt_name.upper())
                if src_pair:
                    src_schema, src_name = src_pair
            tgt_schema, tgt_obj = parse_full_object_name(tgt_name) or ("", tgt_name)
            _push({
                "report_type": "MISMATCHED",
                "object_type": obj_type,
                "source_schema": src_schema,
                "source_name": src_name,
                "target_schema": tgt_schema,
                "target_name": tgt_obj,
                "status": "MISMATCH",
                "reason": "TABLE column mismatch" if (obj_type or "").upper() == "TABLE" else "MISMATCH",
                "detail_json": json.dumps({
                    "missing_columns": sorted(list(missing)) if isinstance(missing, (set, list)) else list(missing or []),
                    "extra_columns": sorted(list(extra)) if isinstance(extra, (set, list)) else list(extra or []),
                    "length_mismatches": length_mismatches,
                    "type_mismatches": type_mismatches,
                }, ensure_ascii=False, default=str)
            })
        for row in extra_index_mismatched:
            table_schema, table_name = parse_full_object_name(row.table) or ("", row.table)
            _push({
                "report_type": "MISMATCHED",
                "object_type": "INDEX",
                "source_schema": table_schema,
                "source_name": table_name,
                "target_schema": table_schema,
                "target_name": table_name,
                "status": "MISMATCH",
                "reason": "INDEX mismatch",
                "detail_json": json.dumps({
                    "missing_indexes": sorted(list(row.missing_indexes)),
                    "extra_indexes": sorted(list(row.extra_indexes)),
                    "detail": row.detail_mismatch,
                }, ensure_ascii=False)
            })
        for row in extra_constraint_mismatched:
            table_schema, table_name = parse_full_object_name(row.table) or ("", row.table)
            _push({
                "report_type": "MISMATCHED",
                "object_type": "CONSTRAINT",
                "source_schema": table_schema,
                "source_name": table_name,
                "target_schema": table_schema,
                "target_name": table_name,
                "status": "MISMATCH",
                "reason": "CONSTRAINT mismatch",
                "detail_json": json.dumps({
                    "missing_constraints": sorted(list(row.missing_constraints)),
                    "extra_constraints": sorted(list(row.extra_constraints)),
                    "detail": row.detail_mismatch,
                    "downgraded_pk": sorted(list(row.downgraded_pk_constraints)),
                }, ensure_ascii=False)
            })
        for row in extra_sequence_mismatched:
            _push({
                "report_type": "MISMATCHED",
                "object_type": "SEQUENCE",
                "source_schema": row.src_schema,
                "source_name": "",
                "target_schema": row.tgt_schema,
                "target_name": "",
                "status": "MISMATCH",
                "reason": row.note or "SEQUENCE mismatch",
                "detail_json": json.dumps({
                    "missing_sequences": sorted(list(row.missing_sequences)),
                    "extra_sequences": sorted(list(row.extra_sequences)),
                    "missing_mappings": row.missing_mappings,
                    "detail": row.detail_mismatch,
                }, ensure_ascii=False, default=str)
            })
        for row in extra_trigger_mismatched:
            table_schema, table_name = parse_full_object_name(row.table) or ("", row.table)
            _push({
                "report_type": "MISMATCHED",
                "object_type": "TRIGGER",
                "source_schema": table_schema,
                "source_name": table_name,
                "target_schema": table_schema,
                "target_name": table_name,
                "status": "MISMATCH",
                "reason": "TRIGGER mismatch",
                "detail_json": json.dumps({
                    "missing_triggers": sorted(list(row.missing_triggers)),
                    "extra_triggers": sorted(list(row.extra_triggers)),
                    "detail": row.detail_mismatch,
                    "missing_mappings": row.missing_mappings,
                }, ensure_ascii=False, default=str)
            })
        for row in trigger_status_rows or []:
            tgt_schema, tgt_name = parse_full_object_name(row.trigger_full or "") or ("", row.trigger_full or "")
            _push({
                "report_type": "MISMATCHED",
                "object_type": "TRIGGER",
                "source_schema": tgt_schema,
                "source_name": tgt_name,
                "target_schema": tgt_schema,
                "target_name": tgt_name,
                "status": "STATUS_DRIFT",
                "reason": "TRIGGER status drift",
                "detail_json": json.dumps({
                    "src_event": row.src_event,
                    "tgt_event": row.tgt_event,
                    "src_enabled": row.src_enabled,
                    "tgt_enabled": row.tgt_enabled,
                    "src_valid": row.src_valid,
                    "tgt_valid": row.tgt_valid,
                    "detail": row.detail,
                    "action_sql": build_trigger_status_fixup_sqls(row, trigger_validity_mode),
                }, ensure_ascii=False, default=str)
            })
        for row in constraint_status_rows or []:
            tgt_schema, tgt_table = parse_full_object_name(row.table_full or "") or ("", row.table_full or "")
            _push({
                "report_type": "MISMATCHED",
                "object_type": "CONSTRAINT",
                "source_schema": tgt_schema,
                "source_name": row.src_constraint,
                "target_schema": tgt_schema,
                "target_name": row.tgt_constraint,
                "status": "STATUS_DRIFT",
                "reason": "CONSTRAINT status drift",
                "detail_json": json.dumps({
                    "table": row.table_full,
                    "constraint_type": row.constraint_type,
                    "src_status": row.src_status,
                    "tgt_status": row.tgt_status,
                    "src_validated": row.src_validated,
                    "tgt_validated": row.tgt_validated,
                    "detail": row.detail,
                    "action_sql": row.action_sql,
                }, ensure_ascii=False, default=str)
            })

    if "ok" in detail_modes:
        for obj_type, tgt_name in ok_rows:
            tgt_schema, tgt_obj = parse_full_object_name(tgt_name) or ("", tgt_name)
            _push({
                "report_type": "OK",
                "object_type": obj_type,
                "source_schema": "",
                "source_name": "",
                "target_schema": tgt_schema,
                "target_name": tgt_obj,
                "status": "OK",
                "reason": "",
                "detail_json": None,
            })

    if "skipped" in detail_modes:
        for obj_type, tgt_name, src_name, reason in skipped_rows:
            src_schema, src_obj = parse_full_object_name(src_name) or ("", src_name)
            tgt_schema, tgt_obj = parse_full_object_name(tgt_name) or ("", tgt_name)
            _push({
                "report_type": "SKIPPED",
                "object_type": obj_type,
                "source_schema": src_schema,
                "source_name": src_obj,
                "target_schema": tgt_schema,
                "target_name": tgt_obj,
                "status": "SKIPPED",
                "reason": reason or "",
                "detail_json": None,
            })

    if max_rows and total_count > max_rows:
        truncated = True
        truncated_count = max(truncated_count, total_count - max_rows)
    return rows, truncated, truncated_count


def _build_report_detail_item_rows(
    tv_results: ReportResults,
    extra_results: ExtraCheckResults,
    support_summary: Optional[SupportClassificationResult],
    table_target_map: Optional[Dict[Tuple[str, str], Tuple[str, str]]],
    max_rows: int,
    trigger_status_rows: Optional[List[TriggerStatusReportRow]] = None,
    constraint_status_rows: Optional[List[ConstraintStatusDriftRow]] = None,
    trigger_validity_mode: str = "off"
) -> Tuple[List[Dict[str, object]], bool, int]:
    rows: List[Dict[str, object]] = []
    truncated = False
    truncated_count = 0

    target_to_source: Dict[str, Tuple[str, str]] = {}
    if table_target_map:
        target_to_source = {
            f"{tgt_schema}.{tgt_name}".upper(): (src_schema, src_name)
            for (src_schema, src_name), (tgt_schema, tgt_name) in table_target_map.items()
        }
    if trigger_status_rows is None:
        trigger_status_rows = list(extra_results.get("trigger_status_drift", []) or [])
    if constraint_status_rows is None:
        constraint_status_rows = list(extra_results.get("constraint_status_drift", []) or [])

    def _push(row: Dict[str, object]) -> None:
        nonlocal truncated, truncated_count
        if max_rows and len(rows) >= max_rows:
            truncated = True
            truncated_count += 1
            return
        rows.append(row)

    def _resolve_src_for_target(target_full: str) -> Tuple[str, str]:
        if not target_full:
            return "", ""
        src_pair = target_to_source.get(target_full.upper())
        return src_pair or ("", "")

    def _split_full(full_name: str) -> Tuple[str, str]:
        parsed = parse_full_object_name(full_name)
        if parsed:
            return parsed
        return "", full_name or ""

    # 1) 支持/阻断/缺失原因（SupportClassification）
    if support_summary:
        for row in (support_summary.missing_detail_rows or []) + (support_summary.unsupported_rows or []):
            report_type = "MISSING" if row.support_state == SUPPORT_STATE_SUPPORTED else "UNSUPPORTED"
            src_schema, src_name = _split_full(row.src_full)
            tgt_schema, tgt_name = _split_full(row.tgt_full)
            base = {
                "report_type": report_type,
                "object_type": row.obj_type,
                "source_schema": src_schema,
                "source_name": src_name,
                "target_schema": tgt_schema,
                "target_name": tgt_name,
                "status": row.support_state,
            }
            if row.reason_code:
                _push({**base, "item_type": "REASON_CODE", "item_key": "", "item_value": row.reason_code})
            if row.dependency:
                _push({**base, "item_type": "DEPENDENCY", "item_key": "", "item_value": row.dependency})
            if row.action:
                _push({**base, "item_type": "ACTION", "item_key": "", "item_value": row.action})
            if row.root_cause:
                _push({**base, "item_type": "ROOT_CAUSE", "item_key": "", "item_value": row.root_cause})
            if row.detail:
                _push({**base, "item_type": "DETAIL", "item_key": "", "item_value": row.detail})

    # 2) TABLE 列差异（主对象）
    for obj_type, tgt_name, missing, extra, length_mismatches, type_mismatches in tv_results.get("mismatched", []):
        if (obj_type or "").upper() != "TABLE":
            continue
        if "获取失败" in (tgt_name or ""):
            continue
        tgt_schema, tgt_obj = _split_full(tgt_name or "")
        src_schema, src_obj = _resolve_src_for_target(tgt_name or "")
        base = {
            "report_type": "MISMATCHED",
            "object_type": "TABLE",
            "source_schema": src_schema,
            "source_name": src_obj,
            "target_schema": tgt_schema,
            "target_name": tgt_obj,
            "status": "MISMATCH",
        }
        for col in sorted(missing or []):
            _push({**base, "item_type": "MISSING_COLUMN", "item_key": col, "item_value": ""})
        for col in sorted(extra or []):
            _push({**base, "item_type": "EXTRA_COLUMN", "item_key": col, "item_value": ""})
        for col, src_len, tgt_len, limit_len, issue_type in (length_mismatches or []):
            _push({
                **base,
                "item_type": "LENGTH_MISMATCH",
                "item_key": col,
                "src_value": str(src_len),
                "tgt_value": str(tgt_len),
                "item_value": f"{issue_type}:{limit_len}"
            })
        for col, src_type, tgt_type, expected_type, issue_type in (type_mismatches or []):
            _push({
                **base,
                "item_type": "TYPE_MISMATCH",
                "item_key": col,
                "src_value": src_type,
                "tgt_value": tgt_type,
                "item_value": f"{issue_type}:{expected_type}"
            })

    # 3) INDEX 差异
    for item in extra_results.get("index_mismatched", []) or []:
        table_schema, table_name = _split_full(item.table or "")
        src_schema, src_name = _resolve_src_for_target(item.table or "")
        base = {
            "report_type": "MISMATCHED",
            "object_type": "INDEX",
            "source_schema": src_schema or table_schema,
            "source_name": src_name or table_name,
            "target_schema": table_schema,
            "target_name": table_name,
            "status": "MISMATCH",
        }
        for idx_name in sorted(item.missing_indexes or []):
            _push({**base, "item_type": "MISSING_INDEX", "item_key": idx_name, "item_value": ""})
        for idx_name in sorted(item.extra_indexes or []):
            _push({**base, "item_type": "EXTRA_INDEX", "item_key": idx_name, "item_value": ""})
        for detail in item.detail_mismatch or []:
            _push({**base, "item_type": "INDEX_DETAIL", "item_key": "", "item_value": detail})

    # 4) CONSTRAINT 差异
    for item in extra_results.get("constraint_mismatched", []) or []:
        table_schema, table_name = _split_full(item.table or "")
        src_schema, src_name = _resolve_src_for_target(item.table or "")
        base = {
            "report_type": "MISMATCHED",
            "object_type": "CONSTRAINT",
            "source_schema": src_schema or table_schema,
            "source_name": src_name or table_name,
            "target_schema": table_schema,
            "target_name": table_name,
            "status": "MISMATCH",
        }
        for name in sorted(item.missing_constraints or []):
            _push({**base, "item_type": "MISSING_CONSTRAINT", "item_key": name, "item_value": ""})
        for name in sorted(item.extra_constraints or []):
            _push({**base, "item_type": "EXTRA_CONSTRAINT", "item_key": name, "item_value": ""})
        for name in sorted(item.downgraded_pk_constraints or []):
            _push({**base, "item_type": "DOWNGRADED_PK", "item_key": name, "item_value": ""})
        for detail in item.detail_mismatch or []:
            _push({**base, "item_type": "CONSTRAINT_DETAIL", "item_key": "", "item_value": detail})

    # 5) SEQUENCE 差异
    for item in extra_results.get("sequence_mismatched", []) or []:
        base = {
            "report_type": "MISMATCHED",
            "object_type": "SEQUENCE",
            "source_schema": item.src_schema or "",
            "source_name": "",
            "target_schema": item.tgt_schema or "",
            "target_name": "",
            "status": "MISMATCH",
        }
        for name in sorted(item.missing_sequences or []):
            _push({**base, "item_type": "MISSING_SEQUENCE", "item_key": name, "item_value": ""})
        for name in sorted(item.extra_sequences or []):
            _push({**base, "item_type": "EXTRA_SEQUENCE", "item_key": name, "item_value": ""})
        for detail in item.detail_mismatch or []:
            _push({**base, "item_type": "SEQUENCE_DETAIL", "item_key": "", "item_value": detail})
        for src_full, tgt_full in (item.missing_mappings or []):
            _push({**base, "item_type": "MISSING_MAPPING", "item_key": src_full, "item_value": tgt_full})

    # 6) TRIGGER 差异
    for item in extra_results.get("trigger_mismatched", []) or []:
        table_schema, table_name = _split_full(item.table or "")
        src_schema, src_name = _resolve_src_for_target(item.table or "")
        base = {
            "report_type": "MISMATCHED",
            "object_type": "TRIGGER",
            "source_schema": src_schema or table_schema,
            "source_name": src_name or table_name,
            "target_schema": table_schema,
            "target_name": table_name,
            "status": "MISMATCH",
        }
        for name in sorted(item.missing_triggers or []):
            _push({**base, "item_type": "MISSING_TRIGGER", "item_key": name, "item_value": ""})
        for name in sorted(item.extra_triggers or []):
            _push({**base, "item_type": "EXTRA_TRIGGER", "item_key": name, "item_value": ""})
        for detail in item.detail_mismatch or []:
            _push({**base, "item_type": "TRIGGER_DETAIL", "item_key": "", "item_value": detail})

    # 6.b) TRIGGER/CONSTRAINT 状态漂移
    for row in trigger_status_rows or []:
        schema_u, trigger_u = _split_full(row.trigger_full or "")
        base = {
            "report_type": "MISMATCHED",
            "object_type": "TRIGGER",
            "source_schema": schema_u,
            "source_name": trigger_u,
            "target_schema": schema_u,
            "target_name": trigger_u,
            "status": "STATUS_DRIFT",
        }
        if row.detail:
            _push({**base, "item_type": "TRIGGER_STATUS_DRIFT", "item_key": "", "item_value": row.detail})
        _push({
            **base,
            "item_type": "TRIGGER_ENABLED",
            "item_key": "ENABLED",
            "src_value": row.src_enabled,
            "tgt_value": row.tgt_enabled,
            "item_value": "",
        })
        _push({
            **base,
            "item_type": "TRIGGER_VALID",
            "item_key": "VALID",
            "src_value": row.src_valid,
            "tgt_value": row.tgt_valid,
            "item_value": "",
        })
        action_sql = " ".join(build_trigger_status_fixup_sqls(row, trigger_validity_mode))
        if action_sql:
            _push({**base, "item_type": "ACTION_SQL", "item_key": "", "item_value": action_sql})

    for row in constraint_status_rows or []:
        schema_u, table_u = _split_full(row.table_full or "")
        base = {
            "report_type": "MISMATCHED",
            "object_type": "CONSTRAINT",
            "source_schema": schema_u,
            "source_name": row.src_constraint,
            "target_schema": schema_u,
            "target_name": row.tgt_constraint,
            "status": "STATUS_DRIFT",
        }
        if row.detail:
            _push({**base, "item_type": "CONSTRAINT_STATUS_DRIFT", "item_key": table_u, "item_value": row.detail})
        _push({
            **base,
            "item_type": "CONSTRAINT_ENABLED",
            "item_key": "ENABLED",
            "src_value": row.src_status,
            "tgt_value": row.tgt_status,
            "item_value": "",
        })
        _push({
            **base,
            "item_type": "CONSTRAINT_VALIDATED",
            "item_key": "VALIDATED",
            "src_value": row.src_validated,
            "tgt_value": row.tgt_validated,
            "item_value": "",
        })
        if row.action_sql and row.action_sql != "-":
            _push({**base, "item_type": "ACTION_SQL", "item_key": "", "item_value": row.action_sql})

    # 7) 不支持扩展对象细节
    for item in extra_results.get("index_unsupported", []) or []:
        table_schema, table_name = _split_full(item.table_full or "")
        base = {
            "report_type": "UNSUPPORTED",
            "object_type": "INDEX",
            "source_schema": table_schema,
            "source_name": table_name,
            "target_schema": table_schema,
            "target_name": table_name,
            "status": item.reason_code or "UNSUPPORTED",
        }
        detail = f"{item.reason_code}: {item.reason}" if item.reason_code else (item.reason or "")
        if item.ob_error_hint:
            detail = f"{detail} | OB_HINT={item.ob_error_hint}" if detail else f"OB_HINT={item.ob_error_hint}"
        if item.columns:
            detail = f"{detail} | COLS={item.columns}" if detail else f"COLS={item.columns}"
        _push({**base, "item_type": "UNSUPPORTED_INDEX", "item_key": item.index_name, "item_value": detail})

    for item in extra_results.get("constraint_unsupported", []) or []:
        table_schema, table_name = _split_full(item.table_full or "")
        base = {
            "report_type": "UNSUPPORTED",
            "object_type": "CONSTRAINT",
            "source_schema": table_schema,
            "source_name": table_name,
            "target_schema": table_schema,
            "target_name": table_name,
            "status": item.reason_code or "UNSUPPORTED",
        }
        detail = f"{item.reason_code}: {item.reason}" if item.reason_code else (item.reason or "")
        if item.ob_error_hint:
            detail = f"{detail} | OB_HINT={item.ob_error_hint}" if detail else f"OB_HINT={item.ob_error_hint}"
        if item.search_condition:
            detail = f"{detail} | COND={item.search_condition}" if detail else f"COND={item.search_condition}"
        _push({**base, "item_type": "UNSUPPORTED_CONSTRAINT", "item_key": item.constraint_name, "item_value": detail})

    return rows, truncated, truncated_count

def _build_report_usability_rows(
    summary: Optional[UsabilitySummary]
) -> List[Dict[str, object]]:
    if not summary:
        return []
    rows: List[Dict[str, object]] = []
    for item in summary.results:
        usable_val: Optional[int] = None
        if item.tgt_usable is True:
            usable_val = 1
        elif item.tgt_usable is False:
            usable_val = 0
        detail_json = json.dumps({
            "status": item.status,
            "src_exists": item.src_exists,
            "src_usable": item.src_usable,
            "tgt_exists": item.tgt_exists,
            "tgt_usable": item.tgt_usable,
            "src_error": item.src_error,
            "tgt_error": item.tgt_error,
            "root_cause": item.root_cause,
            "recommendation": item.recommendation,
            "src_time_ms": item.src_time_ms,
            "tgt_time_ms": item.tgt_time_ms,
        }, ensure_ascii=False)
        rows.append({
            "object_type": item.object_type,
            "schema_name": item.schema,
            "object_name": item.object_name,
            "usable": usable_val,
            "status": item.status,
            "reason": item.root_cause,
            "detail_json": detail_json
        })
    return rows


def _hash_package_compare_row(row: PackageCompareRow) -> str:
    payload = "|".join([
        row.src_full or "",
        row.obj_type or "",
        row.src_status or "",
        row.tgt_full or "",
        row.tgt_status or "",
        row.result or "",
        str(row.error_count),
        normalize_error_text(row.first_error or "")
    ])
    return hashlib.sha256(payload.encode("utf-8")).hexdigest()


def _build_report_package_compare_rows(
    package_results: Optional[PackageCompareResults],
    report_dir: Optional[Path],
    report_timestamp: Optional[str]
) -> List[Dict[str, object]]:
    if not package_results:
        return []
    rows: List[PackageCompareRow] = list(package_results.get("rows") or [])
    if not rows:
        return []
    diff_path: Optional[str] = None
    if report_dir and report_timestamp:
        report_path = Path(report_dir) / f"report_{report_timestamp}.txt"
        pkg_path = derive_package_report_path(report_path)
        if pkg_path.exists():
            diff_path = str(pkg_path.resolve())
        else:
            diff_path = str(pkg_path)
    results: List[Dict[str, object]] = []
    for row in rows:
        tgt_schema, tgt_obj = parse_full_object_name(row.tgt_full) or ("", row.tgt_full)
        summary = (
            f"RESULT={row.result}; SRC_STATUS={row.src_status}; TGT_STATUS={row.tgt_status}; "
            f"ERROR_COUNT={row.error_count}; FIRST_ERROR={normalize_error_text(row.first_error or '')}"
        )
        if len(summary) > 4000:
            summary = summary[:3997] + "..."
        results.append({
            "schema_name": tgt_schema,
            "object_name": tgt_obj,
            "object_type": row.obj_type,
            "src_status": row.src_status,
            "tgt_status": row.tgt_status,
            "diff_status": row.result,
            "diff_hash": _hash_package_compare_row(row),
            "diff_summary": summary,
            "diff_path": diff_path
        })
    return results


def _build_report_trigger_status_rows(
    status_rows: Optional[List[TriggerStatusReportRow]]
) -> List[Dict[str, object]]:
    if not status_rows:
        return []
    results: List[Dict[str, object]] = []
    for row in status_rows:
        schema, name = parse_full_object_name(row.trigger_full) or ("", row.trigger_full)
        results.append({
            "schema_name": schema,
            "trigger_name": name,
            "src_event": row.src_event,
            "tgt_event": row.tgt_event,
            "src_enabled": row.src_enabled,
            "tgt_enabled": row.tgt_enabled,
            "src_valid": row.src_valid,
            "tgt_valid": row.tgt_valid,
            "diff_status": row.detail or "MISMATCH",
            "reason": ""
        })
    return results


def _hash_file(path: Path) -> str:
    hasher = hashlib.sha256()
    with path.open("rb") as f:
        for chunk in iter(lambda: f.read(1024 * 1024), b""):
            hasher.update(chunk)
    return hasher.hexdigest()


def _scan_report_file(path: Path) -> Tuple[int, str]:
    row_count = 0
    fields = ""
    first_data_line = ""
    try:
        with path.open("r", encoding="utf-8", errors="ignore") as f:
            for line in f:
                stripped = line.strip()
                if not stripped:
                    continue
                if stripped.startswith("#"):
                    if stripped.startswith("# 字段说明:"):
                        fields = stripped.split(":", 1)[-1].strip()
                    continue
                if not first_data_line:
                    first_data_line = stripped
                    # 若首行是表头，记录字段后继续统计剩余行
                    if "|" in first_data_line:
                        fields = fields or first_data_line
                        continue
                row_count += 1
    except OSError:
        return 0, ""
    return row_count, fields


def _infer_report_artifact_type(rel_path: str) -> str:
    name = Path(rel_path).name
    if name.startswith("report_index_"):
        return "REPORT_INDEX"
    if name.startswith("report_sql_"):
        return "REPORT_SQL_TEMPLATE"
    if name.startswith("report_"):
        return "REPORT_MAIN"
    if name.startswith("missing_objects_detail_"):
        return "MISSING_DETAIL"
    if name.startswith("unsupported_objects_detail_"):
        return "UNSUPPORTED_DETAIL"
    if name.startswith("extra_mismatch_detail_"):
        return "MISMATCH_DETAIL"
    if name.startswith("status_drift_detail_"):
        return "MISMATCH_DETAIL"
    if name.startswith("constraint_validate_deferred_detail_"):
        return "MISMATCH_DETAIL"
    if name.startswith("package_compare_"):
        return "PACKAGE_COMPARE"
    if name.startswith("trigger_status_report"):
        return "TRIGGER_STATUS"
    if name.startswith("usability_check_detail_"):
        return "USABILITY_DETAIL"
    if name.startswith("dependency_chains_"):
        return "DEPENDENCY_CHAINS"
    if name.startswith("VIEWs_chain_"):
        return "VIEW_CHAIN"
    if name.startswith("object_mapping_"):
        return "OBJECT_MAPPING"
    if name.startswith("remap_conflicts_"):
        return "REMAP_CONFLICT"
    if name == "blacklist_tables.txt":
        return "BLACKLIST_TABLES"
    if name.startswith("fixup_skip_summary_"):
        return "FIXUP_SKIP_SUMMARY"
    if name.startswith("filtered_grants"):
        return "FILTERED_GRANTS"
    if "missed_tables_views_for_OMS" in rel_path:
        return "OMS_MISSING_RULES"
    return "OTHER"


def _infer_artifact_status(
    artifact_type: str,
    store_scope: str,
    detail_modes: Set[str],
    detail_truncated: bool
) -> Tuple[str, str]:
    note = ""
    if store_scope == "full":
        if artifact_type in {"MISSING_DETAIL", "MISMATCH_DETAIL", "UNSUPPORTED_DETAIL"} and detail_truncated:
            return "PARTIAL", "detail_truncated"
        return "IN_DB", ""
    if store_scope == "summary":
        if artifact_type == "REPORT_MAIN":
            return "IN_DB", ""
        return "TXT_ONLY", ""
    if artifact_type in {"MISSING_DETAIL", "MISMATCH_DETAIL", "UNSUPPORTED_DETAIL"}:
        mode_map = {
            "MISSING_DETAIL": "missing",
            "MISMATCH_DETAIL": "mismatched",
            "UNSUPPORTED_DETAIL": "unsupported"
        }
        mode_key = mode_map.get(artifact_type, "")
        if mode_key and mode_key in detail_modes and store_scope in {"core", "full"}:
            if detail_truncated:
                return "PARTIAL", "detail_truncated"
            return "IN_DB", ""
        return "TXT_ONLY", ""
    if artifact_type in {"PACKAGE_COMPARE", "TRIGGER_STATUS", "USABILITY_DETAIL", "FILTERED_GRANTS"}:
        return ("IN_DB", "") if store_scope in {"core", "full"} else ("TXT_ONLY", "")
    if artifact_type in {"DEPENDENCY_CHAINS", "VIEW_CHAIN", "REMAP_CONFLICT", "OBJECT_MAPPING",
                         "BLACKLIST_TABLES", "FIXUP_SKIP_SUMMARY", "OMS_MISSING_RULES"}:
        return ("IN_DB", "") if store_scope == "full" else ("TXT_ONLY", "")
    if artifact_type == "REPORT_INDEX":
        return "TXT_ONLY", ""
    if artifact_type == "REPORT_SQL_TEMPLATE":
        return "TXT_ONLY", ""
    return ("IN_DB", "") if store_scope == "full" else ("TXT_ONLY", "")


def _build_report_artifact_rows(
    report_dir: Optional[Path],
    store_scope: str,
    detail_modes: Set[str],
    detail_truncated: bool
) -> List[Dict[str, object]]:
    if not report_dir or not report_dir.exists():
        return []
    rows: List[Dict[str, object]] = []
    for path in sorted(report_dir.rglob("*.txt")):
        if not path.is_file():
            continue
        rel = _report_index_relpath(report_dir, path)
        artifact_type = _infer_report_artifact_type(rel)
        status, note = _infer_artifact_status(artifact_type, store_scope, detail_modes, detail_truncated)
        row_count, fields = _scan_report_file(path)
        file_hash = ""
        try:
            file_hash = _hash_file(path)
        except OSError:
            file_hash = ""
        rows.append({
            "artifact_type": artifact_type,
            "file_path": str(path.resolve()),
            "file_hash": file_hash,
            "row_count": row_count,
            "field_list": fields,
            "status": status,
            "note": note
        })
    return rows


def _iter_report_artifact_line_rows(report_dir: Optional[Path]) -> Iterator[Dict[str, object]]:
    """
    Yield line-level rows for all txt artifacts in run directory.
    Preserve line order and blank lines for exact replay in DB.
    """
    if not report_dir or not report_dir.exists():
        return
    for path in sorted(report_dir.rglob("*.txt")):
        if not path.is_file():
            continue
        rel = _report_index_relpath(report_dir, path)
        artifact_type = _infer_report_artifact_type(rel)
        abs_path = str(path.resolve())
        try:
            with path.open("r", encoding="utf-8", errors="ignore") as f:
                for line_no, raw_line in enumerate(f, start=1):
                    line_text = raw_line.rstrip("\n")
                    if line_text.endswith("\r"):
                        line_text = line_text[:-1]
                    yield {
                        "artifact_type": artifact_type,
                        "file_path": abs_path,
                        "line_no": line_no,
                        "line_text": line_text,
                    }
        except OSError as exc:
            log.warning("[REPORT_DB] 读取 artifact 文本失败 %s: %s", path, exc)


def _build_report_dependency_rows(
    dependency_report: Optional[DependencyReport],
    expected_pairs: Optional[Set[Tuple[str, str, str, str]]],
    max_rows: int,
    store_expected: bool
) -> Tuple[List[Dict[str, object]], bool, int]:
    if not dependency_report:
        return [], False, 0
    rows: List[Dict[str, object]] = []

    def _split(full: str) -> Tuple[str, str]:
        parsed = parse_full_object_name(full or "")
        if parsed:
            return parsed
        return "", (full or "")

    for issue in dependency_report.get("missing", []):
        dep_schema, dep_name = _split(issue.dependent)
        ref_schema, ref_name = _split(issue.referenced)
        rows.append({
            "dep_schema": dep_schema,
            "dep_name": dep_name,
            "dep_type": issue.dependent_type,
            "ref_schema": ref_schema,
            "ref_name": ref_name,
            "ref_type": issue.referenced_type,
            "edge_status": "MISSING",
            "reason": issue.reason
        })
    for issue in dependency_report.get("unexpected", []):
        dep_schema, dep_name = _split(issue.dependent)
        ref_schema, ref_name = _split(issue.referenced)
        rows.append({
            "dep_schema": dep_schema,
            "dep_name": dep_name,
            "dep_type": issue.dependent_type,
            "ref_schema": ref_schema,
            "ref_name": ref_name,
            "ref_type": issue.referenced_type,
            "edge_status": "UNEXPECTED",
            "reason": issue.reason
        })
    for issue in dependency_report.get("skipped", []):
        dep_schema, dep_name = _split(issue.dependent)
        ref_schema, ref_name = _split(issue.referenced)
        rows.append({
            "dep_schema": dep_schema,
            "dep_name": dep_name,
            "dep_type": issue.dependent_type,
            "ref_schema": ref_schema,
            "ref_name": ref_name,
            "ref_type": issue.referenced_type,
            "edge_status": "SKIPPED",
            "reason": issue.reason
        })

    if store_expected and expected_pairs:
        for dep_full, dep_type, ref_full, ref_type in sorted(expected_pairs):
            dep_schema, dep_name = _split(dep_full)
            ref_schema, ref_name = _split(ref_full)
            rows.append({
                "dep_schema": dep_schema,
                "dep_name": dep_name,
                "dep_type": dep_type,
                "ref_schema": ref_schema,
                "ref_name": ref_name,
                "ref_type": ref_type,
                "edge_status": "EXPECTED",
                "reason": ""
            })

    truncated = False
    truncated_count = 0
    if max_rows and len(rows) > max_rows:
        truncated = True
        truncated_count = len(rows) - max_rows
        rows = rows[:max_rows]
    return rows, truncated, truncated_count


def _build_report_view_chain_rows(
    view_chain_file: Optional[Path],
    max_rows: int
) -> Tuple[List[Dict[str, object]], bool, int]:
    if not view_chain_file or not view_chain_file.exists():
        return [], False, 0
    rows: List[Dict[str, object]] = []
    chain_id = 0
    in_cycles = False
    node_re = re.compile(r"(?P<name>[^\[]+)\[(?P<meta>[^\]]+)\]")
    try:
        with view_chain_file.open("r", encoding="utf-8", errors="ignore") as f:
            for line in f:
                raw = line.strip()
                if not raw or raw.startswith("#"):
                    continue
                if raw.startswith("[CYCLES]"):
                    in_cycles = True
                    continue
                if raw.startswith("- "):
                    raw = raw[2:].strip()
                m_idx = re.match(r"^(\d+)\.\s+(.*)$", raw)
                if m_idx:
                    chain_id = int(m_idx.group(1))
                    raw = m_idx.group(2).strip()
                elif chain_id == 0:
                    chain_id += 1
                parts = [p.strip() for p in raw.split("->") if p.strip()]
                if not parts:
                    continue
                view_schema, view_name = "", ""
                first_match = node_re.search(parts[0])
                if first_match:
                    view_full = first_match.group("name").strip()
                    parsed = parse_full_object_name(view_full)
                    if parsed:
                        view_schema, view_name = parsed
                    else:
                        view_name = view_full
                for idx, part in enumerate(parts, 1):
                    match = node_re.search(part)
                    if not match:
                        continue
                    full = match.group("name").strip()
                    meta = match.group("meta").split("|")
                    node_type = meta[0].strip().upper() if len(meta) > 0 else ""
                    node_exists = meta[1].strip().upper() if len(meta) > 1 else ""
                    grant_status = meta[2].strip().upper() if len(meta) > 2 else ""
                    node_schema, node_name = "", ""
                    parsed = parse_full_object_name(full)
                    if parsed:
                        node_schema, node_name = parsed
                    else:
                        node_name = full
                    rows.append({
                        "view_schema": view_schema,
                        "view_name": view_name,
                        "chain_id": chain_id,
                        "node_index": idx,
                        "node_schema": node_schema,
                        "node_name": node_name,
                        "node_type": node_type,
                        "node_exists": node_exists,
                        "grant_status": grant_status,
                        "cycle_flag": 1 if in_cycles else 0
                    })
    except OSError:
        return [], False, 0

    truncated = False
    truncated_count = 0
    if max_rows and len(rows) > max_rows:
        truncated = True
        truncated_count = len(rows) - max_rows
        rows = rows[:max_rows]
    return rows, truncated, truncated_count


def _build_report_remap_conflict_rows(
    remap_conflicts: Optional[List[Tuple[str, str, str]]],
    max_rows: int
) -> Tuple[List[Dict[str, object]], bool, int]:
    if not remap_conflicts:
        return [], False, 0
    rows: List[Dict[str, object]] = []
    for obj_type, src_full, reason in remap_conflicts:
        src_schema, src_name = parse_full_object_name(src_full) or ("", src_full)
        rows.append({
            "source_schema": src_schema,
            "source_name": src_name,
            "object_type": obj_type,
            "reason": reason,
            "candidates": None
        })
    truncated = False
    truncated_count = 0
    if max_rows and len(rows) > max_rows:
        truncated = True
        truncated_count = len(rows) - max_rows
        rows = rows[:max_rows]
    return rows, truncated, truncated_count


def _build_report_object_mapping_rows(
    full_object_mapping: Optional[FullObjectMapping],
    remap_rules: Optional[RemapRules],
    max_rows: int
) -> Tuple[List[Dict[str, object]], bool, int]:
    if not full_object_mapping:
        return [], False, 0
    rows: List[Dict[str, object]] = []
    rule_map = remap_rules or {}
    for src_full, type_map in full_object_mapping.items():
        src_schema, src_name = parse_full_object_name(src_full) or ("", src_full)
        for obj_type, tgt_full in (type_map or {}).items():
            tgt_schema, tgt_name = parse_full_object_name(tgt_full) or ("", tgt_full)
            map_source = "identity"
            if src_full and rule_map.get(src_full.upper()):
                map_source = "rule"
            elif src_schema and tgt_schema and src_schema != tgt_schema:
                map_source = "inferred"
            rows.append({
                "src_schema": src_schema,
                "src_name": src_name,
                "object_type": obj_type,
                "tgt_schema": tgt_schema,
                "tgt_name": tgt_name,
                "map_source": map_source
            })
    truncated = False
    truncated_count = 0
    if max_rows and len(rows) > max_rows:
        truncated = True
        truncated_count = len(rows) - max_rows
        rows = rows[:max_rows]
    return rows, truncated, truncated_count


def _build_report_blacklist_rows(
    blacklist_rows: Optional[List[BlacklistReportRow]],
    max_rows: int
) -> Tuple[List[Dict[str, object]], bool, int]:
    if not blacklist_rows:
        return [], False, 0
    rows: List[Dict[str, object]] = []
    for row in blacklist_rows:
        rows.append({
            "schema_name": row.schema,
            "table_name": row.table,
            "black_type": row.black_type,
            "data_type": row.data_type,
            "status": row.status,
            "reason": row.reason,
            "detail": row.detail
        })
    truncated = False
    truncated_count = 0
    if max_rows and len(rows) > max_rows:
        truncated = True
        truncated_count = len(rows) - max_rows
        rows = rows[:max_rows]
    return rows, truncated, truncated_count


def _build_report_fixup_skip_rows(
    fixup_skip_summary: Optional[Dict[str, Dict[str, object]]],
    max_rows: int
) -> Tuple[List[Dict[str, object]], bool, int]:
    if not fixup_skip_summary:
        return [], False, 0
    rows: List[Dict[str, object]] = []
    for obj_type, summary in fixup_skip_summary.items():
        missing_total = int(summary.get("missing_total", 0) or 0)
        task_total = int(summary.get("task_total", 0) or 0)
        generated = int(summary.get("generated", 0) or 0)
        skipped = summary.get("skipped", {}) or {}
        if not skipped:
            rows.append({
                "object_type": obj_type,
                "missing_total": missing_total,
                "task_total": task_total,
                "generated_total": generated,
                "skip_reason": "",
                "skip_count": 0
            })
        else:
            for reason, count in skipped.items():
                rows.append({
                    "object_type": obj_type,
                    "missing_total": missing_total,
                    "task_total": task_total,
                    "generated_total": generated,
                    "skip_reason": str(reason),
                    "skip_count": int(count or 0)
                })
    truncated = False
    truncated_count = 0
    if max_rows and len(rows) > max_rows:
        truncated = True
        truncated_count = len(rows) - max_rows
        rows = rows[:max_rows]
    return rows, truncated, truncated_count


def _build_report_oms_missing_rows(
    tv_results: Optional[ReportResults],
    support_state_map: Optional[Dict[Tuple[str, str], ObjectSupportReportRow]],
    blacklisted_tables: Optional[Set[Tuple[str, str]]],
    max_rows: int
) -> Tuple[List[Dict[str, object]], bool, int]:
    if not tv_results:
        return [], False, 0
    rows: List[Dict[str, object]] = []
    support_state_map = support_state_map or {}
    for obj_type, tgt_name, src_name in tv_results.get("missing", []):
        obj_type_u = (obj_type or "").upper()
        if obj_type_u not in ("TABLE", "VIEW"):
            continue
        if support_state_map and "." in src_name:
            src_full = src_name.upper()
            support_row = support_state_map.get((obj_type_u, src_full))
            if support_row and support_row.support_state != SUPPORT_STATE_SUPPORTED:
                continue
        if obj_type_u == "TABLE" and blacklisted_tables:
            src_key = parse_full_object_name(src_name)
            if src_key and src_key in blacklisted_tables:
                continue
        if "." not in tgt_name or "." not in src_name:
            continue
        tgt_schema = tgt_name.split(".")[0].upper()
        src_schema, src_obj = parse_full_object_name(src_name) or ("", src_name)
        tgt_schema2, tgt_obj = parse_full_object_name(tgt_name) or ("", tgt_name)
        rows.append({
            "target_schema": tgt_schema,
            "object_type": obj_type_u,
            "src_schema": src_schema,
            "src_name": src_obj,
            "tgt_schema": tgt_schema2,
            "tgt_name": tgt_obj
        })
    truncated = False
    truncated_count = 0
    if max_rows and len(rows) > max_rows:
        truncated = True
        truncated_count = len(rows) - max_rows
        rows = rows[:max_rows]
    return rows, truncated, truncated_count


def _build_report_detail_insert_all(
    schema_prefix: str,
    report_id: str,
    rows: List[Dict[str, object]]
) -> Optional[str]:
    if not rows:
        return None
    values_sql: List[str] = []
    for row in rows:
        detail_id = f"{report_id}_{uuid.uuid4().hex[:12]}"
        reason = (row.get("reason") or "")
        if len(reason) > 2000:
            reason = reason[:1997] + "..."
        detail_json = row.get("detail_json")
        values_sql.append(
            "INTO {table} (DETAIL_ID, REPORT_ID, REPORT_TYPE, OBJECT_TYPE, SUB_TYPE, "
            "SOURCE_SCHEMA, SOURCE_NAME, TARGET_SCHEMA, TARGET_NAME, STATUS, REASON, BLACKLIST_REASON, DETAIL_JSON) "
            "VALUES ({detail_id}, {report_id}, {report_type}, {object_type}, {sub_type}, "
            "{src_schema}, {src_name}, {tgt_schema}, {tgt_name}, {status}, {reason}, {blacklist_reason}, {detail_json})".format(
                table=f"{schema_prefix}{REPORT_DB_TABLES['detail']}",
                detail_id=sql_quote_literal(detail_id),
                report_id=sql_quote_literal(report_id),
                report_type=sql_quote_literal(row.get("report_type")),
                object_type=sql_quote_literal(row.get("object_type")),
                sub_type=sql_quote_literal(row.get("sub_type")) if row.get("sub_type") else "NULL",
                src_schema=sql_quote_literal(row.get("source_schema")) if row.get("source_schema") else "NULL",
                src_name=sql_quote_literal(row.get("source_name")) if row.get("source_name") else "NULL",
                tgt_schema=sql_quote_literal(row.get("target_schema")) if row.get("target_schema") else "NULL",
                tgt_name=sql_quote_literal(row.get("target_name")) if row.get("target_name") else "NULL",
                status=sql_quote_literal(row.get("status")) if row.get("status") else "NULL",
                reason=sql_quote_literal(reason) if reason else "NULL",
                blacklist_reason=sql_quote_literal(row.get("blacklist_reason")) if row.get("blacklist_reason") else "NULL",
                detail_json=sql_clob_literal(detail_json) if detail_json else "NULL",
            )
        )
    sql = "INSERT ALL\n  " + "\n  ".join(values_sql) + "\nSELECT 1 FROM DUAL"
    return sql


def _insert_report_detail_rows(
    ob_cfg: ObConfig,
    schema_prefix: str,
    report_id: str,
    rows: List[Dict[str, object]],
    batch_size: int
) -> bool:
    if not rows:
        return True
    ok_all = True
    for batch in chunk_list(rows, batch_size):
        insert_sql = _build_report_detail_insert_all(schema_prefix, report_id, batch)
        if not insert_sql:
            continue
        ok, _out, err = obclient_run_sql_commit(ob_cfg, insert_sql)
        if not ok:
            log.warning("[REPORT_DB] INSERT ALL 失败，回退单行插入: %s", err)
            _record_report_db_write_error(
                ob_cfg,
                schema_prefix,
                report_id,
                REPORT_DB_TABLES["detail"],
                insert_sql,
                err
            )
            ok_all = False
            for row in batch:
                insert_sql = _build_report_detail_insert_all(schema_prefix, report_id, [row])
                if not insert_sql:
                    continue
                ok_single, _o, err_single = obclient_run_sql_commit(ob_cfg, insert_sql)
                if not ok_single:
                    ok_all = False
                    log.warning("[REPORT_DB] 单行写入失败: %s", err_single)
                    _record_report_db_write_error(
                        ob_cfg,
                        schema_prefix,
                        report_id,
                        REPORT_DB_TABLES["detail"],
                        insert_sql,
                        err_single
                    )
    return ok_all


def _insert_report_detail_item_rows(
    ob_cfg: ObConfig,
    schema_prefix: str,
    report_id: str,
    rows: List[Dict[str, object]],
    batch_size: int
) -> bool:
    if not rows:
        return True
    ok_all = True
    for batch in chunk_list(rows, batch_size):
        values_sql: List[str] = []
        for row in batch:
            item_id = f"{report_id}_{uuid.uuid4().hex[:12]}"
            values_sql.append(
                "INTO {table} (ITEM_ID, REPORT_ID, REPORT_TYPE, OBJECT_TYPE, SOURCE_SCHEMA, SOURCE_NAME, "
                "TARGET_SCHEMA, TARGET_NAME, ITEM_TYPE, ITEM_KEY, SRC_VALUE, TGT_VALUE, ITEM_VALUE, STATUS) "
                "VALUES ({item_id}, {report_id}, {report_type}, {object_type}, {source_schema}, {source_name}, "
                "{target_schema}, {target_name}, {item_type}, {item_key}, {src_value}, {tgt_value}, {item_value}, {status})".format(
                    table=f"{schema_prefix}{REPORT_DB_TABLES['detail_item']}",
                    item_id=sql_quote_literal(item_id),
                    report_id=sql_quote_literal(report_id),
                    report_type=sql_quote_literal(row.get("report_type")) if row.get("report_type") else "NULL",
                    object_type=sql_quote_literal(row.get("object_type")) if row.get("object_type") else "NULL",
                    source_schema=sql_quote_literal(row.get("source_schema")) if row.get("source_schema") else "NULL",
                    source_name=sql_quote_literal(row.get("source_name")) if row.get("source_name") else "NULL",
                    target_schema=sql_quote_literal(row.get("target_schema")) if row.get("target_schema") else "NULL",
                    target_name=sql_quote_literal(row.get("target_name")) if row.get("target_name") else "NULL",
                    item_type=sql_quote_literal(row.get("item_type")) if row.get("item_type") else "NULL",
                    item_key=sql_quote_literal(row.get("item_key")) if row.get("item_key") else "NULL",
                    src_value=sql_quote_literal(row.get("src_value")) if row.get("src_value") else "NULL",
                    tgt_value=sql_quote_literal(row.get("tgt_value")) if row.get("tgt_value") else "NULL",
                    item_value=sql_clob_literal(row.get("item_value")) if row.get("item_value") else "NULL",
                    status=sql_quote_literal(row.get("status")) if row.get("status") else "NULL",
                )
            )
        insert_sql = "INSERT ALL\n  " + "\n  ".join(values_sql) + "\nSELECT 1 FROM DUAL"
        ok, _out, err = obclient_run_sql_commit(ob_cfg, insert_sql)
        if not ok:
            log.warning("[REPORT_DB] DETAIL_ITEM 批量写入失败，回退单行插入: %s", err)
            _record_report_db_write_error(
                ob_cfg,
                schema_prefix,
                report_id,
                REPORT_DB_TABLES["detail_item"],
                insert_sql,
                err
            )
            ok_all = False
            for row in batch:
                item_id = f"{report_id}_{uuid.uuid4().hex[:12]}"
                single_sql = (
                    "INSERT INTO {table} (ITEM_ID, REPORT_ID, REPORT_TYPE, OBJECT_TYPE, SOURCE_SCHEMA, SOURCE_NAME, "
                    "TARGET_SCHEMA, TARGET_NAME, ITEM_TYPE, ITEM_KEY, SRC_VALUE, TGT_VALUE, ITEM_VALUE, STATUS) "
                    "VALUES ({item_id}, {report_id}, {report_type}, {object_type}, {source_schema}, {source_name}, "
                    "{target_schema}, {target_name}, {item_type}, {item_key}, {src_value}, {tgt_value}, {item_value}, {status})".format(
                        table=f"{schema_prefix}{REPORT_DB_TABLES['detail_item']}",
                        item_id=sql_quote_literal(item_id),
                        report_id=sql_quote_literal(report_id),
                        report_type=sql_quote_literal(row.get("report_type")) if row.get("report_type") else "NULL",
                        object_type=sql_quote_literal(row.get("object_type")) if row.get("object_type") else "NULL",
                        source_schema=sql_quote_literal(row.get("source_schema")) if row.get("source_schema") else "NULL",
                        source_name=sql_quote_literal(row.get("source_name")) if row.get("source_name") else "NULL",
                        target_schema=sql_quote_literal(row.get("target_schema")) if row.get("target_schema") else "NULL",
                        target_name=sql_quote_literal(row.get("target_name")) if row.get("target_name") else "NULL",
                        item_type=sql_quote_literal(row.get("item_type")) if row.get("item_type") else "NULL",
                        item_key=sql_quote_literal(row.get("item_key")) if row.get("item_key") else "NULL",
                        src_value=sql_quote_literal(row.get("src_value")) if row.get("src_value") else "NULL",
                        tgt_value=sql_quote_literal(row.get("tgt_value")) if row.get("tgt_value") else "NULL",
                        item_value=sql_clob_literal(row.get("item_value")) if row.get("item_value") else "NULL",
                        status=sql_quote_literal(row.get("status")) if row.get("status") else "NULL",
                    )
                )
                ok_single, _o, err_single = obclient_run_sql_commit(ob_cfg, single_sql)
                if not ok_single:
                    ok_all = False
                    log.warning("[REPORT_DB] DETAIL_ITEM 单行写入失败: %s", err_single)
                    _record_report_db_write_error(
                        ob_cfg,
                        schema_prefix,
                        report_id,
                        REPORT_DB_TABLES["detail_item"],
                        single_sql,
                        err_single
                    )
    return ok_all


def _insert_report_usability_rows(
    ob_cfg: ObConfig,
    schema_prefix: str,
    report_id: str,
    rows: List[Dict[str, object]],
    batch_size: int
) -> bool:
    if not rows:
        return True
    ok_all = True
    for batch in chunk_list(rows, batch_size):
        values_sql: List[str] = []
        for row in batch:
            values_sql.append(
                "INTO {table} (REPORT_ID, OBJECT_TYPE, SCHEMA_NAME, OBJECT_NAME, USABLE, STATUS, REASON, DETAIL_JSON) "
                "VALUES ({report_id}, {object_type}, {schema_name}, {object_name}, {usable}, {status}, {reason}, {detail_json})".format(
                    table=f"{schema_prefix}{REPORT_DB_TABLES['usability']}",
                    report_id=sql_quote_literal(report_id),
                    object_type=sql_quote_literal(row.get("object_type")),
                    schema_name=sql_quote_literal(row.get("schema_name")) if row.get("schema_name") else "NULL",
                    object_name=sql_quote_literal(row.get("object_name")) if row.get("object_name") else "NULL",
                    usable=str(row.get("usable")) if row.get("usable") is not None else "NULL",
                    status=sql_quote_literal(row.get("status")) if row.get("status") else "NULL",
                    reason=sql_quote_literal(row.get("reason")) if row.get("reason") else "NULL",
                    detail_json=sql_clob_literal(row.get("detail_json")) if row.get("detail_json") else "NULL",
                )
            )
        insert_sql = "INSERT ALL\n  " + "\n  ".join(values_sql) + "\nSELECT 1 FROM DUAL"
        ok, _out, err = obclient_run_sql_commit(ob_cfg, insert_sql)
        if not ok:
            ok_all = False
            log.warning("[REPORT_DB] 写入 usability 失败: %s", err)
            _record_report_db_write_error(
                ob_cfg,
                schema_prefix,
                report_id,
                REPORT_DB_TABLES["usability"],
                insert_sql,
                err
            )
    return ok_all


def _insert_report_package_compare_rows(
    ob_cfg: ObConfig,
    schema_prefix: str,
    report_id: str,
    rows: List[Dict[str, object]],
    batch_size: int
) -> bool:
    if not rows:
        return True
    ok_all = True
    for batch in chunk_list(rows, batch_size):
        values_sql: List[str] = []
        for row in batch:
            summary = row.get("diff_summary") or ""
            if len(summary) > 4000:
                summary = summary[:3997] + "..."
            values_sql.append(
                "INTO {table} (REPORT_ID, SCHEMA_NAME, OBJECT_NAME, OBJECT_TYPE, SRC_STATUS, TGT_STATUS, "
                "DIFF_STATUS, DIFF_HASH, DIFF_SUMMARY, DIFF_PATH) "
                "VALUES ({report_id}, {schema_name}, {object_name}, {object_type}, {src_status}, {tgt_status}, "
                "{diff_status}, {diff_hash}, {diff_summary}, {diff_path})".format(
                    table=f"{schema_prefix}{REPORT_DB_TABLES['package_compare']}",
                    report_id=sql_quote_literal(report_id),
                    schema_name=sql_quote_literal(row.get("schema_name")) if row.get("schema_name") else "NULL",
                    object_name=sql_quote_literal(row.get("object_name")) if row.get("object_name") else "NULL",
                    object_type=sql_quote_literal(row.get("object_type")) if row.get("object_type") else "NULL",
                    src_status=sql_quote_literal(row.get("src_status")) if row.get("src_status") else "NULL",
                    tgt_status=sql_quote_literal(row.get("tgt_status")) if row.get("tgt_status") else "NULL",
                    diff_status=sql_quote_literal(row.get("diff_status")) if row.get("diff_status") else "NULL",
                    diff_hash=sql_quote_literal(row.get("diff_hash")) if row.get("diff_hash") else "NULL",
                    diff_summary=sql_quote_literal(summary) if summary else "NULL",
                    diff_path=sql_quote_literal(row.get("diff_path")) if row.get("diff_path") else "NULL",
                )
            )
        insert_sql = "INSERT ALL\n  " + "\n  ".join(values_sql) + "\nSELECT 1 FROM DUAL"
        ok, _out, err = obclient_run_sql_commit(ob_cfg, insert_sql)
        if not ok:
            ok_all = False
            log.warning("[REPORT_DB] 写入 package_compare 失败: %s", err)
            _record_report_db_write_error(
                ob_cfg,
                schema_prefix,
                report_id,
                REPORT_DB_TABLES["package_compare"],
                insert_sql,
                err
            )
    return ok_all


def _insert_report_trigger_status_rows(
    ob_cfg: ObConfig,
    schema_prefix: str,
    report_id: str,
    rows: List[Dict[str, object]],
    batch_size: int
) -> bool:
    if not rows:
        return True
    ok_all = True
    for batch in chunk_list(rows, batch_size):
        values_sql: List[str] = []
        for row in batch:
            values_sql.append(
                "INTO {table} (REPORT_ID, SCHEMA_NAME, TRIGGER_NAME, SRC_EVENT, TGT_EVENT, "
                "SRC_ENABLED, TGT_ENABLED, SRC_VALID, TGT_VALID, DIFF_STATUS, REASON) "
                "VALUES ({report_id}, {schema_name}, {trigger_name}, {src_event}, {tgt_event}, "
                "{src_enabled}, {tgt_enabled}, {src_valid}, {tgt_valid}, {diff_status}, {reason})".format(
                    table=f"{schema_prefix}{REPORT_DB_TABLES['trigger_status']}",
                    report_id=sql_quote_literal(report_id),
                    schema_name=sql_quote_literal(row.get("schema_name")) if row.get("schema_name") else "NULL",
                    trigger_name=sql_quote_literal(row.get("trigger_name")) if row.get("trigger_name") else "NULL",
                    src_event=sql_quote_literal(row.get("src_event")) if row.get("src_event") else "NULL",
                    tgt_event=sql_quote_literal(row.get("tgt_event")) if row.get("tgt_event") else "NULL",
                    src_enabled=sql_quote_literal(row.get("src_enabled")) if row.get("src_enabled") else "NULL",
                    tgt_enabled=sql_quote_literal(row.get("tgt_enabled")) if row.get("tgt_enabled") else "NULL",
                    src_valid=sql_quote_literal(row.get("src_valid")) if row.get("src_valid") else "NULL",
                    tgt_valid=sql_quote_literal(row.get("tgt_valid")) if row.get("tgt_valid") else "NULL",
                    diff_status=sql_quote_literal(row.get("diff_status")) if row.get("diff_status") else "NULL",
                    reason=sql_quote_literal(row.get("reason")) if row.get("reason") else "NULL",
                )
            )
        insert_sql = "INSERT ALL\n  " + "\n  ".join(values_sql) + "\nSELECT 1 FROM DUAL"
        ok, _out, err = obclient_run_sql_commit(ob_cfg, insert_sql)
        if not ok:
            ok_all = False
            log.warning("[REPORT_DB] 写入 trigger_status 失败: %s", err)
            _record_report_db_write_error(
                ob_cfg,
                schema_prefix,
                report_id,
                REPORT_DB_TABLES["trigger_status"],
                insert_sql,
                err
            )
    return ok_all


def _insert_report_artifact_rows(
    ob_cfg: ObConfig,
    schema_prefix: str,
    report_id: str,
    rows: List[Dict[str, object]],
    batch_size: int
) -> bool:
    if not rows:
        return True
    ok_all = True
    for batch in chunk_list(rows, batch_size):
        values_sql: List[str] = []
        for row in batch:
            values_sql.append(
                "INTO {table} (REPORT_ID, ARTIFACT_TYPE, FILE_PATH, FILE_HASH, ROW_COUNT, FIELD_LIST, STATUS, NOTE) "
                "VALUES ({report_id}, {artifact_type}, {file_path}, {file_hash}, {row_count}, {field_list}, {status}, {note})".format(
                    table=f"{schema_prefix}{REPORT_DB_TABLES['artifact']}",
                    report_id=sql_quote_literal(report_id),
                    artifact_type=sql_quote_literal(row.get("artifact_type")),
                    file_path=sql_quote_literal(row.get("file_path")) if row.get("file_path") else "NULL",
                    file_hash=sql_quote_literal(row.get("file_hash")) if row.get("file_hash") else "NULL",
                    row_count=int(row.get("row_count", 0) or 0),
                    field_list=sql_quote_literal(row.get("field_list")) if row.get("field_list") else "NULL",
                    status=sql_quote_literal(row.get("status")) if row.get("status") else "NULL",
                    note=sql_quote_literal(row.get("note")) if row.get("note") else "NULL",
                )
            )
        insert_sql = "INSERT ALL\n  " + "\n  ".join(values_sql) + "\nSELECT 1 FROM DUAL"
        ok, _out, err = obclient_run_sql_commit(ob_cfg, insert_sql)
        if not ok:
            ok_all = False
            log.warning("[REPORT_DB] 写入 artifact 失败: %s", err)
            _record_report_db_write_error(
                ob_cfg,
                schema_prefix,
                report_id,
                REPORT_DB_TABLES["artifact"],
                insert_sql,
                err
            )
    return ok_all


def _insert_report_artifact_line_rows(
    ob_cfg: ObConfig,
    schema_prefix: str,
    report_id: str,
    row_iter: Iterator[Dict[str, object]],
    batch_size: int
) -> Tuple[bool, int]:
    ok_all = True
    inserted = 0
    batch: List[Dict[str, object]] = []

    def _flush(rows_batch: List[Dict[str, object]]) -> bool:
        if not rows_batch:
            return True
        values_sql: List[str] = []
        for row in rows_batch:
            values_sql.append(
                "INTO {table} (REPORT_ID, ARTIFACT_TYPE, FILE_PATH, LINE_NO, LINE_TEXT) "
                "VALUES ({report_id}, {artifact_type}, {file_path}, {line_no}, {line_text})".format(
                    table=f"{schema_prefix}{REPORT_DB_TABLES['artifact_line']}",
                    report_id=sql_quote_literal(report_id),
                    artifact_type=sql_quote_literal(row.get("artifact_type")),
                    file_path=sql_quote_literal(row.get("file_path")) if row.get("file_path") else "NULL",
                    line_no=int(row.get("line_no", 0) or 0),
                    line_text=sql_clob_literal(row.get("line_text")),
                )
            )
        insert_sql = "INSERT ALL\n  " + "\n  ".join(values_sql) + "\nSELECT 1 FROM DUAL"
        ok, _out, err = obclient_run_sql_commit(ob_cfg, insert_sql)
        if not ok:
            log.warning("[REPORT_DB] 写入 artifact_line 失败: %s", err)
            _record_report_db_write_error(
                ob_cfg,
                schema_prefix,
                report_id,
                REPORT_DB_TABLES["artifact_line"],
                insert_sql,
                err
            )
            return False
        return True

    for row in row_iter:
        batch.append(row)
        if len(batch) >= batch_size:
            if not _flush(batch):
                ok_all = False
            inserted += len(batch)
            batch = []
    if batch:
        if not _flush(batch):
            ok_all = False
        inserted += len(batch)
    return ok_all, inserted


def _mark_artifact_line_partial(
    ob_cfg: ObConfig,
    schema_prefix: str,
    report_id: str
) -> None:
    """
    Downgrade artifact status when full-scope line persistence fails.
    """
    update_sql = (
        f"UPDATE {schema_prefix}{REPORT_DB_TABLES['artifact']} "
        "SET STATUS = 'PARTIAL', NOTE = 'artifact_line_insert_failed' "
        f"WHERE REPORT_ID = {sql_quote_literal(report_id)}"
    )
    ok, _out, err = obclient_run_sql_commit(ob_cfg, update_sql)
    if not ok:
        log.warning("[REPORT_DB] 更新 artifact 覆盖状态失败: %s", err)
        _record_report_db_write_error(
            ob_cfg,
            schema_prefix,
            report_id,
            REPORT_DB_TABLES["artifact"],
            update_sql,
            err
        )


def _insert_report_dependency_rows(
    ob_cfg: ObConfig,
    schema_prefix: str,
    report_id: str,
    rows: List[Dict[str, object]],
    batch_size: int
) -> bool:
    if not rows:
        return True
    ok_all = True
    for batch in chunk_list(rows, batch_size):
        values_sql: List[str] = []
        for row in batch:
            reason = (row.get("reason") or "")
            if len(reason) > 2000:
                reason = reason[:1997] + "..."
            values_sql.append(
                "INTO {table} (REPORT_ID, DEP_SCHEMA, DEP_NAME, DEP_TYPE, REF_SCHEMA, REF_NAME, REF_TYPE, EDGE_STATUS, REASON) "
                "VALUES ({report_id}, {dep_schema}, {dep_name}, {dep_type}, {ref_schema}, {ref_name}, {ref_type}, {edge_status}, {reason})".format(
                    table=f"{schema_prefix}{REPORT_DB_TABLES['dependency']}",
                    report_id=sql_quote_literal(report_id),
                    dep_schema=sql_quote_literal(row.get("dep_schema")) if row.get("dep_schema") else "NULL",
                    dep_name=sql_quote_literal(row.get("dep_name")) if row.get("dep_name") else "NULL",
                    dep_type=sql_quote_literal(row.get("dep_type")) if row.get("dep_type") else "NULL",
                    ref_schema=sql_quote_literal(row.get("ref_schema")) if row.get("ref_schema") else "NULL",
                    ref_name=sql_quote_literal(row.get("ref_name")) if row.get("ref_name") else "NULL",
                    ref_type=sql_quote_literal(row.get("ref_type")) if row.get("ref_type") else "NULL",
                    edge_status=sql_quote_literal(row.get("edge_status")) if row.get("edge_status") else "NULL",
                    reason=sql_quote_literal(reason) if reason else "NULL",
                )
            )
        insert_sql = "INSERT ALL\n  " + "\n  ".join(values_sql) + "\nSELECT 1 FROM DUAL"
        ok, _out, err = obclient_run_sql_commit(ob_cfg, insert_sql)
        if not ok:
            ok_all = False
            log.warning("[REPORT_DB] 写入 dependency 失败: %s", err)
            _record_report_db_write_error(
                ob_cfg,
                schema_prefix,
                report_id,
                REPORT_DB_TABLES["dependency"],
                insert_sql,
                err
            )
    return ok_all


def _insert_report_view_chain_rows(
    ob_cfg: ObConfig,
    schema_prefix: str,
    report_id: str,
    rows: List[Dict[str, object]],
    batch_size: int
) -> bool:
    if not rows:
        return True
    ok_all = True
    for batch in chunk_list(rows, batch_size):
        values_sql: List[str] = []
        for row in batch:
            values_sql.append(
                "INTO {table} (REPORT_ID, VIEW_SCHEMA, VIEW_NAME, CHAIN_ID, NODE_INDEX, NODE_SCHEMA, NODE_NAME, NODE_TYPE, "
                "NODE_EXISTS, GRANT_STATUS, CYCLE_FLAG) "
                "VALUES ({report_id}, {view_schema}, {view_name}, {chain_id}, {node_index}, {node_schema}, {node_name}, "
                "{node_type}, {node_exists}, {grant_status}, {cycle_flag})".format(
                    table=f"{schema_prefix}{REPORT_DB_TABLES['view_chain']}",
                    report_id=sql_quote_literal(report_id),
                    view_schema=sql_quote_literal(row.get("view_schema")) if row.get("view_schema") else "NULL",
                    view_name=sql_quote_literal(row.get("view_name")) if row.get("view_name") else "NULL",
                    chain_id=int(row.get("chain_id", 0) or 0),
                    node_index=int(row.get("node_index", 0) or 0),
                    node_schema=sql_quote_literal(row.get("node_schema")) if row.get("node_schema") else "NULL",
                    node_name=sql_quote_literal(row.get("node_name")) if row.get("node_name") else "NULL",
                    node_type=sql_quote_literal(row.get("node_type")) if row.get("node_type") else "NULL",
                    node_exists=sql_quote_literal(row.get("node_exists")) if row.get("node_exists") else "NULL",
                    grant_status=sql_quote_literal(row.get("grant_status")) if row.get("grant_status") else "NULL",
                    cycle_flag=int(row.get("cycle_flag", 0) or 0),
                )
            )
        insert_sql = "INSERT ALL\n  " + "\n  ".join(values_sql) + "\nSELECT 1 FROM DUAL"
        ok, _out, err = obclient_run_sql_commit(ob_cfg, insert_sql)
        if not ok:
            ok_all = False
            log.warning("[REPORT_DB] 写入 view_chain 失败: %s", err)
            _record_report_db_write_error(
                ob_cfg,
                schema_prefix,
                report_id,
                REPORT_DB_TABLES["view_chain"],
                insert_sql,
                err
            )
    return ok_all


def _insert_report_remap_conflict_rows(
    ob_cfg: ObConfig,
    schema_prefix: str,
    report_id: str,
    rows: List[Dict[str, object]],
    batch_size: int
) -> bool:
    if not rows:
        return True
    ok_all = True
    for batch in chunk_list(rows, batch_size):
        values_sql: List[str] = []
        for row in batch:
            reason = (row.get("reason") or "")
            if len(reason) > 2000:
                reason = reason[:1997] + "..."
            values_sql.append(
                "INTO {table} (REPORT_ID, SOURCE_SCHEMA, SOURCE_NAME, OBJECT_TYPE, REASON, CANDIDATES) "
                "VALUES ({report_id}, {source_schema}, {source_name}, {object_type}, {reason}, {candidates})".format(
                    table=f"{schema_prefix}{REPORT_DB_TABLES['remap_conflict']}",
                    report_id=sql_quote_literal(report_id),
                    source_schema=sql_quote_literal(row.get("source_schema")) if row.get("source_schema") else "NULL",
                    source_name=sql_quote_literal(row.get("source_name")) if row.get("source_name") else "NULL",
                    object_type=sql_quote_literal(row.get("object_type")) if row.get("object_type") else "NULL",
                    reason=sql_quote_literal(reason) if reason else "NULL",
                    candidates=sql_clob_literal(row.get("candidates")) if row.get("candidates") else "NULL",
                )
            )
        insert_sql = "INSERT ALL\n  " + "\n  ".join(values_sql) + "\nSELECT 1 FROM DUAL"
        ok, _out, err = obclient_run_sql_commit(ob_cfg, insert_sql)
        if not ok:
            ok_all = False
            log.warning("[REPORT_DB] 写入 remap_conflict 失败: %s", err)
            _record_report_db_write_error(
                ob_cfg,
                schema_prefix,
                report_id,
                REPORT_DB_TABLES["remap_conflict"],
                insert_sql,
                err
            )
    return ok_all


def _insert_report_object_mapping_rows(
    ob_cfg: ObConfig,
    schema_prefix: str,
    report_id: str,
    rows: List[Dict[str, object]],
    batch_size: int
) -> bool:
    if not rows:
        return True
    ok_all = True
    for batch in chunk_list(rows, batch_size):
        values_sql: List[str] = []
        for row in batch:
            values_sql.append(
                "INTO {table} (REPORT_ID, SRC_SCHEMA, SRC_NAME, OBJECT_TYPE, TGT_SCHEMA, TGT_NAME, MAP_SOURCE) "
                "VALUES ({report_id}, {src_schema}, {src_name}, {object_type}, {tgt_schema}, {tgt_name}, {map_source})".format(
                    table=f"{schema_prefix}{REPORT_DB_TABLES['object_mapping']}",
                    report_id=sql_quote_literal(report_id),
                    src_schema=sql_quote_literal(row.get("src_schema")) if row.get("src_schema") else "NULL",
                    src_name=sql_quote_literal(row.get("src_name")) if row.get("src_name") else "NULL",
                    object_type=sql_quote_literal(row.get("object_type")) if row.get("object_type") else "NULL",
                    tgt_schema=sql_quote_literal(row.get("tgt_schema")) if row.get("tgt_schema") else "NULL",
                    tgt_name=sql_quote_literal(row.get("tgt_name")) if row.get("tgt_name") else "NULL",
                    map_source=sql_quote_literal(row.get("map_source")) if row.get("map_source") else "NULL",
                )
            )
        insert_sql = "INSERT ALL\n  " + "\n  ".join(values_sql) + "\nSELECT 1 FROM DUAL"
        ok, _out, err = obclient_run_sql_commit(ob_cfg, insert_sql)
        if not ok:
            ok_all = False
            log.warning("[REPORT_DB] 写入 object_mapping 失败: %s", err)
            _record_report_db_write_error(
                ob_cfg,
                schema_prefix,
                report_id,
                REPORT_DB_TABLES["object_mapping"],
                insert_sql,
                err
            )
    return ok_all


def _insert_report_blacklist_rows(
    ob_cfg: ObConfig,
    schema_prefix: str,
    report_id: str,
    rows: List[Dict[str, object]],
    batch_size: int
) -> bool:
    if not rows:
        return True
    ok_all = True
    for batch in chunk_list(rows, batch_size):
        values_sql: List[str] = []
        for row in batch:
            detail = (row.get("detail") or "")
            if len(detail) > 2000:
                detail = detail[:1997] + "..."
            values_sql.append(
                "INTO {table} (REPORT_ID, SCHEMA_NAME, TABLE_NAME, BLACK_TYPE, DATA_TYPE, STATUS, REASON, DETAIL) "
                "VALUES ({report_id}, {schema_name}, {table_name}, {black_type}, {data_type}, {status}, {reason}, {detail})".format(
                    table=f"{schema_prefix}{REPORT_DB_TABLES['blacklist']}",
                    report_id=sql_quote_literal(report_id),
                    schema_name=sql_quote_literal(row.get("schema_name")) if row.get("schema_name") else "NULL",
                    table_name=sql_quote_literal(row.get("table_name")) if row.get("table_name") else "NULL",
                    black_type=sql_quote_literal(row.get("black_type")) if row.get("black_type") else "NULL",
                    data_type=sql_quote_literal(row.get("data_type")) if row.get("data_type") else "NULL",
                    status=sql_quote_literal(row.get("status")) if row.get("status") else "NULL",
                    reason=sql_quote_literal(row.get("reason")) if row.get("reason") else "NULL",
                    detail=sql_quote_literal(detail) if detail else "NULL",
                )
            )
        insert_sql = "INSERT ALL\n  " + "\n  ".join(values_sql) + "\nSELECT 1 FROM DUAL"
        ok, _out, err = obclient_run_sql_commit(ob_cfg, insert_sql)
        if not ok:
            ok_all = False
            log.warning("[REPORT_DB] 写入 blacklist 失败: %s", err)
            _record_report_db_write_error(
                ob_cfg,
                schema_prefix,
                report_id,
                REPORT_DB_TABLES["blacklist"],
                insert_sql,
                err
            )
    return ok_all


def _insert_report_fixup_skip_rows(
    ob_cfg: ObConfig,
    schema_prefix: str,
    report_id: str,
    rows: List[Dict[str, object]],
    batch_size: int
) -> bool:
    if not rows:
        return True
    ok_all = True
    for batch in chunk_list(rows, batch_size):
        values_sql: List[str] = []
        for row in batch:
            values_sql.append(
                "INTO {table} (REPORT_ID, OBJECT_TYPE, MISSING_TOTAL, TASK_TOTAL, GENERATED_TOTAL, SKIP_REASON, SKIP_COUNT) "
                "VALUES ({report_id}, {object_type}, {missing_total}, {task_total}, {generated_total}, {skip_reason}, {skip_count})".format(
                    table=f"{schema_prefix}{REPORT_DB_TABLES['fixup_skip']}",
                    report_id=sql_quote_literal(report_id),
                    object_type=sql_quote_literal(row.get("object_type")) if row.get("object_type") else "NULL",
                    missing_total=int(row.get("missing_total", 0) or 0),
                    task_total=int(row.get("task_total", 0) or 0),
                    generated_total=int(row.get("generated_total", 0) or 0),
                    skip_reason=sql_quote_literal(row.get("skip_reason")) if row.get("skip_reason") else "NULL",
                    skip_count=int(row.get("skip_count", 0) or 0),
                )
            )
        insert_sql = "INSERT ALL\n  " + "\n  ".join(values_sql) + "\nSELECT 1 FROM DUAL"
        ok, _out, err = obclient_run_sql_commit(ob_cfg, insert_sql)
        if not ok:
            ok_all = False
            log.warning("[REPORT_DB] 写入 fixup_skip 失败: %s", err)
            _record_report_db_write_error(
                ob_cfg,
                schema_prefix,
                report_id,
                REPORT_DB_TABLES["fixup_skip"],
                insert_sql,
                err
            )
    return ok_all


def _insert_report_oms_missing_rows(
    ob_cfg: ObConfig,
    schema_prefix: str,
    report_id: str,
    rows: List[Dict[str, object]],
    batch_size: int
) -> bool:
    if not rows:
        return True
    ok_all = True
    for batch in chunk_list(rows, batch_size):
        values_sql: List[str] = []
        for row in batch:
            values_sql.append(
                "INTO {table} (REPORT_ID, TARGET_SCHEMA, OBJECT_TYPE, SRC_SCHEMA, SRC_NAME, TGT_SCHEMA, TGT_NAME) "
                "VALUES ({report_id}, {target_schema}, {object_type}, {src_schema}, {src_name}, {tgt_schema}, {tgt_name})".format(
                    table=f"{schema_prefix}{REPORT_DB_TABLES['oms_missing']}",
                    report_id=sql_quote_literal(report_id),
                    target_schema=sql_quote_literal(row.get("target_schema")) if row.get("target_schema") else "NULL",
                    object_type=sql_quote_literal(row.get("object_type")) if row.get("object_type") else "NULL",
                    src_schema=sql_quote_literal(row.get("src_schema")) if row.get("src_schema") else "NULL",
                    src_name=sql_quote_literal(row.get("src_name")) if row.get("src_name") else "NULL",
                    tgt_schema=sql_quote_literal(row.get("tgt_schema")) if row.get("tgt_schema") else "NULL",
                    tgt_name=sql_quote_literal(row.get("tgt_name")) if row.get("tgt_name") else "NULL",
                )
            )
        insert_sql = "INSERT ALL\n  " + "\n  ".join(values_sql) + "\nSELECT 1 FROM DUAL"
        ok, _out, err = obclient_run_sql_commit(ob_cfg, insert_sql)
        if not ok:
            ok_all = False
            log.warning("[REPORT_DB] 写入 oms_missing 失败: %s", err)
            _record_report_db_write_error(
                ob_cfg,
                schema_prefix,
                report_id,
                REPORT_DB_TABLES["oms_missing"],
                insert_sql,
                err
            )
    return ok_all


def _insert_report_grant_rows(
    ob_cfg: ObConfig,
    schema_prefix: str,
    report_id: str,
    grant_plan: GrantPlan,
    batch_size: int
) -> bool:
    if not grant_plan:
        return True
    rows: List[Dict[str, object]] = []
    for grantee, entries in (grant_plan.object_grants or {}).items():
        for entry in entries:
            schema, name = parse_full_object_name(entry.object_full) or ("", entry.object_full)
            rows.append({
                "grant_type": "OBJECT",
                "grantee": grantee,
                "grantor": "",
                "privilege": entry.privilege,
                "target_schema": schema,
                "target_name": name,
                "target_type": "",
                "with_grant_option": 1 if entry.grantable else 0,
                "status": "PLANNED",
                "filter_reason": ""
            })
    for grantee, entries in (grant_plan.sys_privs or {}).items():
        for entry in entries:
            rows.append({
                "grant_type": "SYSTEM",
                "grantee": grantee,
                "grantor": "",
                "privilege": entry.privilege,
                "target_schema": "",
                "target_name": "",
                "target_type": "",
                "with_grant_option": 1 if entry.admin_option else 0,
                "status": "PLANNED",
                "filter_reason": ""
            })
    for grantee, entries in (grant_plan.role_privs or {}).items():
        for entry in entries:
            rows.append({
                "grant_type": "ROLE",
                "grantee": grantee,
                "grantor": "",
                "privilege": entry.role,
                "target_schema": "",
                "target_name": "",
                "target_type": "",
                "with_grant_option": 1 if entry.admin_option else 0,
                "status": "PLANNED",
                "filter_reason": ""
            })
    for filtered in (grant_plan.filtered_grants or []):
        schema, name = parse_full_object_name(filtered.object_full) or ("", filtered.object_full)
        rows.append({
            "grant_type": filtered.category or "UNKNOWN",
            "grantee": filtered.grantee,
            "grantor": "",
            "privilege": filtered.privilege,
            "target_schema": schema,
            "target_name": name,
            "target_type": "",
            "with_grant_option": 0,
            "status": "FILTERED",
            "filter_reason": filtered.reason,
        })

    if not rows:
        return True

    ok_all = True
    for batch in chunk_list(rows, batch_size):
        values_sql: List[str] = []
        for row in batch:
            grant_id = f"{report_id}_{uuid.uuid4().hex[:12]}"
            values_sql.append(
                "INTO {table} (GRANT_ID, REPORT_ID, GRANT_TYPE, GRANTEE, GRANTOR, PRIVILEGE, "
                "TARGET_SCHEMA, TARGET_NAME, TARGET_TYPE, WITH_GRANT_OPTION, STATUS, FILTER_REASON) "
                "VALUES ({grant_id}, {report_id}, {grant_type}, {grantee}, {grantor}, {privilege}, "
                "{target_schema}, {target_name}, {target_type}, {with_grant_option}, {status}, {filter_reason})".format(
                    table=f"{schema_prefix}{REPORT_DB_TABLES['grants']}",
                    grant_id=sql_quote_literal(grant_id),
                    report_id=sql_quote_literal(report_id),
                    grant_type=sql_quote_literal(row.get("grant_type")),
                    grantee=sql_quote_literal(row.get("grantee")),
                    grantor=sql_quote_literal(row.get("grantor")) if row.get("grantor") else "NULL",
                    privilege=sql_quote_literal(row.get("privilege")) if row.get("privilege") else "NULL",
                    target_schema=sql_quote_literal(row.get("target_schema")) if row.get("target_schema") else "NULL",
                    target_name=sql_quote_literal(row.get("target_name")) if row.get("target_name") else "NULL",
                    target_type=sql_quote_literal(row.get("target_type")) if row.get("target_type") else "NULL",
                    with_grant_option=str(int(row.get("with_grant_option", 0))),
                    status=sql_quote_literal(row.get("status")) if row.get("status") else "NULL",
                    filter_reason=sql_quote_literal(row.get("filter_reason")) if row.get("filter_reason") else "NULL",
                )
            )
        insert_sql = "INSERT ALL\n  " + "\n  ".join(values_sql) + "\nSELECT 1 FROM DUAL"
        ok, _out, err = obclient_run_sql_commit(ob_cfg, insert_sql)
        if not ok:
            ok_all = False
            log.warning("[REPORT_DB] GRANT 批量写入失败: %s", err)
            _record_report_db_write_error(
                ob_cfg,
                schema_prefix,
                report_id,
                REPORT_DB_TABLES["grants"],
                insert_sql,
                err
            )
    return ok_all


def _build_report_counts_rows(
    object_counts_summary: Optional[ObjectCountSummary],
    unsupported_counts: Optional[Dict[str, int]]
) -> List[Dict[str, object]]:
    if not object_counts_summary:
        return []
    oracle_counts = dict(object_counts_summary.get("oracle", {}))
    ob_counts = dict(object_counts_summary.get("oceanbase", {}))
    missing_counts = dict(object_counts_summary.get("missing", {}))
    extra_counts = dict(object_counts_summary.get("extra", {}))
    unsupported_counts = dict(unsupported_counts or {})
    all_types = sorted(set(oracle_counts) | set(ob_counts) | set(missing_counts) | set(extra_counts) | set(unsupported_counts))
    rows: List[Dict[str, object]] = []
    for obj_type in all_types:
        rows.append({
            "object_type": obj_type,
            "oracle_count": int(oracle_counts.get(obj_type, 0) or 0),
            "oceanbase_count": int(ob_counts.get(obj_type, 0) or 0),
            "missing_count": int(missing_counts.get(obj_type, 0) or 0),
            "unsupported_count": int(unsupported_counts.get(obj_type, 0) or 0),
            "extra_count": int(extra_counts.get(obj_type, 0) or 0),
        })
    return rows


def _insert_report_counts_rows(
    ob_cfg: ObConfig,
    schema_prefix: str,
    report_id: str,
    rows: List[Dict[str, object]],
    batch_size: int
) -> bool:
    if not rows:
        return True
    ok_all = True
    for batch in chunk_list(rows, batch_size):
        values_sql: List[str] = []
        for row in batch:
            values_sql.append(
                "INTO {table} (REPORT_ID, OBJECT_TYPE, ORACLE_COUNT, OCEANBASE_COUNT, "
                "MISSING_COUNT, UNSUPPORTED_COUNT, EXTRA_COUNT) "
                "VALUES ({report_id}, {object_type}, {oracle_count}, {oceanbase_count}, "
                "{missing_count}, {unsupported_count}, {extra_count})".format(
                    table=f"{schema_prefix}{REPORT_DB_TABLES['counts']}",
                    report_id=sql_quote_literal(report_id),
                    object_type=sql_quote_literal(row.get("object_type")),
                    oracle_count=int(row.get("oracle_count", 0)),
                    oceanbase_count=int(row.get("oceanbase_count", 0)),
                    missing_count=int(row.get("missing_count", 0)),
                    unsupported_count=int(row.get("unsupported_count", 0)),
                    extra_count=int(row.get("extra_count", 0)),
                )
            )
        insert_sql = "INSERT ALL\n  " + "\n  ".join(values_sql) + "\nSELECT 1 FROM DUAL"
        ok, _out, err = obclient_run_sql_commit(ob_cfg, insert_sql)
        if not ok:
            ok_all = False
            log.warning("[REPORT_DB] COUNT 批量写入失败: %s", err)
            _record_report_db_write_error(
                ob_cfg,
                schema_prefix,
                report_id,
                REPORT_DB_TABLES["counts"],
                insert_sql,
                err
            )
    return ok_all



def _build_report_db_view_ddls(schema_prefix: str) -> Dict[str, str]:
    detail = f"{schema_prefix}{REPORT_DB_TABLES['detail']}"
    grants = f"{schema_prefix}{REPORT_DB_TABLES['grants']}"
    usability = f"{schema_prefix}{REPORT_DB_TABLES['usability']}"
    summary = f"{schema_prefix}{REPORT_DB_TABLES['summary']}"
    counts = f"{schema_prefix}{REPORT_DB_TABLES['counts']}"
    blacklist = f"{schema_prefix}{REPORT_DB_TABLES['blacklist']}"
    resolution = f"{schema_prefix}{REPORT_DB_TABLES['resolution']}"

    actions_view = f"{schema_prefix}{REPORT_DB_VIEWS['actions']}"
    object_profile_view = f"{schema_prefix}{REPORT_DB_VIEWS['object_profile']}"
    trends_view = f"{schema_prefix}{REPORT_DB_VIEWS['trends']}"
    pending_actions_view = f"{schema_prefix}{REPORT_DB_VIEWS['pending_actions']}"
    grant_class_view = f"{schema_prefix}{REPORT_DB_VIEWS['grant_class']}"
    usability_class_view = f"{schema_prefix}{REPORT_DB_VIEWS['usability_class']}"

    ddls: Dict[str, str] = {}
    ddls[actions_view] = f"""
CREATE OR REPLACE VIEW {actions_view} AS
SELECT report_id,
       'FIXUP' AS action_type,
       object_type,
       source_schema AS schema_name,
       source_name AS object_name,
       target_schema,
       target_name,
       report_type,
       status,
       reason
  FROM {detail}
 WHERE report_type IN ('MISSING','MISMATCHED')
   AND (status IS NULL OR status NOT IN ('UNSUPPORTED','BLOCKED'))
UNION ALL
SELECT report_id,
       'REFACTOR' AS action_type,
       object_type,
       source_schema,
       source_name,
       target_schema,
       target_name,
       report_type,
       status,
       reason
  FROM {detail}
 WHERE report_type = 'UNSUPPORTED'
   AND status = 'UNSUPPORTED'
UNION ALL
SELECT report_id,
       'DEPENDENCY' AS action_type,
       object_type,
       source_schema,
       source_name,
       target_schema,
       target_name,
       report_type,
       status,
       reason
  FROM {detail}
 WHERE report_type = 'UNSUPPORTED'
   AND status = 'BLOCKED'
UNION ALL
SELECT report_id,
       'GRANT_REQUIRED' AS action_type,
       target_type AS object_type,
       target_schema AS schema_name,
       target_name AS object_name,
       target_schema,
       target_name,
       'GRANT' AS report_type,
       status,
       NVL(filter_reason, privilege) AS reason
  FROM {grants}
 WHERE status = 'MISSING'
UNION ALL
SELECT report_id,
       'VERIFY' AS action_type,
       object_type,
       schema_name,
       object_name,
       NULL AS target_schema,
       NULL AS target_name,
       'USABILITY' AS report_type,
       status,
       reason
  FROM {usability}
 WHERE status <> 'OK'
"""

    ddls[object_profile_view] = f"""
CREATE OR REPLACE VIEW {object_profile_view} AS
SELECT d.report_id,
       d.object_type,
       d.source_schema,
       d.source_name,
       d.target_schema,
       d.target_name,
       d.report_type,
       d.status AS detail_status,
       d.reason AS detail_reason,
       u.status AS usability_status,
       u.usable AS usability_usable,
       u.reason AS usability_reason,
       b.black_type AS blacklist_type,
       b.status AS blacklist_status,
       b.reason AS blacklist_reason
  FROM {detail} d
  LEFT JOIN {usability} u
    ON u.report_id = d.report_id
   AND u.object_type = d.object_type
   AND u.schema_name = d.target_schema
   AND u.object_name = d.target_name
  LEFT JOIN {blacklist} b
    ON b.report_id = d.report_id
   AND b.schema_name = d.source_schema
   AND b.table_name = d.source_name
"""

    ddls[trends_view] = f"""
CREATE OR REPLACE VIEW {trends_view} AS
SELECT s.report_id,
       s.run_timestamp,
       s.tool_version,
       c.object_type,
       c.oracle_count,
       c.oceanbase_count,
       c.missing_count,
       c.unsupported_count,
       c.extra_count
  FROM {summary} s
  JOIN {counts} c
    ON s.report_id = c.report_id
"""

    ddls[pending_actions_view] = f"""
CREATE OR REPLACE VIEW {pending_actions_view} AS
SELECT a.report_id,
       a.action_type,
       a.object_type,
       a.schema_name,
       a.object_name,
       a.target_schema,
       a.target_name,
       a.report_type,
       a.status,
       a.reason,
       r.resolution_status,
       r.resolved_by,
       r.resolved_at,
       r.note
  FROM {actions_view} a
  LEFT JOIN {resolution} r
    ON r.report_id = a.report_id
   AND r.object_type = a.object_type
   AND r.schema_name = a.schema_name
   AND r.object_name = a.object_name
   AND r.action_type = a.action_type
 WHERE NVL(r.resolution_status, 'OPEN') NOT IN ('RESOLVED','CLOSED')
"""

    ddls[grant_class_view] = f"""
CREATE OR REPLACE VIEW {grant_class_view} AS
SELECT report_id,
       grant_type,
       grantee,
       grantor,
       privilege,
       target_schema,
       target_name,
       target_type,
       with_grant_option,
       status,
       filter_reason,
       CASE
         WHEN status = 'MISSING' AND with_grant_option = 1 THEN 'MISSING_WITH_GRANT_OPTION'
         WHEN status = 'MISSING' THEN 'MISSING'
         WHEN status = 'EXTRA' THEN 'EXTRA'
         WHEN status = 'FILTERED' THEN 'FILTERED'
         WHEN status IS NULL THEN 'UNKNOWN'
         ELSE status
       END AS grant_class
  FROM {grants}
"""

    ddls[usability_class_view] = f"""
CREATE OR REPLACE VIEW {usability_class_view} AS
SELECT report_id,
       object_type,
       schema_name,
       object_name,
       usable,
       status,
       reason,
       CASE
         WHEN status = 'OK' OR usable = 1 THEN 'OK'
         WHEN reason IS NULL THEN 'UNKNOWN'
         WHEN INSTR(UPPER(reason), 'ORA-00942') > 0
              OR INSTR(UPPER(reason), 'NOT EXIST') > 0
              OR INSTR(UPPER(reason), 'NO SUCH TABLE') > 0 THEN 'NOT_FOUND'
         WHEN INSTR(UPPER(reason), 'ORA-01031') > 0
              OR INSTR(UPPER(reason), 'INSUFFICIENT') > 0
              OR INSTR(UPPER(reason), 'PERMISSION') > 0 THEN 'PERMISSION'
         WHEN INSTR(UPPER(reason), 'ORA-00900') > 0
              OR INSTR(UPPER(reason), 'ORA-00933') > 0
              OR INSTR(UPPER(reason), 'ORA-00936') > 0
              OR INSTR(UPPER(reason), 'ORA-00904') > 0 THEN 'SYNTAX'
         WHEN INSTR(UPPER(reason), 'TIMEOUT') > 0
              OR INSTR(UPPER(reason), 'TIMED OUT') > 0 THEN 'TIMEOUT'
         WHEN INSTR(UPPER(reason), 'LOOPING CHAIN') > 0 THEN 'SYNONYM_LOOP'
         WHEN INSTR(UPPER(reason), 'DEPEND') > 0
              OR INSTR(UPPER(reason), 'BLOCK') > 0 THEN 'DEPENDENCY'
         ELSE 'OTHER'
       END AS reason_class
  FROM {usability}
"""
    return ddls


def ensure_report_db_views_exist(
    ob_cfg: ObConfig,
    settings: Dict
) -> List[Dict[str, object]]:
    artifacts: List[Dict[str, object]] = []
    store_scope = settings.get("report_db_store_scope", "full")
    schema_prefix = build_report_db_schema_prefix(settings)
    ddls = _build_report_db_view_ddls(schema_prefix)
    if store_scope not in {"core", "full"}:
        for view_name in ddls:
            artifacts.append({
                "artifact_type": "REPORT_DB_VIEW",
                "file_path": view_name,
                "file_hash": "",
                "row_count": 0,
                "field_list": "",
                "status": "SKIPPED",
                "note": "store_scope=summary"
            })
        return artifacts

    for view_name, ddl in ddls.items():
        ok, _out, err = obclient_run_sql_commit(ob_cfg, ddl)
        if ok:
            artifacts.append({
                "artifact_type": "REPORT_DB_VIEW",
                "file_path": view_name,
                "file_hash": "",
                "row_count": 0,
                "field_list": "",
                "status": "IN_DB",
                "note": ""
            })
        else:
            note = normalize_error_text(err or "")
            if len(note) > 900:
                note = note[:900] + "..."
            artifacts.append({
                "artifact_type": "REPORT_DB_VIEW",
                "file_path": view_name,
                "file_hash": "",
                "row_count": 0,
                "field_list": "",
                "status": "FAILED",
                "note": note
            })
    return artifacts


def _render_report_sql_template(content: str, report_id: str) -> str:
    if not content:
        return ""
    rendered = content.replace(":report_id", f"'{report_id}'")
    return f"# report_id={report_id}\n{rendered}"


def _find_report_sql_playbook() -> Optional[Path]:
    base_dir = Path(__file__).resolve().parent
    candidates = sorted(base_dir.glob("HOW_TO_READ_REPORTS_IN_OB_*_sqls.txt"))
    if candidates:
        def _score(path: Path) -> int:
            match = re.search(r"_([0-9]+)_sqls\\.txt$", path.name)
            if match:
                try:
                    return int(match.group(1))
                except ValueError:
                    return -1
            return -1
        return max(candidates, key=_score)
    legacy = base_dir / "HOW_TO_READ_REPORTS_IN_OB.txt"
    if legacy.exists():
        return legacy
    return None


def build_report_sql_template_file(
    report_dir: Optional[Path],
    report_timestamp: str,
    report_id: str
) -> Optional[Path]:
    if not report_dir:
        return None
    how_to = _find_report_sql_playbook()
    if not how_to or not how_to.exists():
        return None
    try:
        content = how_to.read_text(encoding="utf-8")
    except OSError:
        return None
    rendered = _render_report_sql_template(content, report_id)
    if not rendered:
        return None
    output = report_dir / f"report_sql_{report_timestamp}.txt"
    try:
        output.write_text(rendered, encoding="utf-8")
    except OSError:
        return None
    return output


def purge_report_db_retention(
    ob_cfg: ObConfig,
    schema_prefix: str,
    retention_days: int
) -> None:
    """
    按保留期清理报告数据。
    先清理子表，再清理 summary，避免仅 summary 变更导致子表累计膨胀。
    """
    days = int(retention_days or 0)
    if days <= 0:
        return
    cutoff_expr = f"SYSTIMESTAMP - INTERVAL '{days}' DAY"
    summary_table = f"{schema_prefix}{REPORT_DB_TABLES['summary']}"
    child_tables = [name for key, name in REPORT_DB_TABLES.items() if key != "summary"]

    for table_name in child_tables:
        delete_child_sql = (
            f"DELETE FROM {schema_prefix}{table_name} "
            f"WHERE REPORT_ID IN (SELECT REPORT_ID FROM {summary_table} "
            f"WHERE RUN_TIMESTAMP < {cutoff_expr})"
        )
        ok_child, _o, err_child = obclient_run_sql_commit(ob_cfg, delete_child_sql)
        if not ok_child:
            log.warning("[REPORT_DB] 保留期清理子表失败 %s: %s", table_name, err_child)

    delete_summary_sql = (
        f"DELETE FROM {summary_table} "
        f"WHERE RUN_TIMESTAMP < {cutoff_expr}"
    )
    ok_sum, _out, err_sum = obclient_run_sql_commit(ob_cfg, delete_summary_sql)
    if not ok_sum:
        log.warning("[REPORT_DB] 保留期清理 summary 失败: %s", err_sum)




def save_report_to_db(
    ob_cfg: ObConfig,
    settings: Dict,
    run_summary: RunSummary,
    run_summary_ctx: RunSummaryContext,
    report_timestamp: str,
    report_dir: Optional[Path],
    tv_results: ReportResults,
    extra_results: ExtraCheckResults,
    support_summary: Optional[SupportClassificationResult],
    endpoint_info: Dict[str, str],
    object_counts_summary: Optional[ObjectCountSummary],
    table_target_map: Optional[Dict[Tuple[str, str], Tuple[str, str]]],
    target_schemas: Optional[Set[str]],
    grant_plan: Optional[GrantPlan] = None,
    usability_summary: Optional[UsabilitySummary] = None,
    package_results: Optional[PackageCompareResults] = None,
    trigger_status_rows: Optional[List[TriggerStatusReportRow]] = None,
    constraint_status_rows: Optional[List[ConstraintStatusDriftRow]] = None,
    dependency_report: Optional[DependencyReport] = None,
    expected_dependency_pairs: Optional[Set[Tuple[str, str, str, str]]] = None,
    view_chain_file: Optional[Path] = None,
    remap_conflicts: Optional[List[Tuple[str, str, str]]] = None,
    full_object_mapping: Optional[FullObjectMapping] = None,
    remap_rules: Optional[RemapRules] = None,
    blacklist_report_rows: Optional[List[BlacklistReportRow]] = None,
    fixup_skip_summary: Optional[Dict[str, Dict[str, object]]] = None,
    blacklisted_table_keys: Optional[Set[Tuple[str, str]]] = None
) -> Tuple[bool, Optional[str]]:
    if not settings.get("report_to_db", False):
        return True, None

    schema_prefix = build_report_db_schema_prefix(settings)
    ok, err = ensure_report_db_tables_exist(ob_cfg, settings)
    if not ok:
        log.error("[REPORT_DB] 创建报告表失败: %s", err)
        if settings.get("report_db_fail_abort"):
            return False, err
        return True, None

    report_id = generate_report_id(report_timestamp)
    report_dir_val = str(report_dir.resolve()) if report_dir else ""
    store_scope = settings.get("report_db_store_scope", "full")
    detail_modes = settings.get("report_db_detail_mode_set") or set(DEFAULT_REPORT_DB_DETAIL_MODES)
    detail_item_enable = bool(settings.get("report_db_detail_item_enable", False))
    if store_scope != "full":
        detail_item_enable = False
    detail_item_max_rows = int(settings.get("report_db_detail_item_max_rows", 0) or 0)

    missing_count = len(tv_results.get("missing", []))
    mismatched_count = len(tv_results.get("mismatched", []))
    ok_count = len(tv_results.get("ok", []))
    skipped_count = len(tv_results.get("skipped", []))
    unsupported_by_type = build_unsupported_summary_counts(support_summary, extra_results)
    unsupported_count = sum(int(v or 0) for v in unsupported_by_type.values())

    # SQL template file (pre-filled report_id) for DB report queries
    build_report_sql_template_file(report_dir, report_timestamp, report_id)

    # Ensure analytic views exist (read-only, report DB)
    view_artifacts = ensure_report_db_views_exist(ob_cfg, settings)

    extra_missing_counts = summarize_extra_missing_counts(extra_results)
    index_missing_total = int(extra_missing_counts.get("INDEX", 0) or 0)
    index_mismatch_total = len(extra_results.get("index_mismatched", []))
    constraint_missing_total = int(extra_missing_counts.get("CONSTRAINT", 0) or 0)
    constraint_mismatch_total = len(extra_results.get("constraint_mismatched", []))
    trigger_missing_total = int(extra_missing_counts.get("TRIGGER", 0) or 0)
    sequence_missing_total = int(extra_missing_counts.get("SEQUENCE", 0) or 0)

    if missing_count == 0 and mismatched_count == 0:
        conclusion = "PASS"
        conclusion_detail = "所有校验对象均已通过"
    elif missing_count > 0:
        conclusion = "FAIL"
        conclusion_detail = f"存在 {missing_count} 个缺失对象"
    else:
        conclusion = "WARN"
        conclusion_detail = f"存在 {mismatched_count} 个不匹配对象"

    if store_scope in {"core", "full"}:
        detail_rows, detail_truncated, detail_truncated_count = _build_report_detail_rows(
            settings,
            tv_results,
            extra_results,
            support_summary,
            table_target_map,
            trigger_status_rows=trigger_status_rows,
            constraint_status_rows=constraint_status_rows,
            trigger_validity_mode=settings.get("trigger_validity_sync_mode", "compile")
        )
    else:
        detail_rows, detail_truncated, detail_truncated_count = [], False, 0
    detail_item_rows: List[Dict[str, object]] = []
    detail_item_truncated = False
    detail_item_truncated_count = 0
    if detail_item_enable and store_scope == "full":
        detail_item_rows, detail_item_truncated, detail_item_truncated_count = _build_report_detail_item_rows(
            tv_results,
            extra_results,
            support_summary,
            table_target_map,
            detail_item_max_rows,
            trigger_status_rows=trigger_status_rows,
            constraint_status_rows=constraint_status_rows,
            trigger_validity_mode=settings.get("trigger_validity_sync_mode", "compile")
        )
        if detail_item_truncated:
            detail_truncated = True
            detail_truncated_count += detail_item_truncated_count

    full_json = None
    if settings.get("report_db_save_full_json"):
        full_json = json.dumps({
            "missing": tv_results.get("missing", []),
            "mismatched": tv_results.get("mismatched", []),
            "ok_count": ok_count,
            "skipped_count": skipped_count,
        }, ensure_ascii=False, default=str)

    enabled_primary_types = settings.get("enabled_primary_types", set())
    enabled_extra_types = settings.get("enabled_extra_types", set())
    target_schema_list = sorted(target_schemas or set())

    insert_sql = f"""
INSERT INTO {schema_prefix}{REPORT_DB_TABLES['summary']} (
    REPORT_ID, RUN_TIMESTAMP, DURATION_SECONDS,
    SOURCE_HOST, SOURCE_PORT, SOURCE_SERVICE, SOURCE_USER, SOURCE_SCHEMAS,
    TARGET_HOST, TARGET_PORT, TARGET_TENANT, TARGET_USER, TARGET_SCHEMAS,
    CHECK_PRIMARY_TYPES, CHECK_EXTRA_TYPES, FIXUP_ENABLED, GRANT_ENABLED,
    TOTAL_CHECKED, MISSING_COUNT, MISMATCHED_COUNT, OK_COUNT, SKIPPED_COUNT, UNSUPPORTED_COUNT,
    INDEX_MISSING, INDEX_MISMATCHED, CONSTRAINT_MISSING, CONSTRAINT_MISMATCH,
    TRIGGER_MISSING, SEQUENCE_MISSING, DETAIL_TRUNCATED, DETAIL_TRUNCATED_COUNT,
    CONCLUSION, CONCLUSION_DETAIL, FULL_REPORT_JSON, TOOL_VERSION, HOSTNAME, RUN_DIR
) VALUES (
    {sql_quote_literal(report_id)},
    TO_TIMESTAMP({sql_quote_literal(report_timestamp.replace('_', ''))}, 'YYYYMMDDHH24MISS'),
    {run_summary.total_seconds:.2f},
    {sql_quote_literal(endpoint_info.get("source_host", ""))},
    {int(endpoint_info.get("source_port", 0) or 0)},
    {sql_quote_literal(endpoint_info.get("source_service", ""))},
    {sql_quote_literal(endpoint_info.get("source_user", ""))},
    {sql_clob_literal(",".join(settings.get("source_schemas_list", [])))},
    {sql_quote_literal(endpoint_info.get("target_host", ""))},
    {int(endpoint_info.get("target_port", 0) or 0)},
    {sql_quote_literal(endpoint_info.get("target_tenant", ""))},
    {sql_quote_literal(endpoint_info.get("target_user", ""))},
    {sql_clob_literal(",".join(target_schema_list))},
    {sql_quote_literal(",".join(sorted(enabled_primary_types)))},
    {sql_quote_literal(",".join(sorted(enabled_extra_types)))},
    {1 if run_summary_ctx.fixup_enabled else 0},
    {1 if run_summary_ctx.enable_grant_generation else 0},
    {run_summary_ctx.total_checked},
    {missing_count},
    {mismatched_count},
    {ok_count},
    {skipped_count},
    {unsupported_count},
    {index_missing_total},
    {index_mismatch_total},
    {constraint_missing_total},
    {constraint_mismatch_total},
    {trigger_missing_total},
    {sequence_missing_total},
    {1 if detail_truncated else 0},
    {detail_truncated_count},
    {sql_quote_literal(conclusion)},
    {sql_quote_literal(conclusion_detail)},
    {sql_clob_literal(full_json) if full_json else "NULL"},
    {sql_quote_literal(__version__)},
    {sql_quote_literal(socket.gethostname())},
    {sql_quote_literal(report_dir_val)}
)
"""

    ok, _out, err = obclient_run_sql_commit(ob_cfg, insert_sql)
    if not ok:
        log.error("[REPORT_DB] 写入主报告失败: %s", err)
        _record_report_db_write_error(
            ob_cfg,
            schema_prefix,
            report_id,
            REPORT_DB_TABLES["summary"],
            insert_sql,
            err
        )
        if settings.get("report_db_fail_abort"):
            return False, err
        return True, report_id

    batch_size = int(settings.get("report_db_insert_batch", 200) or 200)
    max_rows = int(settings.get("report_db_detail_max_rows", 0) or 0)
    count_rows = _build_report_counts_rows(object_counts_summary, unsupported_by_type)
    if count_rows:
        ok_counts = _insert_report_counts_rows(ob_cfg, schema_prefix, report_id, count_rows, batch_size)
        if not ok_counts and settings.get("report_db_fail_abort"):
            return False, "写入 DIFF_REPORT_COUNTS 失败"
    if store_scope in {"core", "full"}:
        _insert_report_detail_rows(ob_cfg, schema_prefix, report_id, detail_rows, batch_size)
        if detail_item_enable and detail_item_rows:
            _insert_report_detail_item_rows(ob_cfg, schema_prefix, report_id, detail_item_rows, batch_size)
        if grant_plan:
            _insert_report_grant_rows(ob_cfg, schema_prefix, report_id, grant_plan, batch_size)
        usability_rows = _build_report_usability_rows(usability_summary)
        if usability_rows:
            _insert_report_usability_rows(ob_cfg, schema_prefix, report_id, usability_rows, batch_size)
        package_rows = _build_report_package_compare_rows(package_results, report_dir, report_timestamp)
        if package_rows:
            _insert_report_package_compare_rows(ob_cfg, schema_prefix, report_id, package_rows, batch_size)
        trigger_rows = _build_report_trigger_status_rows(trigger_status_rows)
        if trigger_rows:
            _insert_report_trigger_status_rows(ob_cfg, schema_prefix, report_id, trigger_rows, batch_size)

    artifact_rows = _build_report_artifact_rows(report_dir, store_scope, detail_modes, detail_truncated)
    if view_artifacts:
        artifact_rows.extend(view_artifacts)
    if artifact_rows:
        _insert_report_artifact_rows(ob_cfg, schema_prefix, report_id, artifact_rows, batch_size)
    if store_scope == "full":
        line_iter = _iter_report_artifact_line_rows(report_dir)
        ok_art_lines, art_line_count = _insert_report_artifact_line_rows(
            ob_cfg,
            schema_prefix,
            report_id,
            line_iter,
            batch_size
        )
        if not ok_art_lines and settings.get("report_db_fail_abort"):
            return False, "写入 DIFF_REPORT_ARTIFACT_LINE 失败"
        if not ok_art_lines:
            _mark_artifact_line_partial(ob_cfg, schema_prefix, report_id)
        if art_line_count > 0:
            log.info("[REPORT_DB] artifact 行级文本已写入: %d", art_line_count)

    if store_scope == "full":
        dep_rows, _dep_trunc, _dep_trunc_cnt = _build_report_dependency_rows(
            dependency_report,
            expected_dependency_pairs,
            max_rows,
            store_expected=True
        )
        if dep_rows:
            _insert_report_dependency_rows(ob_cfg, schema_prefix, report_id, dep_rows, batch_size)

        view_chain_rows, _vc_trunc, _vc_trunc_cnt = _build_report_view_chain_rows(
            view_chain_file,
            max_rows
        )
        if view_chain_rows:
            _insert_report_view_chain_rows(ob_cfg, schema_prefix, report_id, view_chain_rows, batch_size)

        remap_rows, _remap_trunc, _remap_trunc_cnt = _build_report_remap_conflict_rows(
            remap_conflicts,
            max_rows
        )
        if remap_rows:
            _insert_report_remap_conflict_rows(ob_cfg, schema_prefix, report_id, remap_rows, batch_size)

        mapping_rows, _map_trunc, _map_trunc_cnt = _build_report_object_mapping_rows(
            full_object_mapping,
            remap_rules,
            max_rows
        )
        if mapping_rows:
            _insert_report_object_mapping_rows(ob_cfg, schema_prefix, report_id, mapping_rows, batch_size)

        blacklist_rows, _bl_trunc, _bl_trunc_cnt = _build_report_blacklist_rows(
            blacklist_report_rows,
            max_rows
        )
        if blacklist_rows:
            _insert_report_blacklist_rows(ob_cfg, schema_prefix, report_id, blacklist_rows, batch_size)

        skip_rows, _skip_trunc, _skip_trunc_cnt = _build_report_fixup_skip_rows(
            fixup_skip_summary,
            max_rows
        )
        if skip_rows:
            _insert_report_fixup_skip_rows(ob_cfg, schema_prefix, report_id, skip_rows, batch_size)

        oms_rows, _oms_trunc, _oms_trunc_cnt = _build_report_oms_missing_rows(
            tv_results,
            (support_summary.support_state_map if support_summary else None),
            blacklisted_table_keys,
            max_rows
        )
        if oms_rows:
            _insert_report_oms_missing_rows(ob_cfg, schema_prefix, report_id, oms_rows, batch_size)

    retention_days = int(settings.get("report_retention_days", 0) or 0)
    if retention_days > 0:
        purge_report_db_retention(ob_cfg, schema_prefix, retention_days)

    log.info("[REPORT_DB] 报告已写入数据库: report_id=%s", report_id)
    return True, report_id

def collect_blacklisted_missing_tables(
    tv_results: ReportResults,
    blacklist_tables: BlacklistTableMap
) -> BlacklistTableMap:
    """
    从黑名单表中筛选出“当前缺失”的 TABLE，用于统计与输出。
    """
    if not tv_results or not blacklist_tables:
        return {}

    missing_tables: Set[Tuple[str, str]] = set()
    for obj_type, _, src_name in tv_results.get("missing", []):
        if obj_type.upper() != "TABLE":
            continue
        src_key = parse_full_object_name(src_name)
        if src_key:
            missing_tables.add(src_key)

    return {
        key: entries
        for key, entries in blacklist_tables.items()
        if key in missing_tables
    }


def build_unsupported_summary_counts(
    support_summary: Optional[SupportClassificationResult],
    extra_results: Optional[ExtraCheckResults]
) -> Dict[str, int]:
    """
    汇总不支持/阻断数量（含扩展对象），供报告汇总使用。
    """
    counts: Dict[str, int] = defaultdict(int)
    if support_summary:
        for obj_type, data in (support_summary.missing_support_counts or {}).items():
            counts[obj_type] += int(data.get("unsupported", 0) or 0)
            counts[obj_type] += int(data.get("blocked", 0) or 0)
        for obj_type, blocked in (support_summary.extra_blocked_counts or {}).items():
            counts[obj_type] += int(blocked or 0)
    if extra_results:
        counts["INDEX"] += len(extra_results.get("index_unsupported", []) or [])
        counts["CONSTRAINT"] += len(extra_results.get("constraint_unsupported", []) or [])
    return dict(counts)


def summarize_extra_missing_counts(
    extra_results: Optional[ExtraCheckResults]
) -> Dict[str, int]:
    """
    统一扩展对象“缺失对象数”口径（对象级），供 fixup/报告/写库复用。
    """
    if not extra_results:
        return {"INDEX": 0, "CONSTRAINT": 0, "SEQUENCE": 0, "TRIGGER": 0}
    return {
        "INDEX": sum(
            len(item.missing_indexes or set())
            for item in (extra_results.get("index_mismatched", []) or [])
        ),
        "CONSTRAINT": sum(
            len(item.missing_constraints or set())
            for item in (extra_results.get("constraint_mismatched", []) or [])
        ),
        "SEQUENCE": sum(
            len(item.missing_sequences or set())
            for item in (extra_results.get("sequence_mismatched", []) or [])
        ),
        "TRIGGER": sum(
            len(item.missing_triggers or set())
            for item in (extra_results.get("trigger_mismatched", []) or [])
        ),
    }


def build_run_summary(
    ctx: RunSummaryContext,
    tv_results: ReportResults,
    extra_results: ExtraCheckResults,
    comment_results: Dict[str, object],
    dependency_report: DependencyReport,
    remap_conflicts: List[Tuple[str, str, str]],
    extraneous_rules: List[str],
    blacklisted_missing_tables: Optional[BlacklistTableMap],
    report_file: Optional[Path],
    filtered_grants_path: Optional[Path] = None,
    filtered_grants_count: int = 0,
    config_diagnostics: Optional[List[str]] = None,
    fixup_skip_summary: Optional[Dict[str, Dict[str, object]]] = None,
    support_summary: Optional[SupportClassificationResult] = None,
    noise_suppressed_count: int = 0,
    noise_suppressed_path: Optional[Path] = None
) -> RunSummary:
    end_time = datetime.now()
    total_seconds = time.perf_counter() - ctx.start_perf

    phases: List[RunPhaseInfo] = []
    for phase in RUN_PHASE_ORDER:
        if phase in ctx.phase_durations:
            phases.append(RunPhaseInfo(phase, ctx.phase_durations[phase], "完成"))
        else:
            reason = ctx.phase_skip_reasons.get(phase, "跳过")
            phases.append(RunPhaseInfo(phase, None, reason))

    actions_done: List[str] = []
    actions_skipped: List[str] = []

    if ctx.total_checked > 0:
        actions_done.append(
            f"主对象校验: {', '.join(sorted(ctx.enabled_primary_types))} (校验对象 {ctx.total_checked})"
        )
    else:
        actions_skipped.append("主对象校验: 主校验清单为空")

    if ctx.print_only_types:
        actions_done.append(f"仅打印不校验: {', '.join(sorted(ctx.print_only_types))}")

    if ctx.enabled_extra_types:
        actions_done.append(f"扩展对象校验: {', '.join(sorted(ctx.enabled_extra_types))}")
    else:
        actions_skipped.append("扩展对象校验: check_extra_types 为空")

    comment_skip_reason = comment_results.get("skipped_reason")
    if ctx.enable_comment_check:
        if comment_skip_reason:
            actions_skipped.append(f"注释一致性校验: 跳过 ({comment_skip_reason})")
        else:
            actions_done.append("注释一致性校验: 启用")
    else:
        actions_skipped.append("注释一致性校验: check_comments=false")

    if ctx.enable_dependencies_check:
        actions_done.append("依赖校验: 启用")
        if ctx.dependency_chain_file:
            actions_done.append(f"依赖链路输出: {ctx.dependency_chain_file}")
        if ctx.view_chain_file:
            actions_done.append(f"VIEW fixup 链路输出: {ctx.view_chain_file}")
    else:
        actions_skipped.append("依赖校验: check_dependencies=false")

    if ctx.enable_schema_mapping_infer:
        actions_done.append("schema 推导: 启用")
    else:
        actions_skipped.append("schema 推导: infer_schema_mapping=false")

    if ctx.fixup_enabled:
        actions_done.append(f"修补脚本生成: 启用 (目录 {ctx.fixup_dir})")
    else:
        actions_skipped.append("修补脚本生成: generate_fixup=false")

    if ctx.enable_grant_generation:
        if ctx.fixup_enabled:
            actions_done.append(
                "授权脚本生成: 启用 (目录 {miss}, {all})".format(
                    miss=Path(ctx.fixup_dir) / 'grants_miss',
                    all=Path(ctx.fixup_dir) / 'grants_all'
                )
            )
        else:
            actions_skipped.append("授权脚本生成: generate_fixup=false")
    else:
        actions_skipped.append("授权脚本生成: generate_grants=false")

    trigger_summary = ctx.trigger_list_summary or {}
    if trigger_summary.get("enabled"):
        if trigger_summary.get("error"):
            actions_done.append(f"触发器清单: 读取失败，已回退全量触发器 ({trigger_summary.get('error')})")
        elif trigger_summary.get("check_disabled"):
            actions_skipped.append("触发器清单: TRIGGER 未启用检查，仅校验清单格式")
        elif trigger_summary.get("fallback_full"):
            actions_done.append("触发器清单: 为空或无有效条目，已回退全量触发器")
        else:
            actions_done.append(
                "触发器清单: 生效 (列表 {valid_entries}, 命中缺失 {selected_missing})".format(
                    valid_entries=trigger_summary.get("valid_entries", 0),
                    selected_missing=trigger_summary.get("selected_missing", 0)
                )
            )
    else:
        actions_skipped.append("触发器清单: 未配置")

    if report_file:
        actions_done.append(f"报告输出: {Path(report_file).resolve()}")

    missing_count = len(tv_results.get("missing", []))
    mismatched_count = len(tv_results.get("mismatched", []))
    extra_target_cnt = len(tv_results.get("extra_targets", []))
    skipped_count = len(tv_results.get("skipped", []))
    remap_conflict_cnt = len(remap_conflicts)
    extraneous_count = len(extraneous_rules)
    blacklist_missing_cnt = len(blacklisted_missing_tables or {})
    comment_mis_cnt = len(comment_results.get("mismatched", []))
    idx_mis_cnt = len(extra_results.get("index_mismatched", []))
    cons_mis_cnt = len(extra_results.get("constraint_mismatched", []))
    seq_mis_cnt = len(extra_results.get("sequence_mismatched", []))
    trg_mis_cnt = len(extra_results.get("trigger_mismatched", []))
    extra_missing_counts = summarize_extra_missing_counts(extra_results)
    idx_missing_cnt = int(extra_missing_counts.get("INDEX", 0) or 0)
    cons_missing_cnt = int(extra_missing_counts.get("CONSTRAINT", 0) or 0)
    seq_missing_cnt = int(extra_missing_counts.get("SEQUENCE", 0) or 0)
    trg_missing_cnt = int(extra_missing_counts.get("TRIGGER", 0) or 0)
    dep_missing_cnt = len(dependency_report.get("missing", []))
    dep_unexpected_cnt = len(dependency_report.get("unexpected", []))
    dep_skipped_cnt = len(dependency_report.get("skipped", []))
    unsupported_by_type: Dict[str, int] = build_unsupported_summary_counts(support_summary, extra_results)
    findings: List[str] = [
        f"主对象: 缺失 {missing_count}, 不匹配 {mismatched_count}, 多余 {extra_target_cnt}, 仅打印 {skipped_count}"
    ]
    if unsupported_by_type:
        items = ", ".join(f"{k}={v}" for k, v in sorted(unsupported_by_type.items()))
        findings.append(f"缺失中不支持/阻断: {items}")
    if extraneous_count:
        findings.append(f"无效 remap 规则: {extraneous_count}")
    if remap_conflict_cnt:
        findings.append(f"无法推导对象: {remap_conflict_cnt}")
    if blacklist_missing_cnt:
        findings.append(f"黑名单缺失表: {blacklist_missing_cnt}")
    if ctx.enable_comment_check and not comment_skip_reason:
        findings.append(f"注释差异: {comment_mis_cnt}")
    else:
        findings.append(f"注释校验: 跳过 ({comment_skip_reason or '未启用'})")
    if ctx.enabled_extra_types:
        findings.append(
            "扩展对象差异: "
            f"INDEX 缺失对象 {idx_missing_cnt} (差异表 {idx_mis_cnt}), "
            f"CONSTRAINT 缺失对象 {cons_missing_cnt} (差异表 {cons_mis_cnt}), "
            f"SEQUENCE 缺失对象 {seq_missing_cnt} (差异表 {seq_mis_cnt}), "
            f"TRIGGER 缺失对象 {trg_missing_cnt} (差异表 {trg_mis_cnt})"
        )
    if ctx.enable_dependencies_check:
        findings.append(f"依赖差异: 缺失 {dep_missing_cnt}, 额外 {dep_unexpected_cnt}, 跳过 {dep_skipped_cnt}")
    if noise_suppressed_count:
        if noise_suppressed_path:
            findings.append(f"降噪项: {noise_suppressed_count} (见 {noise_suppressed_path})")
        else:
            findings.append(f"降噪项: {noise_suppressed_count}")
    if filtered_grants_count:
        if filtered_grants_path:
            findings.append(f"权限兼容过滤: {filtered_grants_count} 条 (见 {filtered_grants_path})")
        else:
            findings.append(f"权限兼容过滤: {filtered_grants_count} 条")
    if fixup_skip_summary and fixup_skip_summary.get("INDEX"):
        idx_summary = fixup_skip_summary.get("INDEX") or {}
        skipped_map = idx_summary.get("skipped", {}) or {}
        skipped_total = sum(int(v or 0) for v in skipped_map.values())
        findings.append(
            "INDEX 修补: 缺失 {missing}, 生成 {generated}, 跳过 {skipped}".format(
                missing=idx_summary.get("missing_total", 0),
                generated=idx_summary.get("generated", 0),
                skipped=skipped_total
            )
        )

    if trigger_summary.get("enabled"):
        if trigger_summary.get("fallback_full"):
            findings.append(
                "触发器清单: 回退全量，缺失触发器 {missing_not_listed}".format(
                    missing_not_listed=trigger_summary.get("missing_not_listed", 0)
                )
            )
        elif not trigger_summary.get("error"):
            findings.append(
                "触发器清单: 列表 {valid_entries}, 命中缺失 {selected_missing}, 未列出缺失 {missing_not_listed}, "
                "无效 {invalid_entries}, 未找到 {not_found}".format(
                    valid_entries=trigger_summary.get("valid_entries", 0),
                    selected_missing=trigger_summary.get("selected_missing", 0),
                    missing_not_listed=trigger_summary.get("missing_not_listed", 0),
                    invalid_entries=trigger_summary.get("invalid_entries", 0),
                    not_found=trigger_summary.get("not_found", 0)
                )
            )

    attention: List[str] = []
    if missing_count or mismatched_count or extra_target_cnt:
        attention.append("目标端结构与源端不一致，需要处理缺失/差异/多余对象。")
    if remap_conflict_cnt:
        attention.append("存在无法自动推导的对象，需要在 remap_rules.txt 显式配置。")
    if extraneous_count:
        attention.append("remap_rules.txt 存在无效条目，建议清理。")
    if ctx.enable_comment_check and comment_skip_reason:
        attention.append("注释一致性未完成校验，报告中的注释差异可能不完整。")
    if not ctx.enable_dependencies_check:
        attention.append("依赖校验已关闭，依赖差异可能未暴露。")
    if dep_missing_cnt or dep_unexpected_cnt:
        attention.append("依赖关系存在缺失或额外，需要补齐或清理。")
    if blacklist_missing_cnt:
        attention.append("存在黑名单表，未生成 OMS 规则。")
    if trigger_summary.get("error") or trigger_summary.get("invalid_entries"):
        attention.append("触发器清单存在读取失败或无效条目。")
    if not ctx.enable_grant_generation:
        attention.append("授权脚本生成已关闭，权限调整需人工确认。")
    elif ctx.enable_grant_generation and not ctx.fixup_enabled:
        attention.append("授权脚本生成依赖 generate_fixup=true，当前未输出授权脚本。")
    if config_diagnostics:
        attention.append(f"配置诊断提示 {len(config_diagnostics)} 项（详见报告）。")

    next_steps: List[str] = []
    if remap_conflict_cnt:
        next_steps.append("补充 remap_rules.txt，为无法推导对象显式配置映射。")
    if missing_count or mismatched_count or idx_mis_cnt or cons_mis_cnt or seq_mis_cnt or trg_mis_cnt:
        if ctx.fixup_enabled:
            next_steps.append(f"审核并执行 {ctx.fixup_dir} 中的修补脚本。")
        else:
            next_steps.append("如需自动生成修补脚本，请设置 generate_fixup=true。")
    if missing_count:
        if report_file:
            report_parent = Path(report_file).parent
            next_steps.append(
                f"将 {report_parent}/missed_tables_views_for_OMS 下的 schema_T.txt / schema_V.txt 规则提供给 OMS 进行迁移。"
            )
        else:
            next_steps.append("将 missed_tables_views_for_OMS 下的 schema_T.txt / schema_V.txt 规则提供给 OMS 进行迁移。")
    if blacklist_missing_cnt:
        if report_file:
            report_parent = Path(report_file).parent
            next_steps.append(f"查看 {report_parent}/blacklist_tables.txt，确认黑名单表处理方案。")
        else:
            next_steps.append("查看 blacklist_tables.txt，确认黑名单表处理方案。")
    if trigger_summary.get("invalid_entries") or trigger_summary.get("not_found"):
        next_steps.append("修正 trigger_list 清单内容后重新运行。")
    if dep_missing_cnt or dep_unexpected_cnt:
        next_steps.append("根据依赖差异报告补齐编译或授权。")
    if comment_mis_cnt:
        next_steps.append("确认注释差异是否需要修复。")
    if ctx.enable_grant_generation and ctx.fixup_enabled:
        next_steps.append(
            "审核 {miss} 中的授权脚本（全量审计见 {all}）。".format(
                miss=Path(ctx.fixup_dir) / 'grants_miss',
                all=Path(ctx.fixup_dir) / 'grants_all'
            )
        )
    if fixup_skip_summary and fixup_skip_summary.get("INDEX"):
        if report_file:
            report_parent = Path(report_file).parent
            next_steps.append(f"查看 {report_parent}/fixup_skip_summary_*.txt，确认索引修补跳过原因。")
        else:
            next_steps.append("查看 fixup_skip_summary_*.txt，确认索引修补跳过原因。")
    if unsupported_by_type:
        if report_file:
            report_parent = Path(report_file).parent
            next_steps.append(
                f"如需明细，设置 report_detail_mode=split 后查看 {report_parent}/report_index_*.txt，定位 unsupported_<type>_detail_*.txt。"
            )
        else:
            next_steps.append("如需明细，设置 report_detail_mode=split 后查看 report_index_*.txt，定位 unsupported_<type>_detail_*.txt。")

    return RunSummary(
        start_time=ctx.start_time,
        end_time=end_time,
        total_seconds=total_seconds,
        phases=phases,
        actions_done=actions_done,
        actions_skipped=actions_skipped,
        findings=findings,
        attention=attention,
        next_steps=next_steps
    )


def render_run_summary_panel(summary: RunSummary, width: int) -> Panel:
    def render_section(title: str, items: List[str], empty_text: str = "无") -> str:
        lines = [f"[bold]{title}[/bold]"]
        if not items:
            lines.append(f"- {empty_text}")
        else:
            lines.extend([f"- {item}" for item in items])
        return "\n".join(lines)

    phase_lines: List[str] = []
    for phase in summary.phases:
        if phase.duration is not None:
            phase_lines.append(f"- {phase.name}: {format_duration(phase.duration)}")
        else:
            phase_lines.append(f"- {phase.name}: 跳过 ({phase.status})")

    overview = "\n".join([
        "[bold]运行概览[/bold]",
        f"- 开始时间: {summary.start_time.strftime('%Y-%m-%d %H:%M:%S')}",
        f"- 结束时间: {summary.end_time.strftime('%Y-%m-%d %H:%M:%S')}",
        f"- 总耗时: {format_duration(summary.total_seconds)}"
    ])
    phases = "\n".join(["[bold]阶段耗时[/bold]"] + phase_lines)
    actions = render_section("本次执行", summary.actions_done, empty_text="无已执行动作")
    skipped = render_section("本次未执行", summary.actions_skipped, empty_text="无")
    findings = render_section("关键发现", summary.findings, empty_text="无")
    attention = render_section("需要注意", summary.attention, empty_text="无")
    next_steps = render_section("下一步建议", summary.next_steps, empty_text="无")

    text = "\n\n".join([overview, phases, actions, skipped, findings, attention, next_steps])
    return Panel.fit(text, title="[info]运行总结", border_style="info", width=width)


def log_run_summary(summary: RunSummary) -> None:
    log_section("运行总结")
    log.info("开始时间: %s", summary.start_time.strftime("%Y-%m-%d %H:%M:%S"))
    log.info("结束时间: %s", summary.end_time.strftime("%Y-%m-%d %H:%M:%S"))
    log.info("总耗时: %s", format_duration(summary.total_seconds))

    log_subsection("阶段耗时")
    for phase in summary.phases:
        if phase.duration is not None:
            log.info("%s: %s", phase.name, format_duration(phase.duration))
        else:
            log.info("%s: 跳过 (%s)", phase.name, phase.status)

    log_subsection("本次执行")
    for item in summary.actions_done:
        log.info("完成: %s", item)
    for item in summary.actions_skipped:
        log.info("跳过: %s", item)

    log_subsection("关键发现")
    for item in summary.findings:
        log.info("%s", item)

    log_subsection("需要注意")
    if summary.attention:
        for item in summary.attention:
            log.info("%s", item)
    else:
        log.info("无")

    log_subsection("下一步建议")
    if summary.next_steps:
        for item in summary.next_steps:
            log.info("%s", item)
    else:
        log.info("无")

def print_final_report(
    tv_results: ReportResults,
    total_checked: int,
    extra_results: Optional[ExtraCheckResults] = None,
    comment_results: Optional[Dict[str, object]] = None,
    dependency_report: Optional[DependencyReport] = None,
    report_file: Optional[Path] = None,
    object_counts_summary: Optional[ObjectCountSummary] = None,
    endpoint_info: Optional[Dict[str, Dict[str, str]]] = None,
    schema_summary: Optional[Dict[str, List[str]]] = None,
    settings: Optional[Dict] = None,
    blacklisted_missing_tables: Optional[BlacklistTableMap] = None,
    blacklist_report_rows: Optional[List[BlacklistReportRow]] = None,
    trigger_list_summary: Optional[Dict[str, object]] = None,
    trigger_list_rows: Optional[List[TriggerListReportRow]] = None,
    trigger_status_rows: Optional[List[TriggerStatusReportRow]] = None,
    constraint_status_rows: Optional[List[ConstraintStatusDriftRow]] = None,
    package_results: Optional[PackageCompareResults] = None,
    run_summary_ctx: Optional[RunSummaryContext] = None,
    filtered_grants: Optional[List[FilteredGrantEntry]] = None,
    config_diagnostics: Optional[List[str]] = None,
    fixup_skip_summary: Optional[Dict[str, Dict[str, object]]] = None,
    support_summary: Optional[SupportClassificationResult] = None,
    noise_suppressed_details: Optional[List[NoiseSuppressedDetail]] = None,
    usability_summary: Optional[UsabilitySummary] = None
):
    custom_theme = Theme({
        "ok": "green",
        "missing": "red",
        "mismatch": "yellow",
        "info": "cyan",
        "header": "bold magenta",
        "title": "bold white on blue"
    })
    # 从配置读取报告宽度，避免在nohup等非交互式环境下被截断为80
    if settings:
        try:
            report_width = int(settings.get('report_width', 160))
        except (TypeError, ValueError):
            report_width = 160
    else:
        report_width = 160
    console = Console(theme=custom_theme, record=report_file is not None, width=report_width)
    report_detail_mode = (
        settings.get('report_detail_mode', 'split')
        if settings else 'full'
    )
    report_detail_mode = normalize_report_detail_mode(report_detail_mode)
    show_detail_sections = report_detail_mode == "full"
    emit_detail_files = report_detail_mode == "split"
    report_ts: Optional[str] = None
    if report_file:
        report_name = Path(report_file).name
        if report_name.startswith("report_") and report_name.endswith(".txt"):
            report_ts = report_name[len("report_"):-4]

    if extra_results is None:
        extra_results = {
            "index_ok": [], "index_mismatched": [], "index_unsupported": [],
            "constraint_ok": [], "constraint_mismatched": [], "sequence_ok": [], "sequence_mismatched": [],
            "trigger_ok": [], "trigger_mismatched": [], "constraint_unsupported": [],
            "trigger_status_drift": [], "constraint_status_drift": [],
        }
    if comment_results is None:
        comment_results = {
            "ok": [],
            "mismatched": [],
            "skipped_reason": "未执行注释比对。"
        }
    if dependency_report is None:
        dependency_report = {
            "missing": [],
            "unexpected": [],
            "skipped": []
        }
    trigger_status_rows = trigger_status_rows or []
    constraint_status_rows = constraint_status_rows or []
    noise_suppressed_details = noise_suppressed_details or []
    if schema_summary is None:
        schema_summary = {
            "source_missing": [],
            "target_missing": [],
            "target_extra": []
        }

    log.info("所有校验已完成。正在生成最终报告...")

    ok_count = len(tv_results['ok'])
    missing_count = len(tv_results['missing'])
    mismatched_count = len(tv_results['mismatched'])
    skipped_count = len(tv_results.get('skipped', []))
    remap_conflicts = tv_results.get('remap_conflicts', [])
    remap_conflict_cnt = len(remap_conflicts)
    extraneous_count = len(tv_results['extraneous'])
    idx_ok_cnt = len(extra_results.get("index_ok", []))
    idx_mis_cnt = len(extra_results.get("index_mismatched", []))
    idx_unsupported_cnt = len(extra_results.get("index_unsupported", []) or [])
    cons_ok_cnt = len(extra_results.get("constraint_ok", []))
    cons_mis_cnt = len(extra_results.get("constraint_mismatched", []))
    seq_ok_cnt = len(extra_results.get("sequence_ok", []))
    seq_mis_cnt = len(extra_results.get("sequence_mismatched", []))
    trg_ok_cnt = len(extra_results.get("trigger_ok", []))
    trg_mis_cnt = len(extra_results.get("trigger_mismatched", []))
    trg_status_drift_cnt = len(trigger_status_rows)
    cons_status_drift_cnt = len(constraint_status_rows)
    extra_missing_counts = summarize_extra_missing_counts(extra_results)
    idx_missing_cnt = int(extra_missing_counts.get("INDEX", 0) or 0)
    cons_missing_cnt = int(extra_missing_counts.get("CONSTRAINT", 0) or 0)
    seq_missing_cnt = int(extra_missing_counts.get("SEQUENCE", 0) or 0)
    trg_missing_cnt = int(extra_missing_counts.get("TRIGGER", 0) or 0)
    comment_ok_cnt = len(comment_results.get("ok", []))
    comment_mis_cnt = len(comment_results.get("mismatched", []))
    comment_skip_reason = comment_results.get("skipped_reason")
    column_order_mismatches = list(tv_results.get("column_order_mismatched", []) or [])
    column_order_mismatch_cnt = len(column_order_mismatches)
    noise_suppressed_count = len(noise_suppressed_details)
    extra_target_cnt = len(tv_results.get('extra_targets', []))
    dep_missing_cnt = len(dependency_report.get("missing", []))
    dep_unexpected_cnt = len(dependency_report.get("unexpected", []))
    dep_skipped_cnt = len(dependency_report.get("skipped", []))
    usability_checked_cnt = 0
    usability_usable_cnt = 0
    usability_unusable_cnt = 0
    usability_expected_unusable_cnt = 0
    usability_unexpected_usable_cnt = 0
    usability_timeout_cnt = 0
    usability_skipped_cnt = 0
    usability_sampled_out_cnt = 0
    if usability_summary:
        usability_checked_cnt = usability_summary.total_checked
        usability_usable_cnt = usability_summary.total_usable
        usability_unusable_cnt = usability_summary.total_unusable
        usability_expected_unusable_cnt = usability_summary.total_expected_unusable
        usability_unexpected_usable_cnt = usability_summary.total_unexpected_usable
        usability_timeout_cnt = usability_summary.total_timeout
        usability_skipped_cnt = usability_summary.total_skipped
        usability_sampled_out_cnt = usability_summary.total_sampled_out
    source_missing_schema_cnt = len(schema_summary.get("source_missing", []))
    package_rows: List[PackageCompareRow] = []
    package_diff_rows: List[PackageCompareRow] = []
    package_summary: Dict[str, int] = {}
    if package_results:
        package_rows = list(package_results.get("rows") or [])
        package_diff_rows = list(package_results.get("diff_rows") or [])
        def _package_sort_key(row: PackageCompareRow) -> Tuple[str, str, int, str, str]:
            parsed = parse_full_object_name(row.src_full) or parse_full_object_name(row.tgt_full)
            owner, name = parsed if parsed else ("", "")
            type_u = (row.obj_type or "").upper()
            type_rank = 0 if type_u == "PACKAGE" else 1 if type_u == "PACKAGE BODY" else 2
            return (owner, name, type_rank, row.src_full, row.tgt_full)
        if package_diff_rows:
            package_diff_rows = sorted(package_diff_rows, key=_package_sort_key)
        package_summary = dict(package_results.get("summary") or {})
    package_missing_cnt = int(package_summary.get("MISSING_TARGET", 0) or 0)
    package_src_invalid_cnt = int(package_summary.get("SOURCE_INVALID", 0) or 0)
    package_tgt_invalid_cnt = int(package_summary.get("TARGET_INVALID", 0) or 0)
    package_status_mismatch_cnt = int(package_summary.get("STATUS_MISMATCH", 0) or 0)
    support_counts = dict(support_summary.missing_support_counts) if support_summary else {}
    unsupported_rows = list(support_summary.unsupported_rows) if support_summary else []
    missing_detail_rows = list(support_summary.missing_detail_rows) if support_summary else []
    extra_missing_rows = list(support_summary.extra_missing_rows) if support_summary else []
    extra_blocked_counts = dict(support_summary.extra_blocked_counts) if support_summary else {}
    view_constraint_cleaned_rows = list(support_summary.view_constraint_cleaned_rows) if support_summary else []
    view_constraint_uncleanable_rows = list(support_summary.view_constraint_uncleanable_rows) if support_summary else []
    extra_constraint_unsupported = list(extra_results.get("constraint_unsupported") or [])
    unsupported_detail_rows = list(unsupported_rows)
    if extra_constraint_unsupported:
        unsupported_detail_rows.extend(convert_constraint_unsupported_rows(extra_constraint_unsupported))
    unsupported_summary_counts = build_unsupported_summary_counts(support_summary, extra_results)
    blocked_index_rows = _filter_blocked_support_rows(unsupported_rows, "INDEX") if unsupported_rows else []
    blocked_constraint_rows = _filter_blocked_support_rows(unsupported_rows, "CONSTRAINT") if unsupported_rows else []
    blocked_trigger_rows = _filter_blocked_support_rows(unsupported_rows, "TRIGGER") if unsupported_rows else []
    missing_supported_rows = [
        row for row in missing_detail_rows
        if row.support_state == SUPPORT_STATE_SUPPORTED
    ]
    combined_missing_rows = list(missing_detail_rows)
    if extra_missing_rows:
        combined_missing_rows.extend(extra_missing_rows)
        missing_supported_rows.extend(extra_missing_rows)
    missing_supported_cnt = len(missing_supported_rows)
    unsupported_blocked_cnt = len([
        row for row in unsupported_rows
        if row.support_state in {SUPPORT_STATE_UNSUPPORTED, SUPPORT_STATE_BLOCKED}
    ])

    console.print(Panel.fit(f"[bold]数据库对象迁移校验报告 (V{__version__} - Rich)[/bold]", style="title"))
    console.print(f"[info]项目主页: {REPO_URL} | 问题反馈: {REPO_ISSUES_URL}[/info]")
    console.print("")

    section_width = 140
    count_table_kwargs: Dict[str, object] = {"width": section_width, "expand": False}
    TYPE_COL_WIDTH = 16
    OBJECT_COL_WIDTH = 42
    DETAIL_COL_WIDTH = 90

    def format_endpoint_block(info: Dict[str, str], is_oracle: bool) -> str:
        lines: List[str] = []
        if not info:
            return "无可用信息"
        if info.get("version"):
            lines.append(f"版本: {info['version']}")
        if is_oracle:
            if info.get("cdb_mode"):
                lines.append(f"CDB/PDB: {info['cdb_mode']}")
            if info.get("container"):
                lines.append(f"容器: {info['container']}")
            if info.get("service_name"):
                lines.append(f"服务名: {info['service_name']}")
        else:
            if info.get("current_database"):
                lines.append(f"当前库: {info['current_database']}")
            if info.get("connection_id"):
                lines.append(f"连接 ID: {info['connection_id']}")
            if info.get("ssl"):
                lines.append(f"SSL: {info['ssl']}")
        host = info.get("host")
        port = info.get("port")
        if host or port:
            lines.append(f"地址: {host or ''}:{port or ''}")
        user_label = "连接用户" if is_oracle else "当前用户"
        user_value = info.get("current_user") or info.get("user") or info.get("configured_user", "")
        if is_oracle and user_value:
            user_value = str(user_value).upper()
        lines.append(f"{user_label}: {user_value}")
        if is_oracle and info.get("dsn"):
            lines.append(f"DSN: {info['dsn']}")
        return "\n".join([line for line in lines if line.strip()]) or "无可用信息"

    if endpoint_info:
        src_info = endpoint_info.get("oracle", {})
        tgt_info = endpoint_info.get("oceanbase", {})
        env_table = Table(title="[header]源/目标环境", width=section_width)
        env_table.add_column("源 (Oracle)", width=section_width // 2)
        env_table.add_column("目标 (OceanBase)", width=section_width // 2)
        env_table.add_row(
            format_endpoint_block(src_info, True),
            format_endpoint_block(tgt_info, False)
        )
        console.print(env_table)
        console.print("")

    unsupported_total = 0
    if support_counts:
        for data in support_counts.values():
            unsupported_total += int(data.get("unsupported", 0) or 0)
            unsupported_total += int(data.get("blocked", 0) or 0)
    idx_blocked_cnt = extra_blocked_counts.get("INDEX", 0)
    cons_blocked_cnt = extra_blocked_counts.get("CONSTRAINT", 0)
    trg_blocked_cnt = extra_blocked_counts.get("TRIGGER", 0)
    cons_unsupported_cnt = len(extra_results.get("constraint_unsupported", []) or [])

    def build_execution_conclusion() -> Panel:
        ext_mismatch_cnt = idx_missing_cnt + cons_missing_cnt + seq_missing_cnt + trg_missing_cnt
        actionable_cnt = missing_count + mismatched_count + ext_mismatch_cnt
        blocked_total = (
            unsupported_total
            + idx_blocked_cnt
            + idx_unsupported_cnt
            + cons_blocked_cnt
            + cons_unsupported_cnt
            + trg_blocked_cnt
        )
        status = "通过" if actionable_cnt == 0 and blocked_total == 0 else "需处理"
        lines: List[str] = [
            f"状态: {status} | 总校验对象: {total_checked}",
            f"主对象: 缺失 {missing_count}, 不匹配 {mismatched_count}, 多余 {extra_target_cnt}",
            (
                "扩展对象差异(缺失对象): "
                f"索引 {idx_missing_cnt}, 约束 {cons_missing_cnt}, "
                f"序列 {seq_missing_cnt}, 触发器 {trg_missing_cnt}"
            ),
        ]
        if missing_supported_cnt or unsupported_blocked_cnt:
            lines.append(
                f"迁移聚焦: 缺失可修补 {missing_supported_cnt}, 不兼容/阻断 {unsupported_blocked_cnt}"
            )
        if blocked_total:
            lines.append(f"不支持/阻断: {blocked_total}")
        next_steps: List[str] = []
        if actionable_cnt:
            if settings and not bool(settings.get("generate_fixup", True)):
                next_steps.append("如需生成修补脚本，请开启 generate_fixup")
            else:
                next_steps.append("查看 fixup_scripts/ 下的缺失与修补脚本")
        if blocked_total:
            suffix = f"_{report_ts}" if report_ts else "_*"
            next_steps.append(f"查看 unsupported_objects_detail{suffix}.txt 或 blacklist_tables.txt")
        if report_ts:
            next_steps.append(f"查看 migration_focus_{report_ts}.txt 汇总迁移动作清单")
        if next_steps:
            lines.append("下一步: " + "；".join(next_steps))
        return Panel.fit("\n".join(lines), title="[header]执行结论", border_style="info")

    console.print(build_execution_conclusion())
    console.print("")

    # --- 综合概要 ---
    summary_table = Table(
        title="[header]综合概要",
        show_header=False,
        box=None,
        width=section_width,
        pad_edge=False,
        padding=(0, 1)
    )
    summary_table.add_column("Category", justify="left", width=24, no_wrap=True)
    summary_table.add_column("Details", justify="left", width=section_width - 28)

    schema_text = Text()
    schema_text.append("源 schema 未获取到对象: ", style="mismatch")
    schema_text.append(f"{source_missing_schema_cnt}")
    summary_table.add_row("[bold]Schema 覆盖[/bold]", schema_text)

    blacklist_missing_cnt = len(blacklisted_missing_tables or {})
    primary_text = Text()
    primary_text.append(f"总计校验对象 (来自源库): {total_checked}\n")
    primary_text.append("一致: ", style="ok")
    primary_text.append(f"{ok_count}\n")
    primary_text.append("缺失: ", style="missing")
    primary_text.append(f"{missing_count}\n")
    if unsupported_total:
        primary_text.append("缺失(不支持/阻断): ", style="mismatch")
        primary_text.append(f"{unsupported_total}\n", style="mismatch")
    primary_text.append("不匹配 (表列/长度): ", style="mismatch")
    primary_text.append(f"{mismatched_count}\n")
    primary_text.append("多余: ", style="mismatch")
    primary_text.append(f"{extra_target_cnt}\n")
    if skipped_count:
        primary_text.append("仅打印: ", style="info")
        primary_text.append(f"{skipped_count}\n")
    primary_text.append("无效 Remap 规则: ", style="mismatch")
    primary_text.append(f"{extraneous_count}")
    if remap_conflict_cnt:
        primary_text.append("\n")
        primary_text.append("无法推导: ", style="mismatch")
        primary_text.append(f"{remap_conflict_cnt}")
    summary_table.add_row("[bold]主对象 (TABLE/VIEW/etc.)[/bold]", primary_text)

    if settings is not None:
        gate_text = Text()
        ob_version_text = (settings.get("ob_version") or "").strip() or "unknown"
        gate_text.append("OB版本: ", style="info")
        gate_text.append(ob_version_text)
        gate_text.append("\ninterval分区补齐: ", style="info")
        gate_text.append(
            "开启" if settings.get("effective_interval_fixup_enabled") else "关闭",
            style="ok" if settings.get("effective_interval_fixup_enabled") else "mismatch"
        )
        gate_text.append(
            f" (mode={settings.get('generate_interval_partition_fixup_mode', '-')}, "
            f"{(settings.get('ob_feature_gate_decisions') or {}).get('interval', '-')})",
            style="info"
        )
        gate_text.append("\nMATERIALIZED VIEW: ", style="info")
        mview_enabled = bool(settings.get("effective_mview_enabled"))
        gate_text.append(
            "校验+修补" if mview_enabled else "仅打印",
            style="ok" if mview_enabled else "mismatch"
        )
        gate_text.append(
            f" (mode={settings.get('mview_check_fixup_mode', '-')}, "
            f"{(settings.get('ob_feature_gate_decisions') or {}).get('mview', '-')})",
            style="info"
        )
        summary_table.add_row("[bold]版本门控[/bold]", gate_text)

    focus_text = Text()
    focus_text.append("缺失可修补: ", style="missing")
    focus_text.append(f"{missing_supported_cnt}")
    focus_text.append("\n不兼容/阻断: ", style="mismatch")
    focus_text.append(f"{unsupported_blocked_cnt}")
    if report_ts:
        focus_text.append("\n详见: ", style="info")
        focus_text.append(f"migration_focus_{report_ts}.txt", style="info")
    summary_table.add_row("[bold]迁移聚焦[/bold]", focus_text)

    if package_rows:
        pkg_text = Text()
        pkg_text.append("源端无效: ", style="mismatch")
        pkg_text.append(f"{package_src_invalid_cnt}\n")
        pkg_text.append("目标缺失: ", style="missing")
        pkg_text.append(f"{package_missing_cnt}\n")
        pkg_text.append("目标无效: ", style="mismatch")
        pkg_text.append(f"{package_tgt_invalid_cnt}\n")
        pkg_text.append("状态不一致: ", style="mismatch")
        pkg_text.append(f"{package_status_mismatch_cnt}")
        if report_file:
            pkg_report_hint = derive_package_report_path(Path(report_file))
            pkg_text.append("\n")
            pkg_text.append(f"详见: {pkg_report_hint.name}", style="info")
        summary_table.add_row("[bold]PACKAGE/PKG BODY[/bold]", pkg_text)

    comment_text = Text()
    if comment_skip_reason:
        comment_text.append(str(comment_skip_reason), style="info")
    else:
        comment_text.append("一致: ", style="ok")
        comment_text.append(f"{comment_ok_cnt}\n")
        comment_text.append("差异: ", style="mismatch")
        comment_text.append(f"{comment_mis_cnt}")
    summary_table.add_row("[bold]注释一致性[/bold]", comment_text)

    if view_constraint_cleaned_rows or view_constraint_uncleanable_rows:
        vc_text = Text()
        vc_text.append("清洗: ", style="info")
        vc_text.append(str(len(view_constraint_cleaned_rows)), style="info")
        vc_text.append("\n无法清洗: ", style="mismatch")
        vc_text.append(str(len(view_constraint_uncleanable_rows)), style="mismatch")
        if emit_detail_files and report_ts:
            vc_text.append("\n详见: ", style="info")
            vc_text.append(
                f"view_constraint_cleaned_detail_{report_ts}.txt / "
                f"view_constraint_uncleanable_detail_{report_ts}.txt",
                style="info"
            )
        summary_table.add_row("[bold]VIEW 约束清洗[/bold]", vc_text)

    if column_order_mismatch_cnt:
        order_text = Text()
        order_text.append("差异: ", style="mismatch")
        order_text.append(f"{column_order_mismatch_cnt}", style="mismatch")
        if emit_detail_files and report_ts:
            order_text.append("\n")
            order_text.append(
                f"详见: column_order_mismatch_detail_{report_ts}.txt",
                style="info"
            )
        summary_table.add_row("[bold]列顺序差异[/bold]", order_text)

    if noise_suppressed_count:
        noise_text = Text()
        noise_text.append("降噪项: ", style="info")
        noise_text.append(f"{noise_suppressed_count}")
        if emit_detail_files and report_ts:
            noise_text.append("\n")
            noise_text.append(f"详见: noise_suppressed_detail_{report_ts}.txt", style="info")
        summary_table.add_row("[bold]降噪统计[/bold]", noise_text)

    ext_text = Text()
    ext_text.append("索引: ", style="info")
    ext_text.append(f"一致表 {idx_ok_cnt} / ", style="ok")
    ext_text.append(f"缺失对象 {idx_missing_cnt}", style="mismatch")
    ext_text.append(f" (差异表 {idx_mis_cnt})", style="info")
    if idx_blocked_cnt or idx_unsupported_cnt:
        ext_text.append(
            f" (不支持/阻断 {idx_blocked_cnt + idx_unsupported_cnt})",
            style="mismatch"
        )
    ext_text.append("\n")
    ext_text.append("约束: ", style="info")
    ext_text.append(f"一致表 {cons_ok_cnt} / ", style="ok")
    ext_text.append(f"缺失对象 {cons_missing_cnt}", style="mismatch")
    ext_text.append(f" (差异表 {cons_mis_cnt})", style="info")
    if cons_blocked_cnt or cons_unsupported_cnt:
        ext_text.append(
            f" (不支持/阻断 {cons_blocked_cnt + cons_unsupported_cnt})",
            style="mismatch"
        )
    ext_text.append("\n")
    ext_text.append("序列: ", style="info")
    ext_text.append(f"一致 schema {seq_ok_cnt} / ", style="ok")
    ext_text.append(f"缺失对象 {seq_missing_cnt}", style="mismatch")
    ext_text.append(f" (差异 schema {seq_mis_cnt})\n", style="info")
    ext_text.append("触发器: ", style="info")
    ext_text.append(f"一致表 {trg_ok_cnt} / ", style="ok")
    ext_text.append(f"缺失对象 {trg_missing_cnt}", style="mismatch")
    ext_text.append(f" (差异表 {trg_mis_cnt})", style="info")
    if trg_blocked_cnt:
        ext_text.append(f" (不支持/阻断 {trg_blocked_cnt})", style="mismatch")
    summary_table.add_row("[bold]扩展对象 (INDEX/SEQ/etc.)[/bold]", ext_text)

    if trigger_list_summary and trigger_list_summary.get("enabled"):
        filter_text = Text()
        if trigger_list_summary.get("error"):
            filter_text.append(
                f"trigger_list 读取失败: {trigger_list_summary.get('error')} (已回退全量触发器)",
                style="mismatch"
            )
        elif trigger_list_summary.get("check_disabled"):
            filter_text.append("TRIGGER 未启用检查，清单仅做格式校验。", style="mismatch")
        elif trigger_list_summary.get("fallback_full"):
            filter_text.append("清单为空或无有效条目，已回退全量触发器。", style="info")
        else:
            filter_text.append("列表: ", style="info")
            filter_text.append(str(trigger_list_summary.get("valid_entries", 0)))
            filter_text.append("  命中缺失: ", style="info")
            filter_text.append(str(trigger_list_summary.get("selected_missing", 0)), style="ok")
            filter_text.append("  未列出缺失: ", style="info")
            filter_text.append(str(trigger_list_summary.get("missing_not_listed", 0)), style="mismatch")
            invalid_cnt = trigger_list_summary.get("invalid_entries", 0) or 0
            not_found_cnt = trigger_list_summary.get("not_found", 0) or 0
            not_missing_cnt = trigger_list_summary.get("not_missing", 0) or 0
            if invalid_cnt:
                filter_text.append("  无效: ", style="mismatch")
                filter_text.append(str(invalid_cnt), style="mismatch")
            if not_found_cnt:
                filter_text.append("  未找到: ", style="mismatch")
                filter_text.append(str(not_found_cnt), style="mismatch")
            if not_missing_cnt:
                filter_text.append("  非缺失: ", style="info")
                filter_text.append(str(not_missing_cnt), style="info")
        summary_table.add_row("[bold]触发器筛选[/bold]", filter_text)

    if trigger_status_rows or constraint_status_rows:
        status_text = Text()
        status_text.append("TRIGGER 差异: ", style="mismatch")
        status_text.append(str(trg_status_drift_cnt), style="mismatch")
        status_text.append("\nCONSTRAINT 差异: ", style="mismatch")
        status_text.append(str(cons_status_drift_cnt), style="mismatch")
        if report_file:
            status_text.append("\n详见: trigger_status_report.txt / ", style="info")
            status_text.append(f"status_drift_detail_{report_ts or '*'}.txt", style="info")
        summary_table.add_row("[bold]状态漂移[/bold]", status_text)

    dep_text = Text()
    dep_text.append("缺失依赖: ", style="missing")
    dep_text.append(f"{dep_missing_cnt}  ")
    dep_text.append("多余依赖: ", style="mismatch")
    dep_text.append(f"{dep_unexpected_cnt}  ")
    dep_text.append("跳过: ", style="info")
    dep_text.append(f"{dep_skipped_cnt}")
    summary_table.add_row("[bold]依赖关系[/bold]", dep_text)

    if usability_summary:
        usability_text = Text()
        usability_text.append("校验对象: ", style="info")
        usability_text.append(str(usability_checked_cnt))
        if usability_sampled_out_cnt:
            usability_text.append(f" (抽样跳过 {usability_sampled_out_cnt})", style="info")
        usability_text.append("\n可用: ", style="ok")
        usability_text.append(str(usability_usable_cnt), style="ok")
        usability_text.append("  不可用: ", style="mismatch")
        usability_text.append(str(usability_unusable_cnt), style="mismatch")
        usability_text.append("\n预期不可用: ", style="info")
        usability_text.append(str(usability_expected_unusable_cnt), style="info")
        usability_text.append("  意外可用: ", style="mismatch")
        usability_text.append(str(usability_unexpected_usable_cnt), style="mismatch")
        usability_text.append("\n超时: ", style="mismatch")
        usability_text.append(str(usability_timeout_cnt), style="mismatch")
        if usability_skipped_cnt:
            usability_text.append("  跳过: ", style="info")
            usability_text.append(str(usability_skipped_cnt), style="info")
        if report_ts:
            usability_text.append("\n详见: ", style="info")
            usability_text.append(f"usability_check_detail_{report_ts}.txt", style="info")
        summary_table.add_row("[bold]对象可用性 (VIEW/SYNONYM)[/bold]", usability_text)

    console.print(summary_table)
    console.print("")
    console.print("")

    if not show_detail_sections:
        detail_hint_lines = [
            f"report_detail_mode={report_detail_mode}，主报告仅保留概要。"
        ]
        if settings.get("report_to_db", False):
            report_dir_effective = settings.get("report_dir_effective") or ""
            store_scope = str(settings.get("report_db_store_scope", "full") or "full").strip().lower()
            if store_scope == "full":
                db_hint = (
                    "DB 覆盖范围提醒: full 模式下 run 目录 txt 已逐行入库 "
                    "(DIFF_REPORT_ARTIFACT_LINE)；可在数据库内完整查询文本报告。"
                )
            else:
                db_hint = (
                    "DB 覆盖范围提醒: 缺失/不支持/阻断/可用性/触发器状态/包对比摘要已入库；"
                    "依赖链/格式化/黑名单/索引等细节仍在 run 目录文本中。"
                )
            if report_dir_effective:
                db_hint = f"{db_hint} run_dir={report_dir_effective}"
            detail_hint_lines.append(db_hint)
        if emit_detail_files:
            detail_hint_lines.append(
                "详细差异请查看分拆报告：missing_objects_detail / unsupported_objects_detail / extra_mismatch_detail 等。"
            )
            if report_ts:
                detail_hint_lines.append(f"示例: missing_objects_detail_{report_ts}.txt")
                detail_hint_lines.append(f"报告索引: report_index_{report_ts}.txt")
                detail_hint_lines.append(
                    "按类型明细: missing_<type>_detail / unsupported_<type>_detail (含 ROOT_CAUSE，详见 report_index)"
                )
            if column_order_mismatch_cnt and report_ts:
                detail_hint_lines.append(f"列顺序差异明细: column_order_mismatch_detail_{report_ts}.txt")
            if noise_suppressed_count and report_ts:
                detail_hint_lines.append(f"降噪明细: noise_suppressed_detail_{report_ts}.txt")
        if extra_results.get("index_unsupported"):
            suffix = f"_{report_ts}" if report_ts else ""
            detail_hint_lines.append(
                f"索引语法不支持明细(仅DESC): indexes_unsupported_detail{suffix}.txt"
            )
        if extra_results.get("constraint_unsupported"):
            suffix = f"_{report_ts}" if report_ts else ""
            detail_hint_lines.append(
                f"约束语法不支持明细(DEFERRABLE/自引用外键等): constraints_unsupported_detail{suffix}.txt"
            )
        if view_constraint_cleaned_rows:
            suffix = f"_{report_ts}" if report_ts else ""
            detail_hint_lines.append(
                f"VIEW 约束清洗明细: view_constraint_cleaned_detail{suffix}.txt"
            )
        if view_constraint_uncleanable_rows:
            suffix = f"_{report_ts}" if report_ts else ""
            detail_hint_lines.append(
                f"VIEW 约束无法清洗明细: view_constraint_uncleanable_detail{suffix}.txt"
            )
        if usability_summary and report_ts:
            detail_hint_lines.append(
                f"对象可用性明细: usability_check_detail_{report_ts}.txt"
            )
        if emit_detail_files and report_ts:
            if blocked_index_rows:
                detail_hint_lines.append(
                    f"索引依赖阻断明细: indexes_blocked_detail_{report_ts}.txt"
                )
            if blocked_constraint_rows:
                detail_hint_lines.append(
                    f"约束依赖阻断明细: constraints_blocked_detail_{report_ts}.txt"
                )
            if blocked_trigger_rows:
                detail_hint_lines.append(
                    f"触发器依赖阻断明细: triggers_blocked_detail_{report_ts}.txt"
                )
        console.print(Panel.fit("\n".join(detail_hint_lines), style="info", width=section_width))

    if config_diagnostics:
        diag_table = Table(title="[header]0.c 配置诊断[/header]", width=section_width)
        diag_table.add_column("提示", style="mismatch")
        for item in config_diagnostics:
            diag_table.add_row(item)
        console.print(diag_table)
        console.print("")

    if fixup_skip_summary and fixup_skip_summary.get("INDEX"):
        idx_summary = fixup_skip_summary.get("INDEX") or {}
        skip_table = Table(title="[header]0.d Fixup 跳过汇总 (INDEX)[/header]", width=section_width)
        skip_table.add_column("指标", style="info", width=36)
        skip_table.add_column("数量", justify="right")
        skip_table.add_row("缺失总数", str(idx_summary.get("missing_total", 0)))
        skip_table.add_row("进入生成任务", str(idx_summary.get("task_total", 0)))
        skip_table.add_row("实际生成脚本", str(idx_summary.get("generated", 0)))
        skipped_map = idx_summary.get("skipped", {}) or {}
        if skipped_map:
            for reason, count in sorted(skipped_map.items()):
                skip_table.add_row(f"跳过: {reason}", str(count))
        console.print(skip_table)
        console.print("")

    def summarize_actions() -> Panel:
        modify_counts = OrderedDict()
        modify_counts["TABLE (列差异修补)"] = len(tv_results.get('mismatched', []))

        addition_counts: Dict[str, int] = defaultdict(int)
        for obj_type, _, _ in tv_results.get('missing', []):
            addition_counts[obj_type.upper()] += 1
        for item in extra_results.get("index_mismatched", []):
            addition_counts["INDEX"] += len(item.missing_indexes)
        for item in extra_results.get("constraint_mismatched", []):
            addition_counts["CONSTRAINT"] += len(item.missing_constraints)
        for item in extra_results.get("sequence_mismatched", []):
            addition_counts["SEQUENCE"] += len(item.missing_sequences)
        for item in extra_results.get("trigger_mismatched", []):
            addition_counts["TRIGGER"] += len(item.missing_triggers)
        if trigger_list_summary and trigger_list_summary.get("enabled"):
            if not trigger_list_summary.get("fallback_full") and not trigger_list_summary.get("error"):
                addition_counts["TRIGGER"] = int(trigger_list_summary.get("selected_missing", 0) or 0)

        def format_block(title: str, data: OrderedDict) -> str:
            lines = [f"[bold]{title}[/bold]"]
            entries = [(k, v) for k, v in data.items() if v > 0]
            if not entries:
                lines.append("  - 无")
            else:
                for k, v in entries:
                    lines.append(f"  - {k}: {v}")
            return "\n".join(lines)

        def format_add_block(title: str, data_map: Dict[str, int]) -> str:
            lines = [f"[bold]{title}[/bold]"]
            entries = [(k, v) for k, v in sorted(data_map.items()) if v > 0]
            if not entries:
                lines.append("  - 无")
            else:
                for k, v in entries:
                    lines.append(f"  - {k}: {v}")
            return "\n".join(lines)

        text = "\n\n".join([
            format_block("需要在目标端修改的对象", modify_counts),
            format_add_block("需要在目标端新增的对象", addition_counts)
        ])
        return Panel.fit(text, title="[info]执行摘要", border_style="info", width=section_width)

    console.print(summarize_actions())

    if schema_summary:
        schema_table = Table(title="[header]0.a Schema 覆盖详情", width=section_width)
        schema_table.add_column("类别", style="info", width=36)
        schema_table.add_column("Schema 列表", style="info")
        has_row = False
        if schema_summary.get("source_missing"):
            schema_table.add_row(
                "源端未获取到对象",
                ", ".join(schema_summary["source_missing"])
            )
            has_row = True
        if has_row:
            console.print(schema_table)

    if object_counts_summary:
        count_table = Table(title="[header]0.b 检查汇总", **count_table_kwargs)
        count_table.add_column("对象类型", style="info", width=TYPE_COL_WIDTH)
        count_table.add_column("Oracle (应校验)", justify="right", width=18)
        count_table.add_column("OceanBase (命中)", justify="right", width=18)
        count_table.add_column("缺失", justify="right", width=8)
        count_table.add_column("不支持/阻断", justify="right", width=12)
        count_table.add_column("多余", justify="right", width=8)
        oracle_counts = dict(object_counts_summary.get("oracle", {}))
        ob_counts = dict(object_counts_summary.get("oceanbase", {}))
        missing_counts = dict(object_counts_summary.get("missing", {}))
        extra_counts = dict(object_counts_summary.get("extra", {}))
        count_types = sorted(set(oracle_counts) | set(ob_counts) | set(missing_counts) | set(extra_counts))
        for obj_type in count_types:
            ora_val = oracle_counts.get(obj_type, 0)
            ob_val = ob_counts.get(obj_type, 0)
            miss_val = missing_counts.get(obj_type, 0)
            extra_val = extra_counts.get(obj_type, 0)
            unsupported_val = int(unsupported_summary_counts.get(obj_type, 0) or 0)
            count_table.add_row(
                obj_type,
                str(ora_val),
                str(ob_val),
                f"[missing]{miss_val}[/missing]" if miss_val else "0",
                f"[mismatch]{unsupported_val}[/mismatch]" if unsupported_val else "0",
                f"[mismatch]{extra_val}[/mismatch]" if extra_val else "0"
            )
        console.print(count_table)

    # --- 1. 缺失的主对象 ---
    if show_detail_sections and tv_results['missing']:
        table = Table(title=f"[header]1. 缺失的主对象 (共 {missing_count} 个) — 按目标 schema 分组[/header]", width=section_width)
        SCHEMA_COL_WIDTH = 18
        table.add_column("目标 Schema", style="info", width=SCHEMA_COL_WIDTH)
        table.add_column("类型", style="info", width=TYPE_COL_WIDTH)
        table.add_column("缺失对象 (源名[=目标名])", style="info")

        grouped_missing: Dict[str, List[Tuple[str, str, str]]] = defaultdict(list)
        for obj_type, tgt_name, src_name in tv_results['missing']:
            tgt_schema = tgt_name.split('.', 1)[0] if '.' in tgt_name else tgt_name
            grouped_missing[tgt_schema.upper()].append((obj_type, tgt_name, src_name))

        grouped_items = sorted(grouped_missing.items())
        for tgt_schema, items in grouped_items:
            sorted_items = sorted(items, key=lambda x: (x[0], x[1], x[2]))
            for idx, (obj_type, tgt_name, src_name) in enumerate(sorted_items):
                table.add_row(
                    tgt_schema if idx == 0 else "",
                    f"[{obj_type}]",
                    format_missing_mapping(src_name, tgt_name),
                    end_section=(idx == len(sorted_items) - 1)
                )
        console.print(table)

    if show_detail_sections and tv_results.get('extra_targets'):
        extra_target_count = len(tv_results['extra_targets'])
        table = Table(title=f"[header]1.b 目标端多出的对象 (共 {extra_target_count} 个)", width=section_width)
        table.add_column("类型", style="info", width=TYPE_COL_WIDTH)
        table.add_column("目标对象(多余)", style="info")
        for obj_type, tgt_name in tv_results['extra_targets']:
            table.add_row(f"[{obj_type}]", tgt_name)
        console.print(table)

    if show_detail_sections and tv_results.get('skipped'):
        skipped_items = tv_results['skipped']
        table = Table(title=f"[header]1.c 仅打印未校验的主对象 (共 {len(skipped_items)} 个)", width=section_width)
        table.add_column("类型", style="info", width=TYPE_COL_WIDTH)
        table.add_column("对象 (源名[=目标名])", style="info", width=OBJECT_COL_WIDTH)
        table.add_column("原因", style="info", width=DETAIL_COL_WIDTH)
        for obj_type, tgt_name, src_name, reason in skipped_items:
            table.add_row(
                f"[{obj_type}]",
                format_missing_mapping(src_name, tgt_name),
                reason or ""
            )
        console.print(table)

    if show_detail_sections and package_diff_rows:
        table = Table(
            title=f"[header]1.d PACKAGE/PKG BODY 差异 (共 {len(package_diff_rows)} 个)",
            width=section_width
        )
        table.add_column("类型", style="info", width=TYPE_COL_WIDTH)
        table.add_column("对象 (源名[=目标名])", style="info", width=OBJECT_COL_WIDTH)
        table.add_column("源状态", style="info", width=12)
        table.add_column("目标状态", style="info", width=12)
        table.add_column("结果", style="info", width=16)
        table.add_column("错误摘要", style="info", width=DETAIL_COL_WIDTH)
        for row in package_diff_rows:
            error_hint = "-"
            if row.error_count:
                error_hint = f"{row.error_count} | {row.first_error}" if row.first_error else str(row.error_count)
            table.add_row(
                f"[{row.obj_type}]",
                format_missing_mapping(row.src_full, row.tgt_full),
                row.src_status,
                row.tgt_status,
                row.result,
                error_hint
            )
        console.print(table)

    if show_detail_sections and remap_conflicts:
        table = Table(title=f"[header]1.e 无法自动推导的对象 (共 {remap_conflict_cnt} 个)", width=section_width)
        table.add_column("类型", style="info", width=TYPE_COL_WIDTH)
        table.add_column("对象 (源端)", style="info", width=OBJECT_COL_WIDTH)
        table.add_column("原因", style="info", width=DETAIL_COL_WIDTH)
        for obj_type, src_name, reason in remap_conflicts:
            table.add_row(
                f"[{obj_type}]",
                src_name,
                reason or ""
            )
        console.print(table)

    if show_detail_sections and tv_results['extraneous']:
        table = Table(title=f"[header]1.f 无效的 Remap 规则 (共 {extraneous_count} 个)", width=section_width)
        table.add_column("在 remap_rules.txt 中定义, 但在源端 Oracle 中未找到的对象", style="info", width=section_width - 6)
        for item in tv_results['extraneous']:
            table.add_row(item, style="mismatch")
        console.print(table)

    # --- 2. 列不匹配的表 ---
    if show_detail_sections and tv_results['mismatched']:
        table = Table(title=f"[header]2. 不匹配的表 (共 {mismatched_count} 个)", width=section_width)
        table.add_column("表名", style="info", width=OBJECT_COL_WIDTH)
        table.add_column("差异详情", width=DETAIL_COL_WIDTH)
        for obj_type, tgt_name, missing, extra, length_mismatches, type_mismatches in tv_results['mismatched']:
            details = Text()
            if "获取失败" in tgt_name:
                details.append(f"源端列信息获取失败", style="missing")
            else:
                if missing:
                    details.append(f"- 缺失列: {sorted(list(missing))}\n", style="missing")
                if extra:
                    details.append(f"+ 多余列: {sorted(list(extra))}\n", style="mismatch")
                if type_mismatches:
                    details.append("* 类型不匹配:\n", style="mismatch")
                    for issue in type_mismatches:
                        col, src_type, tgt_type, expected_type, issue_type = issue
                        details.append(
                            f"    - {col}: 源={src_type}, 目标={tgt_type}, 期望={expected_type} ({issue_type})\n"
                        )
                if length_mismatches:
                    details.append("* 长度不匹配 (VARCHAR/2):\n", style="mismatch")
                    for issue in length_mismatches:
                        col, src_len, tgt_len, limit_len, issue_type = issue
                        if issue_type == 'char_mismatch':
                            details.append(
                                f"    - {col}: 源={src_len} CHAR, 目标={tgt_len}, 要求一致\n"
                            )
                        elif issue_type == 'short':
                            details.append(
                                f"    - {col}: 源={src_len} BYTE, 目标={tgt_len}, 期望下限={limit_len}\n"
                            )
                        else:
                            details.append(
                                f"    - {col}: 源={src_len} BYTE, 目标={tgt_len}, 上限允许={limit_len}\n"
                            )
            table.add_row(tgt_name, details)
        console.print(table)

    if show_detail_sections and column_order_mismatches:
        table = Table(
            title=f"[header]2.b 列顺序差异 (共 {column_order_mismatch_cnt} 张表)",
            width=section_width
        )
        table.add_column("表名", style="info", width=OBJECT_COL_WIDTH)
        table.add_column("源端顺序", width=DETAIL_COL_WIDTH // 2)
        table.add_column("目标顺序", width=DETAIL_COL_WIDTH // 2)
        for item in column_order_mismatches:
            src_seq = ", ".join(item.src_order) if item.src_order else "-"
            tgt_seq = ", ".join(item.tgt_order) if item.tgt_order else "-"
            table.add_row(item.table, src_seq, tgt_seq)
        console.print(table)

    comment_mismatches = comment_results.get("mismatched", [])
    if show_detail_sections and comment_skip_reason:
        console.print(Panel.fit(str(comment_skip_reason), style="info", width=section_width))
    if show_detail_sections and comment_mismatches:
        table = Table(title=f"[header]3. 表/列注释一致性检查 (共 {len(comment_mismatches)} 张表差异)", width=section_width)
        table.add_column("表名", style="info", width=OBJECT_COL_WIDTH)
        table.add_column("差异详情", width=DETAIL_COL_WIDTH)
        for item in comment_mismatches:
            details = Text()
            if item.table_comment:
                src_cmt, tgt_cmt = item.table_comment
                details.append(
                    f"* 表注释不一致: src={shorten_comment_preview(src_cmt)}, "
                    f"tgt={shorten_comment_preview(tgt_cmt)}\n",
                    style="mismatch"
                )
            if item.missing_columns:
                details.append(f"- 缺失列注释: {sorted(item.missing_columns)}\n", style="missing")
            if item.extra_columns:
                details.append(f"+ 额外列注释: {sorted(item.extra_columns)}\n", style="mismatch")
            for col, src_cmt, tgt_cmt in item.column_comment_diffs:
                details.append(
                    f"  - {col}: src={shorten_comment_preview(src_cmt)}, "
                    f"tgt={shorten_comment_preview(tgt_cmt)}\n"
                )
            table.add_row(item.table, details)
        console.print(table)

    def render_missing_mapping_lines(mappings: List[Tuple[str, str]]) -> str:
        """格式化缺失对象的映射行，remap 时显示 src=tgt。"""
        formatted = [format_missing_mapping(src, tgt) for src, tgt in mappings]
        return "\n".join([item for item in formatted if item])

    def build_missing_text(
        mappings: List[Tuple[str, str]],
        has_missing: bool,
        include_header: bool = True
    ) -> Text:
        """根据缺失映射构造 Text，按需添加“- 缺失:”标题。"""
        if not has_missing:
            return Text("")
        lines = render_missing_mapping_lines(mappings)
        if not lines:
            return Text("")
        header = Text("- 缺失:\n", style="missing") if include_header else Text("")
        return header + Text(lines + "\n", style="missing")

    # --- 4. 扩展对象差异 ---
    def print_ext_mismatch_table(title, items, headers, render_func):
        if not show_detail_sections:
            return
        if not items:
            return
        table = Table(title=f"[header]{title} (共 {len(items)} 项差异)", width=section_width)
        table.add_column(headers[0], style="info", width=OBJECT_COL_WIDTH)
        table.add_column(headers[1], width=DETAIL_COL_WIDTH)
        for item in items:
            table.add_row(*render_func(item))
        console.print(table)

    print_ext_mismatch_table(
        "4. 索引一致性检查", extra_results["index_mismatched"], ["表名", "差异详情"],
        lambda item: (
            Text(item.table),
            Text(f"- 缺失: {sorted(item.missing_indexes)}\n" if item.missing_indexes else "", style="missing") +
            Text(f"+ 多余: {sorted(item.extra_indexes)}\n" if item.extra_indexes else "", style="mismatch") +
            Text('\n'.join([f"* {d}" for d in item.detail_mismatch]))
        )
    )
    print_ext_mismatch_table(
        "5. 约束 (PK/UK/FK/CHECK) 一致性检查", extra_results["constraint_mismatched"], ["表名", "差异详情"],
        lambda item: (
            Text(item.table),
            Text(f"- 缺失: {sorted(item.missing_constraints)}\n" if item.missing_constraints else "", style="missing") +
            Text(f"+ 多余: {sorted(item.extra_constraints)}\n" if item.extra_constraints else "", style="mismatch") +
            Text('\n'.join([f"* {d}" for d in item.detail_mismatch]))
        )
    )
    print_ext_mismatch_table(
        "6. 序列 (SEQUENCE) 一致性检查", extra_results["sequence_mismatched"], ["Schema 映射", "差异详情"],
        lambda item: (
            Text(f"{item.src_schema}->{item.tgt_schema}"),
            build_missing_text(item.missing_mappings or [], bool(item.missing_sequences))
            + (
                Text(f"+ 多余: {sorted(item.extra_sequences)}\n", style="mismatch")
                if item.extra_sequences else Text("")
            )
            + (Text(f"* {item.note}\n", style="missing") if item.note else Text(""))
            + (
                Text("".join([f"* {d}\n" for d in item.detail_mismatch]), style="mismatch")
                if item.detail_mismatch else Text("")
            )
        )
    )
    print_ext_mismatch_table(
        "7. 触发器 (TRIGGER) 一致性检查", extra_results["trigger_mismatched"], ["表名", "差异详情"],
        lambda item: (
            Text(item.table),
            build_missing_text(item.missing_mappings or [], bool(item.missing_triggers))
            + (
                Text(f"+ 多余: {sorted(item.extra_triggers)}\n", style="mismatch")
                if item.extra_triggers else Text("")
            )
            + Text('\n'.join([f"* {d}" for d in item.detail_mismatch]))
        )
    )

    dep_total = dep_missing_cnt + dep_unexpected_cnt + dep_skipped_cnt
    if show_detail_sections and dep_total:
        dep_table = Table(title=f"[header]8. 依赖关系校验 (共 {dep_total} 项)", width=section_width)
        dep_table.add_column("类别", style="info", width=12)
        dep_table.add_column("依赖对象", style="info", width=OBJECT_COL_WIDTH)
        dep_table.add_column("依赖类型", style="info", width=TYPE_COL_WIDTH)
        dep_table.add_column("被依赖对象", style="info", width=OBJECT_COL_WIDTH)
        dep_table.add_column("被依赖类型", style="info", width=TYPE_COL_WIDTH)
        dep_table.add_column("修复建议", width=DETAIL_COL_WIDTH)

        def render_dep_rows(label: str, entries: List[DependencyIssue], style: str) -> None:
            for issue in entries:
                dep_table.add_row(
                    f"[{style}]{label}[/{style}]",
                    issue.dependent,
                    issue.dependent_type,
                    issue.referenced,
                    issue.referenced_type,
                    issue.reason
                )

        render_dep_rows("缺失", dependency_report.get("missing", []), "missing")
        render_dep_rows("多余", dependency_report.get("unexpected", []), "mismatch")
        render_dep_rows("跳过", dependency_report.get("skipped", []), "info")
        console.print(dep_table)

    mview_fixup_line = "fixup_scripts/materialized_view : MATERIALIZED VIEW 缺失对象修补脚本\n"
    if settings is not None and not bool(settings.get("effective_mview_enabled", False)):
        mview_fixup_line = "fixup_scripts/materialized_view : MATERIALIZED VIEW 当前仅打印不生成\n"

    interval_fixup_line = "fixup_scripts/table_alter/interval_add_<cutoff> : interval 分区补齐脚本\n\n"
    if settings is not None and not bool(settings.get("effective_interval_fixup_enabled", False)):
        interval_fixup_line = (
            "fixup_scripts/table_alter/interval_add_<cutoff> : interval 分区补齐脚本（当前门控关闭）\n\n"
        )

    # --- 提示 ---
    fixup_panel = Panel.fit(
        "[bold]Fixup 脚本生成目录[/bold]\n\n"
        "fixup_scripts/table         : 缺失 TABLE 的 CREATE 脚本\n"
        "fixup_scripts/tables_unsupported : 不支持 TABLE 的 DDL (默认不执行)\n"
        "fixup_scripts/tables_unsupported/temporary : 不支持临时表 DDL\n"
        "fixup_scripts/view_prereq_grants : VIEW 前置授权 (依赖对象)\n"
        "fixup_scripts/view          : 缺失 VIEW 的 CREATE 脚本\n"
        "fixup_scripts/view_post_grants : VIEW 创建后授权 (同步源端权限)\n"
        + mview_fixup_line +
        "fixup_scripts/procedure     : 缺失 PROCEDURE 的 CREATE 脚本\n"
        "fixup_scripts/function      : 缺失 FUNCTION 的 CREATE 脚本\n"
        "fixup_scripts/package       : 缺失 PACKAGE 的 CREATE 脚本\n"
        "fixup_scripts/package_body  : 缺失 PACKAGE BODY 的 CREATE 脚本\n"
        "fixup_scripts/synonym       : 缺失 SYNONYM 的 CREATE 脚本\n"
        "fixup_scripts/job           : 缺失 JOB 的 CREATE 脚本\n"
        "fixup_scripts/schedule      : 缺失 SCHEDULE 的 CREATE 脚本\n"
        "fixup_scripts/type          : 缺失 TYPE 的 CREATE 脚本\n"
        "fixup_scripts/type_body     : 缺失 TYPE BODY 的 CREATE 脚本\n"
        "fixup_scripts/index         : 缺失 INDEX 的 CREATE 脚本\n"
        "fixup_scripts/constraint    : 缺失约束的 CREATE 脚本\n"
        "fixup_scripts/sequence      : 缺失 SEQUENCE 的 CREATE 脚本\n"
        "fixup_scripts/trigger       : 缺失 TRIGGER 的 CREATE 脚本\n"
        "fixup_scripts/constraint_validate_later : 约束后置 VALIDATE 脚本（脏数据清理后执行）\n"
        "fixup_scripts/unsupported/* : 不支持/阻断对象 DDL (默认不执行)\n"
        "fixup_scripts/compile       : 依赖重编译脚本 (ALTER ... COMPILE)\n"
        "fixup_scripts/grants_miss   : 缺失授权脚本 (对象/角色/系统)\n"
        "fixup_scripts/grants_all    : 全量授权脚本 (对象/角色/系统)\n"
        "fixup_scripts/table_alter   : 列不匹配 TABLE 的 ALTER 修补脚本\n"
        + interval_fixup_line +
        "[bold]请在 OceanBase 执行前逐一人工审核上述脚本。[/bold]",
        title="[info]提示",
        border_style="info"
    )
    console.print(fixup_panel)
    run_summary: Optional[RunSummary] = None
    if run_summary_ctx:
        run_summary_ctx.phase_durations["报告输出"] = time.perf_counter() - run_summary_ctx.report_start_perf
        filtered_grants_count = len(filtered_grants or [])
        filtered_grants_path = None
        if filtered_grants_count and report_file:
            filtered_grants_path = Path(report_file).parent / "filtered_grants.txt"
        noise_detail_path = None
        if noise_suppressed_count and report_file and emit_detail_files and report_ts:
            noise_detail_path = Path(report_file).parent / f"noise_suppressed_detail_{report_ts}.txt"
        run_summary = build_run_summary(
            run_summary_ctx,
            tv_results,
            extra_results,
            comment_results,
            dependency_report,
            remap_conflicts,
            tv_results.get("extraneous", []),
            blacklisted_missing_tables,
            report_file,
            filtered_grants_path=filtered_grants_path,
            filtered_grants_count=filtered_grants_count,
            config_diagnostics=config_diagnostics,
            fixup_skip_summary=fixup_skip_summary,
            support_summary=support_summary,
            noise_suppressed_count=noise_suppressed_count,
            noise_suppressed_path=noise_detail_path
        )
        console.print(render_run_summary_panel(run_summary, section_width))

    if report_file:
        report_path = Path(report_file)
        index_entries: List[ReportIndexEntry] = []

        def _add_index_entry(category: str, path: Optional[Path], rows: Optional[int], description: str) -> None:
            if not path:
                return
            rel_path = _report_index_relpath(report_path.parent, path)
            if not rel_path:
                return
            row_text = str(rows) if rows is not None else "-"
            index_entries.append(ReportIndexEntry(category, rel_path, row_text, description))

        _add_index_entry("REPORT", report_path, None, "主报告")
        blacklisted_table_keys: Set[Tuple[str, str]] = set()
        for key, entries in (blacklisted_missing_tables or {}).items():
            if is_long_only_blacklist(entries):
                continue
            blacklisted_table_keys.add(key)
        export_dir = export_missing_table_view_mappings(
            tv_results,
            report_path.parent,
            blacklisted_tables=blacklisted_table_keys,
            support_state_map=(support_summary.support_state_map if support_summary else None)
        )
        _add_index_entry("DIR", export_dir, None, "缺失 TABLE/VIEW 规则目录")
        package_report_path = None
        if package_rows:
            package_report_path = export_package_compare_report(
                package_rows,
                derive_package_report_path(report_path)
            )
        _add_index_entry("DETAIL", package_report_path, len(package_rows) if package_rows else None, "PACKAGE/PKG BODY 对比明细")
        blacklist_path = export_blacklist_tables(blacklist_report_rows or [], report_path.parent)
        _add_index_entry("AUX", blacklist_path, len(blacklist_report_rows or []), "黑名单表清单")
        trigger_report_path = None
        if trigger_list_summary or trigger_status_rows:
            trigger_report_path = export_trigger_status_report(
                trigger_list_rows or [],
                trigger_list_summary,
                trigger_status_rows or [],
                report_path.parent
            )
        trigger_row_count = (len(trigger_list_rows or []) + len(trigger_status_rows or [])) if trigger_report_path else None
        _add_index_entry("AUX", trigger_report_path, trigger_row_count, "触发器状态/清单报告")
        status_drift_path = export_status_drift_detail(
            trigger_status_rows or [],
            constraint_status_rows or [],
            report_path.parent,
            report_ts,
            trigger_validity_mode=settings.get("trigger_validity_sync_mode", "compile") if settings else "compile"
        )
        _add_index_entry(
            "DETAIL",
            status_drift_path,
            len(trigger_status_rows or []) + len(constraint_status_rows or []),
            "触发器/约束状态漂移明细"
        )
        filtered_grants_path = export_filtered_grants(
            filtered_grants or [],
            report_path.parent
        )
        _add_index_entry("AUX", filtered_grants_path, len(filtered_grants or []), "过滤授权清单")
        fixup_skip_path = export_fixup_skip_summary(
            fixup_skip_summary or {},
            report_path.parent,
            report_ts
        )
        _add_index_entry("AUX", fixup_skip_path, None, "Fixup 跳过汇总")
        missing_detail_path = None
        unsupported_detail_path = None
        migration_focus_path = None
        view_constraint_cleaned_path = None
        view_constraint_uncleanable_path = None
        missing_by_type_paths: Dict[str, Optional[Path]] = {}
        unsupported_by_type_paths: Dict[str, Optional[Path]] = {}
        if emit_detail_files:
            missing_detail_path = export_missing_objects_detail(
                missing_detail_rows,
                report_path.parent,
                report_ts
            )
            unsupported_detail_path = export_unsupported_objects_detail(
                unsupported_detail_rows,
                report_path.parent,
                report_ts
            )
            view_constraint_cleaned_path = export_view_constraint_cleaned_detail(
                view_constraint_cleaned_rows,
                report_path.parent,
                report_ts
            )
            view_constraint_uncleanable_path = export_view_constraint_uncleanable_detail(
                view_constraint_uncleanable_rows,
                report_path.parent,
                report_ts
            )
            missing_by_type_paths = export_missing_by_type(
                combined_missing_rows,
                report_path.parent,
                report_ts
            )
            unsupported_by_type_paths = export_unsupported_by_type(
                unsupported_rows,
                report_path.parent,
                report_ts
            )
        if report_ts:
            migration_focus_path = export_migration_focus_report(
                combined_missing_rows,
                unsupported_rows,
                report_path.parent,
                report_ts
            )
        _add_index_entry(
            "DETAIL",
            missing_detail_path,
            len(missing_detail_rows or []),
            "缺失对象支持性明细"
        )
        _add_index_entry(
            "DETAIL",
            unsupported_detail_path,
            len(unsupported_detail_rows or []),
            "不支持/阻断对象明细"
        )
        _add_index_entry(
            "DETAIL",
            view_constraint_cleaned_path,
            len(view_constraint_cleaned_rows or []),
            "VIEW 列清单约束清洗明细"
        )
        _add_index_entry(
            "DETAIL",
            view_constraint_uncleanable_path,
            len(view_constraint_uncleanable_rows or []),
            "VIEW 列清单约束无法清洗明细"
        )
        for obj_type, path in sorted(missing_by_type_paths.items()):
            if not path:
                continue
            type_rows = [
                row for row in (missing_detail_rows + extra_missing_rows)
                if row.support_state == SUPPORT_STATE_SUPPORTED and row.obj_type.upper() == obj_type
            ]
            _add_index_entry(
                "DETAIL",
                path,
                len(type_rows),
                f"缺失 {obj_type} 明细"
            )
        for obj_type, path in sorted(unsupported_by_type_paths.items()):
            if not path:
                continue
            type_rows = [
                row for row in unsupported_rows
                if row.support_state in {SUPPORT_STATE_UNSUPPORTED, SUPPORT_STATE_BLOCKED}
                and row.obj_type.upper() == obj_type
            ]
            _add_index_entry(
                "DETAIL",
                path,
                len(type_rows),
                f"不支持/阻断 {obj_type} 明细"
            )
        _add_index_entry(
            "DETAIL",
            migration_focus_path,
            (missing_supported_cnt + unsupported_blocked_cnt),
            "迁移聚焦清单"
        )
        index_unsupported_path = export_indexes_unsupported_detail(
            extra_results.get("index_unsupported", []) or [],
            report_path.parent,
            report_ts
        )
        _add_index_entry(
            "DETAIL",
            index_unsupported_path,
            len(extra_results.get("index_unsupported", []) or []),
            "索引语法不支持明细(仅DESC)"
        )
        constraint_unsupported_path = export_constraints_unsupported_detail(
            extra_results.get("constraint_unsupported", []) or [],
            report_path.parent,
            report_ts
        )
        _add_index_entry(
            "DETAIL",
            constraint_unsupported_path,
            len(extra_results.get("constraint_unsupported", []) or []),
            "约束语法不支持明细(DEFERRABLE/自引用外键等)"
        )
        deferred_validate_detail_path = None
        deferred_validate_count = int((settings or {}).get("_constraint_validate_deferred_count", 0) or 0)
        deferred_validate_path_raw = str((settings or {}).get("_constraint_validate_deferred_report_path", "") or "").strip()
        if deferred_validate_path_raw:
            deferred_validate_detail_path = Path(deferred_validate_path_raw)
        elif report_ts:
            candidate = report_path.parent / f"constraint_validate_deferred_detail_{report_ts}.txt"
            if candidate.exists():
                deferred_validate_detail_path = candidate
        _add_index_entry(
            "DETAIL",
            deferred_validate_detail_path,
            deferred_validate_count or None,
            "缺失约束后置 VALIDATE 明细"
        )
        blocked_index_path = None
        blocked_constraint_path = None
        blocked_trigger_path = None
        if emit_detail_files and report_ts:
            blocked_index_path = export_indexes_blocked_detail(
                blocked_index_rows,
                report_path.parent,
                report_ts
            )
            blocked_constraint_path = export_constraints_blocked_detail(
                blocked_constraint_rows,
                report_path.parent,
                report_ts
            )
            blocked_trigger_path = export_triggers_blocked_detail(
                blocked_trigger_rows,
                report_path.parent,
                report_ts
            )
        _add_index_entry(
            "DETAIL",
            blocked_index_path,
            len(blocked_index_rows),
            "索引依赖阻断明细"
        )
        _add_index_entry(
            "DETAIL",
            blocked_constraint_path,
            len(blocked_constraint_rows),
            "约束依赖阻断明细"
        )
        _add_index_entry(
            "DETAIL",
            blocked_trigger_path,
            len(blocked_trigger_rows),
            "触发器依赖阻断明细"
        )
        extra_targets_path = None
        skipped_detail_path = None
        mismatched_tables_path = None
        column_order_mismatch_path = None
        comment_mismatch_path = None
        usability_detail_path = None
        extra_mismatch_path = None
        dependency_detail_path = None
        noise_suppressed_path = None
        if usability_summary and report_ts:
            usability_detail_path = export_usability_check_detail(
                usability_summary,
                report_path.parent,
                report_ts
            )
        if emit_detail_files and report_ts:
            extra_targets_path = export_extra_targets_detail(
                tv_results.get("extra_targets", []),
                report_path.parent,
                report_ts
            )
            skipped_detail_path = export_skipped_objects_detail(
                tv_results.get("skipped", []),
                report_path.parent,
                report_ts
            )
            mismatched_tables_path = export_mismatched_tables_detail(
                tv_results.get("mismatched", []),
                report_path.parent,
                report_ts
            )
            column_order_mismatch_path = export_column_order_mismatch_detail(
                tv_results.get("column_order_mismatched", []) or [],
                report_path.parent,
                report_ts
            )
            comment_mismatch_path = export_comment_mismatch_detail(
                comment_results.get("mismatched", []),
                report_path.parent,
                report_ts
            )
            extra_mismatch_path = export_extra_mismatch_detail(
                extra_results,
                report_path.parent,
                report_ts
            )
            dependency_detail_path = export_dependency_detail(
                dependency_report,
                report_path.parent,
                report_ts
            )
            noise_suppressed_path = export_noise_suppressed_detail(
                noise_suppressed_details,
                report_path.parent,
                report_ts
            )
        _add_index_entry(
            "DETAIL",
            extra_targets_path,
            len(tv_results.get("extra_targets", []) or []),
            "目标端多余对象明细"
        )
        _add_index_entry(
            "DETAIL",
            skipped_detail_path,
            len(tv_results.get("skipped", []) or []),
            "仅打印未校验对象明细"
        )
        _add_index_entry(
            "DETAIL",
            mismatched_tables_path,
            len(tv_results.get("mismatched", []) or []),
            "表列不匹配明细"
        )
        _add_index_entry(
            "DETAIL",
            column_order_mismatch_path,
            column_order_mismatch_cnt,
            "列顺序差异明细"
        )
        _add_index_entry(
            "DETAIL",
            comment_mismatch_path,
            len(comment_results.get("mismatched", []) or []),
            "注释差异明细"
        )
        _add_index_entry(
            "DETAIL",
            usability_detail_path,
            len(usability_summary.results) if usability_summary else 0,
            "对象可用性校验明细"
        )
        extra_mismatch_count = (
            len(extra_results.get("index_mismatched", []) or [])
            + len(extra_results.get("constraint_mismatched", []) or [])
            + len(extra_results.get("sequence_mismatched", []) or [])
            + len(extra_results.get("trigger_mismatched", []) or [])
        )
        _add_index_entry(
            "DETAIL",
            extra_mismatch_path,
            extra_mismatch_count,
            "扩展对象差异明细"
        )
        dep_detail_count = dep_missing_cnt + dep_unexpected_cnt + dep_skipped_cnt
        _add_index_entry(
            "DETAIL",
            dependency_detail_path,
            dep_detail_count,
            "依赖关系差异明细"
        )
        _add_index_entry(
            "DETAIL",
            noise_suppressed_path,
            len(noise_suppressed_details or []),
            "降噪明细"
        )
        if report_ts:
            mapping_path = report_path.parent / f"object_mapping_{report_ts}.txt"
            if mapping_path.exists():
                _add_index_entry("AUX", mapping_path, None, "全量对象映射")
            conflict_path = report_path.parent / f"remap_conflicts_{report_ts}.txt"
            if conflict_path.exists():
                _add_index_entry("AUX", conflict_path, None, "无法自动推导对象")
            ddl_format_path = report_path.parent / f"ddl_format_report_{report_ts}.txt"
            if ddl_format_path.exists():
                _add_index_entry("AUX", ddl_format_path, None, "DDL 格式化报告")
            ddl_punct_path = report_path.parent / f"ddl_punct_clean_{report_ts}.txt"
            if ddl_punct_path.exists():
                _add_index_entry("AUX", ddl_punct_path, None, "全角标点清洗报告")
            ddl_hint_path = report_path.parent / f"ddl_hint_clean_{report_ts}.txt"
            if ddl_hint_path.exists():
                _add_index_entry("AUX", ddl_hint_path, None, "DDL hint 清洗报告")
            dep_chain_path = report_path.parent / f"dependency_chains_{report_ts}.txt"
            if dep_chain_path.exists():
                _add_index_entry("AUX", dep_chain_path, None, "依赖链条导出")
            view_chain_path = report_path.parent / f"VIEWs_chain_{report_ts}.txt"
            if view_chain_path.exists():
                _add_index_entry("AUX", view_chain_path, None, "VIEW 依赖链")
        try:
            report_path.parent.mkdir(parents=True, exist_ok=True)
            report_text = console.export_text(clear=False)
            # 生成便于 vi/less 查看且无颜色/框线的纯文本报告
            plain_text = strip_ansi_text(report_text).translate(BOX_ASCII_TRANS)
            report_path.write_text(plain_text, encoding='utf-8')
            console.print(f"[info]报告已保存(纯文本): {report_path}")
            if export_dir:
                log.info("缺失 TABLE/VIEW 映射已输出到: %s", export_dir)
            if package_report_path:
                log.info("PACKAGE 对比明细已输出到: %s", package_report_path)
            if blacklist_path:
                log.info("黑名单表清单已输出到: %s", blacklist_path)
            if trigger_report_path:
                log.info("触发器状态/清单报告已输出到: %s", trigger_report_path)
            if status_drift_path:
                log.info("状态漂移明细已输出到: %s", status_drift_path)
            if filtered_grants_path:
                log.info("过滤授权清单已输出到: %s", filtered_grants_path)
            if fixup_skip_path:
                log.info("Fixup 跳过汇总已输出到: %s", fixup_skip_path)
            if missing_detail_path:
                log.info("缺失对象支持性明细已输出到: %s", missing_detail_path)
            if unsupported_detail_path:
                log.info("不支持/阻断对象明细已输出到: %s", unsupported_detail_path)
            if missing_by_type_paths:
                for path in sorted(p for p in missing_by_type_paths.values() if p):
                    log.info("缺失按类型明细已输出到: %s", path)
            if unsupported_by_type_paths:
                for path in sorted(p for p in unsupported_by_type_paths.values() if p):
                    log.info("不支持按类型明细已输出到: %s", path)
            if index_unsupported_path:
                log.info("索引不支持明细已输出到: %s", index_unsupported_path)
            if constraint_unsupported_path:
                log.info("约束不支持明细已输出到: %s", constraint_unsupported_path)
            if blocked_index_path:
                log.info("索引阻断明细已输出到: %s", blocked_index_path)
            if blocked_constraint_path:
                log.info("约束阻断明细已输出到: %s", blocked_constraint_path)
            if blocked_trigger_path:
                log.info("触发器阻断明细已输出到: %s", blocked_trigger_path)
            if migration_focus_path:
                log.info("迁移聚焦清单已输出到: %s", migration_focus_path)
            if extra_targets_path:
                log.info("目标端多余对象明细已输出到: %s", extra_targets_path)
            if skipped_detail_path:
                log.info("仅打印未校验对象明细已输出到: %s", skipped_detail_path)
            if mismatched_tables_path:
                log.info("表列不匹配明细已输出到: %s", mismatched_tables_path)
            if column_order_mismatch_path:
                log.info("列顺序差异明细已输出到: %s", column_order_mismatch_path)
            if comment_mismatch_path:
                log.info("注释差异明细已输出到: %s", comment_mismatch_path)
            if usability_detail_path:
                log.info("对象可用性明细已输出到: %s", usability_detail_path)
            if extra_mismatch_path:
                log.info("扩展对象差异明细已输出到: %s", extra_mismatch_path)
            if noise_suppressed_path:
                log.info("降噪明细已输出到: %s", noise_suppressed_path)
            if dependency_detail_path:
                log.info("依赖关系差异明细已输出到: %s", dependency_detail_path)
            existing_paths = {entry.path for entry in index_entries}
            for item in report_path.parent.iterdir():
                rel_path = _report_index_relpath(report_path.parent, item)
                if not rel_path or rel_path in existing_paths:
                    continue
                category, description = _infer_report_index_meta(item)
                index_entries.append(ReportIndexEntry(category, rel_path, "-", description))
                existing_paths.add(rel_path)
            index_path = export_report_index(
                index_entries,
                report_path.parent,
                report_ts,
                report_detail_mode
            )
            if index_path:
                log.info("报告索引已输出到: %s", index_path)
        except OSError as exc:
            console.print(f"[missing]报告写入失败: {exc}")

    console.print(Panel.fit("[bold]报告结束[/bold]", style="title"))

    return run_summary


# ====================== 主函数 ======================

def parse_cli_args() -> argparse.Namespace:
    """解析命令行参数，允许自定义 config.ini 路径并展示功能说明。"""
    desc = textwrap.dedent(
        f"""\
        OceanBase Comparator Toolkit v{__version__}
        - 一次转储，本地对比：Oracle Thick Mode + 少量 obclient 调用，全部比对在内存完成。
        - 覆盖对象：TABLE/VIEW/MVIEW/PLSQL/TYPE/JOB/SCHEDULE + INDEX/CONSTRAINT/SEQUENCE/TRIGGER。
        - 校验规则：表列名集合 + VARCHAR/VARCHAR2 长度窗口 [ceil(1.5x), ceil(2.5x)]；其余对象校验存在性/列组合。
        - 注释校验：基于 DBA_TAB_COMMENTS / DBA_COL_COMMENTS 的表/列注释一致性检查（可通过 check_comments 开关关闭）。
        - 依赖校验：加载 DBA_DEPENDENCIES，映射后对比，缺失则生成 ALTER ... COMPILE。
        - 授权生成：基于 DBA_TAB_PRIVS/DBA_SYS_PRIVS/DBA_ROLE_PRIVS + 依赖推导生成授权脚本。
        - Fix-up 输出：缺失对象 CREATE、表列 ALTER ADD/MODIFY、依赖 COMPILE、授权脚本，按类型落地到 fixup_scripts/*。
        """
    )
    epilog = textwrap.dedent(
        """\
        配置提示 (config.ini):
          [ORACLE_SOURCE] user/password/dsn (Thick Mode)
          [OCEANBASE_TARGET] executable/host/port/user_string/password (obclient)
          [SETTINGS] source_schemas, remap_file, oracle_client_lib_dir, dbcat_*，输出目录等
          可选开关：
            check_primary_types     限制主对象类型（默认全量）
            check_extra_types       限制扩展对象 (index,constraint,sequence,trigger)
            synonym_check_scope     同义词校验范围 (all/public_only，默认 public_only)
            extra_check_workers     扩展对象校验并发进程数（默认 16）
            extra_check_chunk_size  扩展对象校验批量表数量（默认 200）
            extra_check_progress_interval 扩展对象校验进度日志间隔（秒）
            fixup_schemas           仅对指定目标 schema 生成订正 SQL（逗号分隔，留空为全部）
            fixup_types             仅生成指定对象类型的订正 SQL（留空为全部，例如 TABLE,TRIGGER）
            fixup_idempotent_mode   修补脚本幂等模式 (off/guard/replace/drop_create)
            fixup_idempotent_types  幂等模式作用对象类型（逗号分隔，留空用默认）
            fixup_drop_sys_c_columns 是否对目标端额外 SYS_C* 列生成 ALTER TABLE FORCE (true/false)
            synonym_fixup_scope     同义词修补范围 (all/public_only，默认 public_only)
            trigger_list            仅生成指定触发器清单 (每行 SCHEMA.TRIGGER_NAME)
            trigger_qualify_schema  触发器 DDL 是否强制补全 schema 前缀 (true/false)
            check_status_drift_types 状态漂移检查范围 (trigger,constraint)
            generate_status_fixup   true/false 控制状态漂移修复脚本生成
            status_fixup_types      状态修复对象类型 (trigger,constraint)
            constraint_status_sync_mode 约束状态同步模式 (enabled_only/full)
            constraint_missing_fixup_validate_mode 缺失约束 VALIDATE 策略 (safe_novalidate/source/force_validate)
            trigger_validity_sync_mode 触发器有效性同步模式 (off/compile)
            sequence_remap_policy   SEQUENCE 目标 schema 推导策略 (infer/source_only/dominant_table)
            blacklist_name_patterns 表名黑名单关键字（逗号分隔，默认 _RENAME）
            blacklist_name_patterns_file 表名黑名单关键字文件（每行一条）
            report_detail_mode      报告内容模式 (full/split/summary)
            report_dir_layout       报告目录布局 (flat/per_run)
            view_compat_rules_path  VIEW 兼容规则 JSON (可选)
            view_dblink_policy      VIEW DBLINK 处理策略 (block/allow)
            view_constraint_cleanup VIEW 列清单约束清洗策略 (auto/force/off)
            column_visibility_policy 列可见性(INVISIBLE)处理策略 (auto/enforce/ignore)
            ddl_format_enable       true/false 启用 SQLcl DDL 格式化
            ddl_format_types        DDL 格式化对象类型列表（逗号分隔）
            sqlcl_bin               SQLcl 根目录或 bin/sql 路径
            ddl_format_timeout      SQLcl 批次超时（秒，0 不超时）
            check_dependencies      true/false 控制依赖校验
            check_column_order      true/false 控制列顺序校验（默认 false）
            generate_grants         true/false 控制授权脚本生成
            grant_tab_privs_scope   owner/owner_or_grantee 控制 DBA_TAB_PRIVS 抽取范围
            grant_merge_privileges  true/false 合并同对象多权限授权
            grant_merge_grantees    true/false 合并同权限多 grantee 授权
            grant_supported_sys_privs    逗号分隔系统权限清单（留空自动探测）
            grant_supported_object_privs 逗号分隔对象权限清单（留空使用默认白名单）
            grant_include_oracle_maintained_roles true/false 是否生成 Oracle 维护角色
            generate_fixup          true/false 控制是否生成脚本

        用法示例:
          python schema_diff_reconciler.py                   # 使用当前目录 config.ini
          python schema_diff_reconciler.py /path/to/conf.ini # 指定配置
        输出:
          main_reports/run_<ts>/report_<ts>.txt  Rich 报告文本 (report_dir_layout=per_run)
          main_reports/run_<ts>/report_index_<ts>.txt  报告索引 (报告与明细文件清单)
          main_reports/run_<ts>/package_compare_<ts>.txt  PACKAGE/PKG BODY 对比明细
          main_reports/run_<ts>/missed_tables_views_for_OMS/ 按 schema 输出缺失 TABLE/VIEW 规则 (schema_T.txt / schema_V.txt)
          main_reports/run_<ts>/blacklist_tables.txt 黑名单表清单 (含 LONG 转换校验状态)
          main_reports/run_<ts>/trigger_status_report.txt  触发器状态/清单报告 (trigger_list 或触发器状态差异时生成)
          main_reports/run_<ts>/status_drift_detail_<ts>.txt 触发器/约束状态漂移明细
          main_reports/run_<ts>/filtered_grants.txt 过滤掉的不兼容 GRANT 权限清单
          main_reports/run_<ts>/ddl_format_report_<ts>.txt SQLcl DDL 格式化报告 (ddl_format_enable=true)
          main_reports/run_<ts>/missing_objects_detail_<ts>.txt 缺失对象支持性明细 (report_detail_mode=split)
          main_reports/run_<ts>/unsupported_objects_detail_<ts>.txt 不支持/阻断对象明细 (report_detail_mode=split)
          main_reports/run_<ts>/view_constraint_cleaned_detail_<ts>.txt VIEW 列清单约束清洗明细 (report_detail_mode=split)
          main_reports/run_<ts>/view_constraint_uncleanable_detail_<ts>.txt VIEW 列清单约束无法清洗明细 (report_detail_mode=split)
          main_reports/run_<ts>/extra_mismatch_detail_<ts>.txt 扩展对象差异明细 (report_detail_mode=split)
          main_reports/run_<ts>/missing_<TYPE>_detail_<ts>.txt 按类型缺失明细 (report_detail_mode=split)
          main_reports/run_<ts>/unsupported_<TYPE>_detail_<ts>.txt 按类型不支持/阻断明细 (含 ROOT_CAUSE)
          fixup_scripts/                按类型分类的订正 SQL
        项目信息:
          主页: {repo_url}
          反馈: {issues_url}
        """
    ).format(repo_url=REPO_URL, issues_url=REPO_ISSUES_URL)
    parser = argparse.ArgumentParser(
        description=desc,
        epilog=epilog,
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    parser.add_argument(
        "config",
        nargs="?",
        default="config.ini",
        help="config.ini path (default: ./config.ini)",
    )
    parser.add_argument(
        "--wizard",
        action="store_true",
        help="启动交互式配置向导：缺失/无效项时提示输入并写回配置，然后继续运行主流程。",
    )
    return parser.parse_args()


def main():
    """主执行函数"""
    args = parse_cli_args()
    config_file = args.config
    config_path = Path(config_file).resolve()
    run_start_time = datetime.now()
    run_start_perf = time.perf_counter()
    phase_durations: Dict[str, float] = OrderedDict()
    phase_skip_reasons: Dict[str, str] = {}

    if args.wizard:
        run_config_wizard(config_path)

    # 1) 加载配置 + 初始化
    with phase_timer("加载配置与初始化", phase_durations):
        ora_cfg, ob_cfg, settings = load_config(str(config_path))
        # 为本次运行初始化日志文件（尽量早，以便记录后续步骤）
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        setup_run_logging(settings, timestamp)
        log_section("启动与配置")
        log.info("配置文件: %s", config_path)
        validate_runtime_paths(settings, ob_cfg)

        log.info("OceanBase Comparator Toolkit v%s", __version__)
        log.info("项目主页: %s (问题反馈: %s)", REPO_URL, REPO_ISSUES_URL)
        enabled_primary_types: Set[str] = set(settings.get('enabled_primary_types') or set(PRIMARY_OBJECT_TYPES))
        enabled_extra_types: Set[str] = set(settings.get('enabled_extra_types') or set(EXTRA_OBJECT_CHECK_TYPES))
        enable_dependencies_check: bool = bool(settings.get('enable_dependencies_check', True))
        enable_comment_check: bool = bool(settings.get('enable_comment_check', True))
        enable_column_order_check: bool = bool(settings.get('enable_column_order_check', False))
        enable_grant_generation: bool = bool(settings.get('enable_grant_generation', True))
        enable_usability_check: bool = bool(settings.get('check_object_usability', False))

        # 初始化 Oracle Instant Client (Thick Mode)
        init_oracle_client_from_settings(settings)

        oracle_env_info = collect_oracle_env_info(ora_cfg)
        ob_env_info = collect_ob_env_info(ob_cfg)
        endpoint_info = {
            "oracle": oracle_env_info,
            "oceanbase": ob_env_info
        }
        ob_version = extract_ob_version_number(ob_env_info.get("version", ""))
        gate_info = apply_ob_feature_gates(settings, ob_version)

        print_only_primary_types = set(settings.get("effective_print_only_primary_types") or PRINT_ONLY_PRIMARY_TYPES)
        print_only_types = enabled_primary_types & print_only_primary_types
        checked_primary_types = enabled_primary_types - print_only_types
        enabled_object_types = enabled_primary_types | enabled_extra_types

        log.info(
            "本次启用的主对象类型: %s",
            ", ".join(sorted(enabled_primary_types))
        )
        if print_only_types:
            log.info(
                "以下主对象类型仅打印不校验: %s",
                ", ".join(sorted(print_only_types))
            )
        log.info(
            "本次启用的扩展校验: %s",
            ", ".join(sorted(enabled_extra_types)) if enabled_extra_types else "<无>"
        )
        if not enable_dependencies_check:
            log.info("已根据配置跳过依赖关系校验。")
        if not enable_comment_check:
            log.info("已根据配置关闭注释一致性校验。")
        if enable_column_order_check:
            log.info("已开启列顺序校验（check_column_order=true）。")
        if not enable_grant_generation:
            log.info("已根据配置关闭授权脚本生成。")
        if enable_usability_check:
            log.info("已开启 VIEW/SYNONYM 可用性校验。")
        log.info(
            "OB 特性门控: ob_version=%s, gate=%s, interval(mode=%s)->%s, mview(mode=%s)->%s",
            gate_info.get("version") or "unknown",
            OB_FEATURE_GATE_VERSION,
            gate_info.get("interval_mode"),
            "enabled" if gate_info.get("interval_enabled") else "disabled",
            gate_info.get("mview_mode"),
            "enabled" if gate_info.get("mview_enabled") else "print-only",
        )
        if not gate_info.get("version_known"):
            log.warning(
                "OB 版本不可识别，auto 门控已回退旧行为：interval=enabled, mview=print-only。"
            )

        config_diagnostics = collect_fixup_config_diagnostics(
            settings,
            enabled_primary_types,
            enabled_extra_types
        )
        if config_diagnostics:
            log_subsection("配置诊断")
            for item in config_diagnostics:
                log.warning("[CONFIG] %s", item)

    generate_fixup_enabled = settings.get('generate_fixup', 'true').strip().lower() in ('true', '1', 'yes')
    if not enabled_extra_types:
        phase_skip_reasons["扩展对象校验"] = "check_extra_types 为空"
    if not enable_usability_check:
        phase_skip_reasons["对象可用性校验"] = "check_object_usability=false"
    if not enable_dependencies_check:
        phase_skip_reasons["依赖/授权校验"] = "check_dependencies=false"
    if not generate_fixup_enabled:
        phase_skip_reasons["修补脚本生成"] = "generate_fixup=false"

    log_section("对象映射准备")
    with phase_timer("对象映射准备", phase_durations):
        # 2) 加载 Remap 规则
        remap_rules = load_remap_rules(settings['remap_file'])

        # 3) 加载源端主对象 (TABLE/VIEW/PROC/FUNC/PACKAGE/PACKAGE BODY/SYNONYM)
        source_objects = get_source_objects(
            ora_cfg,
            settings['source_schemas_list'],
            synonym_check_scope=settings.get('synonym_check_scope', 'public_only')
        )

        # 4) 验证 Remap 规则
        extraneous_rules = validate_remap_rules(remap_rules, source_objects, settings.get("remap_file"))
        schema_mapping_from_tables: Optional[Dict[str, str]] = None
        
        # 4.1) 获取依附对象（如 TRIGGER）的父表映射，用于 one-to-many schema 拆分场景
        object_parent_map = get_object_parent_tables(
            ora_cfg,
            settings['source_schemas_list'],
            enabled_object_types=enabled_object_types
        )
        
        # 4.2) 加载源端依赖关系（用于智能推导一对多场景的目标 schema）
        oracle_dependencies_internal: List[DependencyRecord] = []
        oracle_dependencies_for_grants: List[DependencyRecord] = []
        source_dependencies_set: Optional[SourceDependencySet] = None
        source_schema_set = {s.upper() for s in settings.get("source_schemas_list", []) if s}
        # 依赖既用于缺失依赖校验，也用于 one-to-many remap 推导；grant 生成亦依赖依赖链
        infer_candidate_types = enabled_object_types - NO_INFER_SCHEMA_TYPES - {'TABLE', 'INDEX', 'CONSTRAINT'}
        need_dependency_infer = enable_dependencies_check or (
            bool(settings.get("enable_schema_mapping_infer", True)) and bool(infer_candidate_types)
        )
        need_grant_dependencies = enable_grant_generation and generate_fixup_enabled
        need_dependency_load = need_dependency_infer or enable_dependencies_check or need_grant_dependencies
        if need_dependency_load:
            include_external_refs = bool(need_grant_dependencies)
            oracle_dependencies_for_grants = load_oracle_dependencies(
                ora_cfg,
                settings['source_schemas_list'],
                object_types=enabled_object_types,
                include_external_refs=include_external_refs
            )
            if include_external_refs:
                oracle_dependencies_internal = [
                    dep for dep in oracle_dependencies_for_grants
                    if (dep.referenced_owner or "").upper() in source_schema_set
                ]
            else:
                oracle_dependencies_internal = list(oracle_dependencies_for_grants)
            if need_dependency_infer or enable_dependencies_check:
                # 转换为简化格式：(dep_owner, dep_name, dep_type, ref_owner, ref_name, ref_type)
                source_dependencies_set = {
                    (dep.owner.upper(), dep.name.upper(), dep.object_type.upper(),
                     dep.referenced_owner.upper(), dep.referenced_name.upper(), dep.referenced_type.upper())
                    for dep in oracle_dependencies_internal
                }
        dependency_graph: DependencyGraph = build_dependency_graph(source_dependencies_set) if source_dependencies_set else {}
        view_dependency_map: Dict[Tuple[str, str], Set[str]] = build_view_dependency_map(
            source_dependencies_set
        ) if source_dependencies_set else {}
        # 4.2.b) 预计算递归依赖表集合（性能优化：避免每对象 DFS）
        transitive_table_cache: Optional[TransitiveTableCache] = None
        if settings.get("enable_schema_mapping_infer") and dependency_graph:
            log.info("正在预计算依赖图的递归 TABLE/MVIEW 引用缓存以加速 remap 推导...")
            transitive_table_cache = precompute_transitive_table_cache(
                dependency_graph,
                object_parent_map=object_parent_map
            )
            log.info("递归依赖表缓存完成，共 %d 个节点。", len(transitive_table_cache))
        # 4.3) 缓存同义词元数据，供 PUBLIC 等大规模同义词快速生成 DDL
        synonym_meta = load_synonym_metadata(
            ora_cfg,
            settings['source_schemas_list'],
            allowed_target_schemas=settings['source_schemas_list']
        )

        sequence_policy = settings.get("sequence_remap_policy", "source_only")
        # 5) 先不推导 schema，生成基础映射/清单，用于推导 TABLE 唯一映射
        remap_conflicts: RemapConflictMap = {}
        base_full_mapping = build_full_object_mapping(
            source_objects,
            remap_rules,
            schema_mapping=None,
            object_parent_map=object_parent_map,
            transitive_table_cache=transitive_table_cache,
            source_dependencies=source_dependencies_set,
            dependency_graph=dependency_graph,
            enabled_types=enabled_object_types,
            remap_conflicts=remap_conflicts,
            sequence_remap_policy=sequence_policy
        )
        base_master_list = generate_master_list(
            source_objects,
            remap_rules,
            enabled_primary_types=enabled_primary_types,
            schema_mapping=None,
            precomputed_mapping=base_full_mapping,
            object_parent_map=object_parent_map,
            transitive_table_cache=transitive_table_cache,
            source_dependencies=source_dependencies_set,
            dependency_graph=dependency_graph,
            remap_conflicts=remap_conflicts,
            sequence_remap_policy=sequence_policy
        )
        if settings.get("enable_schema_mapping_infer"):
            schema_mapping_from_tables = build_schema_mapping(base_master_list)
        schema_mapping_for_grants = derive_schema_mapping_from_rules(remap_rules)
        if schema_mapping_from_tables:
            schema_mapping_for_grants.update(schema_mapping_from_tables)

        # 6) 基于 TABLE 推导的 schema 映射（仅作用于非 TABLE 对象）+ 依赖分析，重建映射与校验清单
        full_object_mapping = build_full_object_mapping(
            source_objects,
            remap_rules,
            schema_mapping=schema_mapping_from_tables,
            object_parent_map=object_parent_map,
            transitive_table_cache=transitive_table_cache,
            source_dependencies=source_dependencies_set,
            dependency_graph=dependency_graph,
            enabled_types=enabled_object_types,
            remap_conflicts=remap_conflicts,
            sequence_remap_policy=sequence_policy
        )
        master_list = generate_master_list(
            source_objects,
            remap_rules,
            enabled_primary_types=enabled_primary_types,
            schema_mapping=schema_mapping_from_tables,
            precomputed_mapping=full_object_mapping,
            object_parent_map=object_parent_map,
            transitive_table_cache=transitive_table_cache,
            source_dependencies=source_dependencies_set,
            dependency_graph=dependency_graph,
            remap_conflicts=remap_conflicts,
            sequence_remap_policy=sequence_policy
        )
        expected_dependency_pairs: Set[Tuple[str, str, str, str]] = set()
        skipped_dependency_pairs: List[DependencyIssue] = []
        if enable_dependencies_check:
            expected_dependency_pairs, skipped_dependency_pairs = build_expected_dependency_pairs(
                oracle_dependencies_internal,
                full_object_mapping
            )
        target_schemas: Set[str] = set()
        for type_map in full_object_mapping.values():
            for tgt_name in type_map.values():
                try:
                    schema, _ = tgt_name.split('.', 1)
                    target_schemas.add(schema.upper())
                except ValueError:
                    continue
        target_table_pairs = collect_table_pairs(master_list, use_target=True)

    report_dir_setting = settings.get('report_dir', 'main_reports').strip() or 'main_reports'
    report_layout = settings.get('report_dir_layout', 'per_run')
    report_root = Path(report_dir_setting)
    report_dir = report_root / f"run_{timestamp}" if report_layout == "per_run" else report_root
    report_dir.mkdir(parents=True, exist_ok=True)
    settings['report_dir_effective'] = str(report_dir)
    report_path = report_dir / f"report_{timestamp}.txt"
    log.info(f"本次报告将输出到: {report_path}")

    # 输出全量 remap 推导结果，便于人工审核
    mapping_path = report_dir / f"object_mapping_{timestamp}.txt"
    mapping_written = export_full_object_mapping(full_object_mapping, mapping_path)
    if mapping_written:
        log.info("全量对象映射已输出: %s", mapping_written)

    remap_conflict_items: List[Tuple[str, str, str]] = []
    if remap_conflicts:
        remap_conflict_items = [
            (obj_type, src_full, reason)
            for (src_full, obj_type), reason in sorted(remap_conflicts.items())
        ]
        conflict_path = report_dir / f"remap_conflicts_{timestamp}.txt"
        conflict_written = export_remap_conflicts(remap_conflicts, conflict_path)
        if conflict_written:
            log.info("无法自动推导的对象已输出: %s", conflict_written)

    dependency_report: DependencyReport = {
        "missing": [],
        "unexpected": [],
        "skipped": skipped_dependency_pairs
    }
    dependency_chain_file: Optional[Path] = None
    view_chain_file: Optional[Path] = None
    grant_plan: Optional[GrantPlan] = None
    object_counts_summary: Optional[ObjectCountSummary] = None
    schema_summary: Optional[Dict[str, List[str]]] = None

    if not master_list:
        for phase in (
            "OceanBase 元数据转储",
            "Oracle 元数据转储",
            "主对象校验",
            "扩展对象校验",
            "对象可用性校验",
            "依赖/授权校验",
            "修补脚本生成"
        ):
            phase_skip_reasons[phase] = "主校验清单为空"

        log.info("主校验清单为空，程序结束。")
        tv_results: ReportResults = {
            "missing": [],
            "mismatched": [],
            "ok": [],
            "skipped": [],
            "column_order_mismatched": [],
            "column_order_skipped": [],
            "remap_conflicts": remap_conflict_items,
            "extraneous": extraneous_rules,
            "extra_targets": []
        }
        package_results: PackageCompareResults = {"rows": [], "summary": {}, "diff_rows": []}
        extra_results: ExtraCheckResults = {
            "index_ok": [],
            "index_mismatched": [],
            "index_unsupported": [],
            "constraint_ok": [],
            "constraint_mismatched": [],
            "constraint_unsupported": [],
            "sequence_ok": [],
            "sequence_mismatched": [],
            "trigger_ok": [],
            "trigger_mismatched": [],
            "trigger_status_drift": [],
            "constraint_status_drift": [],
        }
        comment_results = {
            "ok": [],
            "mismatched": [],
            "skipped_reason": "主校验清单为空，未执行注释比对。"
        }
        report_start_perf = time.perf_counter()
        trigger_summary_stub = None
        trigger_list_path = settings.get("trigger_list", "").strip()
        if trigger_list_path:
            trigger_summary_stub = {
                "enabled": True,
                "path": trigger_list_path,
                "valid_entries": 0,
                "selected_missing": 0,
                "missing_not_listed": 0,
                "invalid_entries": 0,
                "not_found": 0,
                "check_disabled": True,
                "error": ""
            }
        run_summary_ctx = RunSummaryContext(
            start_time=run_start_time,
            start_perf=run_start_perf,
            phase_durations=phase_durations,
            phase_skip_reasons=phase_skip_reasons,
            enabled_primary_types=enabled_primary_types,
            enabled_extra_types=enabled_extra_types,
            print_only_types=print_only_types,
            total_checked=0,
            enable_dependencies_check=enable_dependencies_check,
            enable_comment_check=enable_comment_check,
            enable_grant_generation=enable_grant_generation,
            enable_schema_mapping_infer=settings.get("enable_schema_mapping_infer", True),
            fixup_enabled=generate_fixup_enabled,
            fixup_dir=settings.get('fixup_dir', 'fixup_scripts') or 'fixup_scripts',
            dependency_chain_file=None,
            view_chain_file=None,
            trigger_list_summary=trigger_summary_stub,
            report_start_perf=report_start_perf
        )
        run_summary = print_final_report(
            tv_results,
            0,
            extra_results,
            comment_results,
            dependency_report,
            report_path,
            object_counts_summary,
            endpoint_info,
            schema_summary,
            settings,
            blacklisted_missing_tables={},
            blacklist_report_rows=[],
            trigger_list_summary=None,
            trigger_list_rows=None,
            package_results=package_results,
            run_summary_ctx=run_summary_ctx,
            filtered_grants=None,
            config_diagnostics=config_diagnostics,
            fixup_skip_summary=None,
            usability_summary=None
        )
        if run_summary:
            log_run_summary(run_summary)
            db_ok, _ = save_report_to_db(
                ob_cfg,
                settings,
                run_summary,
                run_summary_ctx,
                timestamp,
                report_dir,
                tv_results,
                extra_results,
                None,
                endpoint_info,
                None,
                None,
                set(),
                grant_plan=None,
                usability_summary=None,
                package_results=package_results,
                trigger_status_rows=None,
                dependency_report=dependency_report,
                expected_dependency_pairs=None,
                view_chain_file=None,
                remap_conflicts=remap_conflict_items,
                full_object_mapping=None,
                remap_rules=None,
                blacklist_report_rows=None,
                fixup_skip_summary=None,
                blacklisted_table_keys=set()
            )
            if not db_ok:
                abort_run()
        return

    log_section("元数据转储")
    with phase_timer("OceanBase 元数据转储", phase_durations):
        log_subsection("OceanBase 元数据")
        # 6) 计算目标端 schema 集合并一次性 dump OB 元数据
        tracked_types = set(checked_primary_types) | (set(enabled_extra_types) & set(ALL_TRACKED_OBJECT_TYPES))
        if not tracked_types:
            tracked_types = {'TABLE'}

        ob_meta = dump_ob_metadata(
            ob_cfg,
            target_schemas,
            tracked_object_types=tracked_types,
            synonym_check_scope=settings.get('synonym_check_scope', 'public_only'),
            include_tab_columns='TABLE' in enabled_primary_types,
            include_column_order=enable_column_order_check,
            include_indexes='INDEX' in enabled_extra_types,
            include_constraints='CONSTRAINT' in enabled_extra_types,
            include_triggers='TRIGGER' in enabled_extra_types,
            include_sequences='SEQUENCE' in enabled_extra_types,
            include_comments=enable_comment_check,
            include_roles=enable_grant_generation,
            target_table_pairs=target_table_pairs if enable_comment_check else set()
        )
        ob_dependencies: Set[Tuple[str, str, str, str]] = set()
        if enable_dependencies_check:
            ob_dependencies = load_ob_dependencies(
                ob_cfg,
                target_schemas,
                object_types=enabled_object_types
            )

        schema_summary = compute_schema_coverage(
            settings['source_schemas_list'],
            source_objects,
            target_schemas,
            ob_meta
        )

    # 7) 主对象校验
    with phase_timer("Oracle 元数据转储", phase_durations):
        log_subsection("Oracle 元数据")
        oracle_meta = dump_oracle_metadata(
            ora_cfg,
            master_list,
            settings,
            include_indexes='INDEX' in enabled_extra_types,
            include_constraints='CONSTRAINT' in enabled_extra_types,
            include_triggers='TRIGGER' in enabled_extra_types,
            include_sequences='SEQUENCE' in enabled_extra_types,
            include_comments=enable_comment_check,
            include_privileges=enable_grant_generation,
            include_interval_partitions=bool(
                settings.get("generate_interval_partition_fixup", False)
            ) and generate_fixup_enabled
        )

        table_target_map = build_table_target_map(master_list)
        blacklist_report_rows = build_blacklist_report_rows(
            oracle_meta.blacklist_tables,
            table_target_map,
            oracle_meta,
            ob_meta
        )

    log_section("差异校验")
    monitored_types: Tuple[str, ...] = tuple(
        t for t in OBJECT_COUNT_TYPES
        if (t.upper() in checked_primary_types) or (t.upper() in enabled_extra_types)
    ) or ('TABLE',)

    with phase_timer("主对象校验", phase_durations):
        object_counts_summary = compute_object_counts(full_object_mapping, ob_meta, oracle_meta, monitored_types)
        tv_results = check_primary_objects(
            master_list,
            extraneous_rules,
            ob_meta,
            oracle_meta,
            enabled_primary_types,
            print_only_types,
            settings.get("effective_print_only_primary_reasons"),
            settings=settings
        )
        supplement_missing_views_from_mapping(
            tv_results,
            full_object_mapping,
            ob_meta,
            enabled_primary_types
        )
        tv_results["remap_conflicts"] = remap_conflict_items
        package_results = compare_package_objects(
            master_list,
            oracle_meta,
            ob_meta,
            enabled_primary_types
        )
        blacklisted_missing_tables = collect_blacklisted_missing_tables(
            tv_results,
            oracle_meta.blacklist_tables
        )
        blacklisted_table_keys = set(oracle_meta.blacklist_tables.keys()) if oracle_meta.blacklist_tables else set()
        comment_results = check_comments(
            master_list,
            oracle_meta,
            ob_meta,
            enable_comment_check
        )

    # 8) 扩展对象校验 (索引/约束/序列/触发器)
    extra_check_ctx = phase_timer("扩展对象校验", phase_durations) if enabled_extra_types else nullcontext()
    with extra_check_ctx:
        extra_results = check_extra_objects(
            settings,
            master_list,
            ob_meta,
            oracle_meta,
            full_object_mapping,
            enabled_extra_types
        )

    support_summary = classify_missing_objects(
        ora_cfg,
        settings,
        tv_results,
        extra_results,
        oracle_meta,
        ob_meta,
        full_object_mapping,
        source_objects,
        dependency_graph,
        object_parent_map,
        table_target_map,
        synonym_meta
    )

    noise_result = apply_noise_suppression(tv_results, extra_results, comment_results)
    tv_results = noise_result.tv_results
    extra_results = noise_result.extra_results
    comment_results = noise_result.comment_results
    noise_suppressed_details = noise_result.suppressed_details

    usability_summary: Optional[UsabilitySummary] = None
    if enable_usability_check:
        with phase_timer("对象可用性校验", phase_durations):
            usability_summary = check_object_usability(
                settings,
                master_list,
                tv_results,
                ob_cfg,
                ora_cfg,
                ob_meta,
                enabled_primary_types,
                support_summary=support_summary,
                dependency_graph=dependency_graph,
                source_objects=source_objects,
                full_object_mapping=full_object_mapping,
                synonym_meta_map=synonym_meta
            )

    extra_results_for_report = filter_trigger_results_for_unsupported_tables(
        extra_results,
        support_summary.unsupported_table_keys if support_summary else None,
        table_target_map
    )
    object_counts_summary = reconcile_object_counts_summary(
        object_counts_summary,
        tv_results,
        extra_results_for_report,
        package_results
    )

    status_drift_types: Set[str] = set(settings.get("check_status_drift_type_set", set()) or set())

    trigger_status_rows: List[TriggerStatusReportRow] = []
    if 'TRIGGER' in enabled_extra_types and "TRIGGER" in status_drift_types:
        trigger_status_rows = collect_trigger_status_rows(
            oracle_meta,
            ob_meta,
            full_object_mapping,
            unsupported_table_keys=(support_summary.unsupported_table_keys if support_summary else None)
        )
    constraint_status_rows: List[ConstraintStatusDriftRow] = []
    if 'CONSTRAINT' in enabled_extra_types and "CONSTRAINT" in status_drift_types:
        constraint_status_rows = collect_constraint_status_drift_rows(
            oracle_meta,
            ob_meta,
            master_list,
            full_object_mapping,
            sync_mode=settings.get("constraint_status_sync_mode", "enabled_only"),
            unsupported_table_keys=(support_summary.unsupported_table_keys if support_summary else None)
        )
    extra_results["trigger_status_drift"] = trigger_status_rows
    extra_results["constraint_status_drift"] = constraint_status_rows

    trigger_list_summary: Optional[Dict[str, object]] = None
    trigger_list_rows: Optional[List[TriggerListReportRow]] = None
    trigger_filter_entries: Optional[Set[str]] = None
    trigger_filter_enabled = False
    trigger_list_path = settings.get("trigger_list", "").strip()
    if trigger_list_path:
        entries, invalid_entries, duplicate_entries, total_lines, read_error = parse_trigger_list_file(trigger_list_path)
        trigger_list_rows, trigger_list_summary = build_trigger_list_report(
            trigger_list_path,
            entries,
            invalid_entries,
            duplicate_entries,
            total_lines,
            read_error,
            extra_results_for_report,
            oracle_meta,
            ob_meta,
            full_object_mapping,
            trigger_check_enabled='TRIGGER' in enabled_extra_types
        )
        trigger_filter_entries = entries
        if trigger_list_summary.get("error"):
            log.warning("trigger_list 读取失败，将回退全量触发器生成: %s", trigger_list_summary.get("error"))
            trigger_filter_enabled = False
        elif trigger_list_summary.get("check_disabled"):
            log.warning("TRIGGER 未启用检查，trigger_list 将不会用于缺失触发器筛选。")
            trigger_filter_enabled = False
        elif trigger_list_summary.get("fallback_full"):
            log.warning("trigger_list 为空或无有效条目，已回退全量触发器生成。")
            trigger_filter_enabled = False
        else:
            trigger_filter_enabled = True
            log.info(
                "trigger_list 生效: 列表=%d, 命中缺失=%d, 未列出缺失=%d。",
                trigger_list_summary.get("valid_entries", 0),
                trigger_list_summary.get("selected_missing", 0),
                trigger_list_summary.get("missing_not_listed", 0)
            )

    if enable_dependencies_check:
        with phase_timer("依赖/授权校验", phase_durations):
            dependency_grant_catalog: Optional[ObGrantCatalog] = None
            if expected_dependency_pairs:
                dependency_grantees: Set[str] = set()
                for dep_full, _dep_type, ref_full, ref_type in expected_dependency_pairs:
                    if "." not in dep_full or "." not in ref_full:
                        continue
                    dep_schema = dep_full.split(".", 1)[0].upper()
                    ref_schema = ref_full.split(".", 1)[0].upper()
                    if dep_schema == ref_schema:
                        continue
                    if not GRANT_PRIVILEGE_BY_TYPE.get((ref_type or "").upper()):
                        continue
                    dependency_grantees.add(dep_schema)
                if dependency_grantees:
                    dependency_grant_catalog = load_ob_grant_catalog(ob_cfg, dependency_grantees)
                    if dependency_grant_catalog is None:
                        log.warning("[DEPENDENCY] 读取 OB 授权目录失败，将跳过依赖授权评估。")
            dependency_report = check_dependencies_against_ob(
                expected_dependency_pairs,
                ob_dependencies,
                skipped_dependency_pairs,
                ob_meta,
                ob_grant_catalog=dependency_grant_catalog
            )
            if settings.get("print_dependency_chains", True):
                dep_chain_path = report_dir / f"dependency_chains_{timestamp}.txt"
                source_dep_pairs = (
                    to_raw_dependency_pairs(oracle_dependencies_internal)
                    if oracle_dependencies_internal else set()
                )
                dependency_chain_file = export_dependency_chains(
                    expected_dependency_pairs,
                    dep_chain_path,
                    source_pairs=source_dep_pairs
                )
                if dependency_chain_file:
                    log.info("依赖链路已输出: %s", dependency_chain_file)
                else:
                    log.info("依赖链路输出已跳过（无数据或写入失败）。")
    else:
        dependency_report = {
            "missing": [],
            "unexpected": [],
            "skipped": []
        }

    log_section("修补脚本与报告")
    # 9) 生成目标端订正 SQL
    fixup_skip_summary: Dict[str, Dict[str, object]] = {}
    if generate_fixup_enabled:
        fixup_dir_label = settings.get('fixup_dir', 'fixup_scripts') or 'fixup_scripts'
        log.info('已开启修补脚本生成，开始写入 %s 目录...', fixup_dir_label)
        with phase_timer("修补脚本生成", phase_durations):
            grant_plan = None
            if enable_grant_generation:
                try:
                    grant_progress_interval = float(settings.get('progress_log_interval', 10))
                except (TypeError, ValueError):
                    grant_progress_interval = 10.0
                grant_progress_interval = max(1.0, grant_progress_interval)
                supported_sys_privs = settings.get('grant_supported_sys_privs_set', set())
                if not supported_sys_privs:
                    supported_sys_privs = load_ob_supported_sys_privs(ob_cfg)
                supported_object_privs = settings.get('grant_supported_object_privs_set', set())
                if not supported_object_privs:
                    supported_object_privs = set(DEFAULT_SUPPORTED_OBJECT_PRIVS)
                include_oracle_maintained_roles = bool(settings.get('grant_include_oracle_maintained_roles', False))
                ob_roles: Optional[Set[str]] = None
                if ob_meta and ob_meta.roles:
                    ob_roles = set(ob_meta.roles)
                elif enable_grant_generation:
                    ob_roles = load_ob_roles(ob_cfg)
                ob_users = load_ob_users(ob_cfg) if enable_grant_generation else None
                if not supported_sys_privs:
                    log.warning("[GRANT] 未获取到 OceanBase 系统权限清单，将仅依据 Oracle 权限合法性过滤。")
                log.info(
                    "[GRANT] 权限过滤参数：sys_privs=%d, obj_privs=%d, include_oracle_maintained_roles=%s, ob_roles=%d, ob_users=%d",
                    len(supported_sys_privs),
                    len(supported_object_privs),
                    "true" if include_oracle_maintained_roles else "false",
                    len(ob_roles or set()),
                    len(ob_users or set())
                )
                log.info(
                    "[GRANT] 开始生成授权计划：对象权限=%d, 系统权限=%d, 角色授权=%d, 依赖记录=%d。",
                    len(oracle_meta.object_privileges),
                    len(oracle_meta.sys_privileges),
                    len(oracle_meta.role_privileges),
                    len(oracle_dependencies_for_grants)
                )
                grant_plan = build_grant_plan(
                    oracle_meta,
                    full_object_mapping,
                    remap_rules,
                    source_objects,
                    schema_mapping_for_grants,
                    object_parent_map,
                    dependency_graph,
                    transitive_table_cache,
                    source_dependencies_set,
                    source_schema_set,
                    remap_conflicts,
                    synonym_meta,
                    supported_sys_privs=supported_sys_privs,
                    supported_object_privs=supported_object_privs,
                    oracle_sys_privs_map=oracle_meta.system_privilege_map,
                    oracle_obj_privs_map=oracle_meta.table_privilege_map,
                    oracle_roles=oracle_meta.role_metadata,
                    ob_roles=ob_roles,
                    ob_users=ob_users,
                    include_oracle_maintained_roles=include_oracle_maintained_roles,
                    dependencies=oracle_dependencies_for_grants,
                    progress_interval=grant_progress_interval,
                    sequence_remap_policy=sequence_policy
                )
                object_grant_cnt = sum(len(v) for v in grant_plan.object_grants.values())
                sys_grant_cnt = sum(len(v) for v in grant_plan.sys_privs.values())
                role_grant_cnt = sum(len(v) for v in grant_plan.role_privs.values())
                log.info(
                    "[GRANT] 授权计划生成完成：对象权限=%d, 系统权限=%d, 角色授权=%d",
                    object_grant_cnt,
                    sys_grant_cnt,
                    role_grant_cnt
                )
                if grant_plan.filtered_grants:
                    report_dir_hint = settings.get("report_dir_effective") or settings.get("report_dir", "main_reports")
                    log.warning(
                        "[GRANT] 已过滤不兼容权限 %d 条，将输出 %s/filtered_grants.txt。",
                        len(grant_plan.filtered_grants),
                        report_dir_hint
                    )
            else:
                log.info("[GRANT] generate_grants=false，授权脚本生成已关闭。")

            view_chain_file = generate_fixup_scripts(
                ora_cfg,
                ob_cfg,
                settings,
                tv_results,
                extra_results,
                master_list,
                oracle_meta,
                full_object_mapping,
                remap_rules,
                grant_plan,
                enable_grant_generation,
                dependency_report,
                ob_meta,
                expected_dependency_pairs,
                synonym_meta,
                trigger_filter_entries,
                trigger_filter_enabled,
                package_results=package_results,
                report_dir=report_dir,
                report_timestamp=timestamp,
                fixup_skip_summary=fixup_skip_summary,
                trigger_status_rows=trigger_status_rows,
                constraint_status_rows=constraint_status_rows,
                support_state_map=support_summary.support_state_map,
                unsupported_table_keys=support_summary.unsupported_table_keys,
                view_compat_map=support_summary.view_compat_map,
                view_dependency_map=view_dependency_map
            )
    else:
        log.info('已根据配置跳过修补脚本生成，仅打印对比报告。')
        if enable_grant_generation:
            log.info("[GRANT] generate_fixup=false，授权脚本生成已跳过。")

    # 10) 输出最终报告
    total_checked = sum(
        1 for _, _, obj_type in master_list
        if obj_type.upper() in checked_primary_types
    )
    report_start_perf = time.perf_counter()
    run_summary_ctx = RunSummaryContext(
        start_time=run_start_time,
        start_perf=run_start_perf,
        phase_durations=phase_durations,
        phase_skip_reasons=phase_skip_reasons,
        enabled_primary_types=enabled_primary_types,
        enabled_extra_types=enabled_extra_types,
        print_only_types=print_only_types,
        total_checked=total_checked,
        enable_dependencies_check=enable_dependencies_check,
        enable_comment_check=enable_comment_check,
        enable_grant_generation=enable_grant_generation,
        enable_schema_mapping_infer=settings.get("enable_schema_mapping_infer", True),
        fixup_enabled=generate_fixup_enabled,
        fixup_dir=settings.get('fixup_dir', 'fixup_scripts') or 'fixup_scripts',
        dependency_chain_file=dependency_chain_file,
        view_chain_file=view_chain_file,
        trigger_list_summary=trigger_list_summary,
        report_start_perf=report_start_perf
        )
    run_summary = print_final_report(
        tv_results,
        total_checked,
        extra_results_for_report,
        comment_results,
        dependency_report,
        report_path,
        object_counts_summary,
        endpoint_info,
        schema_summary,
        settings,
        blacklisted_missing_tables,
        blacklist_report_rows,
        trigger_list_summary,
        trigger_list_rows,
        trigger_status_rows,
        constraint_status_rows,
        package_results=package_results,
        run_summary_ctx=run_summary_ctx,
        filtered_grants=(grant_plan.filtered_grants if grant_plan else None),
        config_diagnostics=config_diagnostics,
        fixup_skip_summary=fixup_skip_summary,
        support_summary=support_summary,
        noise_suppressed_details=noise_suppressed_details,
        usability_summary=usability_summary
    )
    if run_summary:
        log_run_summary(run_summary)
        db_ok, _ = save_report_to_db(
            ob_cfg,
            settings,
            run_summary,
            run_summary_ctx,
            timestamp,
            report_dir,
            tv_results,
            extra_results_for_report,
            support_summary,
            endpoint_info,
            object_counts_summary,
            table_target_map,
            target_schemas,
            grant_plan=grant_plan,
            usability_summary=usability_summary,
            package_results=package_results,
            trigger_status_rows=trigger_status_rows,
            constraint_status_rows=constraint_status_rows,
            dependency_report=dependency_report,
            expected_dependency_pairs=expected_dependency_pairs,
            view_chain_file=view_chain_file,
            remap_conflicts=remap_conflict_items,
            full_object_mapping=full_object_mapping,
            remap_rules=remap_rules,
            blacklist_report_rows=blacklist_report_rows,
            fixup_skip_summary=fixup_skip_summary,
            blacklisted_table_keys=blacklisted_table_keys
        )
        if not db_ok:
            abort_run()


if __name__ == "__main__":
    try:
        main()
    except FatalError as exc:
        try:
            log.error("运行终止: %s", exc)
        except Exception:
            print(f"运行终止: {exc}", file=sys.stderr)
        sys.exit(1)
